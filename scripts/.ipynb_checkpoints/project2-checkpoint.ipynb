{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_processor as dp\n",
    "from implementations import *\n",
    "from proj1_helpers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_train = tX[:int(tX.shape[0]*0.8)]\n",
    "y_train = y[:int(tX.shape[0]*0.8)]\n",
    "\n",
    "tX_val = tX[int(tX.shape[0]*0.8):]\n",
    "y_val = y[int(tX.shape[0]*0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_train, filler = dp.fill_nan(tX_train, nan_value=-999, method='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_val, _ = dp.fill_nan(tX_val, nan_value=-999, method='use_filler', filler=filler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_train,_ = dp.remove_outliers(tX_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200000, 30), (50000, 30))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX_train.shape,tX_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After the minimal training, we try to do some extra data processing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building polynomial in order to do feature augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_train_aug, tX_val_aug = dp.poly_features(tX_train,2), dp.poly_features(tX_val,2)\n",
    "tX_train_aug, tX_val_aug = dp.add_ones(tX_train_aug), dp.add_ones(tX_val_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200000, 61), (50000, 61))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX_train_aug.shape, tX_val_aug.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill NaN. Apply LS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, mse = least_squares(y_train, tX_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.  1. -1. ...  1. -1. -1.]\n",
      "(200000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.740765"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict_labels(w, tX_train)\n",
    "print(y_pred)\n",
    "print(y_pred.shape)\n",
    "np.mean(y_train==y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1. -1. -1. ... -1.  1. -1.]\n",
      "(50000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.73982"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict_labels(w, tX_val)\n",
    "print(y_pred)\n",
    "print(y_pred.shape)\n",
    "np.mean(y_val==y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200000,), 68548)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape, np.sum(y_train > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for deg train:0.385785\n",
      "Accuracy for deg val:0.64388\n",
      "475.54004162132316\n"
     ]
    }
   ],
   "source": [
    "w2, mse2 = least_squares(y_train, tX_train_aug)\n",
    "y_pred2 = predict_labels(w2, tX_train_aug)\n",
    "print('Accuracy for deg train:' + str(np.mean(y_train==y_pred2)))\n",
    "y_pred2 = predict_labels(w2, tX_val_aug)\n",
    "print('Accuracy for deg val:' +str(np.mean(y_val==y_pred2)))\n",
    "print(mse2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001: VAL=0.73982\n",
      "0.001: VAL=0.73982\n",
      "0.01: VAL=0.73986\n",
      "0.1: VAL=0.73988\n",
      "1: VAL=0.73988\n",
      "10.0: VAL=0.73986\n",
      "100.0: VAL=0.73982\n",
      "1000.0: VAL=0.7393\n"
     ]
    }
   ],
   "source": [
    "for l in [1e-4, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3]:\n",
    "    w, mse = ridge_regression(y_train, tX_train, l)\n",
    "    y_pred = predict_labels(w, tX_val)\n",
    "    print(str(l)+\": VAL=\"+str(np.mean(y_val==y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Squares GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros(tX.shape[1])#np.random.normal(loc=0, scale=1, size=(tX.shape[1],1))\n",
    "max_iters = 1000\n",
    "gamma = 3e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/999): loss=0.5\n",
      "Gradient Descent(1/999): loss=0.474999342622941\n",
      "Gradient Descent(2/999): loss=0.4721726106390642\n",
      "Gradient Descent(3/999): loss=0.46968451001295397\n",
      "Gradient Descent(4/999): loss=0.4673086476825307\n",
      "Gradient Descent(5/999): loss=0.4650369766970005\n",
      "Gradient Descent(6/999): loss=0.4628639688549106\n",
      "Gradient Descent(7/999): loss=0.46078441979937124\n",
      "Gradient Descent(8/999): loss=0.4587934104117789\n",
      "Gradient Descent(9/999): loss=0.4568862908711271\n",
      "Gradient Descent(10/999): loss=0.45505866580197263\n",
      "Gradient Descent(11/999): loss=0.45330638024552466\n",
      "Gradient Descent(12/999): loss=0.4516255064060496\n",
      "Gradient Descent(13/999): loss=0.45001233112954286\n",
      "Gradient Descent(14/999): loss=0.44846334407405913\n",
      "Gradient Descent(15/999): loss=0.4469752265333793\n",
      "Gradient Descent(16/999): loss=0.4455448408778337\n",
      "Gradient Descent(17/999): loss=0.4441692205781283\n",
      "Gradient Descent(18/999): loss=0.4428455607799212\n",
      "Gradient Descent(19/999): loss=0.4415712093986918\n",
      "Gradient Descent(20/999): loss=0.44034365870614434\n",
      "Gradient Descent(21/999): loss=0.439160537380974\n",
      "Gradient Descent(22/999): loss=0.4380196029983438\n",
      "Gradient Descent(23/999): loss=0.43691873493382943\n",
      "Gradient Descent(24/999): loss=0.4358559276589424\n",
      "Gradient Descent(25/999): loss=0.4348292844066015\n",
      "Gradient Descent(26/999): loss=0.433837011186127\n",
      "Gradient Descent(27/999): loss=0.4328774111284517\n",
      "Gradient Descent(28/999): loss=0.43194887914332064\n",
      "Gradient Descent(29/999): loss=0.431049896871253\n",
      "Gradient Descent(30/999): loss=0.43017902791399115\n",
      "Gradient Descent(31/999): loss=0.42933491332806506\n",
      "Gradient Descent(32/999): loss=0.4285162673669473\n",
      "Gradient Descent(33/999): loss=0.42772187345807405\n",
      "Gradient Descent(34/999): loss=0.42695058040176964\n",
      "Gradient Descent(35/999): loss=0.42620129877982565\n",
      "Gradient Descent(36/999): loss=0.4254729975621611\n",
      "Gradient Descent(37/999): loss=0.424764700900632\n",
      "Gradient Descent(38/999): loss=0.4240754850996599\n",
      "Gradient Descent(39/999): loss=0.42340447575392004\n",
      "Gradient Descent(40/999): loss=0.4227508450438696\n",
      "Gradient Descent(41/999): loss=0.4221138091804024\n",
      "Gradient Descent(42/999): loss=0.42149262599040266\n",
      "Gradient Descent(43/999): loss=0.42088659263541905\n",
      "Gradient Descent(44/999): loss=0.420295043456113\n",
      "Gradient Descent(45/999): loss=0.4197173479355406\n",
      "Gradient Descent(46/999): loss=0.4191529087747083\n",
      "Gradient Descent(47/999): loss=0.41860116007420883\n",
      "Gradient Descent(48/999): loss=0.4180615656160805\n",
      "Gradient Descent(49/999): loss=0.41753361724035937\n",
      "Gradient Descent(50/999): loss=0.417016833311102\n",
      "Gradient Descent(51/999): loss=0.4165107572669367\n",
      "Gradient Descent(52/999): loss=0.4160149562514806\n",
      "Gradient Descent(53/999): loss=0.4155290198192173\n",
      "Gradient Descent(54/999): loss=0.41505255871266794\n",
      "Gradient Descent(55/999): loss=0.41458520370692387\n",
      "Gradient Descent(56/999): loss=0.41412660451782235\n",
      "Gradient Descent(57/999): loss=0.4136764287702549\n",
      "Gradient Descent(58/999): loss=0.4132343610232899\n",
      "Gradient Descent(59/999): loss=0.4128001018489739\n",
      "Gradient Descent(60/999): loss=0.4123733669618503\n",
      "Gradient Descent(61/999): loss=0.41195388639639874\n",
      "Gradient Descent(62/999): loss=0.4115414037297478\n",
      "Gradient Descent(63/999): loss=0.4111356753471665\n",
      "Gradient Descent(64/999): loss=0.41073646974797234\n",
      "Gradient Descent(65/999): loss=0.41034356688962864\n",
      "Gradient Descent(66/999): loss=0.4099567575679203\n",
      "Gradient Descent(67/999): loss=0.40957584283122217\n",
      "Gradient Descent(68/999): loss=0.4092006334269761\n",
      "Gradient Descent(69/999): loss=0.40883094927860175\n",
      "Gradient Descent(70/999): loss=0.4084666189911615\n",
      "Gradient Descent(71/999): loss=0.4081074793841938\n",
      "Gradient Descent(72/999): loss=0.4077533750502165\n",
      "Gradient Descent(73/999): loss=0.40740415793748436\n",
      "Gradient Descent(74/999): loss=0.40705968695566186\n",
      "Gradient Descent(75/999): loss=0.40671982760315073\n",
      "Gradient Descent(76/999): loss=0.4063844516148731\n",
      "Gradient Descent(77/999): loss=0.40605343662938737\n",
      "Gradient Descent(78/999): loss=0.40572666587426715\n",
      "Gradient Descent(79/999): loss=0.40540402786873775\n",
      "Gradient Descent(80/999): loss=0.40508541614261767\n",
      "Gradient Descent(81/999): loss=0.40477072897066874\n",
      "Gradient Descent(82/999): loss=0.40445986912150106\n",
      "Gradient Descent(83/999): loss=0.4041527436202352\n",
      "Gradient Descent(84/999): loss=0.40384926352415895\n",
      "Gradient Descent(85/999): loss=0.40354934371066553\n",
      "Gradient Descent(86/999): loss=0.40325290267679503\n",
      "Gradient Descent(87/999): loss=0.40295986234973963\n",
      "Gradient Descent(88/999): loss=0.4026701479077074\n",
      "Gradient Descent(89/999): loss=0.402383687610578\n",
      "Gradient Descent(90/999): loss=0.40210041263980356\n",
      "Gradient Descent(91/999): loss=0.4018202569470535\n",
      "Gradient Descent(92/999): loss=0.40154315711111443\n",
      "Gradient Descent(93/999): loss=0.40126905220259645\n",
      "Gradient Descent(94/999): loss=0.4009978836560107\n",
      "Gradient Descent(95/999): loss=0.4007295951488156\n",
      "Gradient Descent(96/999): loss=0.4004641324870475\n",
      "Gradient Descent(97/999): loss=0.40020144349717124\n",
      "Gradient Descent(98/999): loss=0.3999414779238113\n",
      "Gradient Descent(99/999): loss=0.39968418733303646\n",
      "Gradient Descent(100/999): loss=0.399429525020896\n",
      "Gradient Descent(101/999): loss=0.3991774459269143\n",
      "Gradient Descent(102/999): loss=0.3989279065522744\n",
      "Gradient Descent(103/999): loss=0.3986808648824314\n",
      "Gradient Descent(104/999): loss=0.3984362803139114\n",
      "Gradient Descent(105/999): loss=0.3981941135850667\n",
      "Gradient Descent(106/999): loss=0.39795432671056874\n",
      "Gradient Descent(107/999): loss=0.3977168829194349\n",
      "Gradient Descent(108/999): loss=0.3974817465963924\n",
      "Gradient Descent(109/999): loss=0.3972488832263994\n",
      "Gradient Descent(110/999): loss=0.3970182593421457\n",
      "Gradient Descent(111/999): loss=0.3967898424743729\n",
      "Gradient Descent(112/999): loss=0.39656360110485755\n",
      "Gradient Descent(113/999): loss=0.3963395046219104\n",
      "Gradient Descent(114/999): loss=0.39611752327825567\n",
      "Gradient Descent(115/999): loss=0.39589762815115825\n",
      "Gradient Descent(116/999): loss=0.39567979110467505\n",
      "Gradient Descent(117/999): loss=0.3954639847539159\n",
      "Gradient Descent(118/999): loss=0.39525018243120186\n",
      "Gradient Descent(119/999): loss=0.39503835815401817\n",
      "Gradient Descent(120/999): loss=0.39482848659466296\n",
      "Gradient Descent(121/999): loss=0.39462054305149924\n",
      "Gradient Descent(122/999): loss=0.3944145034217216\n",
      "Gradient Descent(123/999): loss=0.39421034417555556\n",
      "Gradient Descent(124/999): loss=0.3940080423318111\n",
      "Gradient Descent(125/999): loss=0.3938075754347154\n",
      "Gradient Descent(126/999): loss=0.3936089215319562\n",
      "Gradient Descent(127/999): loss=0.3934120591538687\n",
      "Gradient Descent(128/999): loss=0.3932169672937038\n",
      "Gradient Descent(129/999): loss=0.3930236253889183\n",
      "Gradient Descent(130/999): loss=0.39283201330343276\n",
      "Gradient Descent(131/999): loss=0.3926421113108016\n",
      "Gradient Descent(132/999): loss=0.39245390007824915\n",
      "Gradient Descent(133/999): loss=0.39226736065152085\n",
      "Gradient Descent(134/999): loss=0.39208247444050803\n",
      "Gradient Descent(135/999): loss=0.39189922320560333\n",
      "Gradient Descent(136/999): loss=0.39171758904474613\n",
      "Gradient Descent(137/999): loss=0.3915375543811221\n",
      "Gradient Descent(138/999): loss=0.39135910195148\n",
      "Gradient Descent(139/999): loss=0.3911822147950328\n",
      "Gradient Descent(140/999): loss=0.3910068762429125\n",
      "Gradient Descent(141/999): loss=0.390833069908146\n",
      "Gradient Descent(142/999): loss=0.3906607796761265\n",
      "Gradient Descent(143/999): loss=0.39048998969555315\n",
      "Gradient Descent(144/999): loss=0.39032068436981204\n",
      "Gradient Descent(145/999): loss=0.3901528483487765\n",
      "Gradient Descent(146/999): loss=0.38998646652100316\n",
      "Gradient Descent(147/999): loss=0.38982152400630393\n",
      "Gradient Descent(148/999): loss=0.3896580061486703\n",
      "Gradient Descent(149/999): loss=0.38949589850953614\n",
      "Gradient Descent(150/999): loss=0.38933518686135554\n",
      "Gradient Descent(151/999): loss=0.38917585718148356\n",
      "Gradient Descent(152/999): loss=0.38901789564633943\n",
      "Gradient Descent(153/999): loss=0.38886128862584085\n",
      "Gradient Descent(154/999): loss=0.38870602267809223\n",
      "Gradient Descent(155/999): loss=0.3885520845443154\n",
      "Gradient Descent(156/999): loss=0.38839946114400675\n",
      "Gradient Descent(157/999): loss=0.388248139570315\n",
      "Gradient Descent(158/999): loss=0.3880981070856189\n",
      "Gradient Descent(159/999): loss=0.3879493511173039\n",
      "Gradient Descent(160/999): loss=0.3878018592537198\n",
      "Gradient Descent(161/999): loss=0.3876556192403129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(162/999): loss=0.38751061897592604\n",
      "Gradient Descent(163/999): loss=0.3873668465092516\n",
      "Gradient Descent(164/999): loss=0.3872242900354348\n",
      "Gradient Descent(165/999): loss=0.3870829378928184\n",
      "Gradient Descent(166/999): loss=0.38694277855981807\n",
      "Gradient Descent(167/999): loss=0.38680380065192727\n",
      "Gradient Descent(168/999): loss=0.38666599291883963\n",
      "Gradient Descent(169/999): loss=0.3865293442416863\n",
      "Gradient Descent(170/999): loss=0.3863938436303811\n",
      "Gradient Descent(171/999): loss=0.3862594802210677\n",
      "Gradient Descent(172/999): loss=0.3861262432736638\n",
      "Gradient Descent(173/999): loss=0.3859941221694984\n",
      "Gradient Descent(174/999): loss=0.3858631064090354\n",
      "Gradient Descent(175/999): loss=0.38573318560968184\n",
      "Gradient Descent(176/999): loss=0.38560434950367317\n",
      "Gradient Descent(177/999): loss=0.3854765879360355\n",
      "Gradient Descent(178/999): loss=0.38534989086261817\n",
      "Gradient Descent(179/999): loss=0.38522424834819313\n",
      "Gradient Descent(180/999): loss=0.38509965056462153\n",
      "Gradient Descent(181/999): loss=0.3849760877890783\n",
      "Gradient Descent(182/999): loss=0.38485355040233704\n",
      "Gradient Descent(183/999): loss=0.38473202888710956\n",
      "Gradient Descent(184/999): loss=0.38461151382643893\n",
      "Gradient Descent(185/999): loss=0.38449199590214134\n",
      "Gradient Descent(186/999): loss=0.3843734658932979\n",
      "Gradient Descent(187/999): loss=0.3842559146747893\n",
      "Gradient Descent(188/999): loss=0.38413933321587657\n",
      "Gradient Descent(189/999): loss=0.3840237125788212\n",
      "Gradient Descent(190/999): loss=0.38390904391754577\n",
      "Gradient Descent(191/999): loss=0.3837953184763323\n",
      "Gradient Descent(192/999): loss=0.3836825275885557\n",
      "Gradient Descent(193/999): loss=0.38357066267545187\n",
      "Gradient Descent(194/999): loss=0.38345971524491945\n",
      "Gradient Descent(195/999): loss=0.3833496768903509\n",
      "Gradient Descent(196/999): loss=0.38324053928949525\n",
      "Gradient Descent(197/999): loss=0.38313229420334943\n",
      "Gradient Descent(198/999): loss=0.3830249334750745\n",
      "Gradient Descent(199/999): loss=0.3829184490289415\n",
      "Gradient Descent(200/999): loss=0.3828128328692997\n",
      "Gradient Descent(201/999): loss=0.38270807707956994\n",
      "Gradient Descent(202/999): loss=0.3826041738212613\n",
      "Gradient Descent(203/999): loss=0.38250111533300823\n",
      "Gradient Descent(204/999): loss=0.38239889392963095\n",
      "Gradient Descent(205/999): loss=0.38229750200121404\n",
      "Gradient Descent(206/999): loss=0.3821969320122058\n",
      "Gradient Descent(207/999): loss=0.3820971765005366\n",
      "Gradient Descent(208/999): loss=0.38199822807675304\n",
      "Gradient Descent(209/999): loss=0.38190007942317256\n",
      "Gradient Descent(210/999): loss=0.38180272329305154\n",
      "Gradient Descent(211/999): loss=0.38170615250977163\n",
      "Gradient Descent(212/999): loss=0.38161035996604054\n",
      "Gradient Descent(213/999): loss=0.38151533862310694\n",
      "Gradient Descent(214/999): loss=0.381421081509992\n",
      "Gradient Descent(215/999): loss=0.38132758172273185\n",
      "Gradient Descent(216/999): loss=0.38123483242363526\n",
      "Gradient Descent(217/999): loss=0.3811428268405534\n",
      "Gradient Descent(218/999): loss=0.3810515582661627\n",
      "Gradient Descent(219/999): loss=0.38096102005725835\n",
      "Gradient Descent(220/999): loss=0.38087120563406085\n",
      "Gradient Descent(221/999): loss=0.3807821084795334\n",
      "Gradient Descent(222/999): loss=0.38069372213870944\n",
      "Gradient Descent(223/999): loss=0.38060604021803135\n",
      "Gradient Descent(224/999): loss=0.3805190563846994\n",
      "Gradient Descent(225/999): loss=0.3804327643660305\n",
      "Gradient Descent(226/999): loss=0.38034715794882634\n",
      "Gradient Descent(227/999): loss=0.38026223097875117\n",
      "Gradient Descent(228/999): loss=0.38017797735971826\n",
      "Gradient Descent(229/999): loss=0.3800943910532857\n",
      "Gradient Descent(230/999): loss=0.3800114660780598\n",
      "Gradient Descent(231/999): loss=0.3799291965091086\n",
      "Gradient Descent(232/999): loss=0.3798475764773812\n",
      "Gradient Descent(233/999): loss=0.37976660016913694\n",
      "Gradient Descent(234/999): loss=0.37968626182538046\n",
      "Gradient Descent(235/999): loss=0.3796065557413069\n",
      "Gradient Descent(236/999): loss=0.379527476265751\n",
      "Gradient Descent(237/999): loss=0.3794490178006462\n",
      "Gradient Descent(238/999): loss=0.37937117480048943\n",
      "Gradient Descent(239/999): loss=0.37929394177181225\n",
      "Gradient Descent(240/999): loss=0.3792173132726606\n",
      "Gradient Descent(241/999): loss=0.37914128391207835\n",
      "Gradient Descent(242/999): loss=0.37906584834959917\n",
      "Gradient Descent(243/999): loss=0.3789910012947439\n",
      "Gradient Descent(244/999): loss=0.37891673750652477\n",
      "Gradient Descent(245/999): loss=0.3788430517929537\n",
      "Gradient Descent(246/999): loss=0.37876993901055866\n",
      "Gradient Descent(247/999): loss=0.37869739406390424\n",
      "Gradient Descent(248/999): loss=0.378625411905119\n",
      "Gradient Descent(249/999): loss=0.3785539875334268\n",
      "Gradient Descent(250/999): loss=0.37848311599468526\n",
      "Gradient Descent(251/999): loss=0.37841279238092757\n",
      "Gradient Descent(252/999): loss=0.3783430118299118\n",
      "Gradient Descent(253/999): loss=0.3782737695246732\n",
      "Gradient Descent(254/999): loss=0.37820506069308296\n",
      "Gradient Descent(255/999): loss=0.3781368806074107\n",
      "Gradient Descent(256/999): loss=0.37806922458389336\n",
      "Gradient Descent(257/999): loss=0.37800208798230683\n",
      "Gradient Descent(258/999): loss=0.3779354662055443\n",
      "Gradient Descent(259/999): loss=0.3778693546991979\n",
      "Gradient Descent(260/999): loss=0.37780374895114494\n",
      "Gradient Descent(261/999): loss=0.3777386444911396\n",
      "Gradient Descent(262/999): loss=0.37767403689040757\n",
      "Gradient Descent(263/999): loss=0.37760992176124675\n",
      "Gradient Descent(264/999): loss=0.37754629475663004\n",
      "Gradient Descent(265/999): loss=0.3774831515698145\n",
      "Gradient Descent(266/999): loss=0.3774204879339532\n",
      "Gradient Descent(267/999): loss=0.3773582996217116\n",
      "Gradient Descent(268/999): loss=0.37729658244488806\n",
      "Gradient Descent(269/999): loss=0.3772353322540381\n",
      "Gradient Descent(270/999): loss=0.37717454493810315\n",
      "Gradient Descent(271/999): loss=0.3771142164240418\n",
      "Gradient Descent(272/999): loss=0.37705434267646654\n",
      "Gradient Descent(273/999): loss=0.37699491969728244\n",
      "Gradient Descent(274/999): loss=0.37693594352533205\n",
      "Gradient Descent(275/999): loss=0.3768774102360407\n",
      "Gradient Descent(276/999): loss=0.37681931594106816\n",
      "Gradient Descent(277/999): loss=0.37676165678796253\n",
      "Gradient Descent(278/999): loss=0.37670442895981837\n",
      "Gradient Descent(279/999): loss=0.3766476286749364\n",
      "Gradient Descent(280/999): loss=0.37659125218648987\n",
      "Gradient Descent(281/999): loss=0.37653529578219147\n",
      "Gradient Descent(282/999): loss=0.3764797557839649\n",
      "Gradient Descent(283/999): loss=0.37642462854761927\n",
      "Gradient Descent(284/999): loss=0.37636991046252727\n",
      "Gradient Descent(285/999): loss=0.3763155979513057\n",
      "Gradient Descent(286/999): loss=0.3762616874695008\n",
      "Gradient Descent(287/999): loss=0.37620817550527424\n",
      "Gradient Descent(288/999): loss=0.3761550585790945\n",
      "Gradient Descent(289/999): loss=0.3761023332434303\n",
      "Gradient Descent(290/999): loss=0.37604999608244716\n",
      "Gradient Descent(291/999): loss=0.375998043711707\n",
      "Gradient Descent(292/999): loss=0.37594647277787097\n",
      "Gradient Descent(293/999): loss=0.3758952799584045\n",
      "Gradient Descent(294/999): loss=0.3758444619612864\n",
      "Gradient Descent(295/999): loss=0.3757940155247189\n",
      "Gradient Descent(296/999): loss=0.3757439374168436\n",
      "Gradient Descent(297/999): loss=0.37569422443545697\n",
      "Gradient Descent(298/999): loss=0.37564487340773056\n",
      "Gradient Descent(299/999): loss=0.3755958811899339\n",
      "Gradient Descent(300/999): loss=0.37554724466715833\n",
      "Gradient Descent(301/999): loss=0.3754989607530468\n",
      "Gradient Descent(302/999): loss=0.3754510263895226\n",
      "Gradient Descent(303/999): loss=0.375403438546524\n",
      "Gradient Descent(304/999): loss=0.37535619422173855\n",
      "Gradient Descent(305/999): loss=0.37530929044034284\n",
      "Gradient Descent(306/999): loss=0.3752627242547423\n",
      "Gradient Descent(307/999): loss=0.3752164927443154\n",
      "Gradient Descent(308/999): loss=0.37517059301515854\n",
      "Gradient Descent(309/999): loss=0.37512502219983596\n",
      "Gradient Descent(310/999): loss=0.37507977745712884\n",
      "Gradient Descent(311/999): loss=0.3750348559717894\n",
      "Gradient Descent(312/999): loss=0.37499025495429694\n",
      "Gradient Descent(313/999): loss=0.3749459716406149\n",
      "Gradient Descent(314/999): loss=0.37490200329195167\n",
      "Gradient Descent(315/999): loss=0.3748583471945227\n",
      "Gradient Descent(316/999): loss=0.37481500065931683\n",
      "Gradient Descent(317/999): loss=0.37477196102186183\n",
      "Gradient Descent(318/999): loss=0.37472922564199457\n",
      "Gradient Descent(319/999): loss=0.374686791903633\n",
      "Gradient Descent(320/999): loss=0.3746446572145496\n",
      "Gradient Descent(321/999): loss=0.3746028190061474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(322/999): loss=0.37456127473323847\n",
      "Gradient Descent(323/999): loss=0.37452002187382427\n",
      "Gradient Descent(324/999): loss=0.37447905792887787\n",
      "Gradient Descent(325/999): loss=0.3744383804221285\n",
      "Gradient Descent(326/999): loss=0.37439798689984854\n",
      "Gradient Descent(327/999): loss=0.3743578749306421\n",
      "Gradient Descent(328/999): loss=0.37431804210523495\n",
      "Gradient Descent(329/999): loss=0.3742784860362688\n",
      "Gradient Descent(330/999): loss=0.37423920435809443\n",
      "Gradient Descent(331/999): loss=0.3742001947265695\n",
      "Gradient Descent(332/999): loss=0.3741614548188566\n",
      "Gradient Descent(333/999): loss=0.3741229823332237\n",
      "Gradient Descent(334/999): loss=0.37408477498884707\n",
      "Gradient Descent(335/999): loss=0.3740468305256157\n",
      "Gradient Descent(336/999): loss=0.3740091467039371\n",
      "Gradient Descent(337/999): loss=0.37397172130454537\n",
      "Gradient Descent(338/999): loss=0.37393455212831184\n",
      "Gradient Descent(339/999): loss=0.3738976369960561\n",
      "Gradient Descent(340/999): loss=0.3738609737483596\n",
      "Gradient Descent(341/999): loss=0.37382456024538163\n",
      "Gradient Descent(342/999): loss=0.3737883943666757\n",
      "Gradient Descent(343/999): loss=0.373752474011009\n",
      "Gradient Descent(344/999): loss=0.37371679709618233\n",
      "Gradient Descent(345/999): loss=0.37368136155885284\n",
      "Gradient Descent(346/999): loss=0.373646165354358\n",
      "Gradient Descent(347/999): loss=0.3736112064565413\n",
      "Gradient Descent(348/999): loss=0.37357648285757916\n",
      "Gradient Descent(349/999): loss=0.3735419925678107\n",
      "Gradient Descent(350/999): loss=0.37350773361556766\n",
      "Gradient Descent(351/999): loss=0.37347370404700675\n",
      "Gradient Descent(352/999): loss=0.3734399019259439\n",
      "Gradient Descent(353/999): loss=0.37340632533368934\n",
      "Gradient Descent(354/999): loss=0.3733729723688847\n",
      "Gradient Descent(355/999): loss=0.37333984114734137\n",
      "Gradient Descent(356/999): loss=0.3733069298018807\n",
      "Gradient Descent(357/999): loss=0.37327423648217567\n",
      "Gradient Descent(358/999): loss=0.37324175935459397\n",
      "Gradient Descent(359/999): loss=0.37320949660204256\n",
      "Gradient Descent(360/999): loss=0.3731774464238139\n",
      "Gradient Descent(361/999): loss=0.37314560703543304\n",
      "Gradient Descent(362/999): loss=0.37311397666850726\n",
      "Gradient Descent(363/999): loss=0.37308255357057624\n",
      "Gradient Descent(364/999): loss=0.3730513360049635\n",
      "Gradient Descent(365/999): loss=0.3730203222506304\n",
      "Gradient Descent(366/999): loss=0.3729895106020301\n",
      "Gradient Descent(367/999): loss=0.37295889936896437\n",
      "Gradient Descent(368/999): loss=0.37292848687644004\n",
      "Gradient Descent(369/999): loss=0.3728982714645287\n",
      "Gradient Descent(370/999): loss=0.3728682514882262\n",
      "Gradient Descent(371/999): loss=0.37283842531731415\n",
      "Gradient Descent(372/999): loss=0.37280879133622313\n",
      "Gradient Descent(373/999): loss=0.3727793479438961\n",
      "Gradient Descent(374/999): loss=0.3727500935536543\n",
      "Gradient Descent(375/999): loss=0.37272102659306305\n",
      "Gradient Descent(376/999): loss=0.3726921455038008\n",
      "Gradient Descent(377/999): loss=0.3726634487415275\n",
      "Gradient Descent(378/999): loss=0.37263493477575477\n",
      "Gradient Descent(379/999): loss=0.3726066020897183\n",
      "Gradient Descent(380/999): loss=0.3725784491802501\n",
      "Gradient Descent(381/999): loss=0.3725504745576525\n",
      "Gradient Descent(382/999): loss=0.3725226767455737\n",
      "Gradient Descent(383/999): loss=0.37249505428088375\n",
      "Gradient Descent(384/999): loss=0.37246760571355286\n",
      "Gradient Descent(385/999): loss=0.3724403296065292\n",
      "Gradient Descent(386/999): loss=0.3724132245356192\n",
      "Gradient Descent(387/999): loss=0.3723862890893689\n",
      "Gradient Descent(388/999): loss=0.3723595218689457\n",
      "Gradient Descent(389/999): loss=0.3723329214880216\n",
      "Gradient Descent(390/999): loss=0.3723064865726576\n",
      "Gradient Descent(391/999): loss=0.37228021576118914\n",
      "Gradient Descent(392/999): loss=0.37225410770411327\n",
      "Gradient Descent(393/999): loss=0.372228161063975\n",
      "Gradient Descent(394/999): loss=0.37220237451525695\n",
      "Gradient Descent(395/999): loss=0.372176746744269\n",
      "Gradient Descent(396/999): loss=0.3721512764490385\n",
      "Gradient Descent(397/999): loss=0.3721259623392027\n",
      "Gradient Descent(398/999): loss=0.37210080313590127\n",
      "Gradient Descent(399/999): loss=0.37207579757166975\n",
      "Gradient Descent(400/999): loss=0.3720509443903354\n",
      "Gradient Descent(401/999): loss=0.37202624234691206\n",
      "Gradient Descent(402/999): loss=0.3720016902074974\n",
      "Gradient Descent(403/999): loss=0.3719772867491705\n",
      "Gradient Descent(404/999): loss=0.3719530307598909\n",
      "Gradient Descent(405/999): loss=0.3719289210383982\n",
      "Gradient Descent(406/999): loss=0.37190495639411214\n",
      "Gradient Descent(407/999): loss=0.3718811356470355\n",
      "Gradient Descent(408/999): loss=0.3718574576276551\n",
      "Gradient Descent(409/999): loss=0.3718339211768459\n",
      "Gradient Descent(410/999): loss=0.37181052514577584\n",
      "Gradient Descent(411/999): loss=0.37178726839581033\n",
      "Gradient Descent(412/999): loss=0.37176414979841876\n",
      "Gradient Descent(413/999): loss=0.3717411682350814\n",
      "Gradient Descent(414/999): loss=0.3717183225971973\n",
      "Gradient Descent(415/999): loss=0.3716956117859929\n",
      "Gradient Descent(416/999): loss=0.37167303471243235\n",
      "Gradient Descent(417/999): loss=0.3716505902971269\n",
      "Gradient Descent(418/999): loss=0.37162827747024696\n",
      "Gradient Descent(419/999): loss=0.3716060951714341\n",
      "Gradient Descent(420/999): loss=0.37158404234971415\n",
      "Gradient Descent(421/999): loss=0.3715621179634106\n",
      "Gradient Descent(422/999): loss=0.37154032098005935\n",
      "Gradient Descent(423/999): loss=0.37151865037632464\n",
      "Gradient Descent(424/999): loss=0.3714971051379142\n",
      "Gradient Descent(425/999): loss=0.3714756842594974\n",
      "Gradient Descent(426/999): loss=0.371454386744622\n",
      "Gradient Descent(427/999): loss=0.3714332116056339\n",
      "Gradient Descent(428/999): loss=0.37141215786359494\n",
      "Gradient Descent(429/999): loss=0.37139122454820384\n",
      "Gradient Descent(430/999): loss=0.37137041069771753\n",
      "Gradient Descent(431/999): loss=0.37134971535887146\n",
      "Gradient Descent(432/999): loss=0.37132913758680275\n",
      "Gradient Descent(433/999): loss=0.37130867644497284\n",
      "Gradient Descent(434/999): loss=0.3712883310050916\n",
      "Gradient Descent(435/999): loss=0.37126810034704166\n",
      "Gradient Descent(436/999): loss=0.3712479835588031\n",
      "Gradient Descent(437/999): loss=0.3712279797363803\n",
      "Gradient Descent(438/999): loss=0.371208087983727\n",
      "Gradient Descent(439/999): loss=0.37118830741267556\n",
      "Gradient Descent(440/999): loss=0.37116863714286297\n",
      "Gradient Descent(441/999): loss=0.37114907630166066\n",
      "Gradient Descent(442/999): loss=0.37112962402410354\n",
      "Gradient Descent(443/999): loss=0.37111027945281977\n",
      "Gradient Descent(444/999): loss=0.3710910417379612\n",
      "Gradient Descent(445/999): loss=0.3710719100371347\n",
      "Gradient Descent(446/999): loss=0.37105288351533494\n",
      "Gradient Descent(447/999): loss=0.37103396134487515\n",
      "Gradient Descent(448/999): loss=0.3710151427053225\n",
      "Gradient Descent(449/999): loss=0.37099642678342915\n",
      "Gradient Descent(450/999): loss=0.3709778127730698\n",
      "Gradient Descent(451/999): loss=0.37095929987517395\n",
      "Gradient Descent(452/999): loss=0.37094088729766306\n",
      "Gradient Descent(453/999): loss=0.3709225742553866\n",
      "Gradient Descent(454/999): loss=0.37090435997005816\n",
      "Gradient Descent(455/999): loss=0.37088624367019396\n",
      "Gradient Descent(456/999): loss=0.3708682245910497\n",
      "Gradient Descent(457/999): loss=0.37085030197455987\n",
      "Gradient Descent(458/999): loss=0.37083247506927725\n",
      "Gradient Descent(459/999): loss=0.3708147431303112\n",
      "Gradient Descent(460/999): loss=0.37079710541926963\n",
      "Gradient Descent(461/999): loss=0.3707795612041986\n",
      "Gradient Descent(462/999): loss=0.37076210975952445\n",
      "Gradient Descent(463/999): loss=0.37074475036599513\n",
      "Gradient Descent(464/999): loss=0.37072748231062363\n",
      "Gradient Descent(465/999): loss=0.37071030488662965\n",
      "Gradient Descent(466/999): loss=0.3706932173933844\n",
      "Gradient Descent(467/999): loss=0.37067621913635374\n",
      "Gradient Descent(468/999): loss=0.3706593094270438\n",
      "Gradient Descent(469/999): loss=0.370642487582945\n",
      "Gradient Descent(470/999): loss=0.3706257529274782\n",
      "Gradient Descent(471/999): loss=0.370609104789941\n",
      "Gradient Descent(472/999): loss=0.3705925425054541\n",
      "Gradient Descent(473/999): loss=0.37057606541490834\n",
      "Gradient Descent(474/999): loss=0.37055967286491287\n",
      "Gradient Descent(475/999): loss=0.3705433642077424\n",
      "Gradient Descent(476/999): loss=0.3705271388012865\n",
      "Gradient Descent(477/999): loss=0.3705109960089987\n",
      "Gradient Descent(478/999): loss=0.3704949351998451\n",
      "Gradient Descent(479/999): loss=0.3704789557482559\n",
      "Gradient Descent(480/999): loss=0.3704630570340745\n",
      "Gradient Descent(481/999): loss=0.37044723844250893\n",
      "Gradient Descent(482/999): loss=0.37043149936408354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(483/999): loss=0.3704158391945908\n",
      "Gradient Descent(484/999): loss=0.37040025733504267\n",
      "Gradient Descent(485/999): loss=0.37038475319162495\n",
      "Gradient Descent(486/999): loss=0.37036932617564927\n",
      "Gradient Descent(487/999): loss=0.3703539757035065\n",
      "Gradient Descent(488/999): loss=0.3703387011966221\n",
      "Gradient Descent(489/999): loss=0.37032350208140924\n",
      "Gradient Descent(490/999): loss=0.37030837778922476\n",
      "Gradient Descent(491/999): loss=0.37029332775632334\n",
      "Gradient Descent(492/999): loss=0.3702783514238143\n",
      "Gradient Descent(493/999): loss=0.37026344823761753\n",
      "Gradient Descent(494/999): loss=0.3702486176484192\n",
      "Gradient Descent(495/999): loss=0.37023385911162987\n",
      "Gradient Descent(496/999): loss=0.3702191720873407\n",
      "Gradient Descent(497/999): loss=0.37020455604028235\n",
      "Gradient Descent(498/999): loss=0.3701900104397823\n",
      "Gradient Descent(499/999): loss=0.37017553475972353\n",
      "Gradient Descent(500/999): loss=0.3701611284785036\n",
      "Gradient Descent(501/999): loss=0.370146791078994\n",
      "Gradient Descent(502/999): loss=0.3701325220484996\n",
      "Gradient Descent(503/999): loss=0.37011832087871843\n",
      "Gradient Descent(504/999): loss=0.3701041870657029\n",
      "Gradient Descent(505/999): loss=0.3700901201098196\n",
      "Gradient Descent(506/999): loss=0.3700761195157115\n",
      "Gradient Descent(507/999): loss=0.3700621847922583\n",
      "Gradient Descent(508/999): loss=0.3700483154525393\n",
      "Gradient Descent(509/999): loss=0.3700345110137948\n",
      "Gradient Descent(510/999): loss=0.37002077099738917\n",
      "Gradient Descent(511/999): loss=0.3700070949287738\n",
      "Gradient Descent(512/999): loss=0.36999348233744994\n",
      "Gradient Descent(513/999): loss=0.36997993275693253\n",
      "Gradient Descent(514/999): loss=0.3699664457247141\n",
      "Gradient Descent(515/999): loss=0.36995302078222875\n",
      "Gradient Descent(516/999): loss=0.3699396574748176\n",
      "Gradient Descent(517/999): loss=0.36992635535169255\n",
      "Gradient Descent(518/999): loss=0.3699131139659022\n",
      "Gradient Descent(519/999): loss=0.3698999328742976\n",
      "Gradient Descent(520/999): loss=0.36988681163749726\n",
      "Gradient Descent(521/999): loss=0.36987374981985427\n",
      "Gradient Descent(522/999): loss=0.3698607469894222\n",
      "Gradient Descent(523/999): loss=0.3698478027179222\n",
      "Gradient Descent(524/999): loss=0.36983491658071027\n",
      "Gradient Descent(525/999): loss=0.36982208815674394\n",
      "Gradient Descent(526/999): loss=0.36980931702855113\n",
      "Gradient Descent(527/999): loss=0.36979660278219667\n",
      "Gradient Descent(528/999): loss=0.3697839450072524\n",
      "Gradient Descent(529/999): loss=0.36977134329676464\n",
      "Gradient Descent(530/999): loss=0.36975879724722277\n",
      "Gradient Descent(531/999): loss=0.3697463064585294\n",
      "Gradient Descent(532/999): loss=0.3697338705339694\n",
      "Gradient Descent(533/999): loss=0.3697214890801794\n",
      "Gradient Descent(534/999): loss=0.36970916170711815\n",
      "Gradient Descent(535/999): loss=0.3696968880280369\n",
      "Gradient Descent(536/999): loss=0.3696846676594495\n",
      "Gradient Descent(537/999): loss=0.3696725002211034\n",
      "Gradient Descent(538/999): loss=0.3696603853359517\n",
      "Gradient Descent(539/999): loss=0.36964832263012265\n",
      "Gradient Descent(540/999): loss=0.36963631173289296\n",
      "Gradient Descent(541/999): loss=0.3696243522766592\n",
      "Gradient Descent(542/999): loss=0.36961244389690945\n",
      "Gradient Descent(543/999): loss=0.36960058623219655\n",
      "Gradient Descent(544/999): loss=0.3695887789241102\n",
      "Gradient Descent(545/999): loss=0.36957702161724965\n",
      "Gradient Descent(546/999): loss=0.36956531395919784\n",
      "Gradient Descent(547/999): loss=0.3695536556004941\n",
      "Gradient Descent(548/999): loss=0.3695420461946076\n",
      "Gradient Descent(549/999): loss=0.36953048539791217\n",
      "Gradient Descent(550/999): loss=0.3695189728696591\n",
      "Gradient Descent(551/999): loss=0.36950750827195294\n",
      "Gradient Descent(552/999): loss=0.36949609126972516\n",
      "Gradient Descent(553/999): loss=0.3694847215307094\n",
      "Gradient Descent(554/999): loss=0.36947339872541635\n",
      "Gradient Descent(555/999): loss=0.36946212252710914\n",
      "Gradient Descent(556/999): loss=0.36945089261177894\n",
      "Gradient Descent(557/999): loss=0.3694397086581208\n",
      "Gradient Descent(558/999): loss=0.369428570347509\n",
      "Gradient Descent(559/999): loss=0.3694174773639743\n",
      "Gradient Descent(560/999): loss=0.36940642939417934\n",
      "Gradient Descent(561/999): loss=0.3693954261273959\n",
      "Gradient Descent(562/999): loss=0.3693844672554818\n",
      "Gradient Descent(563/999): loss=0.36937355247285747\n",
      "Gradient Descent(564/999): loss=0.3693626814764833\n",
      "Gradient Descent(565/999): loss=0.36935185396583764\n",
      "Gradient Descent(566/999): loss=0.369341069642894\n",
      "Gradient Descent(567/999): loss=0.36933032821209866\n",
      "Gradient Descent(568/999): loss=0.3693196293803502\n",
      "Gradient Descent(569/999): loss=0.36930897285697556\n",
      "Gradient Descent(570/999): loss=0.36929835835371055\n",
      "Gradient Descent(571/999): loss=0.3692877855846773\n",
      "Gradient Descent(572/999): loss=0.3692772542663637\n",
      "Gradient Descent(573/999): loss=0.36926676411760206\n",
      "Gradient Descent(574/999): loss=0.36925631485954924\n",
      "Gradient Descent(575/999): loss=0.3692459062156648\n",
      "Gradient Descent(576/999): loss=0.36923553791169167\n",
      "Gradient Descent(577/999): loss=0.36922520967563577\n",
      "Gradient Descent(578/999): loss=0.3692149212377456\n",
      "Gradient Descent(579/999): loss=0.3692046723304924\n",
      "Gradient Descent(580/999): loss=0.36919446268855194\n",
      "Gradient Descent(581/999): loss=0.369184292048783\n",
      "Gradient Descent(582/999): loss=0.3691741601502091\n",
      "Gradient Descent(583/999): loss=0.3691640667339999\n",
      "Gradient Descent(584/999): loss=0.3691540115434513\n",
      "Gradient Descent(585/999): loss=0.3691439943239676\n",
      "Gradient Descent(586/999): loss=0.3691340148230419\n",
      "Gradient Descent(587/999): loss=0.36912407279023896\n",
      "Gradient Descent(588/999): loss=0.3691141679771752\n",
      "Gradient Descent(589/999): loss=0.3691043001375028\n",
      "Gradient Descent(590/999): loss=0.36909446902689\n",
      "Gradient Descent(591/999): loss=0.3690846744030044\n",
      "Gradient Descent(592/999): loss=0.3690749160254946\n",
      "Gradient Descent(593/999): loss=0.3690651936559735\n",
      "Gradient Descent(594/999): loss=0.3690555070580005\n",
      "Gradient Descent(595/999): loss=0.3690458559970647\n",
      "Gradient Descent(596/999): loss=0.3690362402405677\n",
      "Gradient Descent(597/999): loss=0.36902665955780733\n",
      "Gradient Descent(598/999): loss=0.36901711371996\n",
      "Gradient Descent(599/999): loss=0.36900760250006537\n",
      "Gradient Descent(600/999): loss=0.36899812567300916\n",
      "Gradient Descent(601/999): loss=0.36898868301550714\n",
      "Gradient Descent(602/999): loss=0.36897927430608934\n",
      "Gradient Descent(603/999): loss=0.36896989932508406\n",
      "Gradient Descent(604/999): loss=0.36896055785460197\n",
      "Gradient Descent(605/999): loss=0.3689512496785202\n",
      "Gradient Descent(606/999): loss=0.36894197458246775\n",
      "Gradient Descent(607/999): loss=0.3689327323538093\n",
      "Gradient Descent(608/999): loss=0.3689235227816305\n",
      "Gradient Descent(609/999): loss=0.3689143456567222\n",
      "Gradient Descent(610/999): loss=0.36890520077156697\n",
      "Gradient Descent(611/999): loss=0.3688960879203228\n",
      "Gradient Descent(612/999): loss=0.3688870068988093\n",
      "Gradient Descent(613/999): loss=0.3688779575044923\n",
      "Gradient Descent(614/999): loss=0.3688689395364712\n",
      "Gradient Descent(615/999): loss=0.36885995279546263\n",
      "Gradient Descent(616/999): loss=0.36885099708378777\n",
      "Gradient Descent(617/999): loss=0.36884207220535786\n",
      "Gradient Descent(618/999): loss=0.36883317796566006\n",
      "Gradient Descent(619/999): loss=0.3688243141717445\n",
      "Gradient Descent(620/999): loss=0.3688154806322095\n",
      "Gradient Descent(621/999): loss=0.3688066771571889\n",
      "Gradient Descent(622/999): loss=0.36879790355833886\n",
      "Gradient Descent(623/999): loss=0.36878915964882364\n",
      "Gradient Descent(624/999): loss=0.36878044524330356\n",
      "Gradient Descent(625/999): loss=0.36877176015792107\n",
      "Gradient Descent(626/999): loss=0.36876310421028835\n",
      "Gradient Descent(627/999): loss=0.3687544772194746\n",
      "Gradient Descent(628/999): loss=0.36874587900599287\n",
      "Gradient Descent(629/999): loss=0.368737309391788\n",
      "Gradient Descent(630/999): loss=0.368728768200224\n",
      "Gradient Descent(631/999): loss=0.36872025525607144\n",
      "Gradient Descent(632/999): loss=0.3687117703854959\n",
      "Gradient Descent(633/999): loss=0.3687033134160454\n",
      "Gradient Descent(634/999): loss=0.3686948841766378\n",
      "Gradient Descent(635/999): loss=0.36868648249755054\n",
      "Gradient Descent(636/999): loss=0.36867810821040725\n",
      "Gradient Descent(637/999): loss=0.3686697611481669\n",
      "Gradient Descent(638/999): loss=0.36866144114511185\n",
      "Gradient Descent(639/999): loss=0.36865314803683685\n",
      "Gradient Descent(640/999): loss=0.36864488166023734\n",
      "Gradient Descent(641/999): loss=0.3686366418534978\n",
      "Gradient Descent(642/999): loss=0.3686284284560815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(643/999): loss=0.36862024130871873\n",
      "Gradient Descent(644/999): loss=0.3686120802533962\n",
      "Gradient Descent(645/999): loss=0.36860394513334577\n",
      "Gradient Descent(646/999): loss=0.3685958357930344\n",
      "Gradient Descent(647/999): loss=0.3685877520781525\n",
      "Gradient Descent(648/999): loss=0.36857969383560424\n",
      "Gradient Descent(649/999): loss=0.36857166091349675\n",
      "Gradient Descent(650/999): loss=0.3685636531611294\n",
      "Gradient Descent(651/999): loss=0.36855567042898413\n",
      "Gradient Descent(652/999): loss=0.3685477125687146\n",
      "Gradient Descent(653/999): loss=0.368539779433137\n",
      "Gradient Descent(654/999): loss=0.3685318708762186\n",
      "Gradient Descent(655/999): loss=0.36852398675306974\n",
      "Gradient Descent(656/999): loss=0.3685161269199318\n",
      "Gradient Descent(657/999): loss=0.36850829123416945\n",
      "Gradient Descent(658/999): loss=0.3685004795542598\n",
      "Gradient Descent(659/999): loss=0.36849269173978316\n",
      "Gradient Descent(660/999): loss=0.3684849276514132\n",
      "Gradient Descent(661/999): loss=0.36847718715090877\n",
      "Gradient Descent(662/999): loss=0.36846947010110276\n",
      "Gradient Descent(663/999): loss=0.3684617763658944\n",
      "Gradient Descent(664/999): loss=0.368454105810239\n",
      "Gradient Descent(665/999): loss=0.3684464583001401\n",
      "Gradient Descent(666/999): loss=0.36843883370263886\n",
      "Gradient Descent(667/999): loss=0.36843123188580706\n",
      "Gradient Descent(668/999): loss=0.3684236527187363\n",
      "Gradient Descent(669/999): loss=0.3684160960715308\n",
      "Gradient Descent(670/999): loss=0.3684085618152982\n",
      "Gradient Descent(671/999): loss=0.36840104982214034\n",
      "Gradient Descent(672/999): loss=0.3683935599651459\n",
      "Gradient Descent(673/999): loss=0.3683860921183812\n",
      "Gradient Descent(674/999): loss=0.3683786461568818\n",
      "Gradient Descent(675/999): loss=0.3683712219566447\n",
      "Gradient Descent(676/999): loss=0.3683638193946196\n",
      "Gradient Descent(677/999): loss=0.3683564383487011\n",
      "Gradient Descent(678/999): loss=0.3683490786977204\n",
      "Gradient Descent(679/999): loss=0.3683417403214376\n",
      "Gradient Descent(680/999): loss=0.36833442310053355\n",
      "Gradient Descent(681/999): loss=0.36832712691660185\n",
      "Gradient Descent(682/999): loss=0.3683198516521415\n",
      "Gradient Descent(683/999): loss=0.36831259719054865\n",
      "Gradient Descent(684/999): loss=0.3683053634161095\n",
      "Gradient Descent(685/999): loss=0.3682981502139923\n",
      "Gradient Descent(686/999): loss=0.36829095747024004\n",
      "Gradient Descent(687/999): loss=0.3682837850717627\n",
      "Gradient Descent(688/999): loss=0.3682766329063306\n",
      "Gradient Descent(689/999): loss=0.36826950086256627\n",
      "Gradient Descent(690/999): loss=0.3682623888299374\n",
      "Gradient Descent(691/999): loss=0.36825529669875023\n",
      "Gradient Descent(692/999): loss=0.36824822436014165\n",
      "Gradient Descent(693/999): loss=0.36824117170607257\n",
      "Gradient Descent(694/999): loss=0.3682341386293209\n",
      "Gradient Descent(695/999): loss=0.36822712502347427\n",
      "Gradient Descent(696/999): loss=0.368220130782924\n",
      "Gradient Descent(697/999): loss=0.3682131558028571\n",
      "Gradient Descent(698/999): loss=0.3682061999792507\n",
      "Gradient Descent(699/999): loss=0.3681992632088645\n",
      "Gradient Descent(700/999): loss=0.3681923453892344\n",
      "Gradient Descent(701/999): loss=0.36818544641866635\n",
      "Gradient Descent(702/999): loss=0.36817856619622913\n",
      "Gradient Descent(703/999): loss=0.36817170462174825\n",
      "Gradient Descent(704/999): loss=0.3681648615957997\n",
      "Gradient Descent(705/999): loss=0.3681580370197035\n",
      "Gradient Descent(706/999): loss=0.3681512307955172\n",
      "Gradient Descent(707/999): loss=0.3681444428260298\n",
      "Gradient Descent(708/999): loss=0.36813767301475564\n",
      "Gradient Descent(709/999): loss=0.36813092126592845\n",
      "Gradient Descent(710/999): loss=0.3681241874844948\n",
      "Gradient Descent(711/999): loss=0.3681174715761083\n",
      "Gradient Descent(712/999): loss=0.3681107734471242\n",
      "Gradient Descent(713/999): loss=0.3681040930045922\n",
      "Gradient Descent(714/999): loss=0.36809743015625207\n",
      "Gradient Descent(715/999): loss=0.3680907848105268\n",
      "Gradient Descent(716/999): loss=0.3680841568765175\n",
      "Gradient Descent(717/999): loss=0.3680775462639972\n",
      "Gradient Descent(718/999): loss=0.36807095288340513\n",
      "Gradient Descent(719/999): loss=0.36806437664584235\n",
      "Gradient Descent(720/999): loss=0.3680578174630642\n",
      "Gradient Descent(721/999): loss=0.3680512752474764\n",
      "Gradient Descent(722/999): loss=0.36804474991212915\n",
      "Gradient Descent(723/999): loss=0.3680382413707112\n",
      "Gradient Descent(724/999): loss=0.36803174953754536\n",
      "Gradient Descent(725/999): loss=0.3680252743275826\n",
      "Gradient Descent(726/999): loss=0.36801881565639694\n",
      "Gradient Descent(727/999): loss=0.3680123734401801\n",
      "Gradient Descent(728/999): loss=0.36800594759573696\n",
      "Gradient Descent(729/999): loss=0.36799953804047936\n",
      "Gradient Descent(730/999): loss=0.36799314469242206\n",
      "Gradient Descent(731/999): loss=0.3679867674701771\n",
      "Gradient Descent(732/999): loss=0.36798040629294876\n",
      "Gradient Descent(733/999): loss=0.3679740610805294\n",
      "Gradient Descent(734/999): loss=0.3679677317532932\n",
      "Gradient Descent(735/999): loss=0.36796141823219314\n",
      "Gradient Descent(736/999): loss=0.36795512043875384\n",
      "Gradient Descent(737/999): loss=0.36794883829506936\n",
      "Gradient Descent(738/999): loss=0.367942571723796\n",
      "Gradient Descent(739/999): loss=0.36793632064815013\n",
      "Gradient Descent(740/999): loss=0.36793008499190083\n",
      "Gradient Descent(741/999): loss=0.3679238646793677\n",
      "Gradient Descent(742/999): loss=0.3679176596354148\n",
      "Gradient Descent(743/999): loss=0.36791146978544664\n",
      "Gradient Descent(744/999): loss=0.36790529505540376\n",
      "Gradient Descent(745/999): loss=0.36789913537175795\n",
      "Gradient Descent(746/999): loss=0.36789299066150805\n",
      "Gradient Descent(747/999): loss=0.3678868608521761\n",
      "Gradient Descent(748/999): loss=0.3678807458718018\n",
      "Gradient Descent(749/999): loss=0.36787464564893907\n",
      "Gradient Descent(750/999): loss=0.36786856011265195\n",
      "Gradient Descent(751/999): loss=0.3678624891925098\n",
      "Gradient Descent(752/999): loss=0.3678564328185836\n",
      "Gradient Descent(753/999): loss=0.3678503909214411\n",
      "Gradient Descent(754/999): loss=0.36784436343214394\n",
      "Gradient Descent(755/999): loss=0.367838350282242\n",
      "Gradient Descent(756/999): loss=0.36783235140377096\n",
      "Gradient Descent(757/999): loss=0.36782636672924673\n",
      "Gradient Descent(758/999): loss=0.3678203961916634\n",
      "Gradient Descent(759/999): loss=0.36781443972448685\n",
      "Gradient Descent(760/999): loss=0.36780849726165354\n",
      "Gradient Descent(761/999): loss=0.36780256873756423\n",
      "Gradient Descent(762/999): loss=0.3677966540870818\n",
      "Gradient Descent(763/999): loss=0.3677907532455266\n",
      "Gradient Descent(764/999): loss=0.36778486614867334\n",
      "Gradient Descent(765/999): loss=0.36777899273274656\n",
      "Gradient Descent(766/999): loss=0.36777313293441743\n",
      "Gradient Descent(767/999): loss=0.36776728669080055\n",
      "Gradient Descent(768/999): loss=0.36776145393944903\n",
      "Gradient Descent(769/999): loss=0.367755634618352\n",
      "Gradient Descent(770/999): loss=0.36774982866593064\n",
      "Gradient Descent(771/999): loss=0.3677440360210349\n",
      "Gradient Descent(772/999): loss=0.3677382566229393\n",
      "Gradient Descent(773/999): loss=0.3677324904113404\n",
      "Gradient Descent(774/999): loss=0.36772673732635247\n",
      "Gradient Descent(775/999): loss=0.3677209973085047\n",
      "Gradient Descent(776/999): loss=0.36771527029873763\n",
      "Gradient Descent(777/999): loss=0.36770955623839974\n",
      "Gradient Descent(778/999): loss=0.3677038550692441\n",
      "Gradient Descent(779/999): loss=0.36769816673342487\n",
      "Gradient Descent(780/999): loss=0.3676924911734945\n",
      "Gradient Descent(781/999): loss=0.3676868283324006\n",
      "Gradient Descent(782/999): loss=0.3676811781534816\n",
      "Gradient Descent(783/999): loss=0.3676755405804648\n",
      "Gradient Descent(784/999): loss=0.367669915557463\n",
      "Gradient Descent(785/999): loss=0.3676643030289705\n",
      "Gradient Descent(786/999): loss=0.36765870293986114\n",
      "Gradient Descent(787/999): loss=0.3676531152353843\n",
      "Gradient Descent(788/999): loss=0.36764753986116255\n",
      "Gradient Descent(789/999): loss=0.3676419767631884\n",
      "Gradient Descent(790/999): loss=0.3676364258878207\n",
      "Gradient Descent(791/999): loss=0.36763088718178266\n",
      "Gradient Descent(792/999): loss=0.3676253605921584\n",
      "Gradient Descent(793/999): loss=0.36761984606639003\n",
      "Gradient Descent(794/999): loss=0.3676143435522744\n",
      "Gradient Descent(795/999): loss=0.3676088529979615\n",
      "Gradient Descent(796/999): loss=0.3676033743519495\n",
      "Gradient Descent(797/999): loss=0.3675979075630845\n",
      "Gradient Descent(798/999): loss=0.3675924525805553\n",
      "Gradient Descent(799/999): loss=0.36758700935389266\n",
      "Gradient Descent(800/999): loss=0.3675815778329647\n",
      "Gradient Descent(801/999): loss=0.36757615796797566\n",
      "Gradient Descent(802/999): loss=0.36757074970946263\n",
      "Gradient Descent(803/999): loss=0.3675653530082927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(804/999): loss=0.36755996781566064\n",
      "Gradient Descent(805/999): loss=0.3675545940830857\n",
      "Gradient Descent(806/999): loss=0.36754923176240994\n",
      "Gradient Descent(807/999): loss=0.36754388080579475\n",
      "Gradient Descent(808/999): loss=0.367538541165719\n",
      "Gradient Descent(809/999): loss=0.3675332127949758\n",
      "Gradient Descent(810/999): loss=0.36752789564667043\n",
      "Gradient Descent(811/999): loss=0.3675225896742179\n",
      "Gradient Descent(812/999): loss=0.3675172948313401\n",
      "Gradient Descent(813/999): loss=0.36751201107206355\n",
      "Gradient Descent(814/999): loss=0.36750673835071723\n",
      "Gradient Descent(815/999): loss=0.3675014766219297\n",
      "Gradient Descent(816/999): loss=0.367496225840627\n",
      "Gradient Descent(817/999): loss=0.36749098596202995\n",
      "Gradient Descent(818/999): loss=0.3674857569416525\n",
      "Gradient Descent(819/999): loss=0.36748053873529846\n",
      "Gradient Descent(820/999): loss=0.3674753312990602\n",
      "Gradient Descent(821/999): loss=0.3674701345893153\n",
      "Gradient Descent(822/999): loss=0.36746494856272505\n",
      "Gradient Descent(823/999): loss=0.36745977317623213\n",
      "Gradient Descent(824/999): loss=0.3674546083870579\n",
      "Gradient Descent(825/999): loss=0.3674494541527005\n",
      "Gradient Descent(826/999): loss=0.36744431043093273\n",
      "Gradient Descent(827/999): loss=0.3674391771798\n",
      "Gradient Descent(828/999): loss=0.3674340543576179\n",
      "Gradient Descent(829/999): loss=0.36742894192296954\n",
      "Gradient Descent(830/999): loss=0.3674238398347049\n",
      "Gradient Descent(831/999): loss=0.3674187480519371\n",
      "Gradient Descent(832/999): loss=0.36741366653404145\n",
      "Gradient Descent(833/999): loss=0.36740859524065284\n",
      "Gradient Descent(834/999): loss=0.367403534131664\n",
      "Gradient Descent(835/999): loss=0.36739848316722307\n",
      "Gradient Descent(836/999): loss=0.3673934423077321\n",
      "Gradient Descent(837/999): loss=0.36738841151384444\n",
      "Gradient Descent(838/999): loss=0.3673833907464635\n",
      "Gradient Descent(839/999): loss=0.36737837996674\n",
      "Gradient Descent(840/999): loss=0.36737337913607054\n",
      "Gradient Descent(841/999): loss=0.3673683882160958\n",
      "Gradient Descent(842/999): loss=0.36736340716869786\n",
      "Gradient Descent(843/999): loss=0.36735843595599915\n",
      "Gradient Descent(844/999): loss=0.36735347454036044\n",
      "Gradient Descent(845/999): loss=0.3673485228843782\n",
      "Gradient Descent(846/999): loss=0.3673435809508836\n",
      "Gradient Descent(847/999): loss=0.3673386487029404\n",
      "Gradient Descent(848/999): loss=0.36733372610384346\n",
      "Gradient Descent(849/999): loss=0.36732881311711585\n",
      "Gradient Descent(850/999): loss=0.3673239097065085\n",
      "Gradient Descent(851/999): loss=0.36731901583599735\n",
      "Gradient Descent(852/999): loss=0.36731413146978215\n",
      "Gradient Descent(853/999): loss=0.36730925657228436\n",
      "Gradient Descent(854/999): loss=0.36730439110814617\n",
      "Gradient Descent(855/999): loss=0.3672995350422275\n",
      "Gradient Descent(856/999): loss=0.36729468833960566\n",
      "Gradient Descent(857/999): loss=0.36728985096557265\n",
      "Gradient Descent(858/999): loss=0.367285022885634\n",
      "Gradient Descent(859/999): loss=0.36728020406550715\n",
      "Gradient Descent(860/999): loss=0.3672753944711195\n",
      "Gradient Descent(861/999): loss=0.3672705940686072\n",
      "Gradient Descent(862/999): loss=0.3672658028243127\n",
      "Gradient Descent(863/999): loss=0.3672610207047844\n",
      "Gradient Descent(864/999): loss=0.3672562476767742\n",
      "Gradient Descent(865/999): loss=0.3672514837072358\n",
      "Gradient Descent(866/999): loss=0.36724672876332415\n",
      "Gradient Descent(867/999): loss=0.36724198281239256\n",
      "Gradient Descent(868/999): loss=0.367237245821992\n",
      "Gradient Descent(869/999): loss=0.3672325177598697\n",
      "Gradient Descent(870/999): loss=0.3672277985939672\n",
      "Gradient Descent(871/999): loss=0.36722308829241884\n",
      "Gradient Descent(872/999): loss=0.3672183868235507\n",
      "Gradient Descent(873/999): loss=0.3672136941558788\n",
      "Gradient Descent(874/999): loss=0.36720901025810765\n",
      "Gradient Descent(875/999): loss=0.36720433509912875\n",
      "Gradient Descent(876/999): loss=0.3671996686480197\n",
      "Gradient Descent(877/999): loss=0.3671950108740422\n",
      "Gradient Descent(878/999): loss=0.3671903617466407\n",
      "Gradient Descent(879/999): loss=0.36718572123544113\n",
      "Gradient Descent(880/999): loss=0.3671810893102496\n",
      "Gradient Descent(881/999): loss=0.3671764659410511\n",
      "Gradient Descent(882/999): loss=0.3671718510980073\n",
      "Gradient Descent(883/999): loss=0.3671672447514566\n",
      "Gradient Descent(884/999): loss=0.3671626468719119\n",
      "Gradient Descent(885/999): loss=0.3671580574300592\n",
      "Gradient Descent(886/999): loss=0.36715347639675666\n",
      "Gradient Descent(887/999): loss=0.3671489037430329\n",
      "Gradient Descent(888/999): loss=0.3671443394400865\n",
      "Gradient Descent(889/999): loss=0.36713978345928383\n",
      "Gradient Descent(890/999): loss=0.3671352357721579\n",
      "Gradient Descent(891/999): loss=0.3671306963504079\n",
      "Gradient Descent(892/999): loss=0.3671261651658969\n",
      "Gradient Descent(893/999): loss=0.36712164219065124\n",
      "Gradient Descent(894/999): loss=0.36711712739685903\n",
      "Gradient Descent(895/999): loss=0.3671126207568694\n",
      "Gradient Descent(896/999): loss=0.36710812224319045\n",
      "Gradient Descent(897/999): loss=0.3671036318284888\n",
      "Gradient Descent(898/999): loss=0.36709914948558825\n",
      "Gradient Descent(899/999): loss=0.3670946751874678\n",
      "Gradient Descent(900/999): loss=0.36709020890726235\n",
      "Gradient Descent(901/999): loss=0.3670857506182591\n",
      "Gradient Descent(902/999): loss=0.36708130029389874\n",
      "Gradient Descent(903/999): loss=0.36707685790777217\n",
      "Gradient Descent(904/999): loss=0.36707242343362134\n",
      "Gradient Descent(905/999): loss=0.36706799684533653\n",
      "Gradient Descent(906/999): loss=0.36706357811695645\n",
      "Gradient Descent(907/999): loss=0.36705916722266635\n",
      "Gradient Descent(908/999): loss=0.36705476413679694\n",
      "Gradient Descent(909/999): loss=0.3670503688338239\n",
      "Gradient Descent(910/999): loss=0.3670459812883664\n",
      "Gradient Descent(911/999): loss=0.367041601475186\n",
      "Gradient Descent(912/999): loss=0.3670372293691854\n",
      "Gradient Descent(913/999): loss=0.3670328649454084\n",
      "Gradient Descent(914/999): loss=0.3670285081790371\n",
      "Gradient Descent(915/999): loss=0.36702415904539304\n",
      "Gradient Descent(916/999): loss=0.36701981751993407\n",
      "Gradient Descent(917/999): loss=0.3670154835782548\n",
      "Gradient Descent(918/999): loss=0.3670111571960847\n",
      "Gradient Descent(919/999): loss=0.36700683834928804\n",
      "Gradient Descent(920/999): loss=0.36700252701386166\n",
      "Gradient Descent(921/999): loss=0.3669982231659354\n",
      "Gradient Descent(922/999): loss=0.3669939267817699\n",
      "Gradient Descent(923/999): loss=0.3669896378377561\n",
      "Gradient Descent(924/999): loss=0.36698535631041485\n",
      "Gradient Descent(925/999): loss=0.3669810821763948\n",
      "Gradient Descent(926/999): loss=0.3669768154124725\n",
      "Gradient Descent(927/999): loss=0.36697255599555084\n",
      "Gradient Descent(928/999): loss=0.36696830390265844\n",
      "Gradient Descent(929/999): loss=0.36696405911094865\n",
      "Gradient Descent(930/999): loss=0.3669598215976983\n",
      "Gradient Descent(931/999): loss=0.36695559134030775\n",
      "Gradient Descent(932/999): loss=0.36695136831629876\n",
      "Gradient Descent(933/999): loss=0.3669471525033144\n",
      "Gradient Descent(934/999): loss=0.3669429438791182\n",
      "Gradient Descent(935/999): loss=0.3669387424215927\n",
      "Gradient Descent(936/999): loss=0.3669345481087389\n",
      "Gradient Descent(937/999): loss=0.3669303609186757\n",
      "Gradient Descent(938/999): loss=0.3669261808296388\n",
      "Gradient Descent(939/999): loss=0.3669220078199797\n",
      "Gradient Descent(940/999): loss=0.3669178418681647\n",
      "Gradient Descent(941/999): loss=0.3669136829527746\n",
      "Gradient Descent(942/999): loss=0.36690953105250396\n",
      "Gradient Descent(943/999): loss=0.36690538614615964\n",
      "Gradient Descent(944/999): loss=0.36690124821266024\n",
      "Gradient Descent(945/999): loss=0.3668971172310353\n",
      "Gradient Descent(946/999): loss=0.36689299318042484\n",
      "Gradient Descent(947/999): loss=0.36688887604007797\n",
      "Gradient Descent(948/999): loss=0.3668847657893528\n",
      "Gradient Descent(949/999): loss=0.36688066240771494\n",
      "Gradient Descent(950/999): loss=0.3668765658747372\n",
      "Gradient Descent(951/999): loss=0.3668724761700988\n",
      "Gradient Descent(952/999): loss=0.36686839327358445\n",
      "Gradient Descent(953/999): loss=0.36686431716508333\n",
      "Gradient Descent(954/999): loss=0.36686024782458915\n",
      "Gradient Descent(955/999): loss=0.3668561852321985\n",
      "Gradient Descent(956/999): loss=0.36685212936811107\n",
      "Gradient Descent(957/999): loss=0.3668480802126277\n",
      "Gradient Descent(958/999): loss=0.3668440377461509\n",
      "Gradient Descent(959/999): loss=0.36684000194918326\n",
      "Gradient Descent(960/999): loss=0.36683597280232727\n",
      "Gradient Descent(961/999): loss=0.36683195028628424\n",
      "Gradient Descent(962/999): loss=0.3668279343818539\n",
      "Gradient Descent(963/999): loss=0.3668239250699332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(964/999): loss=0.36681992233151633\n",
      "Gradient Descent(965/999): loss=0.366815926147694\n",
      "Gradient Descent(966/999): loss=0.3668119364996515\n",
      "Gradient Descent(967/999): loss=0.36680795336866967\n",
      "Gradient Descent(968/999): loss=0.3668039767361237\n",
      "Gradient Descent(969/999): loss=0.3668000065834819\n",
      "Gradient Descent(970/999): loss=0.36679604289230516\n",
      "Gradient Descent(971/999): loss=0.3667920856442477\n",
      "Gradient Descent(972/999): loss=0.36678813482105416\n",
      "Gradient Descent(973/999): loss=0.3667841904045605\n",
      "Gradient Descent(974/999): loss=0.36678025237669365\n",
      "Gradient Descent(975/999): loss=0.3667763207194688\n",
      "Gradient Descent(976/999): loss=0.36677239541499174\n",
      "Gradient Descent(977/999): loss=0.3667684764454557\n",
      "Gradient Descent(978/999): loss=0.3667645637931418\n",
      "Gradient Descent(979/999): loss=0.3667606574404187\n",
      "Gradient Descent(980/999): loss=0.36675675736974156\n",
      "Gradient Descent(981/999): loss=0.36675286356365133\n",
      "Gradient Descent(982/999): loss=0.36674897600477463\n",
      "Gradient Descent(983/999): loss=0.36674509467582267\n",
      "Gradient Descent(984/999): loss=0.3667412195595912\n",
      "Gradient Descent(985/999): loss=0.3667373506389593\n",
      "Gradient Descent(986/999): loss=0.3667334878968889\n",
      "Gradient Descent(987/999): loss=0.36672963131642533\n",
      "Gradient Descent(988/999): loss=0.3667257808806947\n",
      "Gradient Descent(989/999): loss=0.36672193657290547\n",
      "Gradient Descent(990/999): loss=0.3667180983763463\n",
      "Gradient Descent(991/999): loss=0.3667142662743863\n",
      "Gradient Descent(992/999): loss=0.36671044025047433\n",
      "Gradient Descent(993/999): loss=0.36670662028813816\n",
      "Gradient Descent(994/999): loss=0.36670280637098474\n",
      "Gradient Descent(995/999): loss=0.3666989984826984\n",
      "Gradient Descent(996/999): loss=0.36669519660704153\n",
      "Gradient Descent(997/999): loss=0.36669140072785333\n",
      "Gradient Descent(998/999): loss=0.36668761082904966\n",
      "Gradient Descent(999/999): loss=0.366683826894622\n"
     ]
    }
   ],
   "source": [
    "w, mse = least_squares_GD(np.float16(y_train), np.float16(tX_train), np.float16(initial_w), max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.721525"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict_labels(w, tX_train)\n",
    "np.mean(y_train.reshape(-1,1)==y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7235"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict_labels(w, tX_val)\n",
    "np.mean(y_val.reshape(-1,1)==y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Squares SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros(tX.shape[1])#np.random.normal(loc=0, scale=1, size=(tX.shape[1],1))\n",
    "max_iters = 1000\n",
    "gamma = 3e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/999): loss=0.5948267886066436\n",
      "Gradient Descent(1/999): loss=23.918287456359867\n",
      "Gradient Descent(2/999): loss=6.482131631984115\n",
      "Gradient Descent(3/999): loss=1.072019238193035\n",
      "Gradient Descent(4/999): loss=0.5620987787550688\n",
      "Gradient Descent(5/999): loss=0.7132101563155653\n",
      "Gradient Descent(6/999): loss=0.4416907335579395\n",
      "Gradient Descent(7/999): loss=0.9546526320773362\n",
      "Gradient Descent(8/999): loss=0.44025807523488997\n",
      "Gradient Descent(9/999): loss=0.622820172495842\n",
      "Gradient Descent(10/999): loss=0.7327715994256736\n",
      "Gradient Descent(11/999): loss=0.4805811141961813\n",
      "Gradient Descent(12/999): loss=2.547217366743088\n",
      "Gradient Descent(13/999): loss=1.6598004800122974\n",
      "Gradient Descent(14/999): loss=10.665810132827756\n",
      "Gradient Descent(15/999): loss=3.6932393459528683\n",
      "Gradient Descent(16/999): loss=3.2435152139967673\n",
      "Gradient Descent(17/999): loss=1.271850050452948\n",
      "Gradient Descent(18/999): loss=1.708876825043559\n",
      "Gradient Descent(19/999): loss=0.6463847397458552\n",
      "Gradient Descent(20/999): loss=1.2367377821934222\n",
      "Gradient Descent(21/999): loss=0.42434024762809275\n",
      "Gradient Descent(22/999): loss=1.1161592226743697\n",
      "Gradient Descent(23/999): loss=184.3957147753906\n",
      "Gradient Descent(24/999): loss=31.82333335377276\n",
      "Gradient Descent(25/999): loss=3.164414755451083\n",
      "Gradient Descent(26/999): loss=2.4412171898692847\n",
      "Gradient Descent(27/999): loss=2.8293220120221374\n",
      "Gradient Descent(28/999): loss=2.9860410023438932\n",
      "Gradient Descent(29/999): loss=2.6526376205867535\n",
      "Gradient Descent(30/999): loss=2.888489938376545\n",
      "Gradient Descent(31/999): loss=6.165935746034979\n",
      "Gradient Descent(32/999): loss=1.5643249367409946\n",
      "Gradient Descent(33/999): loss=1.7183963099855182\n",
      "Gradient Descent(34/999): loss=1.516492523713708\n",
      "Gradient Descent(35/999): loss=1.6013364644694328\n",
      "Gradient Descent(36/999): loss=2.299284913481474\n",
      "Gradient Descent(37/999): loss=1.6676106221699718\n",
      "Gradient Descent(38/999): loss=13.249627849121092\n",
      "Gradient Descent(39/999): loss=7.09118230966568\n",
      "Gradient Descent(40/999): loss=2.915862446448803\n",
      "Gradient Descent(41/999): loss=2.2293584312564136\n",
      "Gradient Descent(42/999): loss=121.36205763671875\n",
      "Gradient Descent(43/999): loss=18.660214225521088\n",
      "Gradient Descent(44/999): loss=2.3458805411148065\n",
      "Gradient Descent(45/999): loss=0.771377046790719\n",
      "Gradient Descent(46/999): loss=0.4529210606825352\n",
      "Gradient Descent(47/999): loss=3.252370638285875\n",
      "Gradient Descent(48/999): loss=1.7802431542450197\n",
      "Gradient Descent(49/999): loss=3.795635986250639\n",
      "Gradient Descent(50/999): loss=4.486953200089334\n",
      "Gradient Descent(51/999): loss=2.1022520168048144\n",
      "Gradient Descent(52/999): loss=2.837225831127763\n",
      "Gradient Descent(53/999): loss=3.474587364355326\n",
      "Gradient Descent(54/999): loss=3.3468088790625337\n",
      "Gradient Descent(55/999): loss=1.706018770904541\n",
      "Gradient Descent(56/999): loss=1.7595648038893938\n",
      "Gradient Descent(57/999): loss=3.815087528179288\n",
      "Gradient Descent(58/999): loss=2.643659572609067\n",
      "Gradient Descent(59/999): loss=2.097819794996381\n",
      "Gradient Descent(60/999): loss=2.9744807864475256\n",
      "Gradient Descent(61/999): loss=3.243415258507132\n",
      "Gradient Descent(62/999): loss=3.288731995747089\n",
      "Gradient Descent(63/999): loss=1.5131096402549746\n",
      "Gradient Descent(64/999): loss=2.1829212297481297\n",
      "Gradient Descent(65/999): loss=1.928840721630454\n",
      "Gradient Descent(66/999): loss=1.5180938405764102\n",
      "Gradient Descent(67/999): loss=1.2356054956662652\n",
      "Gradient Descent(68/999): loss=1.5349573898750544\n",
      "Gradient Descent(69/999): loss=1.996675549145341\n",
      "Gradient Descent(70/999): loss=2.1581586142385003\n",
      "Gradient Descent(71/999): loss=0.7022438422638179\n",
      "Gradient Descent(72/999): loss=3.887793109911681\n",
      "Gradient Descent(73/999): loss=0.5845116340124608\n",
      "Gradient Descent(74/999): loss=0.4512329136145115\n",
      "Gradient Descent(75/999): loss=0.7542078969573973\n",
      "Gradient Descent(76/999): loss=0.49512331839144225\n",
      "Gradient Descent(77/999): loss=0.589974533985853\n",
      "Gradient Descent(78/999): loss=0.8071319148218633\n",
      "Gradient Descent(79/999): loss=1.202079316686988\n",
      "Gradient Descent(80/999): loss=0.9078434538972378\n",
      "Gradient Descent(81/999): loss=0.873360042794943\n",
      "Gradient Descent(82/999): loss=0.8115621174836158\n",
      "Gradient Descent(83/999): loss=0.8223653665590287\n",
      "Gradient Descent(84/999): loss=0.8056009457814696\n",
      "Gradient Descent(85/999): loss=0.5643522934317587\n",
      "Gradient Descent(86/999): loss=1.0907311501455308\n",
      "Gradient Descent(87/999): loss=0.6044623557358981\n",
      "Gradient Descent(88/999): loss=1.141223600887656\n",
      "Gradient Descent(89/999): loss=0.5047634692251682\n",
      "Gradient Descent(90/999): loss=0.47007657613575454\n",
      "Gradient Descent(91/999): loss=0.6133414637690782\n",
      "Gradient Descent(92/999): loss=0.9982322121632099\n",
      "Gradient Descent(93/999): loss=0.8573483354473115\n",
      "Gradient Descent(94/999): loss=0.8116613938969373\n",
      "Gradient Descent(95/999): loss=0.7698452027475836\n",
      "Gradient Descent(96/999): loss=0.489802333278656\n",
      "Gradient Descent(97/999): loss=0.553918047747016\n",
      "Gradient Descent(98/999): loss=0.8762830804049969\n",
      "Gradient Descent(99/999): loss=12.45166554766655\n",
      "Gradient Descent(100/999): loss=0.7214196095848083\n",
      "Gradient Descent(101/999): loss=4.789337584173083\n",
      "Gradient Descent(102/999): loss=0.6148359396034478\n",
      "Gradient Descent(103/999): loss=1.533062437953949\n",
      "Gradient Descent(104/999): loss=1.5735596308690312\n",
      "Gradient Descent(105/999): loss=0.47481781302392484\n",
      "Gradient Descent(106/999): loss=1.2822556394439937\n",
      "Gradient Descent(107/999): loss=0.4088048126518727\n",
      "Gradient Descent(108/999): loss=0.42541473560273646\n",
      "Gradient Descent(109/999): loss=0.4492028641003371\n",
      "Gradient Descent(110/999): loss=0.42787945596754545\n",
      "Gradient Descent(111/999): loss=0.5291216867542267\n",
      "Gradient Descent(112/999): loss=0.557024565270543\n",
      "Gradient Descent(113/999): loss=0.6918877783203123\n",
      "Gradient Descent(114/999): loss=1.9509159867995978\n",
      "Gradient Descent(115/999): loss=0.6177634188473226\n",
      "Gradient Descent(116/999): loss=0.7464859860605001\n",
      "Gradient Descent(117/999): loss=1.3711357672828435\n",
      "Gradient Descent(118/999): loss=1.6739003278756144\n",
      "Gradient Descent(119/999): loss=0.5855377742171289\n",
      "Gradient Descent(120/999): loss=8.01190567936182\n",
      "Gradient Descent(121/999): loss=3.231591836464405\n",
      "Gradient Descent(122/999): loss=1.7034207011097666\n",
      "Gradient Descent(123/999): loss=1.9535428884464503\n",
      "Gradient Descent(124/999): loss=2.107569318289757\n",
      "Gradient Descent(125/999): loss=1.0898975104063748\n",
      "Gradient Descent(126/999): loss=0.8456985674214363\n",
      "Gradient Descent(127/999): loss=0.8972853559613225\n",
      "Gradient Descent(128/999): loss=0.6769819574791194\n",
      "Gradient Descent(129/999): loss=0.7294893660622835\n",
      "Gradient Descent(130/999): loss=0.8589250000220534\n",
      "Gradient Descent(131/999): loss=1.3738646060734987\n",
      "Gradient Descent(132/999): loss=0.6746952553993463\n",
      "Gradient Descent(133/999): loss=0.6352889138174058\n",
      "Gradient Descent(134/999): loss=0.48296508188903337\n",
      "Gradient Descent(135/999): loss=0.6929440549230577\n",
      "Gradient Descent(136/999): loss=0.4736685826486348\n",
      "Gradient Descent(137/999): loss=0.844021286343336\n",
      "Gradient Descent(138/999): loss=0.6310285888892412\n",
      "Gradient Descent(139/999): loss=0.5864988821464777\n",
      "Gradient Descent(140/999): loss=0.7533828565847873\n",
      "Gradient Descent(141/999): loss=0.5179553950101137\n",
      "Gradient Descent(142/999): loss=0.7283074197268484\n",
      "Gradient Descent(143/999): loss=0.5960767476606369\n",
      "Gradient Descent(144/999): loss=0.620156301241517\n",
      "Gradient Descent(145/999): loss=0.9212231431651114\n",
      "Gradient Descent(146/999): loss=0.6616304323071241\n",
      "Gradient Descent(147/999): loss=0.7580105676007272\n",
      "Gradient Descent(148/999): loss=0.7024281691074371\n",
      "Gradient Descent(149/999): loss=0.671419889937043\n",
      "Gradient Descent(150/999): loss=1.0838183148556946\n",
      "Gradient Descent(151/999): loss=1.329448260730505\n",
      "Gradient Descent(152/999): loss=0.9172620912623406\n",
      "Gradient Descent(153/999): loss=1.2071727345484495\n",
      "Gradient Descent(154/999): loss=0.6952452554482221\n",
      "Gradient Descent(155/999): loss=0.7580899781280757\n",
      "Gradient Descent(156/999): loss=5.064171897354127\n",
      "Gradient Descent(157/999): loss=0.4444335049003363\n",
      "Gradient Descent(158/999): loss=1.02260311157763\n",
      "Gradient Descent(159/999): loss=0.495227561572194\n",
      "Gradient Descent(160/999): loss=0.6663948785215617\n",
      "Gradient Descent(161/999): loss=0.5465319810706375\n",
      "Gradient Descent(162/999): loss=0.8717797262251374\n",
      "Gradient Descent(163/999): loss=0.740575754547119\n",
      "Gradient Descent(164/999): loss=0.5869343596315383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(165/999): loss=0.4267675859737396\n",
      "Gradient Descent(166/999): loss=0.5267206713360548\n",
      "Gradient Descent(167/999): loss=0.5204919648849964\n",
      "Gradient Descent(168/999): loss=0.6383018074393271\n",
      "Gradient Descent(169/999): loss=0.5979811944115162\n",
      "Gradient Descent(170/999): loss=0.5381523439627885\n",
      "Gradient Descent(171/999): loss=0.4349727834844589\n",
      "Gradient Descent(172/999): loss=0.5552750364941359\n",
      "Gradient Descent(173/999): loss=0.47861381176471707\n",
      "Gradient Descent(174/999): loss=0.5411288452345133\n",
      "Gradient Descent(175/999): loss=0.8320371055948733\n",
      "Gradient Descent(176/999): loss=0.750480238226056\n",
      "Gradient Descent(177/999): loss=0.7821323653179407\n",
      "Gradient Descent(178/999): loss=0.4649884309452772\n",
      "Gradient Descent(179/999): loss=0.7176039593595267\n",
      "Gradient Descent(180/999): loss=0.9474488932836055\n",
      "Gradient Descent(181/999): loss=0.6946846099376681\n",
      "Gradient Descent(182/999): loss=6.129021634216307\n",
      "Gradient Descent(183/999): loss=3.102127190757394\n",
      "Gradient Descent(184/999): loss=1.8381360068053005\n",
      "Gradient Descent(185/999): loss=1.976288860131502\n",
      "Gradient Descent(186/999): loss=15.509728264770507\n",
      "Gradient Descent(187/999): loss=2.031507277495861\n",
      "Gradient Descent(188/999): loss=4.051046255601644\n",
      "Gradient Descent(189/999): loss=1.469388259213567\n",
      "Gradient Descent(190/999): loss=1.374115315206647\n",
      "Gradient Descent(191/999): loss=2.3017641145849232\n",
      "Gradient Descent(192/999): loss=1.7986255727660656\n",
      "Gradient Descent(193/999): loss=4.54238896713078\n",
      "Gradient Descent(194/999): loss=1.9098158862262966\n",
      "Gradient Descent(195/999): loss=2.4721096463567016\n",
      "Gradient Descent(196/999): loss=1.2717895453387498\n",
      "Gradient Descent(197/999): loss=1.0824056940335036\n",
      "Gradient Descent(198/999): loss=1.513011345975399\n",
      "Gradient Descent(199/999): loss=1.4040135183984044\n",
      "Gradient Descent(200/999): loss=1.000429938004017\n",
      "Gradient Descent(201/999): loss=1.0462276299059388\n",
      "Gradient Descent(202/999): loss=21.4565961328125\n",
      "Gradient Descent(203/999): loss=2.188613300010562\n",
      "Gradient Descent(204/999): loss=1.6363960861790177\n",
      "Gradient Descent(205/999): loss=1.4337119680655006\n",
      "Gradient Descent(206/999): loss=0.7622251282554865\n",
      "Gradient Descent(207/999): loss=0.6372634759932757\n",
      "Gradient Descent(208/999): loss=0.4306749987506866\n",
      "Gradient Descent(209/999): loss=0.5461945031410453\n",
      "Gradient Descent(210/999): loss=0.5129478970450163\n",
      "Gradient Descent(211/999): loss=18.30834492645264\n",
      "Gradient Descent(212/999): loss=5.660142707784175\n",
      "Gradient Descent(213/999): loss=1.0467675150758027\n",
      "Gradient Descent(214/999): loss=0.5836894758927822\n",
      "Gradient Descent(215/999): loss=0.7529661605280636\n",
      "Gradient Descent(216/999): loss=0.46811326527357106\n",
      "Gradient Descent(217/999): loss=0.721176855252385\n",
      "Gradient Descent(218/999): loss=1.9111609103041884\n",
      "Gradient Descent(219/999): loss=1.6329656253796816\n",
      "Gradient Descent(220/999): loss=0.420476878964305\n",
      "Gradient Descent(221/999): loss=1.0281120748651027\n",
      "Gradient Descent(222/999): loss=1.0125919831293821\n",
      "Gradient Descent(223/999): loss=0.7268801141917705\n",
      "Gradient Descent(224/999): loss=0.42503972749471675\n",
      "Gradient Descent(225/999): loss=0.5044508868122101\n",
      "Gradient Descent(226/999): loss=0.5413461237621308\n",
      "Gradient Descent(227/999): loss=0.8388090652483702\n",
      "Gradient Descent(228/999): loss=0.4347052149277926\n",
      "Gradient Descent(229/999): loss=0.7265159113615752\n",
      "Gradient Descent(230/999): loss=0.4207044916009902\n",
      "Gradient Descent(231/999): loss=0.6583179833787678\n",
      "Gradient Descent(232/999): loss=0.46503301250874995\n",
      "Gradient Descent(233/999): loss=0.5495997913706303\n",
      "Gradient Descent(234/999): loss=1.0318352860021593\n",
      "Gradient Descent(235/999): loss=0.5005178315973282\n",
      "Gradient Descent(236/999): loss=0.8666708045500519\n",
      "Gradient Descent(237/999): loss=0.5806343840301036\n",
      "Gradient Descent(238/999): loss=0.8879709254854918\n",
      "Gradient Descent(239/999): loss=1.145175343568325\n",
      "Gradient Descent(240/999): loss=0.6001871496617793\n",
      "Gradient Descent(241/999): loss=1.7806799126833677\n",
      "Gradient Descent(242/999): loss=1.6777534128421547\n",
      "Gradient Descent(243/999): loss=0.7829419218176604\n",
      "Gradient Descent(244/999): loss=1.0371282637584212\n",
      "Gradient Descent(245/999): loss=0.4655298465925456\n",
      "Gradient Descent(246/999): loss=0.4334349701124429\n",
      "Gradient Descent(247/999): loss=11.515065853899122\n",
      "Gradient Descent(248/999): loss=8.257116809942127\n",
      "Gradient Descent(249/999): loss=1.3614043289387225\n",
      "Gradient Descent(250/999): loss=1.3749085396581893\n",
      "Gradient Descent(251/999): loss=1.3881267617148159\n",
      "Gradient Descent(252/999): loss=1.0582396620762349\n",
      "Gradient Descent(253/999): loss=0.9684113143932819\n",
      "Gradient Descent(254/999): loss=1.0568586443066597\n",
      "Gradient Descent(255/999): loss=1.8481535610646007\n",
      "Gradient Descent(256/999): loss=2.2787242316770557\n",
      "Gradient Descent(257/999): loss=0.8774162696850301\n",
      "Gradient Descent(258/999): loss=0.8999701147323849\n",
      "Gradient Descent(259/999): loss=13.719408148828743\n",
      "Gradient Descent(260/999): loss=7.246892614244223\n",
      "Gradient Descent(261/999): loss=30.912381068725587\n",
      "Gradient Descent(262/999): loss=10.188694052070378\n",
      "Gradient Descent(263/999): loss=0.5956561744534971\n",
      "Gradient Descent(264/999): loss=3.1269539418429133\n",
      "Gradient Descent(265/999): loss=4.745249493489862\n",
      "Gradient Descent(266/999): loss=4.393261583999991\n",
      "Gradient Descent(267/999): loss=10.444405328238604\n",
      "Gradient Descent(268/999): loss=1.9632568767797944\n",
      "Gradient Descent(269/999): loss=1.3271399692732095\n",
      "Gradient Descent(270/999): loss=1.5642416153275966\n",
      "Gradient Descent(271/999): loss=1.5462393927508589\n",
      "Gradient Descent(272/999): loss=1.9923851867282387\n",
      "Gradient Descent(273/999): loss=2.180356021392941\n",
      "Gradient Descent(274/999): loss=1.7024140249890087\n",
      "Gradient Descent(275/999): loss=0.9042389571398495\n",
      "Gradient Descent(276/999): loss=1.4057973828071357\n",
      "Gradient Descent(277/999): loss=0.7021417471975088\n",
      "Gradient Descent(278/999): loss=0.699075356515646\n",
      "Gradient Descent(279/999): loss=0.9284632751554249\n",
      "Gradient Descent(280/999): loss=0.6975051110255719\n",
      "Gradient Descent(281/999): loss=0.6711802022385596\n",
      "Gradient Descent(282/999): loss=0.6402018795919419\n",
      "Gradient Descent(283/999): loss=0.967110629799962\n",
      "Gradient Descent(284/999): loss=0.7037942439860103\n",
      "Gradient Descent(285/999): loss=2.0530567825078965\n",
      "Gradient Descent(286/999): loss=2.8168204530930514\n",
      "Gradient Descent(287/999): loss=0.7737242036354541\n",
      "Gradient Descent(288/999): loss=1.7603943551820518\n",
      "Gradient Descent(289/999): loss=2.119286992599368\n",
      "Gradient Descent(290/999): loss=0.5855566476285456\n",
      "Gradient Descent(291/999): loss=0.607367984725833\n",
      "Gradient Descent(292/999): loss=1.1135663693225384\n",
      "Gradient Descent(293/999): loss=0.5515339032137394\n",
      "Gradient Descent(294/999): loss=0.5437133025020361\n",
      "Gradient Descent(295/999): loss=0.579675438580513\n",
      "Gradient Descent(296/999): loss=0.5553831875365972\n",
      "Gradient Descent(297/999): loss=0.4890258716148137\n",
      "Gradient Descent(298/999): loss=0.5620008901041745\n",
      "Gradient Descent(299/999): loss=0.5061154232567548\n",
      "Gradient Descent(300/999): loss=0.7956947509557007\n",
      "Gradient Descent(301/999): loss=1.3136818191587922\n",
      "Gradient Descent(302/999): loss=3.055102318205237\n",
      "Gradient Descent(303/999): loss=2.1127933611285687\n",
      "Gradient Descent(304/999): loss=1.5032286608040337\n",
      "Gradient Descent(305/999): loss=0.7999798210382462\n",
      "Gradient Descent(306/999): loss=0.5571545065760612\n",
      "Gradient Descent(307/999): loss=1.3656441131436825\n",
      "Gradient Descent(308/999): loss=1.4706347484242916\n",
      "Gradient Descent(309/999): loss=2.349026853997707\n",
      "Gradient Descent(310/999): loss=0.5852220569741725\n",
      "Gradient Descent(311/999): loss=0.6575725805705784\n",
      "Gradient Descent(312/999): loss=0.6166140264970065\n",
      "Gradient Descent(313/999): loss=1.2800144382590055\n",
      "Gradient Descent(314/999): loss=5.645663703234195\n",
      "Gradient Descent(315/999): loss=1.3702737733221055\n",
      "Gradient Descent(316/999): loss=1.3080278147423265\n",
      "Gradient Descent(317/999): loss=1.5147850661122797\n",
      "Gradient Descent(318/999): loss=1.2573379265838864\n",
      "Gradient Descent(319/999): loss=1.7600821031552554\n",
      "Gradient Descent(320/999): loss=1.0535677419382334\n",
      "Gradient Descent(321/999): loss=1.4703104025065898\n",
      "Gradient Descent(322/999): loss=1.01181519474566\n",
      "Gradient Descent(323/999): loss=1.1548249104368686\n",
      "Gradient Descent(324/999): loss=1.0506160606217385\n",
      "Gradient Descent(325/999): loss=1.2109273043388127\n",
      "Gradient Descent(326/999): loss=0.8543007655709982\n",
      "Gradient Descent(327/999): loss=1.3823414478224518\n",
      "Gradient Descent(328/999): loss=1.5324267179209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(329/999): loss=1.426836585037708\n",
      "Gradient Descent(330/999): loss=1.2919340658706429\n",
      "Gradient Descent(331/999): loss=0.7967910266155005\n",
      "Gradient Descent(332/999): loss=0.9917094595992567\n",
      "Gradient Descent(333/999): loss=1.0230497395956517\n",
      "Gradient Descent(334/999): loss=1.1999708811807634\n",
      "Gradient Descent(335/999): loss=1.5353533273774385\n",
      "Gradient Descent(336/999): loss=0.7344069871914387\n",
      "Gradient Descent(337/999): loss=1.058842031903267\n",
      "Gradient Descent(338/999): loss=0.49292584601759915\n",
      "Gradient Descent(339/999): loss=1.0560212452411648\n",
      "Gradient Descent(340/999): loss=1.3264357104158404\n",
      "Gradient Descent(341/999): loss=0.42647337322771556\n",
      "Gradient Descent(342/999): loss=0.5620027696287633\n",
      "Gradient Descent(343/999): loss=0.4250540415519476\n",
      "Gradient Descent(344/999): loss=0.44547410776257523\n",
      "Gradient Descent(345/999): loss=0.9277320336997509\n",
      "Gradient Descent(346/999): loss=0.5028871032416821\n",
      "Gradient Descent(347/999): loss=0.537125194787979\n",
      "Gradient Descent(348/999): loss=0.5711607508975268\n",
      "Gradient Descent(349/999): loss=1.08324905574739\n",
      "Gradient Descent(350/999): loss=0.6143180669188498\n",
      "Gradient Descent(351/999): loss=0.4508515240466595\n",
      "Gradient Descent(352/999): loss=0.501249960334301\n",
      "Gradient Descent(353/999): loss=2.0860446073269845\n",
      "Gradient Descent(354/999): loss=1.8856027745270731\n",
      "Gradient Descent(355/999): loss=1.206628308853507\n",
      "Gradient Descent(356/999): loss=3.4367895603686573\n",
      "Gradient Descent(357/999): loss=0.5340794686102867\n",
      "Gradient Descent(358/999): loss=0.5251568613260985\n",
      "Gradient Descent(359/999): loss=0.5488464235037565\n",
      "Gradient Descent(360/999): loss=0.5745952685385941\n",
      "Gradient Descent(361/999): loss=0.6981376785504817\n",
      "Gradient Descent(362/999): loss=0.4284701591324806\n",
      "Gradient Descent(363/999): loss=0.4654865170603991\n",
      "Gradient Descent(364/999): loss=0.6155159893083573\n",
      "Gradient Descent(365/999): loss=2.292139935035109\n",
      "Gradient Descent(366/999): loss=0.4115083373105527\n",
      "Gradient Descent(367/999): loss=0.4042986336821317\n",
      "Gradient Descent(368/999): loss=0.4095449329131842\n",
      "Gradient Descent(369/999): loss=0.7963021298009157\n",
      "Gradient Descent(370/999): loss=0.7554682359480859\n",
      "Gradient Descent(371/999): loss=0.4490236350417137\n",
      "Gradient Descent(372/999): loss=0.5897806939208508\n",
      "Gradient Descent(373/999): loss=1.0936182074761391\n",
      "Gradient Descent(374/999): loss=0.39931972812950606\n",
      "Gradient Descent(375/999): loss=0.9868810957813264\n",
      "Gradient Descent(376/999): loss=0.5933120807790758\n",
      "Gradient Descent(377/999): loss=1.0030202014267444\n",
      "Gradient Descent(378/999): loss=0.7256891727668047\n",
      "Gradient Descent(379/999): loss=0.7650869414615633\n",
      "Gradient Descent(380/999): loss=0.8612804141581059\n",
      "Gradient Descent(381/999): loss=0.5582151601034403\n",
      "Gradient Descent(382/999): loss=0.4848290514177085\n",
      "Gradient Descent(383/999): loss=0.6432590046894553\n",
      "Gradient Descent(384/999): loss=0.4550829810017348\n",
      "Gradient Descent(385/999): loss=0.5871889488732815\n",
      "Gradient Descent(386/999): loss=0.41353952754557133\n",
      "Gradient Descent(387/999): loss=0.4856759871995449\n",
      "Gradient Descent(388/999): loss=0.6705368078106642\n",
      "Gradient Descent(389/999): loss=1.3633330058664084\n",
      "Gradient Descent(390/999): loss=0.48765627648293974\n",
      "Gradient Descent(391/999): loss=0.8941688448268175\n",
      "Gradient Descent(392/999): loss=0.532592779239416\n",
      "Gradient Descent(393/999): loss=0.6236927591270208\n",
      "Gradient Descent(394/999): loss=0.45940137403547765\n",
      "Gradient Descent(395/999): loss=0.44263954084515567\n",
      "Gradient Descent(396/999): loss=0.5988815289705993\n",
      "Gradient Descent(397/999): loss=2.674774763438702\n",
      "Gradient Descent(398/999): loss=0.6858381133151055\n",
      "Gradient Descent(399/999): loss=2.499391913337111\n",
      "Gradient Descent(400/999): loss=0.7095465447813273\n",
      "Gradient Descent(401/999): loss=2.021721639022827\n",
      "Gradient Descent(402/999): loss=0.6572618520855902\n",
      "Gradient Descent(403/999): loss=0.8880725460940597\n",
      "Gradient Descent(404/999): loss=0.9125853276550769\n",
      "Gradient Descent(405/999): loss=0.7088340948098898\n",
      "Gradient Descent(406/999): loss=1.1499650048214198\n",
      "Gradient Descent(407/999): loss=0.6313321523976325\n",
      "Gradient Descent(408/999): loss=0.7082091986465453\n",
      "Gradient Descent(409/999): loss=1.2259487422782183\n",
      "Gradient Descent(410/999): loss=0.7055209260261059\n",
      "Gradient Descent(411/999): loss=1.0335226485723257\n",
      "Gradient Descent(412/999): loss=0.9351638178598883\n",
      "Gradient Descent(413/999): loss=0.9406382500565051\n",
      "Gradient Descent(414/999): loss=1.1599899030792713\n",
      "Gradient Descent(415/999): loss=0.38435419001340876\n",
      "Gradient Descent(416/999): loss=0.5452188557702303\n",
      "Gradient Descent(417/999): loss=0.39008878113448614\n",
      "Gradient Descent(418/999): loss=0.5064270050424338\n",
      "Gradient Descent(419/999): loss=0.42128178202569483\n",
      "Gradient Descent(420/999): loss=0.3967862267911434\n",
      "Gradient Descent(421/999): loss=0.8855716406035423\n",
      "Gradient Descent(422/999): loss=1.9620679775613543\n",
      "Gradient Descent(423/999): loss=31.057115123351217\n",
      "Gradient Descent(424/999): loss=17.24032920782029\n",
      "Gradient Descent(425/999): loss=7.673392556583285\n",
      "Gradient Descent(426/999): loss=85.93743333007811\n",
      "Gradient Descent(427/999): loss=7.876619918070436\n",
      "Gradient Descent(428/999): loss=3.965879105265141\n",
      "Gradient Descent(429/999): loss=6.282053991779685\n",
      "Gradient Descent(430/999): loss=2.601353615542054\n",
      "Gradient Descent(431/999): loss=1.773704937278032\n",
      "Gradient Descent(432/999): loss=2.672287471894622\n",
      "Gradient Descent(433/999): loss=3.063528562906981\n",
      "Gradient Descent(434/999): loss=2.117163903465867\n",
      "Gradient Descent(435/999): loss=1.8286666000699996\n",
      "Gradient Descent(436/999): loss=1.9207824136358502\n",
      "Gradient Descent(437/999): loss=2.8839591952443118\n",
      "Gradient Descent(438/999): loss=5.937454021458032\n",
      "Gradient Descent(439/999): loss=5.731341668255329\n",
      "Gradient Descent(440/999): loss=1.9874143152832986\n",
      "Gradient Descent(441/999): loss=1.1224672843980787\n",
      "Gradient Descent(442/999): loss=1.0914077387309074\n",
      "Gradient Descent(443/999): loss=0.9431384221452473\n",
      "Gradient Descent(444/999): loss=1.2553557236272095\n",
      "Gradient Descent(445/999): loss=1.3929242880928516\n",
      "Gradient Descent(446/999): loss=1.843698510944247\n",
      "Gradient Descent(447/999): loss=2.755128032158017\n",
      "Gradient Descent(448/999): loss=1.667256450516581\n",
      "Gradient Descent(449/999): loss=1.3828243138480187\n",
      "Gradient Descent(450/999): loss=1.4260771294844152\n",
      "Gradient Descent(451/999): loss=1.3463119288545846\n",
      "Gradient Descent(452/999): loss=1.5504561641633512\n",
      "Gradient Descent(453/999): loss=2.5308781811821466\n",
      "Gradient Descent(454/999): loss=1.487160808172226\n",
      "Gradient Descent(455/999): loss=2.0653697056788207\n",
      "Gradient Descent(456/999): loss=1.5390145106357338\n",
      "Gradient Descent(457/999): loss=1.1980429382610323\n",
      "Gradient Descent(458/999): loss=9.867624266333578\n",
      "Gradient Descent(459/999): loss=3.2068414411056043\n",
      "Gradient Descent(460/999): loss=0.8076655079501869\n",
      "Gradient Descent(461/999): loss=0.7546843074899913\n",
      "Gradient Descent(462/999): loss=0.7557608594870568\n",
      "Gradient Descent(463/999): loss=0.7667241580748557\n",
      "Gradient Descent(464/999): loss=1.1490096331501005\n",
      "Gradient Descent(465/999): loss=0.8039720935857296\n",
      "Gradient Descent(466/999): loss=0.6933991994112729\n",
      "Gradient Descent(467/999): loss=0.7943204849505423\n",
      "Gradient Descent(468/999): loss=4.496235953282714\n",
      "Gradient Descent(469/999): loss=2.3469097067886597\n",
      "Gradient Descent(470/999): loss=1.3979985408556463\n",
      "Gradient Descent(471/999): loss=0.5381440337496995\n",
      "Gradient Descent(472/999): loss=0.8105203992033003\n",
      "Gradient Descent(473/999): loss=0.9232886633747817\n",
      "Gradient Descent(474/999): loss=1.8952296923124794\n",
      "Gradient Descent(475/999): loss=0.895661495346427\n",
      "Gradient Descent(476/999): loss=0.5282015990436079\n",
      "Gradient Descent(477/999): loss=0.6678414791095258\n",
      "Gradient Descent(478/999): loss=0.7249870655804871\n",
      "Gradient Descent(479/999): loss=1.5958912312567235\n",
      "Gradient Descent(480/999): loss=0.6883683771067857\n",
      "Gradient Descent(481/999): loss=14.398102437744141\n",
      "Gradient Descent(482/999): loss=7.656381957397461\n",
      "Gradient Descent(483/999): loss=0.4147622238200903\n",
      "Gradient Descent(484/999): loss=1.8774575944876672\n",
      "Gradient Descent(485/999): loss=0.8615942438769342\n",
      "Gradient Descent(486/999): loss=0.9988714418798685\n",
      "Gradient Descent(487/999): loss=1.6044847182559967\n",
      "Gradient Descent(488/999): loss=0.45281563812494274\n",
      "Gradient Descent(489/999): loss=0.974225388519168\n",
      "Gradient Descent(490/999): loss=0.7407134081470969\n",
      "Gradient Descent(491/999): loss=0.6114576774203777\n",
      "Gradient Descent(492/999): loss=0.5254614397644997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(493/999): loss=0.5861444668972493\n",
      "Gradient Descent(494/999): loss=0.8633255346494911\n",
      "Gradient Descent(495/999): loss=0.434107495753765\n",
      "Gradient Descent(496/999): loss=0.5409223221689464\n",
      "Gradient Descent(497/999): loss=0.7926104688179493\n",
      "Gradient Descent(498/999): loss=0.5347611611008644\n",
      "Gradient Descent(499/999): loss=0.5471947792059183\n",
      "Gradient Descent(500/999): loss=0.660281293503046\n",
      "Gradient Descent(501/999): loss=0.4309758179187774\n",
      "Gradient Descent(502/999): loss=0.42509896345257764\n",
      "Gradient Descent(503/999): loss=0.5218977323716879\n",
      "Gradient Descent(504/999): loss=0.5320056591552497\n",
      "Gradient Descent(505/999): loss=0.5709220310157537\n",
      "Gradient Descent(506/999): loss=0.45990055133521573\n",
      "Gradient Descent(507/999): loss=0.49505604653179647\n",
      "Gradient Descent(508/999): loss=0.42251314749717706\n",
      "Gradient Descent(509/999): loss=0.47457670105814925\n",
      "Gradient Descent(510/999): loss=0.5024171053534747\n",
      "Gradient Descent(511/999): loss=0.6135193176233766\n",
      "Gradient Descent(512/999): loss=0.42723775861024854\n",
      "Gradient Descent(513/999): loss=0.4600195232141018\n",
      "Gradient Descent(514/999): loss=0.7472151414048671\n",
      "Gradient Descent(515/999): loss=0.4246742620605231\n",
      "Gradient Descent(516/999): loss=1.079274639930129\n",
      "Gradient Descent(517/999): loss=1.0804304729789498\n",
      "Gradient Descent(518/999): loss=0.9996025677150489\n",
      "Gradient Descent(519/999): loss=1.1580744041132924\n",
      "Gradient Descent(520/999): loss=0.6570383797973395\n",
      "Gradient Descent(521/999): loss=0.7416568621778488\n",
      "Gradient Descent(522/999): loss=0.684672736415267\n",
      "Gradient Descent(523/999): loss=0.7376941545122861\n",
      "Gradient Descent(524/999): loss=0.5042210601127147\n",
      "Gradient Descent(525/999): loss=0.6023855044060947\n",
      "Gradient Descent(526/999): loss=0.6087479859101772\n",
      "Gradient Descent(527/999): loss=0.7165892515438795\n",
      "Gradient Descent(528/999): loss=15.743161903071403\n",
      "Gradient Descent(529/999): loss=1.4157480057013037\n",
      "Gradient Descent(530/999): loss=0.8244530612331629\n",
      "Gradient Descent(531/999): loss=0.5448139937394859\n",
      "Gradient Descent(532/999): loss=0.9380636993557214\n",
      "Gradient Descent(533/999): loss=1.4458660073816776\n",
      "Gradient Descent(534/999): loss=0.8509558115571738\n",
      "Gradient Descent(535/999): loss=0.6687692246222494\n",
      "Gradient Descent(536/999): loss=1.029587177132964\n",
      "Gradient Descent(537/999): loss=0.4050536014330387\n",
      "Gradient Descent(538/999): loss=0.48284413667142395\n",
      "Gradient Descent(539/999): loss=0.7268524847012757\n",
      "Gradient Descent(540/999): loss=0.9223515754920244\n",
      "Gradient Descent(541/999): loss=0.43025506497383126\n",
      "Gradient Descent(542/999): loss=0.7085988917571305\n",
      "Gradient Descent(543/999): loss=0.7143472579878568\n",
      "Gradient Descent(544/999): loss=0.8756880085152386\n",
      "Gradient Descent(545/999): loss=1.024863902735114\n",
      "Gradient Descent(546/999): loss=2.2099016701364524\n",
      "Gradient Descent(547/999): loss=0.546252801770568\n",
      "Gradient Descent(548/999): loss=0.8081670431816578\n",
      "Gradient Descent(549/999): loss=346.31718687500006\n",
      "Gradient Descent(550/999): loss=4.011499684939384\n",
      "Gradient Descent(551/999): loss=2.3108784791576857\n",
      "Gradient Descent(552/999): loss=2.5891161114007226\n",
      "Gradient Descent(553/999): loss=3.0803183951741455\n",
      "Gradient Descent(554/999): loss=5.335774684954286\n",
      "Gradient Descent(555/999): loss=1.8468133263099196\n",
      "Gradient Descent(556/999): loss=1.8826200757348537\n",
      "Gradient Descent(557/999): loss=1.9407662752139572\n",
      "Gradient Descent(558/999): loss=2.881312774564624\n",
      "Gradient Descent(559/999): loss=2.0550980195510387\n",
      "Gradient Descent(560/999): loss=1.8126165260142084\n",
      "Gradient Descent(561/999): loss=1.9861124613350627\n",
      "Gradient Descent(562/999): loss=1.505951434241533\n",
      "Gradient Descent(563/999): loss=3.4518055882763865\n",
      "Gradient Descent(564/999): loss=1.3082532323366403\n",
      "Gradient Descent(565/999): loss=1.4139356496840716\n",
      "Gradient Descent(566/999): loss=1.2805881290125847\n",
      "Gradient Descent(567/999): loss=1.5050906795305015\n",
      "Gradient Descent(568/999): loss=1.4234289411729575\n",
      "Gradient Descent(569/999): loss=1.3771146493327615\n",
      "Gradient Descent(570/999): loss=1.2767058795237543\n",
      "Gradient Descent(571/999): loss=1.3125440556794403\n",
      "Gradient Descent(572/999): loss=2.790208542314768\n",
      "Gradient Descent(573/999): loss=96.62582984375\n",
      "Gradient Descent(574/999): loss=19.5728032769376\n",
      "Gradient Descent(575/999): loss=12.136502211704252\n",
      "Gradient Descent(576/999): loss=7.258080426656009\n",
      "Gradient Descent(577/999): loss=7.2332269769817605\n",
      "Gradient Descent(578/999): loss=4.866187637167574\n",
      "Gradient Descent(579/999): loss=3.880192097950578\n",
      "Gradient Descent(580/999): loss=6.848302112169861\n",
      "Gradient Descent(581/999): loss=8.750092587994338\n",
      "Gradient Descent(582/999): loss=3.8699034517431268\n",
      "Gradient Descent(583/999): loss=3.415319944705962\n",
      "Gradient Descent(584/999): loss=3.70107561077416\n",
      "Gradient Descent(585/999): loss=51.28670220703126\n",
      "Gradient Descent(586/999): loss=20.254862017822266\n",
      "Gradient Descent(587/999): loss=3.491306734335423\n",
      "Gradient Descent(588/999): loss=2.0637552171832323\n",
      "Gradient Descent(589/999): loss=2.129717165066004\n",
      "Gradient Descent(590/999): loss=0.7354089151763917\n",
      "Gradient Descent(591/999): loss=1.0073544987595082\n",
      "Gradient Descent(592/999): loss=0.9420228938806058\n",
      "Gradient Descent(593/999): loss=1.6285486653441192\n",
      "Gradient Descent(594/999): loss=1.3532269538635016\n",
      "Gradient Descent(595/999): loss=1.162078144196868\n",
      "Gradient Descent(596/999): loss=1.7328501598638297\n",
      "Gradient Descent(597/999): loss=1.495612051854134\n",
      "Gradient Descent(598/999): loss=1.4702429951012135\n",
      "Gradient Descent(599/999): loss=1.4081650048065184\n",
      "Gradient Descent(600/999): loss=0.5977443581187726\n",
      "Gradient Descent(601/999): loss=0.9131709163856507\n",
      "Gradient Descent(602/999): loss=1.3642613709485534\n",
      "Gradient Descent(603/999): loss=0.8507211921638249\n",
      "Gradient Descent(604/999): loss=0.4537979098922014\n",
      "Gradient Descent(605/999): loss=0.8509580532211065\n",
      "Gradient Descent(606/999): loss=0.8234768257784842\n",
      "Gradient Descent(607/999): loss=0.43591571053147316\n",
      "Gradient Descent(608/999): loss=0.6104686958920956\n",
      "Gradient Descent(609/999): loss=0.4604728976637125\n",
      "Gradient Descent(610/999): loss=0.866810793914795\n",
      "Gradient Descent(611/999): loss=1.0615051709961891\n",
      "Gradient Descent(612/999): loss=0.5130520474654436\n",
      "Gradient Descent(613/999): loss=0.4887702414017916\n",
      "Gradient Descent(614/999): loss=0.4509207632631063\n",
      "Gradient Descent(615/999): loss=0.5348304193544389\n",
      "Gradient Descent(616/999): loss=0.8947367539346219\n",
      "Gradient Descent(617/999): loss=0.7591256470459701\n",
      "Gradient Descent(618/999): loss=0.838993620298505\n",
      "Gradient Descent(619/999): loss=0.7047220963329077\n",
      "Gradient Descent(620/999): loss=1.0556559619373085\n",
      "Gradient Descent(621/999): loss=1.1095409637188909\n",
      "Gradient Descent(622/999): loss=2.466019198346734\n",
      "Gradient Descent(623/999): loss=1.9645395709085467\n",
      "Gradient Descent(624/999): loss=0.879373306503296\n",
      "Gradient Descent(625/999): loss=0.7902566149497032\n",
      "Gradient Descent(626/999): loss=0.9686960741728543\n",
      "Gradient Descent(627/999): loss=1.2928275532639029\n",
      "Gradient Descent(628/999): loss=0.7088827321475742\n",
      "Gradient Descent(629/999): loss=0.7568967169457673\n",
      "Gradient Descent(630/999): loss=0.6037009041202068\n",
      "Gradient Descent(631/999): loss=0.6216723635905981\n",
      "Gradient Descent(632/999): loss=0.6634328438156843\n",
      "Gradient Descent(633/999): loss=0.6180669495624302\n",
      "Gradient Descent(634/999): loss=0.8551330737841129\n",
      "Gradient Descent(635/999): loss=1.1951409304296972\n",
      "Gradient Descent(636/999): loss=0.550722751508355\n",
      "Gradient Descent(637/999): loss=1.1843894191509488\n",
      "Gradient Descent(638/999): loss=0.6070455233860017\n",
      "Gradient Descent(639/999): loss=0.579680378035307\n",
      "Gradient Descent(640/999): loss=0.6226004414576292\n",
      "Gradient Descent(641/999): loss=0.8837208895134927\n",
      "Gradient Descent(642/999): loss=0.46365882218003274\n",
      "Gradient Descent(643/999): loss=1.2005483152842522\n",
      "Gradient Descent(644/999): loss=1.1653210481071472\n",
      "Gradient Descent(645/999): loss=0.993061096023917\n",
      "Gradient Descent(646/999): loss=0.7181536774677039\n",
      "Gradient Descent(647/999): loss=0.4659161703509092\n",
      "Gradient Descent(648/999): loss=0.5056115894579887\n",
      "Gradient Descent(649/999): loss=1.2470166527986526\n",
      "Gradient Descent(650/999): loss=2.2990247540014983\n",
      "Gradient Descent(651/999): loss=0.47321457529067984\n",
      "Gradient Descent(652/999): loss=0.8585416648685933\n",
      "Gradient Descent(653/999): loss=0.7180529486691951\n",
      "Gradient Descent(654/999): loss=0.7474206078165769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(655/999): loss=0.47608476152181617\n",
      "Gradient Descent(656/999): loss=3.7205459779953958\n",
      "Gradient Descent(657/999): loss=2.748095379756093\n",
      "Gradient Descent(658/999): loss=1.9833845343691108\n",
      "Gradient Descent(659/999): loss=0.4927812957775594\n",
      "Gradient Descent(660/999): loss=0.5729756646358968\n",
      "Gradient Descent(661/999): loss=0.5507459906196593\n",
      "Gradient Descent(662/999): loss=0.8963071347934005\n",
      "Gradient Descent(663/999): loss=1.7739030647754668\n",
      "Gradient Descent(664/999): loss=1.3548076179856063\n",
      "Gradient Descent(665/999): loss=1.5690865710002182\n",
      "Gradient Descent(666/999): loss=0.414597266292572\n",
      "Gradient Descent(667/999): loss=0.4254462594604493\n",
      "Gradient Descent(668/999): loss=0.46745072169601914\n",
      "Gradient Descent(669/999): loss=0.4309669992125034\n",
      "Gradient Descent(670/999): loss=0.6021123644351958\n",
      "Gradient Descent(671/999): loss=0.6776998422741889\n",
      "Gradient Descent(672/999): loss=0.915358979369998\n",
      "Gradient Descent(673/999): loss=0.48318323985457423\n",
      "Gradient Descent(674/999): loss=0.814146001649499\n",
      "Gradient Descent(675/999): loss=0.8014724404990672\n",
      "Gradient Descent(676/999): loss=1.283668132610321\n",
      "Gradient Descent(677/999): loss=0.5224390510356426\n",
      "Gradient Descent(678/999): loss=1.2370631024456027\n",
      "Gradient Descent(679/999): loss=1.065467341507077\n",
      "Gradient Descent(680/999): loss=0.9670280838096142\n",
      "Gradient Descent(681/999): loss=0.44670994107186796\n",
      "Gradient Descent(682/999): loss=1.0549110409498215\n",
      "Gradient Descent(683/999): loss=0.5349416917276383\n",
      "Gradient Descent(684/999): loss=0.6390786357373\n",
      "Gradient Descent(685/999): loss=0.7042613583707812\n",
      "Gradient Descent(686/999): loss=0.6822294963377713\n",
      "Gradient Descent(687/999): loss=0.5240209328120947\n",
      "Gradient Descent(688/999): loss=0.647617228845954\n",
      "Gradient Descent(689/999): loss=0.6183613746660948\n",
      "Gradient Descent(690/999): loss=0.821183004914522\n",
      "Gradient Descent(691/999): loss=0.7610087974607944\n",
      "Gradient Descent(692/999): loss=0.5695566679751873\n",
      "Gradient Descent(693/999): loss=2.466082158549429\n",
      "Gradient Descent(694/999): loss=3.109517803727387\n",
      "Gradient Descent(695/999): loss=0.8494597797995805\n",
      "Gradient Descent(696/999): loss=0.9046976286512614\n",
      "Gradient Descent(697/999): loss=0.8712613881838323\n",
      "Gradient Descent(698/999): loss=1.163915042963624\n",
      "Gradient Descent(699/999): loss=1.1740969214493036\n",
      "Gradient Descent(700/999): loss=3.640992000467181\n",
      "Gradient Descent(701/999): loss=1.6252651513177159\n",
      "Gradient Descent(702/999): loss=1.0811357759100197\n",
      "Gradient Descent(703/999): loss=0.7839225278049707\n",
      "Gradient Descent(704/999): loss=1.190516443319917\n",
      "Gradient Descent(705/999): loss=0.7870357466274499\n",
      "Gradient Descent(706/999): loss=10.016606238250732\n",
      "Gradient Descent(707/999): loss=3.200853110153675\n",
      "Gradient Descent(708/999): loss=1.1078124374425413\n",
      "Gradient Descent(709/999): loss=0.40859286855399607\n",
      "Gradient Descent(710/999): loss=0.4881924028998613\n",
      "Gradient Descent(711/999): loss=0.5581196567243338\n",
      "Gradient Descent(712/999): loss=0.6798088246625661\n",
      "Gradient Descent(713/999): loss=0.6704497161954641\n",
      "Gradient Descent(714/999): loss=0.990404535051584\n",
      "Gradient Descent(715/999): loss=0.9062328727424144\n",
      "Gradient Descent(716/999): loss=0.7554408307594063\n",
      "Gradient Descent(717/999): loss=0.716309026351571\n",
      "Gradient Descent(718/999): loss=5.321542154011726\n",
      "Gradient Descent(719/999): loss=3.0122042248678205\n",
      "Gradient Descent(720/999): loss=1.7506308054524662\n",
      "Gradient Descent(721/999): loss=1.788430833595395\n",
      "Gradient Descent(722/999): loss=1.0775681157249213\n",
      "Gradient Descent(723/999): loss=0.526055960022211\n",
      "Gradient Descent(724/999): loss=0.6140535874205827\n",
      "Gradient Descent(725/999): loss=0.9878981822431085\n",
      "Gradient Descent(726/999): loss=1.2003226754182577\n",
      "Gradient Descent(727/999): loss=0.5658271142566205\n",
      "Gradient Descent(728/999): loss=0.6771984916871785\n",
      "Gradient Descent(729/999): loss=0.9502504711550472\n",
      "Gradient Descent(730/999): loss=0.5780979143613576\n",
      "Gradient Descent(731/999): loss=0.5530245276796817\n",
      "Gradient Descent(732/999): loss=0.9499771089041233\n",
      "Gradient Descent(733/999): loss=0.9067782754349708\n",
      "Gradient Descent(734/999): loss=0.5902649145317076\n",
      "Gradient Descent(735/999): loss=2.061215847033262\n",
      "Gradient Descent(736/999): loss=0.9144503108531237\n",
      "Gradient Descent(737/999): loss=1.3561333436226843\n",
      "Gradient Descent(738/999): loss=0.9623877812749149\n",
      "Gradient Descent(739/999): loss=1.178414035618305\n",
      "Gradient Descent(740/999): loss=0.8500894687467814\n",
      "Gradient Descent(741/999): loss=0.7308508941525219\n",
      "Gradient Descent(742/999): loss=1.0881308000564576\n",
      "Gradient Descent(743/999): loss=1.2134775020360946\n",
      "Gradient Descent(744/999): loss=0.7162708836460113\n",
      "Gradient Descent(745/999): loss=1.19360916692555\n",
      "Gradient Descent(746/999): loss=1.1630146726483106\n",
      "Gradient Descent(747/999): loss=0.947444864926934\n",
      "Gradient Descent(748/999): loss=0.8311168040716648\n",
      "Gradient Descent(749/999): loss=0.5442669623512031\n",
      "Gradient Descent(750/999): loss=0.6851705212473868\n",
      "Gradient Descent(751/999): loss=2.431951874877214\n",
      "Gradient Descent(752/999): loss=0.5558064121484756\n",
      "Gradient Descent(753/999): loss=0.9140312012165787\n",
      "Gradient Descent(754/999): loss=0.5276744138610363\n",
      "Gradient Descent(755/999): loss=3.0819409748947613\n",
      "Gradient Descent(756/999): loss=29.02850649780273\n",
      "Gradient Descent(757/999): loss=9.99290227961838\n",
      "Gradient Descent(758/999): loss=2.494283879583478\n",
      "Gradient Descent(759/999): loss=1.934437114536166\n",
      "Gradient Descent(760/999): loss=0.9383515515470506\n",
      "Gradient Descent(761/999): loss=0.9414810259008406\n",
      "Gradient Descent(762/999): loss=1.1497999599993225\n",
      "Gradient Descent(763/999): loss=2.5227939138305184\n",
      "Gradient Descent(764/999): loss=0.6221643612796068\n",
      "Gradient Descent(765/999): loss=10.187570598144532\n",
      "Gradient Descent(766/999): loss=4.3870924959057565\n",
      "Gradient Descent(767/999): loss=2.3562019427192205\n",
      "Gradient Descent(768/999): loss=1.0726510663169624\n",
      "Gradient Descent(769/999): loss=1.212322868683934\n",
      "Gradient Descent(770/999): loss=0.8360681304335594\n",
      "Gradient Descent(771/999): loss=0.7368839403676987\n",
      "Gradient Descent(772/999): loss=14.292224696044922\n",
      "Gradient Descent(773/999): loss=3.644242189848423\n",
      "Gradient Descent(774/999): loss=3.4638703482127187\n",
      "Gradient Descent(775/999): loss=1.8953453869324925\n",
      "Gradient Descent(776/999): loss=2.4612104694175727\n",
      "Gradient Descent(777/999): loss=3.1753198547041412\n",
      "Gradient Descent(778/999): loss=17.914658475932477\n",
      "Gradient Descent(779/999): loss=6.530618953226805\n",
      "Gradient Descent(780/999): loss=2.3489278947263954\n",
      "Gradient Descent(781/999): loss=1.130179666236043\n",
      "Gradient Descent(782/999): loss=3.683333566560745\n",
      "Gradient Descent(783/999): loss=0.7843907977557183\n",
      "Gradient Descent(784/999): loss=0.4721285097932815\n",
      "Gradient Descent(785/999): loss=0.6733103183263541\n",
      "Gradient Descent(786/999): loss=0.8940686337667705\n",
      "Gradient Descent(787/999): loss=0.6442176800841091\n",
      "Gradient Descent(788/999): loss=0.4962176231873036\n",
      "Gradient Descent(789/999): loss=12.203549331665037\n",
      "Gradient Descent(790/999): loss=2.5542482397276163\n",
      "Gradient Descent(791/999): loss=1.239794938852191\n",
      "Gradient Descent(792/999): loss=0.8982149547755718\n",
      "Gradient Descent(793/999): loss=85.2450291015625\n",
      "Gradient Descent(794/999): loss=18.865463306427003\n",
      "Gradient Descent(795/999): loss=11.936073506652118\n",
      "Gradient Descent(796/999): loss=269.712781015625\n",
      "Gradient Descent(797/999): loss=50.48255282943725\n",
      "Gradient Descent(798/999): loss=2.311507751601935\n",
      "Gradient Descent(799/999): loss=1.8807342502945663\n",
      "Gradient Descent(800/999): loss=1.84150934564054\n",
      "Gradient Descent(801/999): loss=1.2827929108715057\n",
      "Gradient Descent(802/999): loss=2.1282987052685027\n",
      "Gradient Descent(803/999): loss=2.5435916034579273\n",
      "Gradient Descent(804/999): loss=3.0827363906395435\n",
      "Gradient Descent(805/999): loss=1.7844921685957909\n",
      "Gradient Descent(806/999): loss=1.46010525295794\n",
      "Gradient Descent(807/999): loss=1.7771166788703205\n",
      "Gradient Descent(808/999): loss=44.37066676298381\n",
      "Gradient Descent(809/999): loss=6.818372932415605\n",
      "Gradient Descent(810/999): loss=1.6072915041530134\n",
      "Gradient Descent(811/999): loss=2.1461312803334\n",
      "Gradient Descent(812/999): loss=2.426270424869656\n",
      "Gradient Descent(813/999): loss=2.5976908273661135\n",
      "Gradient Descent(814/999): loss=71.36676502441406\n",
      "Gradient Descent(815/999): loss=7.41188971012354\n",
      "Gradient Descent(816/999): loss=2.95705850763619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(817/999): loss=1.2830815220963956\n",
      "Gradient Descent(818/999): loss=1.4207433948522805\n",
      "Gradient Descent(819/999): loss=1.3727242014908791\n",
      "Gradient Descent(820/999): loss=1.4626854499870536\n",
      "Gradient Descent(821/999): loss=1.1515178243899347\n",
      "Gradient Descent(822/999): loss=1.6419404947167635\n",
      "Gradient Descent(823/999): loss=3.107848146137595\n",
      "Gradient Descent(824/999): loss=0.9489244478112459\n",
      "Gradient Descent(825/999): loss=3.974800354542732\n",
      "Gradient Descent(826/999): loss=0.8426971556770801\n",
      "Gradient Descent(827/999): loss=2.573976096732616\n",
      "Gradient Descent(828/999): loss=0.9773521203303339\n",
      "Gradient Descent(829/999): loss=0.8947859337913989\n",
      "Gradient Descent(830/999): loss=1.2459949031609296\n",
      "Gradient Descent(831/999): loss=1.056648781234026\n",
      "Gradient Descent(832/999): loss=1.579359762094617\n",
      "Gradient Descent(833/999): loss=2.3103788545280697\n",
      "Gradient Descent(834/999): loss=1.643555042654276\n",
      "Gradient Descent(835/999): loss=1.1313159684103729\n",
      "Gradient Descent(836/999): loss=0.9454443010008337\n",
      "Gradient Descent(837/999): loss=1.2154949434435371\n",
      "Gradient Descent(838/999): loss=0.9053491498088836\n",
      "Gradient Descent(839/999): loss=2.1171061367100474\n",
      "Gradient Descent(840/999): loss=1.201024279379845\n",
      "Gradient Descent(841/999): loss=1.1831729617106912\n",
      "Gradient Descent(842/999): loss=1.015449395184517\n",
      "Gradient Descent(843/999): loss=1.0128242608261109\n",
      "Gradient Descent(844/999): loss=4.089543219212294\n",
      "Gradient Descent(845/999): loss=0.6656153445655107\n",
      "Gradient Descent(846/999): loss=0.7424782678264379\n",
      "Gradient Descent(847/999): loss=0.9818980557614567\n",
      "Gradient Descent(848/999): loss=0.74410871104002\n",
      "Gradient Descent(849/999): loss=0.7138229381614923\n",
      "Gradient Descent(850/999): loss=0.7064529809808732\n",
      "Gradient Descent(851/999): loss=0.895575736364126\n",
      "Gradient Descent(852/999): loss=0.8353359628146885\n",
      "Gradient Descent(853/999): loss=7.538356236472726\n",
      "Gradient Descent(854/999): loss=1.8927804040223366\n",
      "Gradient Descent(855/999): loss=1.4491711285042759\n",
      "Gradient Descent(856/999): loss=0.9800556484955549\n",
      "Gradient Descent(857/999): loss=0.9811197822451592\n",
      "Gradient Descent(858/999): loss=1.1198936428070068\n",
      "Gradient Descent(859/999): loss=0.5006218338596821\n",
      "Gradient Descent(860/999): loss=0.49970225298583504\n",
      "Gradient Descent(861/999): loss=0.8000658955842256\n",
      "Gradient Descent(862/999): loss=1.0624712555110452\n",
      "Gradient Descent(863/999): loss=0.9121954703283311\n",
      "Gradient Descent(864/999): loss=0.8358769880867006\n",
      "Gradient Descent(865/999): loss=0.8414527626252173\n",
      "Gradient Descent(866/999): loss=0.8982804841518404\n",
      "Gradient Descent(867/999): loss=0.8409674158078431\n",
      "Gradient Descent(868/999): loss=0.6376884974050525\n",
      "Gradient Descent(869/999): loss=0.44364503795564175\n",
      "Gradient Descent(870/999): loss=0.6629464583158491\n",
      "Gradient Descent(871/999): loss=1.1485990026962758\n",
      "Gradient Descent(872/999): loss=0.4165271285086869\n",
      "Gradient Descent(873/999): loss=0.46986091656148427\n",
      "Gradient Descent(874/999): loss=0.5358227798604966\n",
      "Gradient Descent(875/999): loss=0.5635595142579077\n",
      "Gradient Descent(876/999): loss=0.4662487187922001\n",
      "Gradient Descent(877/999): loss=0.4850493351334333\n",
      "Gradient Descent(878/999): loss=0.5063298148286343\n",
      "Gradient Descent(879/999): loss=0.43738108122587216\n",
      "Gradient Descent(880/999): loss=0.7484876778519154\n",
      "Gradient Descent(881/999): loss=0.5111755312025547\n",
      "Gradient Descent(882/999): loss=0.47087873630642885\n",
      "Gradient Descent(883/999): loss=0.6851177488410474\n",
      "Gradient Descent(884/999): loss=0.4634968830275536\n",
      "Gradient Descent(885/999): loss=0.5047380744427442\n",
      "Gradient Descent(886/999): loss=0.5329222119206192\n",
      "Gradient Descent(887/999): loss=1.1371274361085892\n",
      "Gradient Descent(888/999): loss=0.6983418617975713\n",
      "Gradient Descent(889/999): loss=0.8201562015861272\n",
      "Gradient Descent(890/999): loss=31.231947792968754\n",
      "Gradient Descent(891/999): loss=10.314717818157076\n",
      "Gradient Descent(892/999): loss=2.6742910873264067\n",
      "Gradient Descent(893/999): loss=1.1312558726066353\n",
      "Gradient Descent(894/999): loss=0.6769592405945062\n",
      "Gradient Descent(895/999): loss=0.8257018438410758\n",
      "Gradient Descent(896/999): loss=1.176452990986109\n",
      "Gradient Descent(897/999): loss=1.5910810428333286\n",
      "Gradient Descent(898/999): loss=0.9946453033059837\n",
      "Gradient Descent(899/999): loss=2.4083772756808997\n",
      "Gradient Descent(900/999): loss=1.3203339318507912\n",
      "Gradient Descent(901/999): loss=3.665256854335069\n",
      "Gradient Descent(902/999): loss=0.7682375959026811\n",
      "Gradient Descent(903/999): loss=1.1612418442398311\n",
      "Gradient Descent(904/999): loss=3.557130973716378\n",
      "Gradient Descent(905/999): loss=0.5479479441130162\n",
      "Gradient Descent(906/999): loss=3.877053463397025\n",
      "Gradient Descent(907/999): loss=1.680920986126065\n",
      "Gradient Descent(908/999): loss=0.43048080470204353\n",
      "Gradient Descent(909/999): loss=1.9059435327756404\n",
      "Gradient Descent(910/999): loss=1.0576036290687325\n",
      "Gradient Descent(911/999): loss=1.1716212126260994\n",
      "Gradient Descent(912/999): loss=0.5335392870110274\n",
      "Gradient Descent(913/999): loss=0.7713050903856755\n",
      "Gradient Descent(914/999): loss=3.3166881358480453\n",
      "Gradient Descent(915/999): loss=2.0476907838881013\n",
      "Gradient Descent(916/999): loss=0.889440338625312\n",
      "Gradient Descent(917/999): loss=0.5709928392845393\n",
      "Gradient Descent(918/999): loss=1.0870740779340269\n",
      "Gradient Descent(919/999): loss=1.282640128964782\n",
      "Gradient Descent(920/999): loss=0.7394011210232972\n",
      "Gradient Descent(921/999): loss=1.0549964456576109\n",
      "Gradient Descent(922/999): loss=0.5280619964009523\n",
      "Gradient Descent(923/999): loss=0.46037008872807017\n",
      "Gradient Descent(924/999): loss=0.4865592333585024\n",
      "Gradient Descent(925/999): loss=0.9316030354166033\n",
      "Gradient Descent(926/999): loss=0.5844691377544403\n",
      "Gradient Descent(927/999): loss=0.4025983720159531\n",
      "Gradient Descent(928/999): loss=0.74079622215271\n",
      "Gradient Descent(929/999): loss=0.5859786047631502\n",
      "Gradient Descent(930/999): loss=0.40522846860051154\n",
      "Gradient Descent(931/999): loss=0.5437364432758093\n",
      "Gradient Descent(932/999): loss=0.49622705166816716\n",
      "Gradient Descent(933/999): loss=0.5459993671262264\n",
      "Gradient Descent(934/999): loss=0.5137845007556677\n",
      "Gradient Descent(935/999): loss=1.0023519244486094\n",
      "Gradient Descent(936/999): loss=0.4136809364008904\n",
      "Gradient Descent(937/999): loss=0.7138259776717425\n",
      "Gradient Descent(938/999): loss=0.8089584741169217\n",
      "Gradient Descent(939/999): loss=0.5521503152114152\n",
      "Gradient Descent(940/999): loss=0.5308695114803315\n",
      "Gradient Descent(941/999): loss=0.5952054975116252\n",
      "Gradient Descent(942/999): loss=0.7352500687110424\n",
      "Gradient Descent(943/999): loss=1.0067593292617798\n",
      "Gradient Descent(944/999): loss=0.43291476169824605\n",
      "Gradient Descent(945/999): loss=0.4425695459920168\n",
      "Gradient Descent(946/999): loss=0.5400541959899664\n",
      "Gradient Descent(947/999): loss=1.4511958346098661\n",
      "Gradient Descent(948/999): loss=0.9415880552822351\n",
      "Gradient Descent(949/999): loss=5.299300422509313\n",
      "Gradient Descent(950/999): loss=2.2120202467697863\n",
      "Gradient Descent(951/999): loss=1.3885852247095107\n",
      "Gradient Descent(952/999): loss=1.0323808353298902\n",
      "Gradient Descent(953/999): loss=1.0264604259318115\n",
      "Gradient Descent(954/999): loss=1.723953343975544\n",
      "Gradient Descent(955/999): loss=2.468233873957396\n",
      "Gradient Descent(956/999): loss=0.5133435334914923\n",
      "Gradient Descent(957/999): loss=60.24726164733887\n",
      "Gradient Descent(958/999): loss=7.0837999270445104\n",
      "Gradient Descent(959/999): loss=2.728560765676498\n",
      "Gradient Descent(960/999): loss=1.0847247418278456\n",
      "Gradient Descent(961/999): loss=1.2147233574616911\n",
      "Gradient Descent(962/999): loss=4.866336614105701\n",
      "Gradient Descent(963/999): loss=0.7232872109520434\n",
      "Gradient Descent(964/999): loss=0.5841450597029924\n",
      "Gradient Descent(965/999): loss=0.9206711110103128\n",
      "Gradient Descent(966/999): loss=0.5976363850730657\n",
      "Gradient Descent(967/999): loss=1.157840210158825\n",
      "Gradient Descent(968/999): loss=1.2535740930598973\n",
      "Gradient Descent(969/999): loss=0.48417114377737047\n",
      "Gradient Descent(970/999): loss=0.8021501814097165\n",
      "Gradient Descent(971/999): loss=0.5070480976492165\n",
      "Gradient Descent(972/999): loss=0.6015880093979835\n",
      "Gradient Descent(973/999): loss=1.1346866657882928\n",
      "Gradient Descent(974/999): loss=0.6835528291666508\n",
      "Gradient Descent(975/999): loss=0.8725517988914252\n",
      "Gradient Descent(976/999): loss=0.6026912713319066\n",
      "Gradient Descent(977/999): loss=0.516775875864625\n",
      "Gradient Descent(978/999): loss=0.6171890647739171\n",
      "Gradient Descent(979/999): loss=0.7563131033176185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(980/999): loss=0.9501457487756012\n",
      "Gradient Descent(981/999): loss=0.9206569346904754\n",
      "Gradient Descent(982/999): loss=0.6303025648528336\n",
      "Gradient Descent(983/999): loss=0.4760884416282178\n",
      "Gradient Descent(984/999): loss=0.5046426100170612\n",
      "Gradient Descent(985/999): loss=0.6728682587897777\n",
      "Gradient Descent(986/999): loss=0.6069099208748339\n",
      "Gradient Descent(987/999): loss=0.48249127831220634\n",
      "Gradient Descent(988/999): loss=0.6110678273767233\n",
      "Gradient Descent(989/999): loss=0.4923436442041396\n",
      "Gradient Descent(990/999): loss=0.8132637150108813\n",
      "Gradient Descent(991/999): loss=0.7598894645202161\n",
      "Gradient Descent(992/999): loss=0.7755986096388101\n",
      "Gradient Descent(993/999): loss=0.7989404966562987\n",
      "Gradient Descent(994/999): loss=0.8482243826639653\n",
      "Gradient Descent(995/999): loss=0.6950487541723253\n",
      "Gradient Descent(996/999): loss=0.7698709402674436\n",
      "Gradient Descent(997/999): loss=0.4538547476208211\n",
      "Gradient Descent(998/999): loss=0.46295630400180815\n",
      "Gradient Descent(999/999): loss=0.7138950781029463\n"
     ]
    }
   ],
   "source": [
    "w, mse = least_squares_SGD(np.float16(y_train), np.float16(tX_train), np.float16(initial_w), max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.660535"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict_labels(w, tX_train)\n",
    "np.mean(y_train.reshape(-1,1)==y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6607"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict_labels(w, tX_val)\n",
    "np.mean(y_val.reshape(-1,1)==y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros(tX.shape[1])#np.random.normal(loc=0, scale=1, size=(tX.shape[1],1))\n",
    "max_iters = 1000\n",
    "gamma = 1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_lr = y_train>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(0/999): loss=0.6733910493020513\n",
      "Log Regression(1/999): loss=0.6658206432791178\n",
      "Log Regression(2/999): loss=0.6791503506788689\n",
      "Log Regression(3/999): loss=0.6599997082001471\n",
      "Log Regression(4/999): loss=0.6581950792257761\n",
      "Log Regression(5/999): loss=0.6581358323539759\n",
      "Log Regression(6/999): loss=0.657872374609963\n",
      "Log Regression(7/999): loss=0.6638171659941786\n",
      "Log Regression(8/999): loss=0.6530769247076579\n",
      "Log Regression(9/999): loss=0.6470175924950776\n",
      "Log Regression(10/999): loss=0.6619592757522682\n",
      "Log Regression(11/999): loss=0.6434968285415273\n",
      "Log Regression(12/999): loss=0.6414838294920946\n",
      "Log Regression(13/999): loss=0.6394832706106824\n",
      "Log Regression(14/999): loss=0.6499576706465814\n",
      "Log Regression(15/999): loss=0.6392728133854805\n",
      "Log Regression(16/999): loss=0.6365353798713459\n",
      "Log Regression(17/999): loss=0.6380752153355235\n",
      "Log Regression(18/999): loss=0.6364315777310956\n",
      "Log Regression(19/999): loss=0.6323909844668028\n",
      "Log Regression(20/999): loss=0.6414278635441819\n",
      "Log Regression(21/999): loss=0.6314098624922464\n",
      "Log Regression(22/999): loss=0.6331529022669653\n",
      "Log Regression(23/999): loss=0.6321299632933491\n",
      "Log Regression(24/999): loss=0.6262702141265286\n",
      "Log Regression(25/999): loss=0.6239163961357533\n",
      "Log Regression(26/999): loss=0.6224564143159168\n",
      "Log Regression(27/999): loss=0.6216090368508942\n",
      "Log Regression(28/999): loss=0.6215275315328643\n",
      "Log Regression(29/999): loss=0.6189086553863397\n",
      "Log Regression(30/999): loss=0.6295769256447397\n",
      "Log Regression(31/999): loss=0.6339724688457974\n",
      "Log Regression(32/999): loss=0.620050684339801\n",
      "Log Regression(33/999): loss=0.6172500138263785\n",
      "Log Regression(34/999): loss=0.6200029746788618\n",
      "Log Regression(35/999): loss=0.6143545738565487\n",
      "Log Regression(36/999): loss=0.6135072864716301\n",
      "Log Regression(37/999): loss=0.6251690698445574\n",
      "Log Regression(38/999): loss=0.6126219109959193\n",
      "Log Regression(39/999): loss=0.6139902217772957\n",
      "Log Regression(40/999): loss=0.6202560599680832\n",
      "Log Regression(41/999): loss=0.6136433208359037\n",
      "Log Regression(42/999): loss=0.608988460092993\n",
      "Log Regression(43/999): loss=0.6075780441120747\n",
      "Log Regression(44/999): loss=0.6103043751913032\n",
      "Log Regression(45/999): loss=0.6067976891770727\n",
      "Log Regression(46/999): loss=0.6065966317935697\n",
      "Log Regression(47/999): loss=0.6054601585187775\n",
      "Log Regression(48/999): loss=0.6079624961892415\n",
      "Log Regression(49/999): loss=0.6110209676026919\n",
      "Log Regression(50/999): loss=0.604107404799068\n",
      "Log Regression(51/999): loss=0.6090487445069931\n",
      "Log Regression(52/999): loss=0.6117042491002533\n",
      "Log Regression(53/999): loss=0.6034304316383895\n",
      "Log Regression(54/999): loss=0.6025870829918126\n",
      "Log Regression(55/999): loss=0.603000155865121\n",
      "Log Regression(56/999): loss=0.6016353251876786\n",
      "Log Regression(57/999): loss=0.6009703237919658\n",
      "Log Regression(58/999): loss=0.6012845006280144\n",
      "Log Regression(59/999): loss=0.6063989320430845\n",
      "Log Regression(60/999): loss=0.6030488693692571\n",
      "Log Regression(61/999): loss=0.5997564590018852\n",
      "Log Regression(62/999): loss=0.5984778420208554\n",
      "Log Regression(63/999): loss=0.6019532356263412\n",
      "Log Regression(64/999): loss=0.5984847971582637\n",
      "Log Regression(65/999): loss=0.597682806898178\n",
      "Log Regression(66/999): loss=0.5989210045140734\n",
      "Log Regression(67/999): loss=0.5982038841237722\n",
      "Log Regression(68/999): loss=0.5966021654978799\n",
      "Log Regression(69/999): loss=0.5962116336983534\n",
      "Log Regression(70/999): loss=0.5973803882831236\n",
      "Log Regression(71/999): loss=0.5949957585872594\n",
      "Log Regression(72/999): loss=0.5986151414373858\n",
      "Log Regression(73/999): loss=0.6017302698300778\n",
      "Log Regression(74/999): loss=0.5993505581935485\n",
      "Log Regression(75/999): loss=0.5984981475115083\n",
      "Log Regression(76/999): loss=0.6002159420837169\n",
      "Log Regression(77/999): loss=0.595906228191318\n",
      "Log Regression(78/999): loss=0.59403348809284\n",
      "Log Regression(79/999): loss=0.5921681855303614\n",
      "Log Regression(80/999): loss=0.5942989405297446\n",
      "Log Regression(81/999): loss=0.5914612118677626\n",
      "Log Regression(82/999): loss=0.5917406840450263\n",
      "Log Regression(83/999): loss=0.591554286776406\n",
      "Log Regression(84/999): loss=0.59069918318432\n",
      "Log Regression(85/999): loss=0.5910197607398209\n",
      "Log Regression(86/999): loss=0.5987064295739993\n",
      "Log Regression(87/999): loss=0.5908101761640924\n",
      "Log Regression(88/999): loss=0.5910582377415435\n",
      "Log Regression(89/999): loss=0.5958724564681018\n",
      "Log Regression(90/999): loss=0.5884334270528379\n",
      "Log Regression(91/999): loss=0.5886575020444468\n",
      "Log Regression(92/999): loss=0.6016790732000065\n",
      "Log Regression(93/999): loss=0.5886673490387345\n",
      "Log Regression(94/999): loss=0.5927589259419225\n",
      "Log Regression(95/999): loss=0.5875715955641201\n",
      "Log Regression(96/999): loss=0.5883307921285017\n",
      "Log Regression(97/999): loss=0.5873532325589187\n",
      "Log Regression(98/999): loss=0.5867046580614173\n",
      "Log Regression(99/999): loss=0.5859529567489179\n",
      "Log Regression(100/999): loss=0.5859313743032498\n",
      "Log Regression(101/999): loss=0.5870966301965761\n",
      "Log Regression(102/999): loss=0.5885316972073066\n",
      "Log Regression(103/999): loss=0.5907004554570535\n",
      "Log Regression(104/999): loss=0.5850533327529498\n",
      "Log Regression(105/999): loss=0.5846212344311059\n",
      "Log Regression(106/999): loss=0.5895713630675162\n",
      "Log Regression(107/999): loss=0.5892916880269031\n",
      "Log Regression(108/999): loss=0.5843142534793444\n",
      "Log Regression(109/999): loss=0.5863138406665597\n",
      "Log Regression(110/999): loss=0.583732305384555\n",
      "Log Regression(111/999): loss=0.5863365276092868\n",
      "Log Regression(112/999): loss=0.5836989982849164\n",
      "Log Regression(113/999): loss=0.5839244593728999\n",
      "Log Regression(114/999): loss=0.5837117397055268\n",
      "Log Regression(115/999): loss=0.5824120212026646\n",
      "Log Regression(116/999): loss=0.5969283309318368\n",
      "Log Regression(117/999): loss=0.5842956932467008\n",
      "Log Regression(118/999): loss=0.5810543201102136\n",
      "Log Regression(119/999): loss=0.6004476728238066\n",
      "Log Regression(120/999): loss=0.5807487041486925\n",
      "Log Regression(121/999): loss=0.5842406582635468\n",
      "Log Regression(122/999): loss=0.5799266943367761\n",
      "Log Regression(123/999): loss=0.5796083936366401\n",
      "Log Regression(124/999): loss=0.5816949927729838\n",
      "Log Regression(125/999): loss=0.5818360783997768\n",
      "Log Regression(126/999): loss=0.5804292364399799\n",
      "Log Regression(127/999): loss=0.5784764272863461\n",
      "Log Regression(128/999): loss=0.5842414406474034\n",
      "Log Regression(129/999): loss=0.5779912083880748\n",
      "Log Regression(130/999): loss=0.5782934886374855\n",
      "Log Regression(131/999): loss=0.5776400901490195\n",
      "Log Regression(132/999): loss=0.5774472078685801\n",
      "Log Regression(133/999): loss=0.5804064141209003\n",
      "Log Regression(134/999): loss=0.5803945834332461\n",
      "Log Regression(135/999): loss=0.5767780045227267\n",
      "Log Regression(136/999): loss=0.5769021569014082\n",
      "Log Regression(137/999): loss=0.5760885499124972\n",
      "Log Regression(138/999): loss=0.5761931572860088\n",
      "Log Regression(139/999): loss=0.580224646972938\n",
      "Log Regression(140/999): loss=0.5854597744459124\n",
      "Log Regression(141/999): loss=0.5754258647077403\n",
      "Log Regression(142/999): loss=0.5817800852625357\n",
      "Log Regression(143/999): loss=0.5790186965906527\n",
      "Log Regression(144/999): loss=0.5787536036099418\n",
      "Log Regression(145/999): loss=0.5746722135900688\n",
      "Log Regression(146/999): loss=0.5750336757127814\n",
      "Log Regression(147/999): loss=0.5744003422612827\n",
      "Log Regression(148/999): loss=0.5740226908942951\n",
      "Log Regression(149/999): loss=0.5973576180494423\n",
      "Log Regression(150/999): loss=0.573411082086286\n",
      "Log Regression(151/999): loss=0.5738308429155987\n",
      "Log Regression(152/999): loss=0.577455876410093\n",
      "Log Regression(153/999): loss=0.5860370734323569\n",
      "Log Regression(154/999): loss=0.5728059539282674\n",
      "Log Regression(155/999): loss=0.572873721423745\n",
      "Log Regression(156/999): loss=0.5728066931178012\n",
      "Log Regression(157/999): loss=0.5737334442941455\n",
      "Log Regression(158/999): loss=0.5733912430647276\n",
      "Log Regression(159/999): loss=0.5717078995635727\n",
      "Log Regression(160/999): loss=0.5714318088881114\n",
      "Log Regression(161/999): loss=0.571454034808447\n",
      "Log Regression(162/999): loss=0.5730317413584176\n",
      "Log Regression(163/999): loss=0.5771317511486944\n",
      "Log Regression(164/999): loss=0.5714421196621157\n",
      "Log Regression(165/999): loss=0.5709704274526405\n",
      "Log Regression(166/999): loss=0.578227526706504\n",
      "Log Regression(167/999): loss=0.5757234041328565\n",
      "Log Regression(168/999): loss=0.5722555646574272\n",
      "Log Regression(169/999): loss=0.5738282439731938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(170/999): loss=0.5725698990971164\n",
      "Log Regression(171/999): loss=0.5751018045132454\n",
      "Log Regression(172/999): loss=0.5798455875367179\n",
      "Log Regression(173/999): loss=0.5731842694144367\n",
      "Log Regression(174/999): loss=0.5690740248825776\n",
      "Log Regression(175/999): loss=0.5692588224689149\n",
      "Log Regression(176/999): loss=0.5691484261996678\n",
      "Log Regression(177/999): loss=0.5683368152379623\n",
      "Log Regression(178/999): loss=0.5735821025487755\n",
      "Log Regression(179/999): loss=0.5680545778431715\n",
      "Log Regression(180/999): loss=0.5679840940359132\n",
      "Log Regression(181/999): loss=0.5696246172215338\n",
      "Log Regression(182/999): loss=0.5683601821421993\n",
      "Log Regression(183/999): loss=0.5698337932165771\n",
      "Log Regression(184/999): loss=0.5736013324042647\n",
      "Log Regression(185/999): loss=0.568695223458761\n",
      "Log Regression(186/999): loss=0.5668666548122351\n",
      "Log Regression(187/999): loss=0.5711204534457353\n",
      "Log Regression(188/999): loss=0.5674750557767996\n",
      "Log Regression(189/999): loss=0.5689221913026937\n",
      "Log Regression(190/999): loss=0.5854087532801728\n",
      "Log Regression(191/999): loss=0.5660237398591479\n",
      "Log Regression(192/999): loss=0.5661753179332207\n",
      "Log Regression(193/999): loss=0.5677966588313019\n",
      "Log Regression(194/999): loss=0.5658866467871853\n",
      "Log Regression(195/999): loss=0.5672229719075594\n",
      "Log Regression(196/999): loss=0.5662295445203935\n",
      "Log Regression(197/999): loss=0.5653321107328954\n",
      "Log Regression(198/999): loss=0.5713100658638726\n",
      "Log Regression(199/999): loss=0.5655942618355838\n",
      "Log Regression(200/999): loss=0.5650036314591245\n",
      "Log Regression(201/999): loss=0.5648078083594441\n",
      "Log Regression(202/999): loss=0.5669416653102635\n",
      "Log Regression(203/999): loss=0.5676082670114376\n",
      "Log Regression(204/999): loss=0.5650989978420058\n",
      "Log Regression(205/999): loss=0.566865725249957\n",
      "Log Regression(206/999): loss=0.5649028290384526\n",
      "Log Regression(207/999): loss=0.5654033121274036\n",
      "Log Regression(208/999): loss=0.5649091955427785\n",
      "Log Regression(209/999): loss=0.5648245626164515\n",
      "Log Regression(210/999): loss=0.5644986840194198\n",
      "Log Regression(211/999): loss=0.5644387903659519\n",
      "Log Regression(212/999): loss=0.564297515819074\n",
      "Log Regression(213/999): loss=0.5647255889466958\n",
      "Log Regression(214/999): loss=0.5681734206304341\n",
      "Log Regression(215/999): loss=0.5638889402591774\n",
      "Log Regression(216/999): loss=0.5661426566561379\n",
      "Log Regression(217/999): loss=0.5640166145684475\n",
      "Log Regression(218/999): loss=0.5819281123773322\n",
      "Log Regression(219/999): loss=0.563638337661496\n",
      "Log Regression(220/999): loss=0.5631136275610771\n",
      "Log Regression(221/999): loss=0.5629992595484326\n",
      "Log Regression(222/999): loss=0.5796811740964749\n",
      "Log Regression(223/999): loss=0.563926509095874\n",
      "Log Regression(224/999): loss=0.5623377540219144\n",
      "Log Regression(225/999): loss=0.5623804441435715\n",
      "Log Regression(226/999): loss=0.5652772278799019\n",
      "Log Regression(227/999): loss=0.5659388817587112\n",
      "Log Regression(228/999): loss=0.5622439810788927\n",
      "Log Regression(229/999): loss=0.5672019142981355\n",
      "Log Regression(230/999): loss=0.5624110391943945\n",
      "Log Regression(231/999): loss=0.5697967000951559\n",
      "Log Regression(232/999): loss=0.5623663986492976\n",
      "Log Regression(233/999): loss=0.5615954740114506\n",
      "Log Regression(234/999): loss=0.5620078750893372\n",
      "Log Regression(235/999): loss=0.5617195445234249\n",
      "Log Regression(236/999): loss=0.562187228673061\n",
      "Log Regression(237/999): loss=0.5607424110507597\n",
      "Log Regression(238/999): loss=0.5600702917058173\n",
      "Log Regression(239/999): loss=0.5634332841523421\n",
      "Log Regression(240/999): loss=0.5630176477481214\n",
      "Log Regression(241/999): loss=0.5607017455425752\n",
      "Log Regression(242/999): loss=0.5599641367789441\n",
      "Log Regression(243/999): loss=0.5599652664831335\n",
      "Log Regression(244/999): loss=0.5665012127913563\n",
      "Log Regression(245/999): loss=0.5610530429081633\n",
      "Log Regression(246/999): loss=0.5594903689327738\n",
      "Log Regression(247/999): loss=0.5596256763039782\n",
      "Log Regression(248/999): loss=0.5589489011224892\n",
      "Log Regression(249/999): loss=0.5600782003024213\n",
      "Log Regression(250/999): loss=0.5597698850750142\n",
      "Log Regression(251/999): loss=0.5607582546585687\n",
      "Log Regression(252/999): loss=0.5602383887687749\n",
      "Log Regression(253/999): loss=0.5615334936323675\n",
      "Log Regression(254/999): loss=0.559550467451355\n",
      "Log Regression(255/999): loss=0.5617527781115111\n",
      "Log Regression(256/999): loss=0.5592257446950337\n",
      "Log Regression(257/999): loss=0.5584458236545516\n",
      "Log Regression(258/999): loss=0.5581048718540234\n",
      "Log Regression(259/999): loss=0.5582463323460822\n",
      "Log Regression(260/999): loss=0.5578445060194419\n",
      "Log Regression(261/999): loss=0.5589000333520311\n",
      "Log Regression(262/999): loss=0.5591312134423508\n",
      "Log Regression(263/999): loss=0.5575947887619471\n",
      "Log Regression(264/999): loss=0.5582897763934763\n",
      "Log Regression(265/999): loss=0.5617572555671395\n",
      "Log Regression(266/999): loss=0.5591451823923145\n",
      "Log Regression(267/999): loss=0.5580585534313349\n",
      "Log Regression(268/999): loss=0.5620435055225039\n",
      "Log Regression(269/999): loss=0.5575085904989608\n",
      "Log Regression(270/999): loss=0.5568778556213911\n",
      "Log Regression(271/999): loss=0.55715132484375\n",
      "Log Regression(272/999): loss=0.5623430033058587\n",
      "Log Regression(273/999): loss=0.5659068992317563\n",
      "Log Regression(274/999): loss=0.5566232976048945\n",
      "Log Regression(275/999): loss=0.5563474201553466\n",
      "Log Regression(276/999): loss=0.5563416734904328\n",
      "Log Regression(277/999): loss=0.5572567053916467\n",
      "Log Regression(278/999): loss=0.5601275499601985\n",
      "Log Regression(279/999): loss=0.5706070798653864\n",
      "Log Regression(280/999): loss=0.5562231179124659\n",
      "Log Regression(281/999): loss=0.5566778200664043\n",
      "Log Regression(282/999): loss=0.5562248307194837\n",
      "Log Regression(283/999): loss=0.5563116202820781\n",
      "Log Regression(284/999): loss=0.5575571599855721\n",
      "Log Regression(285/999): loss=0.5561460398198574\n",
      "Log Regression(286/999): loss=0.5607470921304466\n",
      "Log Regression(287/999): loss=0.5558291927475358\n",
      "Log Regression(288/999): loss=0.5559520264589788\n",
      "Log Regression(289/999): loss=0.5551550693684992\n",
      "Log Regression(290/999): loss=0.5559237183017157\n",
      "Log Regression(291/999): loss=0.5571802525628474\n",
      "Log Regression(292/999): loss=0.5549639046310143\n",
      "Log Regression(293/999): loss=0.558224795634806\n",
      "Log Regression(294/999): loss=0.5597978998211457\n",
      "Log Regression(295/999): loss=0.5545171936647444\n",
      "Log Regression(296/999): loss=0.5628231458544257\n",
      "Log Regression(297/999): loss=0.5547420269487311\n",
      "Log Regression(298/999): loss=0.5612252438767656\n",
      "Log Regression(299/999): loss=0.556881291756901\n",
      "Log Regression(300/999): loss=0.5572089460175331\n",
      "Log Regression(301/999): loss=0.5541662647395917\n",
      "Log Regression(302/999): loss=0.5542906502530154\n",
      "Log Regression(303/999): loss=0.5539987575164885\n",
      "Log Regression(304/999): loss=0.5539126872251877\n",
      "Log Regression(305/999): loss=0.5540426344234352\n",
      "Log Regression(306/999): loss=0.5538946868181227\n",
      "Log Regression(307/999): loss=0.5537526650493535\n",
      "Log Regression(308/999): loss=0.5537234019679405\n",
      "Log Regression(309/999): loss=0.5555745911789468\n",
      "Log Regression(310/999): loss=0.5536170803249744\n",
      "Log Regression(311/999): loss=0.5544258352457363\n",
      "Log Regression(312/999): loss=0.5547648843544212\n",
      "Log Regression(313/999): loss=0.5534246253555232\n",
      "Log Regression(314/999): loss=0.5532459204285141\n",
      "Log Regression(315/999): loss=0.5547732095310403\n",
      "Log Regression(316/999): loss=0.5529609107832886\n",
      "Log Regression(317/999): loss=0.5531970670944356\n",
      "Log Regression(318/999): loss=0.552766234912916\n",
      "Log Regression(319/999): loss=0.5539881265634691\n",
      "Log Regression(320/999): loss=0.553985230298975\n",
      "Log Regression(321/999): loss=0.5526490779138227\n",
      "Log Regression(322/999): loss=0.5530400563459789\n",
      "Log Regression(323/999): loss=0.5525183464916541\n",
      "Log Regression(324/999): loss=0.5531167585648543\n",
      "Log Regression(325/999): loss=0.5528630549496888\n",
      "Log Regression(326/999): loss=0.5519999070407992\n",
      "Log Regression(327/999): loss=0.5524269796688457\n",
      "Log Regression(328/999): loss=0.5521129084167922\n",
      "Log Regression(329/999): loss=0.5578770645134359\n",
      "Log Regression(330/999): loss=0.552378546626127\n",
      "Log Regression(331/999): loss=0.554112440524605\n",
      "Log Regression(332/999): loss=0.5533251301974541\n",
      "Log Regression(333/999): loss=0.5525671612759582\n",
      "Log Regression(334/999): loss=0.5516258791397619\n",
      "Log Regression(335/999): loss=0.5546718412741519\n",
      "Log Regression(336/999): loss=0.5526162977614167\n",
      "Log Regression(337/999): loss=0.5530146317867451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(338/999): loss=0.5529299421056523\n",
      "Log Regression(339/999): loss=0.551423767019834\n",
      "Log Regression(340/999): loss=0.5511634823743833\n",
      "Log Regression(341/999): loss=0.5528587721887784\n",
      "Log Regression(342/999): loss=0.5511030574177666\n",
      "Log Regression(343/999): loss=0.5515041954231795\n",
      "Log Regression(344/999): loss=0.553328224732677\n",
      "Log Regression(345/999): loss=0.557056835203508\n",
      "Log Regression(346/999): loss=0.5518336598094145\n",
      "Log Regression(347/999): loss=0.5516307613255022\n",
      "Log Regression(348/999): loss=0.5533149010034227\n",
      "Log Regression(349/999): loss=0.5506017314382581\n",
      "Log Regression(350/999): loss=0.5507796843572594\n",
      "Log Regression(351/999): loss=0.5524852517325275\n",
      "Log Regression(352/999): loss=0.5555562642975636\n",
      "Log Regression(353/999): loss=0.553323379309213\n",
      "Log Regression(354/999): loss=0.5509344897123575\n",
      "Log Regression(355/999): loss=0.5506075924308131\n",
      "Log Regression(356/999): loss=0.550502317472411\n",
      "Log Regression(357/999): loss=0.5499844363229643\n",
      "Log Regression(358/999): loss=0.551133852817582\n",
      "Log Regression(359/999): loss=0.5522058270703897\n",
      "Log Regression(360/999): loss=0.5505618599268707\n",
      "Log Regression(361/999): loss=0.5499094608953498\n",
      "Log Regression(362/999): loss=0.5509645555747729\n",
      "Log Regression(363/999): loss=0.5589229890501553\n",
      "Log Regression(364/999): loss=0.5517740242640513\n",
      "Log Regression(365/999): loss=0.551763310414555\n",
      "Log Regression(366/999): loss=0.5524383257238057\n",
      "Log Regression(367/999): loss=0.5497541229979427\n",
      "Log Regression(368/999): loss=0.5505628883926418\n",
      "Log Regression(369/999): loss=0.555950258477084\n",
      "Log Regression(370/999): loss=0.5499249367013428\n",
      "Log Regression(371/999): loss=0.550323616041601\n",
      "Log Regression(372/999): loss=0.5492796878356128\n",
      "Log Regression(373/999): loss=0.5574627272857655\n",
      "Log Regression(374/999): loss=0.549017217794783\n",
      "Log Regression(375/999): loss=0.5494666609195235\n",
      "Log Regression(376/999): loss=0.5491109609227498\n",
      "Log Regression(377/999): loss=0.549406751458088\n",
      "Log Regression(378/999): loss=0.5505148849880624\n",
      "Log Regression(379/999): loss=0.5491309934633014\n",
      "Log Regression(380/999): loss=0.5524647540801895\n",
      "Log Regression(381/999): loss=0.552290321069872\n",
      "Log Regression(382/999): loss=0.5489418160153107\n",
      "Log Regression(383/999): loss=0.5489261903746142\n",
      "Log Regression(384/999): loss=0.5490518716041664\n",
      "Log Regression(385/999): loss=0.5502116583778137\n",
      "Log Regression(386/999): loss=0.5501249847647383\n",
      "Log Regression(387/999): loss=0.5610895112024002\n",
      "Log Regression(388/999): loss=0.5492162261153732\n",
      "Log Regression(389/999): loss=0.5510138410944638\n",
      "Log Regression(390/999): loss=0.5579943720943878\n",
      "Log Regression(391/999): loss=0.554298896723792\n",
      "Log Regression(392/999): loss=0.5509897399786515\n",
      "Log Regression(393/999): loss=0.5483263204058935\n",
      "Log Regression(394/999): loss=0.5490943408188212\n",
      "Log Regression(395/999): loss=0.5485590730843363\n",
      "Log Regression(396/999): loss=0.5584495018123371\n",
      "Log Regression(397/999): loss=0.5485009505784642\n",
      "Log Regression(398/999): loss=0.5489505633327392\n",
      "Log Regression(399/999): loss=0.5532256546823657\n",
      "Log Regression(400/999): loss=0.5485771267198112\n",
      "Log Regression(401/999): loss=0.5488201219045885\n",
      "Log Regression(402/999): loss=0.55005028347284\n",
      "Log Regression(403/999): loss=0.5484298256407858\n",
      "Log Regression(404/999): loss=0.548048598561277\n",
      "Log Regression(405/999): loss=0.5505564526128419\n",
      "Log Regression(406/999): loss=0.5480555526547158\n",
      "Log Regression(407/999): loss=0.5479843621539752\n",
      "Log Regression(408/999): loss=0.5511298723633431\n",
      "Log Regression(409/999): loss=0.5487679117544446\n",
      "Log Regression(410/999): loss=0.5486512391830694\n",
      "Log Regression(411/999): loss=0.5479979839724812\n",
      "Log Regression(412/999): loss=0.5477231711098126\n",
      "Log Regression(413/999): loss=0.5482504820937439\n",
      "Log Regression(414/999): loss=0.5493376746473689\n",
      "Log Regression(415/999): loss=0.5476550804070293\n",
      "Log Regression(416/999): loss=0.547373529851883\n",
      "Log Regression(417/999): loss=0.547395864598897\n",
      "Log Regression(418/999): loss=0.5482615871229198\n",
      "Log Regression(419/999): loss=0.5478854000084404\n",
      "Log Regression(420/999): loss=0.5476252277808468\n",
      "Log Regression(421/999): loss=0.5475712330180749\n",
      "Log Regression(422/999): loss=0.5500736265112518\n",
      "Log Regression(423/999): loss=0.5473910604159048\n",
      "Log Regression(424/999): loss=0.5471831043530644\n",
      "Log Regression(425/999): loss=0.5470572324762093\n",
      "Log Regression(426/999): loss=0.5497028946208432\n",
      "Log Regression(427/999): loss=0.5497193549759549\n",
      "Log Regression(428/999): loss=0.5494535697076796\n",
      "Log Regression(429/999): loss=0.5472867128839286\n",
      "Log Regression(430/999): loss=0.5471659521089417\n",
      "Log Regression(431/999): loss=0.5482895571484356\n",
      "Log Regression(432/999): loss=0.5471140722673833\n",
      "Log Regression(433/999): loss=0.5584187000227574\n",
      "Log Regression(434/999): loss=0.5468686656442879\n",
      "Log Regression(435/999): loss=0.5474596632625823\n",
      "Log Regression(436/999): loss=0.5669767026687879\n",
      "Log Regression(437/999): loss=0.5501906616699259\n",
      "Log Regression(438/999): loss=0.5483478022162175\n",
      "Log Regression(439/999): loss=0.5481730752013015\n",
      "Log Regression(440/999): loss=0.5464882887677962\n",
      "Log Regression(441/999): loss=0.5471627035210922\n",
      "Log Regression(442/999): loss=0.5492404548903973\n",
      "Log Regression(443/999): loss=0.5510762739847409\n",
      "Log Regression(444/999): loss=0.5468194773861126\n",
      "Log Regression(445/999): loss=0.5473559531540185\n",
      "Log Regression(446/999): loss=0.5463995012586393\n",
      "Log Regression(447/999): loss=0.5481089715037389\n",
      "Log Regression(448/999): loss=0.5465434596300868\n",
      "Log Regression(449/999): loss=0.5461158332762652\n",
      "Log Regression(450/999): loss=0.5470289268664985\n",
      "Log Regression(451/999): loss=0.5460073870290465\n",
      "Log Regression(452/999): loss=0.5463923521576551\n",
      "Log Regression(453/999): loss=0.5465283115495604\n",
      "Log Regression(454/999): loss=0.5467834078299979\n",
      "Log Regression(455/999): loss=0.549267704151356\n",
      "Log Regression(456/999): loss=0.5469733109543978\n",
      "Log Regression(457/999): loss=0.5460630900428678\n",
      "Log Regression(458/999): loss=0.546280737501231\n",
      "Log Regression(459/999): loss=0.549980367983648\n",
      "Log Regression(460/999): loss=0.5504752705570507\n",
      "Log Regression(461/999): loss=0.54656976564622\n",
      "Log Regression(462/999): loss=0.5457556006012398\n",
      "Log Regression(463/999): loss=0.5455403201744268\n",
      "Log Regression(464/999): loss=0.5454847753100344\n",
      "Log Regression(465/999): loss=0.5475483733816292\n",
      "Log Regression(466/999): loss=0.5487031958119742\n",
      "Log Regression(467/999): loss=0.5457584495251249\n",
      "Log Regression(468/999): loss=0.5453327299296963\n",
      "Log Regression(469/999): loss=0.5453361397741029\n",
      "Log Regression(470/999): loss=0.5453591433359412\n",
      "Log Regression(471/999): loss=0.5465883863387033\n",
      "Log Regression(472/999): loss=0.555276029794185\n",
      "Log Regression(473/999): loss=0.5483783023653893\n",
      "Log Regression(474/999): loss=0.5455831750563156\n",
      "Log Regression(475/999): loss=0.545354842231101\n",
      "Log Regression(476/999): loss=0.5502691701205902\n",
      "Log Regression(477/999): loss=0.5458130802247498\n",
      "Log Regression(478/999): loss=0.5451047321303867\n",
      "Log Regression(479/999): loss=0.5450431750333035\n",
      "Log Regression(480/999): loss=0.5450166220292629\n",
      "Log Regression(481/999): loss=0.5455305627769882\n",
      "Log Regression(482/999): loss=0.5477698223948547\n",
      "Log Regression(483/999): loss=0.5510189835657778\n",
      "Log Regression(484/999): loss=0.5483182321444744\n",
      "Log Regression(485/999): loss=0.5458065600110673\n",
      "Log Regression(486/999): loss=0.5458412935169744\n",
      "Log Regression(487/999): loss=0.5451178641026134\n",
      "Log Regression(488/999): loss=0.5453023597731427\n",
      "Log Regression(489/999): loss=0.5465382681306977\n",
      "Log Regression(490/999): loss=0.5449469912972003\n",
      "Log Regression(491/999): loss=0.5447226419401706\n",
      "Log Regression(492/999): loss=0.544983546138431\n",
      "Log Regression(493/999): loss=0.5485400585352366\n",
      "Log Regression(494/999): loss=0.5449117210664247\n",
      "Log Regression(495/999): loss=0.5447680230970613\n",
      "Log Regression(496/999): loss=0.5449795230217647\n",
      "Log Regression(497/999): loss=0.5449431436888393\n",
      "Log Regression(498/999): loss=0.5497772107732812\n",
      "Log Regression(499/999): loss=0.5451436376527022\n",
      "Log Regression(500/999): loss=0.5487272166094105\n",
      "Log Regression(501/999): loss=0.5477285542545741\n",
      "Log Regression(502/999): loss=0.5510403435979768\n",
      "Log Regression(503/999): loss=0.5445253736238586\n",
      "Log Regression(504/999): loss=0.5476711796283378\n",
      "Log Regression(505/999): loss=0.5445194123030614\n",
      "Log Regression(506/999): loss=0.5480637574482324\n",
      "Log Regression(507/999): loss=0.5450987500969906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(508/999): loss=0.5482018652037288\n",
      "Log Regression(509/999): loss=0.5443908167531063\n",
      "Log Regression(510/999): loss=0.544823643687272\n",
      "Log Regression(511/999): loss=0.5462198256678031\n",
      "Log Regression(512/999): loss=0.544852954290529\n",
      "Log Regression(513/999): loss=0.5454176506232196\n",
      "Log Regression(514/999): loss=0.5448528317474249\n",
      "Log Regression(515/999): loss=0.545228865687054\n",
      "Log Regression(516/999): loss=0.5448248268641037\n",
      "Log Regression(517/999): loss=0.5457697225902574\n",
      "Log Regression(518/999): loss=0.5510062661813774\n",
      "Log Regression(519/999): loss=0.5451594289175157\n",
      "Log Regression(520/999): loss=0.5440902510040845\n",
      "Log Regression(521/999): loss=0.5443468309149502\n",
      "Log Regression(522/999): loss=0.5467015070199387\n",
      "Log Regression(523/999): loss=0.5471649597166164\n",
      "Log Regression(524/999): loss=0.5439919701009941\n",
      "Log Regression(525/999): loss=0.5457287156283142\n",
      "Log Regression(526/999): loss=0.5438390500964412\n",
      "Log Regression(527/999): loss=0.5474478029694035\n",
      "Log Regression(528/999): loss=0.5440879632191991\n",
      "Log Regression(529/999): loss=0.5485196482478837\n",
      "Log Regression(530/999): loss=0.5436831978092156\n",
      "Log Regression(531/999): loss=0.5459011707563274\n",
      "Log Regression(532/999): loss=0.543726524568335\n",
      "Log Regression(533/999): loss=0.5453087442164445\n",
      "Log Regression(534/999): loss=0.5445256814633587\n",
      "Log Regression(535/999): loss=0.5434187450028445\n",
      "Log Regression(536/999): loss=0.5456195527119077\n",
      "Log Regression(537/999): loss=0.5438165456039374\n",
      "Log Regression(538/999): loss=0.5471990970039164\n",
      "Log Regression(539/999): loss=0.5445728816749825\n",
      "Log Regression(540/999): loss=0.5447824134470682\n",
      "Log Regression(541/999): loss=0.5442612577473968\n",
      "Log Regression(542/999): loss=0.5455230439714568\n",
      "Log Regression(543/999): loss=0.5471275581618877\n",
      "Log Regression(544/999): loss=0.5518768598453055\n",
      "Log Regression(545/999): loss=0.5618950838311356\n",
      "Log Regression(546/999): loss=0.5435822227962451\n",
      "Log Regression(547/999): loss=0.5448010623469256\n",
      "Log Regression(548/999): loss=0.5434232805169834\n",
      "Log Regression(549/999): loss=0.5467889708196872\n",
      "Log Regression(550/999): loss=0.5440349576806474\n",
      "Log Regression(551/999): loss=0.5444093475733561\n",
      "Log Regression(552/999): loss=0.5451955368532699\n",
      "Log Regression(553/999): loss=0.5433954040477985\n",
      "Log Regression(554/999): loss=0.543220911419539\n",
      "Log Regression(555/999): loss=0.5458456013063341\n",
      "Log Regression(556/999): loss=0.5431617406911642\n",
      "Log Regression(557/999): loss=0.5431111151244867\n",
      "Log Regression(558/999): loss=0.5474485482217748\n",
      "Log Regression(559/999): loss=0.5566137818023209\n",
      "Log Regression(560/999): loss=0.5432572578980776\n",
      "Log Regression(561/999): loss=0.5453785869678451\n",
      "Log Regression(562/999): loss=0.5435534129470465\n",
      "Log Regression(563/999): loss=0.5450449258444771\n",
      "Log Regression(564/999): loss=0.5429077389365773\n",
      "Log Regression(565/999): loss=0.542846606324874\n",
      "Log Regression(566/999): loss=0.5432867958784102\n",
      "Log Regression(567/999): loss=0.5486060577409838\n",
      "Log Regression(568/999): loss=0.5490245059025366\n",
      "Log Regression(569/999): loss=0.546339403786243\n",
      "Log Regression(570/999): loss=0.5426871712003584\n",
      "Log Regression(571/999): loss=0.5500311307157094\n",
      "Log Regression(572/999): loss=0.5428826412068222\n",
      "Log Regression(573/999): loss=0.5427025951763854\n",
      "Log Regression(574/999): loss=0.5450969358525655\n",
      "Log Regression(575/999): loss=0.5423487570900576\n",
      "Log Regression(576/999): loss=0.5425715728968505\n",
      "Log Regression(577/999): loss=0.5427128516955466\n",
      "Log Regression(578/999): loss=0.5456384360585362\n",
      "Log Regression(579/999): loss=0.542373223166835\n",
      "Log Regression(580/999): loss=0.5449858202634906\n",
      "Log Regression(581/999): loss=0.5489638688152698\n",
      "Log Regression(582/999): loss=0.5447734759464469\n",
      "Log Regression(583/999): loss=0.5433010517618999\n",
      "Log Regression(584/999): loss=0.5437065546781144\n",
      "Log Regression(585/999): loss=0.5452130937022795\n",
      "Log Regression(586/999): loss=0.5450075444556308\n",
      "Log Regression(587/999): loss=0.5426721948095282\n",
      "Log Regression(588/999): loss=0.5449120229309049\n",
      "Log Regression(589/999): loss=0.5427023408231305\n",
      "Log Regression(590/999): loss=0.5430123252894596\n",
      "Log Regression(591/999): loss=0.5424992466567379\n",
      "Log Regression(592/999): loss=0.5430661481395188\n",
      "Log Regression(593/999): loss=0.5478364129541381\n",
      "Log Regression(594/999): loss=0.5429396460812023\n",
      "Log Regression(595/999): loss=0.5439415697709659\n",
      "Log Regression(596/999): loss=0.5426813873610463\n",
      "Log Regression(597/999): loss=0.5427995919026778\n",
      "Log Regression(598/999): loss=0.5426943762612897\n",
      "Log Regression(599/999): loss=0.5421348273398401\n",
      "Log Regression(600/999): loss=0.5425973614902282\n",
      "Log Regression(601/999): loss=0.5439043067549663\n",
      "Log Regression(602/999): loss=0.5446701206527546\n",
      "Log Regression(603/999): loss=0.542141999456887\n",
      "Log Regression(604/999): loss=0.5445246914556718\n",
      "Log Regression(605/999): loss=0.5557355837877348\n",
      "Log Regression(606/999): loss=0.5441038358900337\n",
      "Log Regression(607/999): loss=0.542967709089307\n",
      "Log Regression(608/999): loss=0.546704197926343\n",
      "Log Regression(609/999): loss=0.5419827187165474\n",
      "Log Regression(610/999): loss=0.5443534934897235\n",
      "Log Regression(611/999): loss=0.5419690853290218\n",
      "Log Regression(612/999): loss=0.542176611253794\n",
      "Log Regression(613/999): loss=0.5430549272823798\n",
      "Log Regression(614/999): loss=0.5421062599998389\n",
      "Log Regression(615/999): loss=0.5444473400989301\n",
      "Log Regression(616/999): loss=0.5438095132743965\n",
      "Log Regression(617/999): loss=0.5419700287835386\n",
      "Log Regression(618/999): loss=0.5458129357924555\n",
      "Log Regression(619/999): loss=0.5479353694290721\n",
      "Log Regression(620/999): loss=0.5424858847706717\n",
      "Log Regression(621/999): loss=0.5456528185983475\n",
      "Log Regression(622/999): loss=0.5442467879334697\n",
      "Log Regression(623/999): loss=0.5419484304675621\n",
      "Log Regression(624/999): loss=0.5434465137369534\n",
      "Log Regression(625/999): loss=0.544332506735166\n",
      "Log Regression(626/999): loss=0.5445375726499839\n",
      "Log Regression(627/999): loss=0.5416773311733737\n",
      "Log Regression(628/999): loss=0.5419146470169878\n",
      "Log Regression(629/999): loss=0.5416673875815418\n",
      "Log Regression(630/999): loss=0.5417583673362037\n",
      "Log Regression(631/999): loss=0.5416347935224924\n",
      "Log Regression(632/999): loss=0.5420730865477524\n",
      "Log Regression(633/999): loss=0.5451001513943433\n",
      "Log Regression(634/999): loss=0.5450925853358773\n",
      "Log Regression(635/999): loss=0.5432194312164751\n",
      "Log Regression(636/999): loss=0.5417489664695104\n",
      "Log Regression(637/999): loss=0.541564193067513\n",
      "Log Regression(638/999): loss=0.5415864457630243\n",
      "Log Regression(639/999): loss=0.5469340157224148\n",
      "Log Regression(640/999): loss=0.5414571152947831\n",
      "Log Regression(641/999): loss=0.5459965669873509\n",
      "Log Regression(642/999): loss=0.546242966652137\n",
      "Log Regression(643/999): loss=0.5527156038971175\n",
      "Log Regression(644/999): loss=0.5417206292747259\n",
      "Log Regression(645/999): loss=0.5439717267892337\n",
      "Log Regression(646/999): loss=0.5480352268565275\n",
      "Log Regression(647/999): loss=0.5444399325877597\n",
      "Log Regression(648/999): loss=0.5442065495008717\n",
      "Log Regression(649/999): loss=0.5415887001327504\n",
      "Log Regression(650/999): loss=0.5420796419084676\n",
      "Log Regression(651/999): loss=0.5424220890174812\n",
      "Log Regression(652/999): loss=0.5418344799383085\n",
      "Log Regression(653/999): loss=0.5469590543294381\n",
      "Log Regression(654/999): loss=0.5457304295030888\n",
      "Log Regression(655/999): loss=0.5417552878986576\n",
      "Log Regression(656/999): loss=0.5416962629951665\n",
      "Log Regression(657/999): loss=0.5421937195788108\n",
      "Log Regression(658/999): loss=0.5419944679142725\n",
      "Log Regression(659/999): loss=0.5424255121656688\n",
      "Log Regression(660/999): loss=0.5436851128779677\n",
      "Log Regression(661/999): loss=0.5432647502341809\n",
      "Log Regression(662/999): loss=0.5432715062185363\n",
      "Log Regression(663/999): loss=0.5421866305236359\n",
      "Log Regression(664/999): loss=0.5425619324959179\n",
      "Log Regression(665/999): loss=0.5413678767973469\n",
      "Log Regression(666/999): loss=0.5525781935701527\n",
      "Log Regression(667/999): loss=0.5432506717811799\n",
      "Log Regression(668/999): loss=0.5425769530216725\n",
      "Log Regression(669/999): loss=0.5416505252404801\n",
      "Log Regression(670/999): loss=0.5473703901023341\n",
      "Log Regression(671/999): loss=0.5420767329288292\n",
      "Log Regression(672/999): loss=0.5412627078476707\n",
      "Log Regression(673/999): loss=0.546109793650323\n",
      "Log Regression(674/999): loss=0.5425792715944876\n",
      "Log Regression(675/999): loss=0.5414530871407495\n",
      "Log Regression(676/999): loss=0.5428862150354894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(677/999): loss=0.5415280555080819\n",
      "Log Regression(678/999): loss=0.5428139809099993\n",
      "Log Regression(679/999): loss=0.5420659784025293\n",
      "Log Regression(680/999): loss=0.541117830993882\n",
      "Log Regression(681/999): loss=0.5415540233487737\n",
      "Log Regression(682/999): loss=0.5424899412771108\n",
      "Log Regression(683/999): loss=0.5543170720195805\n",
      "Log Regression(684/999): loss=0.5413376988024428\n",
      "Log Regression(685/999): loss=0.5476965636540082\n",
      "Log Regression(686/999): loss=0.5461552381250254\n",
      "Log Regression(687/999): loss=0.5414753485521636\n",
      "Log Regression(688/999): loss=0.5421525921474162\n",
      "Log Regression(689/999): loss=0.5409378995215233\n",
      "Log Regression(690/999): loss=0.5430563274071031\n",
      "Log Regression(691/999): loss=0.5411481154002322\n",
      "Log Regression(692/999): loss=0.5414932355803901\n",
      "Log Regression(693/999): loss=0.5415081396870821\n",
      "Log Regression(694/999): loss=0.5409798286019922\n",
      "Log Regression(695/999): loss=0.5421625142286651\n",
      "Log Regression(696/999): loss=0.5416912099677251\n",
      "Log Regression(697/999): loss=0.5461984180912577\n",
      "Log Regression(698/999): loss=0.5419990304094475\n",
      "Log Regression(699/999): loss=0.5455696567275466\n",
      "Log Regression(700/999): loss=0.5421891944099201\n",
      "Log Regression(701/999): loss=0.5432809587357518\n",
      "Log Regression(702/999): loss=0.5460153492430503\n",
      "Log Regression(703/999): loss=0.5437754063432964\n",
      "Log Regression(704/999): loss=0.5450363225712519\n",
      "Log Regression(705/999): loss=0.5436462416891947\n",
      "Log Regression(706/999): loss=0.5407212526691603\n",
      "Log Regression(707/999): loss=0.5407046749718817\n",
      "Log Regression(708/999): loss=0.5411148640000522\n",
      "Log Regression(709/999): loss=0.5406093036785832\n",
      "Log Regression(710/999): loss=0.5431723202240029\n",
      "Log Regression(711/999): loss=0.5421792517225928\n",
      "Log Regression(712/999): loss=0.5413770115380419\n",
      "Log Regression(713/999): loss=0.5411295203141097\n",
      "Log Regression(714/999): loss=0.5416209194372297\n",
      "Log Regression(715/999): loss=0.5516240387416161\n",
      "Log Regression(716/999): loss=0.5409342076639667\n",
      "Log Regression(717/999): loss=0.5419320308170721\n",
      "Log Regression(718/999): loss=0.5421348965599434\n",
      "Log Regression(719/999): loss=0.5471815753248707\n",
      "Log Regression(720/999): loss=0.5409720106430281\n",
      "Log Regression(721/999): loss=0.5413743389086679\n",
      "Log Regression(722/999): loss=0.5415861853119054\n",
      "Log Regression(723/999): loss=0.5409749104456875\n",
      "Log Regression(724/999): loss=0.5461013903597899\n",
      "Log Regression(725/999): loss=0.5413305046995694\n",
      "Log Regression(726/999): loss=0.5411508872740447\n",
      "Log Regression(727/999): loss=0.5407335777463479\n",
      "Log Regression(728/999): loss=0.5405224802909072\n",
      "Log Regression(729/999): loss=0.5421511667651995\n",
      "Log Regression(730/999): loss=0.5434601857140707\n",
      "Log Regression(731/999): loss=0.540798690179386\n",
      "Log Regression(732/999): loss=0.5410181799084246\n",
      "Log Regression(733/999): loss=0.540485631148808\n",
      "Log Regression(734/999): loss=0.5403052658386395\n",
      "Log Regression(735/999): loss=0.5430471254322551\n",
      "Log Regression(736/999): loss=0.5406917386544231\n",
      "Log Regression(737/999): loss=0.5436475062847085\n",
      "Log Regression(738/999): loss=0.543261507519818\n",
      "Log Regression(739/999): loss=0.5411578752725846\n",
      "Log Regression(740/999): loss=0.5416017755958761\n",
      "Log Regression(741/999): loss=0.54077350250704\n",
      "Log Regression(742/999): loss=0.541917428550677\n",
      "Log Regression(743/999): loss=0.5423496885365419\n",
      "Log Regression(744/999): loss=0.5412953486941093\n",
      "Log Regression(745/999): loss=0.5420575889202076\n",
      "Log Regression(746/999): loss=0.5410421191473525\n",
      "Log Regression(747/999): loss=0.547800570303653\n",
      "Log Regression(748/999): loss=0.5402001821433948\n",
      "Log Regression(749/999): loss=0.5406162892698341\n",
      "Log Regression(750/999): loss=0.5413324481913369\n",
      "Log Regression(751/999): loss=0.5416489217844196\n",
      "Log Regression(752/999): loss=0.5402148456872579\n",
      "Log Regression(753/999): loss=0.5447356319614156\n",
      "Log Regression(754/999): loss=0.5400750327690145\n",
      "Log Regression(755/999): loss=0.5400644626907846\n",
      "Log Regression(756/999): loss=0.5451326889795096\n",
      "Log Regression(757/999): loss=0.5425472907641163\n",
      "Log Regression(758/999): loss=0.5416812298460068\n",
      "Log Regression(759/999): loss=0.5422492053471836\n",
      "Log Regression(760/999): loss=0.5401292782657751\n",
      "Log Regression(761/999): loss=0.5400952364126413\n",
      "Log Regression(762/999): loss=0.5418994309844045\n",
      "Log Regression(763/999): loss=0.550646387981892\n",
      "Log Regression(764/999): loss=0.5411820835663351\n",
      "Log Regression(765/999): loss=0.5400089689352294\n",
      "Log Regression(766/999): loss=0.5452963502637911\n",
      "Log Regression(767/999): loss=0.5465750405057375\n",
      "Log Regression(768/999): loss=0.5403873593801181\n",
      "Log Regression(769/999): loss=0.5402360755948139\n",
      "Log Regression(770/999): loss=0.540062912641294\n",
      "Log Regression(771/999): loss=0.5404912652579409\n",
      "Log Regression(772/999): loss=0.5407995368232487\n",
      "Log Regression(773/999): loss=0.5463753250249088\n",
      "Log Regression(774/999): loss=0.5429320333043514\n",
      "Log Regression(775/999): loss=0.5403631557591264\n",
      "Log Regression(776/999): loss=0.5397921059915114\n",
      "Log Regression(777/999): loss=0.5424661647823205\n",
      "Log Regression(778/999): loss=0.5398809671962371\n",
      "Log Regression(779/999): loss=0.5453568294719523\n",
      "Log Regression(780/999): loss=0.54984453543528\n",
      "Log Regression(781/999): loss=0.5508900369450593\n",
      "Log Regression(782/999): loss=0.5417866467530538\n",
      "Log Regression(783/999): loss=0.5404762137353551\n",
      "Log Regression(784/999): loss=0.5397193251723156\n",
      "Log Regression(785/999): loss=0.543128401549661\n",
      "Log Regression(786/999): loss=0.5405952214264583\n",
      "Log Regression(787/999): loss=0.5399581062804887\n",
      "Log Regression(788/999): loss=0.5429639151894898\n",
      "Log Regression(789/999): loss=0.5402260335141855\n",
      "Log Regression(790/999): loss=0.539889821250194\n",
      "Log Regression(791/999): loss=0.5405757246746644\n",
      "Log Regression(792/999): loss=0.5462478418615099\n",
      "Log Regression(793/999): loss=0.5414525428802989\n",
      "Log Regression(794/999): loss=0.5438785519946583\n",
      "Log Regression(795/999): loss=0.5405611505487079\n",
      "Log Regression(796/999): loss=0.5398120169477169\n",
      "Log Regression(797/999): loss=0.5396331956781586\n",
      "Log Regression(798/999): loss=0.5396176352456683\n",
      "Log Regression(799/999): loss=0.5409474321478676\n",
      "Log Regression(800/999): loss=0.5418579586787935\n",
      "Log Regression(801/999): loss=0.540313007573768\n",
      "Log Regression(802/999): loss=0.543532502558784\n",
      "Log Regression(803/999): loss=0.540107963112901\n",
      "Log Regression(804/999): loss=0.5398452087870494\n",
      "Log Regression(805/999): loss=0.5453657438163902\n",
      "Log Regression(806/999): loss=0.5426361696067783\n",
      "Log Regression(807/999): loss=0.5403487092370358\n",
      "Log Regression(808/999): loss=0.539501828570089\n",
      "Log Regression(809/999): loss=0.5396742370205834\n",
      "Log Regression(810/999): loss=0.5398231846230063\n",
      "Log Regression(811/999): loss=0.54118578259656\n",
      "Log Regression(812/999): loss=0.5395182453775093\n",
      "Log Regression(813/999): loss=0.5396969594380042\n",
      "Log Regression(814/999): loss=0.5402210218542006\n",
      "Log Regression(815/999): loss=0.539384033072522\n",
      "Log Regression(816/999): loss=0.5397418934501254\n",
      "Log Regression(817/999): loss=0.5472969652537032\n",
      "Log Regression(818/999): loss=0.5394129570917285\n",
      "Log Regression(819/999): loss=0.5407969246527577\n",
      "Log Regression(820/999): loss=0.5405807905137375\n",
      "Log Regression(821/999): loss=0.5458916344601858\n",
      "Log Regression(822/999): loss=0.5404761706339207\n",
      "Log Regression(823/999): loss=0.5433177480330051\n",
      "Log Regression(824/999): loss=0.5405565221608912\n",
      "Log Regression(825/999): loss=0.5464053871425291\n",
      "Log Regression(826/999): loss=0.5400300578350554\n",
      "Log Regression(827/999): loss=0.5394299806554878\n",
      "Log Regression(828/999): loss=0.5405001572262398\n",
      "Log Regression(829/999): loss=0.5394972988857728\n",
      "Log Regression(830/999): loss=0.5392325852022185\n",
      "Log Regression(831/999): loss=0.5452287058180351\n",
      "Log Regression(832/999): loss=0.5413005080796962\n",
      "Log Regression(833/999): loss=0.539701019745892\n",
      "Log Regression(834/999): loss=0.5396704453680639\n",
      "Log Regression(835/999): loss=0.5393415327022807\n",
      "Log Regression(836/999): loss=0.5428368113602956\n",
      "Log Regression(837/999): loss=0.5479100029420627\n",
      "Log Regression(838/999): loss=0.5420802377741596\n",
      "Log Regression(839/999): loss=0.5412701814805203\n",
      "Log Regression(840/999): loss=0.539513931839139\n",
      "Log Regression(841/999): loss=0.539816657213169\n",
      "Log Regression(842/999): loss=0.5396017654815999\n",
      "Log Regression(843/999): loss=0.5401690040791469\n",
      "Log Regression(844/999): loss=0.5399154092147953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(845/999): loss=0.539407067637474\n",
      "Log Regression(846/999): loss=0.5391530577375016\n",
      "Log Regression(847/999): loss=0.5397054083943332\n",
      "Log Regression(848/999): loss=0.5401301041765436\n",
      "Log Regression(849/999): loss=0.5392690600035875\n",
      "Log Regression(850/999): loss=0.5431549131566071\n",
      "Log Regression(851/999): loss=0.5390469908724074\n",
      "Log Regression(852/999): loss=0.5420708909758349\n",
      "Log Regression(853/999): loss=0.5389906581195077\n",
      "Log Regression(854/999): loss=0.5425893711672887\n",
      "Log Regression(855/999): loss=0.5389702151303317\n",
      "Log Regression(856/999): loss=0.5422342178226913\n",
      "Log Regression(857/999): loss=0.5406554120319438\n",
      "Log Regression(858/999): loss=0.544182865720048\n",
      "Log Regression(859/999): loss=0.5413989707524254\n",
      "Log Regression(860/999): loss=0.5454459315523573\n",
      "Log Regression(861/999): loss=0.5430414735164323\n",
      "Log Regression(862/999): loss=0.5391048371960266\n",
      "Log Regression(863/999): loss=0.5399528703900972\n",
      "Log Regression(864/999): loss=0.5395874020248194\n",
      "Log Regression(865/999): loss=0.5409458113570479\n",
      "Log Regression(866/999): loss=0.5399889249739331\n",
      "Log Regression(867/999): loss=0.5405960431894193\n",
      "Log Regression(868/999): loss=0.5400740789961023\n",
      "Log Regression(869/999): loss=0.5405922397917831\n",
      "Log Regression(870/999): loss=0.540201742660566\n",
      "Log Regression(871/999): loss=0.540326576265463\n",
      "Log Regression(872/999): loss=0.539321051821315\n",
      "Log Regression(873/999): loss=0.5406679418991711\n",
      "Log Regression(874/999): loss=0.542395048319353\n",
      "Log Regression(875/999): loss=0.5389931905779776\n",
      "Log Regression(876/999): loss=0.5403671607568983\n",
      "Log Regression(877/999): loss=0.5425543672097638\n",
      "Log Regression(878/999): loss=0.5392024806713479\n",
      "Log Regression(879/999): loss=0.5397615339461894\n",
      "Log Regression(880/999): loss=0.5394403808160098\n",
      "Log Regression(881/999): loss=0.5402498065457639\n",
      "Log Regression(882/999): loss=0.5392654826522947\n",
      "Log Regression(883/999): loss=0.5420342634963066\n",
      "Log Regression(884/999): loss=0.5416762123481911\n",
      "Log Regression(885/999): loss=0.5435471373038043\n",
      "Log Regression(886/999): loss=0.5405888678988501\n",
      "Log Regression(887/999): loss=0.5396289213179233\n",
      "Log Regression(888/999): loss=0.5451968087150971\n",
      "Log Regression(889/999): loss=0.5396697997256109\n",
      "Log Regression(890/999): loss=0.5430605046433258\n",
      "Log Regression(891/999): loss=0.5400471646438945\n",
      "Log Regression(892/999): loss=0.538800955533238\n",
      "Log Regression(893/999): loss=0.5429669387676561\n",
      "Log Regression(894/999): loss=0.5389395877676662\n",
      "Log Regression(895/999): loss=0.5393754627623861\n",
      "Log Regression(896/999): loss=0.5409176370037512\n",
      "Log Regression(897/999): loss=0.5386321348377653\n",
      "Log Regression(898/999): loss=0.538976153447412\n",
      "Log Regression(899/999): loss=0.5388712429725697\n",
      "Log Regression(900/999): loss=0.5387918748125095\n",
      "Log Regression(901/999): loss=0.5396978348952572\n",
      "Log Regression(902/999): loss=0.5390068764255346\n",
      "Log Regression(903/999): loss=0.5396873462631678\n",
      "Log Regression(904/999): loss=0.5446349098591883\n",
      "Log Regression(905/999): loss=0.5387417009694592\n",
      "Log Regression(906/999): loss=0.5420649958542109\n",
      "Log Regression(907/999): loss=0.5389282651445731\n",
      "Log Regression(908/999): loss=0.5407046074665978\n",
      "Log Regression(909/999): loss=0.5394297857463819\n",
      "Log Regression(910/999): loss=0.5388689988092448\n",
      "Log Regression(911/999): loss=0.5395371158273556\n",
      "Log Regression(912/999): loss=0.5398214122547802\n",
      "Log Regression(913/999): loss=0.5394605524348364\n",
      "Log Regression(914/999): loss=0.5392306622578108\n",
      "Log Regression(915/999): loss=0.540653278550169\n",
      "Log Regression(916/999): loss=0.5427554510416619\n",
      "Log Regression(917/999): loss=0.5417635239922552\n",
      "Log Regression(918/999): loss=0.5392401485362237\n",
      "Log Regression(919/999): loss=0.5390562939901806\n",
      "Log Regression(920/999): loss=0.5404517366765957\n",
      "Log Regression(921/999): loss=0.5389781535960233\n",
      "Log Regression(922/999): loss=0.5587576464749494\n",
      "Log Regression(923/999): loss=0.5404841249709286\n",
      "Log Regression(924/999): loss=0.5410270978203459\n",
      "Log Regression(925/999): loss=0.5393863049968644\n",
      "Log Regression(926/999): loss=0.5413151640702892\n",
      "Log Regression(927/999): loss=0.5388354623280515\n",
      "Log Regression(928/999): loss=0.5391909685888496\n",
      "Log Regression(929/999): loss=0.544906442546606\n",
      "Log Regression(930/999): loss=0.5476106033730074\n",
      "Log Regression(931/999): loss=0.5460161040387621\n",
      "Log Regression(932/999): loss=0.5384308560315783\n",
      "Log Regression(933/999): loss=0.5383759063929512\n",
      "Log Regression(934/999): loss=0.5397909602480973\n",
      "Log Regression(935/999): loss=0.5421147601553729\n",
      "Log Regression(936/999): loss=0.5390540676983473\n",
      "Log Regression(937/999): loss=0.538392214139256\n",
      "Log Regression(938/999): loss=0.5422928087040639\n",
      "Log Regression(939/999): loss=0.5384116487740696\n",
      "Log Regression(940/999): loss=0.5392783357237307\n",
      "Log Regression(941/999): loss=0.5386302282140368\n",
      "Log Regression(942/999): loss=0.539762688302265\n",
      "Log Regression(943/999): loss=0.5384275834436634\n",
      "Log Regression(944/999): loss=0.5383113075360364\n",
      "Log Regression(945/999): loss=0.5388951808729953\n",
      "Log Regression(946/999): loss=0.5384417028783413\n",
      "Log Regression(947/999): loss=0.5407989138638583\n",
      "Log Regression(948/999): loss=0.539449755290658\n",
      "Log Regression(949/999): loss=0.5383565006576261\n",
      "Log Regression(950/999): loss=0.5382209637010159\n",
      "Log Regression(951/999): loss=0.5398382793697459\n",
      "Log Regression(952/999): loss=0.5412725211686815\n",
      "Log Regression(953/999): loss=0.5393604637923518\n",
      "Log Regression(954/999): loss=0.5390566089018008\n",
      "Log Regression(955/999): loss=0.5418450053411653\n",
      "Log Regression(956/999): loss=0.5393369420471694\n",
      "Log Regression(957/999): loss=0.538904422791819\n",
      "Log Regression(958/999): loss=0.5412094577109712\n",
      "Log Regression(959/999): loss=0.5404132657467378\n",
      "Log Regression(960/999): loss=0.538462019180926\n",
      "Log Regression(961/999): loss=0.5387943148846502\n",
      "Log Regression(962/999): loss=0.541335925797365\n",
      "Log Regression(963/999): loss=0.5388995367765324\n",
      "Log Regression(964/999): loss=0.5392496488384935\n",
      "Log Regression(965/999): loss=0.5384335917060643\n",
      "Log Regression(966/999): loss=0.5393660018315477\n",
      "Log Regression(967/999): loss=0.5389632799358028\n",
      "Log Regression(968/999): loss=0.5394243184684135\n",
      "Log Regression(969/999): loss=0.539918087009162\n",
      "Log Regression(970/999): loss=0.5394933363109956\n",
      "Log Regression(971/999): loss=0.5446552535671925\n",
      "Log Regression(972/999): loss=0.539533771401145\n",
      "Log Regression(973/999): loss=0.542956381999557\n",
      "Log Regression(974/999): loss=0.5567378645859398\n",
      "Log Regression(975/999): loss=0.5389517945304242\n",
      "Log Regression(976/999): loss=0.5384494377911777\n",
      "Log Regression(977/999): loss=0.5381044962750267\n",
      "Log Regression(978/999): loss=0.538143920648107\n",
      "Log Regression(979/999): loss=0.5383190391608831\n",
      "Log Regression(980/999): loss=0.5384861790765154\n",
      "Log Regression(981/999): loss=0.5382515619977546\n",
      "Log Regression(982/999): loss=0.5381863906769663\n",
      "Log Regression(983/999): loss=0.5383626981822504\n",
      "Log Regression(984/999): loss=0.5393592106280849\n",
      "Log Regression(985/999): loss=0.5384138583692262\n",
      "Log Regression(986/999): loss=0.5386631772750939\n",
      "Log Regression(987/999): loss=0.542116557566835\n",
      "Log Regression(988/999): loss=0.5387504820707648\n",
      "Log Regression(989/999): loss=0.5417762419824688\n",
      "Log Regression(990/999): loss=0.541708060181232\n",
      "Log Regression(991/999): loss=0.5448462486440385\n",
      "Log Regression(992/999): loss=0.5403305947770689\n",
      "Log Regression(993/999): loss=0.5411517244512696\n",
      "Log Regression(994/999): loss=0.5401743668601356\n",
      "Log Regression(995/999): loss=0.5494365481123964\n",
      "Log Regression(996/999): loss=0.5395052019895702\n",
      "Log Regression(997/999): loss=0.5407892400920251\n",
      "Log Regression(998/999): loss=0.5397796000428849\n",
      "Log Regression(999/999): loss=0.5403313943377528\n"
     ]
    }
   ],
   "source": [
    "w, mse = logistic_regression(y_train_lr, tX_train, initial_w, max_iters, gamma, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71897"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict_labels(w, tX_train)\n",
    "np.mean(y_train.reshape(-1,1)==y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72184"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict_labels(w, tX_val)\n",
    "np.mean(y_val.reshape(-1,1)==y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Regularized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_lr = y_train>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros(tX.shape[1])#np.random.normal(loc=0, scale=1, size=(tX.shape[1],1))\n",
    "max_iters = 5000\n",
    "gamma = 1e-7\n",
    "lambda_ = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(0/999): loss=0.6847434861837708\n",
      "Log Regression(1/999): loss=0.6740543586581709\n",
      "Log Regression(2/999): loss=0.675833238043273\n",
      "Log Regression(3/999): loss=0.667895811244757\n",
      "Log Regression(4/999): loss=0.6652965741887917\n",
      "Log Regression(5/999): loss=0.6739612263753906\n",
      "Log Regression(6/999): loss=0.6617645032451517\n",
      "Log Regression(7/999): loss=0.6597164490202116\n",
      "Log Regression(8/999): loss=0.6610331107981331\n",
      "Log Regression(9/999): loss=0.6582369194092249\n",
      "Log Regression(10/999): loss=0.6585558529670379\n",
      "Log Regression(11/999): loss=0.666904190729708\n",
      "Log Regression(12/999): loss=0.6714953629361665\n",
      "Log Regression(13/999): loss=0.6581118092206235\n",
      "Log Regression(14/999): loss=0.6532562951414744\n",
      "Log Regression(15/999): loss=0.653325106966108\n",
      "Log Regression(16/999): loss=0.6516822742983085\n",
      "Log Regression(17/999): loss=0.6539942115949839\n",
      "Log Regression(18/999): loss=0.6500018643140412\n",
      "Log Regression(19/999): loss=0.6487127794352043\n",
      "Log Regression(20/999): loss=0.6487937402884153\n",
      "Log Regression(21/999): loss=0.6475595855138552\n",
      "Log Regression(22/999): loss=0.6473229533756235\n",
      "Log Regression(23/999): loss=0.6444239633449181\n",
      "Log Regression(24/999): loss=0.6440963719345486\n",
      "Log Regression(25/999): loss=0.6460428960781154\n",
      "Log Regression(26/999): loss=0.6445320008716613\n",
      "Log Regression(27/999): loss=0.6411558667275886\n",
      "Log Regression(28/999): loss=0.6409181363933303\n",
      "Log Regression(29/999): loss=0.6407829641238475\n",
      "Log Regression(30/999): loss=0.6389417223903513\n",
      "Log Regression(31/999): loss=0.6383872371262044\n",
      "Log Regression(32/999): loss=0.6378290239921832\n",
      "Log Regression(33/999): loss=0.6400808851914411\n",
      "Log Regression(34/999): loss=0.637422537332323\n",
      "Log Regression(35/999): loss=0.6359803411230929\n",
      "Log Regression(36/999): loss=0.6359195521583713\n",
      "Log Regression(37/999): loss=0.6368230455017676\n",
      "Log Regression(38/999): loss=0.6361297653018286\n",
      "Log Regression(39/999): loss=0.6349999985889111\n",
      "Log Regression(40/999): loss=0.6343558053941274\n",
      "Log Regression(41/999): loss=0.6343015280976988\n",
      "Log Regression(42/999): loss=0.6334723534237546\n",
      "Log Regression(43/999): loss=0.6348864835329758\n",
      "Log Regression(44/999): loss=0.6411109737034966\n",
      "Log Regression(45/999): loss=0.6329852248643717\n",
      "Log Regression(46/999): loss=0.6298469863032377\n",
      "Log Regression(47/999): loss=0.6298192249119038\n",
      "Log Regression(48/999): loss=0.6286976287045853\n",
      "Log Regression(49/999): loss=0.6291851612858373\n",
      "Log Regression(50/999): loss=0.6279337890554932\n",
      "Log Regression(51/999): loss=0.6264825287860821\n",
      "Log Regression(52/999): loss=0.6258455120366561\n",
      "Log Regression(53/999): loss=0.6257508396260941\n",
      "Log Regression(54/999): loss=0.6254211384638108\n",
      "Log Regression(55/999): loss=0.6236440792271019\n",
      "Log Regression(56/999): loss=0.6233557883560259\n",
      "Log Regression(57/999): loss=0.6221976218406391\n",
      "Log Regression(58/999): loss=0.6229033976751329\n",
      "Log Regression(59/999): loss=0.6215808702615913\n",
      "Log Regression(60/999): loss=0.6209218940872332\n",
      "Log Regression(61/999): loss=0.6283290222537168\n",
      "Log Regression(62/999): loss=0.6356146247936192\n",
      "Log Regression(63/999): loss=0.6211057712549949\n",
      "Log Regression(64/999): loss=0.6187245198119045\n",
      "Log Regression(65/999): loss=0.6192570345334792\n",
      "Log Regression(66/999): loss=0.618490742979136\n",
      "Log Regression(67/999): loss=0.6174221804793347\n",
      "Log Regression(68/999): loss=0.6168230241630576\n",
      "Log Regression(69/999): loss=0.6210111530568924\n",
      "Log Regression(70/999): loss=0.6174829250969247\n",
      "Log Regression(71/999): loss=0.617353201069555\n",
      "Log Regression(72/999): loss=0.6154318761174778\n",
      "Log Regression(73/999): loss=0.6161668726116875\n",
      "Log Regression(74/999): loss=0.620324768490759\n",
      "Log Regression(75/999): loss=0.6168967253447326\n",
      "Log Regression(76/999): loss=0.6161646284592251\n",
      "Log Regression(77/999): loss=0.61445369117456\n",
      "Log Regression(78/999): loss=0.6151220050804556\n",
      "Log Regression(79/999): loss=0.6165617049313912\n",
      "Log Regression(80/999): loss=0.6117811712992636\n",
      "Log Regression(81/999): loss=0.6162350092040295\n",
      "Log Regression(82/999): loss=0.6107488255007532\n",
      "Log Regression(83/999): loss=0.6118298081357255\n",
      "Log Regression(84/999): loss=0.6106147011960423\n",
      "Log Regression(85/999): loss=0.6101131046058718\n",
      "Log Regression(86/999): loss=0.6107585165089829\n",
      "Log Regression(87/999): loss=0.6100369757684232\n",
      "Log Regression(88/999): loss=0.60909137891962\n",
      "Log Regression(89/999): loss=0.6097446484262012\n",
      "Log Regression(90/999): loss=0.613577492100101\n",
      "Log Regression(91/999): loss=0.6117012998176788\n",
      "Log Regression(92/999): loss=0.6081397935762318\n",
      "Log Regression(93/999): loss=0.613584657192297\n",
      "Log Regression(94/999): loss=0.6132127054827452\n",
      "Log Regression(95/999): loss=0.6084241653794245\n",
      "Log Regression(96/999): loss=0.6070344740476021\n",
      "Log Regression(97/999): loss=0.6076339609667178\n",
      "Log Regression(98/999): loss=0.6064076594853295\n",
      "Log Regression(99/999): loss=0.6054199286895035\n",
      "Log Regression(100/999): loss=0.6066390885041418\n",
      "Log Regression(101/999): loss=0.6061212100387995\n",
      "Log Regression(102/999): loss=0.6056749397993642\n",
      "Log Regression(103/999): loss=0.604268515705461\n",
      "Log Regression(104/999): loss=0.6044116326622261\n",
      "Log Regression(105/999): loss=0.603835925486733\n",
      "Log Regression(106/999): loss=0.6035790617495759\n",
      "Log Regression(107/999): loss=0.6058609971900376\n",
      "Log Regression(108/999): loss=0.6054910739943522\n",
      "Log Regression(109/999): loss=0.6037082351821511\n",
      "Log Regression(110/999): loss=0.603175421263994\n",
      "Log Regression(111/999): loss=0.6085109229436546\n",
      "Log Regression(112/999): loss=0.6023578205149441\n",
      "Log Regression(113/999): loss=0.606337398486024\n",
      "Log Regression(114/999): loss=0.6089187976272646\n",
      "Log Regression(115/999): loss=0.6017923878560898\n",
      "Log Regression(116/999): loss=0.6061525288229647\n",
      "Log Regression(117/999): loss=0.6029438961682478\n",
      "Log Regression(118/999): loss=0.6036379423597721\n",
      "Log Regression(119/999): loss=0.6024899887967315\n",
      "Log Regression(120/999): loss=0.6039890364435601\n",
      "Log Regression(121/999): loss=0.6008057158475889\n",
      "Log Regression(122/999): loss=0.6035207326255488\n",
      "Log Regression(123/999): loss=0.601356566853436\n",
      "Log Regression(124/999): loss=0.5996705787165364\n",
      "Log Regression(125/999): loss=0.6003306369631328\n",
      "Log Regression(126/999): loss=0.6017132299419135\n",
      "Log Regression(127/999): loss=0.6033438212089565\n",
      "Log Regression(128/999): loss=0.6002081539642753\n",
      "Log Regression(129/999): loss=0.5999658822685315\n",
      "Log Regression(130/999): loss=0.5982659668771193\n",
      "Log Regression(131/999): loss=0.5981681180874143\n",
      "Log Regression(132/999): loss=0.5983327524422624\n",
      "Log Regression(133/999): loss=0.5984177739967692\n",
      "Log Regression(134/999): loss=0.5979634290007786\n",
      "Log Regression(135/999): loss=0.598796868595834\n",
      "Log Regression(136/999): loss=0.5977456219951368\n",
      "Log Regression(137/999): loss=0.5975460946989055\n",
      "Log Regression(138/999): loss=0.5971260683859005\n",
      "Log Regression(139/999): loss=0.5986746620912393\n",
      "Log Regression(140/999): loss=0.5971920062994485\n",
      "Log Regression(141/999): loss=0.5995332802598543\n",
      "Log Regression(142/999): loss=0.5976467168792536\n",
      "Log Regression(143/999): loss=0.5958574373228408\n",
      "Log Regression(144/999): loss=0.5988568007744994\n",
      "Log Regression(145/999): loss=0.6018470782430768\n",
      "Log Regression(146/999): loss=0.5957091245374311\n",
      "Log Regression(147/999): loss=0.5994899223496409\n",
      "Log Regression(148/999): loss=0.5959791940436006\n",
      "Log Regression(149/999): loss=0.5949703530425529\n",
      "Log Regression(150/999): loss=0.5978411484506425\n",
      "Log Regression(151/999): loss=0.5956906739514611\n",
      "Log Regression(152/999): loss=0.593935217791761\n",
      "Log Regression(153/999): loss=0.593808436492457\n",
      "Log Regression(154/999): loss=0.5937952626272387\n",
      "Log Regression(155/999): loss=0.5933869395616854\n",
      "Log Regression(156/999): loss=0.5934076412287363\n",
      "Log Regression(157/999): loss=0.593178529153832\n",
      "Log Regression(158/999): loss=0.5930800756199166\n",
      "Log Regression(159/999): loss=0.5926844205056667\n",
      "Log Regression(160/999): loss=0.5924056161117319\n",
      "Log Regression(161/999): loss=0.5924096282168129\n",
      "Log Regression(162/999): loss=0.5970354113685663\n",
      "Log Regression(163/999): loss=0.5923004518807595\n",
      "Log Regression(164/999): loss=0.5928918596344223\n",
      "Log Regression(165/999): loss=0.5927899252739527\n",
      "Log Regression(166/999): loss=0.5930877170163263\n",
      "Log Regression(167/999): loss=0.5952017314420811\n",
      "Log Regression(168/999): loss=0.5921576856934041\n",
      "Log Regression(169/999): loss=0.593801310836588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(170/999): loss=0.5915051709336991\n",
      "Log Regression(171/999): loss=0.5939238873924769\n",
      "Log Regression(172/999): loss=0.5906280579767912\n",
      "Log Regression(173/999): loss=0.5901596642237159\n",
      "Log Regression(174/999): loss=0.5943488381869433\n",
      "Log Regression(175/999): loss=0.5960959092662437\n",
      "Log Regression(176/999): loss=0.5896919798954876\n",
      "Log Regression(177/999): loss=0.589649543168129\n",
      "Log Regression(178/999): loss=0.5950302151935393\n",
      "Log Regression(179/999): loss=0.5894572805837844\n",
      "Log Regression(180/999): loss=0.590072520916506\n",
      "Log Regression(181/999): loss=0.5915200875001166\n",
      "Log Regression(182/999): loss=0.58902674952098\n",
      "Log Regression(183/999): loss=0.5889047512984115\n",
      "Log Regression(184/999): loss=0.5888224680001533\n",
      "Log Regression(185/999): loss=0.5891209318101802\n",
      "Log Regression(186/999): loss=0.5896810116957186\n",
      "Log Regression(187/999): loss=0.5889089521623677\n",
      "Log Regression(188/999): loss=0.5891032241094253\n",
      "Log Regression(189/999): loss=0.5886109934198536\n",
      "Log Regression(190/999): loss=0.5880214426398397\n",
      "Log Regression(191/999): loss=0.5886951685041282\n",
      "Log Regression(192/999): loss=0.5894914131157799\n",
      "Log Regression(193/999): loss=0.5898831847849638\n",
      "Log Regression(194/999): loss=0.5872290539122896\n",
      "Log Regression(195/999): loss=0.5867720426853535\n",
      "Log Regression(196/999): loss=0.5873409326316212\n",
      "Log Regression(197/999): loss=0.5878600563901871\n",
      "Log Regression(198/999): loss=0.5871094371896597\n",
      "Log Regression(199/999): loss=0.588077734793783\n",
      "Log Regression(200/999): loss=0.5869110927670741\n",
      "Log Regression(201/999): loss=0.5884439267277135\n",
      "Log Regression(202/999): loss=0.5861407278846039\n",
      "Log Regression(203/999): loss=0.5859044321797019\n",
      "Log Regression(204/999): loss=0.5858244503129391\n",
      "Log Regression(205/999): loss=0.5874489461055019\n",
      "Log Regression(206/999): loss=0.588413147971511\n",
      "Log Regression(207/999): loss=0.587064329994915\n",
      "Log Regression(208/999): loss=0.5879648125251064\n",
      "Log Regression(209/999): loss=0.5887881032493026\n",
      "Log Regression(210/999): loss=0.5850990339643617\n",
      "Log Regression(211/999): loss=0.589480588297604\n",
      "Log Regression(212/999): loss=0.5902021463039292\n",
      "Log Regression(213/999): loss=0.5848670624994649\n",
      "Log Regression(214/999): loss=0.5845057154453729\n",
      "Log Regression(215/999): loss=0.5837705986002648\n",
      "Log Regression(216/999): loss=0.5852143835000966\n",
      "Log Regression(217/999): loss=0.5841237679675662\n",
      "Log Regression(218/999): loss=0.5861959326743392\n",
      "Log Regression(219/999): loss=0.5850196433229407\n",
      "Log Regression(220/999): loss=0.5842814486584217\n",
      "Log Regression(221/999): loss=0.5836314974720465\n",
      "Log Regression(222/999): loss=0.5858266724429579\n",
      "Log Regression(223/999): loss=0.5830401061713842\n",
      "Log Regression(224/999): loss=0.5831890162316745\n",
      "Log Regression(225/999): loss=0.589104423386478\n",
      "Log Regression(226/999): loss=0.5834629455567827\n",
      "Log Regression(227/999): loss=0.5851274603818943\n",
      "Log Regression(228/999): loss=0.5829628835229276\n",
      "Log Regression(229/999): loss=0.5871520896547563\n",
      "Log Regression(230/999): loss=0.5827614923420977\n",
      "Log Regression(231/999): loss=0.5848416505298412\n",
      "Log Regression(232/999): loss=0.5833733598270298\n",
      "Log Regression(233/999): loss=0.5824273736322206\n",
      "Log Regression(234/999): loss=0.5820414281666298\n",
      "Log Regression(235/999): loss=0.5845239812682191\n",
      "Log Regression(236/999): loss=0.5820109651175829\n",
      "Log Regression(237/999): loss=0.5842837060963647\n",
      "Log Regression(238/999): loss=0.5909144442205665\n",
      "Log Regression(239/999): loss=0.5809005282829299\n",
      "Log Regression(240/999): loss=0.5808317046139783\n",
      "Log Regression(241/999): loss=0.5811078991757214\n",
      "Log Regression(242/999): loss=0.5804940729261059\n",
      "Log Regression(243/999): loss=0.5825187200923653\n",
      "Log Regression(244/999): loss=0.5846344518198987\n",
      "Log Regression(245/999): loss=0.5841830727737279\n",
      "Log Regression(246/999): loss=0.5799591737634363\n",
      "Log Regression(247/999): loss=0.5798847928809863\n",
      "Log Regression(248/999): loss=0.57976631728572\n",
      "Log Regression(249/999): loss=0.5796889259739161\n",
      "Log Regression(250/999): loss=0.5811050391492785\n",
      "Log Regression(251/999): loss=0.5914649808705624\n",
      "Log Regression(252/999): loss=0.5796615639780612\n",
      "Log Regression(253/999): loss=0.5791771266938791\n",
      "Log Regression(254/999): loss=0.5791652781435593\n",
      "Log Regression(255/999): loss=0.5787734916203947\n",
      "Log Regression(256/999): loss=0.5788346414145468\n",
      "Log Regression(257/999): loss=0.5829317735025358\n",
      "Log Regression(258/999): loss=0.5793862860826372\n",
      "Log Regression(259/999): loss=0.5823465775027431\n",
      "Log Regression(260/999): loss=0.5794455454676362\n",
      "Log Regression(261/999): loss=0.5850148916968794\n",
      "Log Regression(262/999): loss=0.5792858304089188\n",
      "Log Regression(263/999): loss=0.580078145678462\n",
      "Log Regression(264/999): loss=0.581091497776906\n",
      "Log Regression(265/999): loss=0.5780591058680371\n",
      "Log Regression(266/999): loss=0.5784528354445115\n",
      "Log Regression(267/999): loss=0.5781189970356447\n",
      "Log Regression(268/999): loss=0.578368381687871\n",
      "Log Regression(269/999): loss=0.5779044163732473\n",
      "Log Regression(270/999): loss=0.5775122926150185\n",
      "Log Regression(271/999): loss=0.5771405602838204\n",
      "Log Regression(272/999): loss=0.5784936946588022\n",
      "Log Regression(273/999): loss=0.5772469193932508\n",
      "Log Regression(274/999): loss=0.5781743302317622\n",
      "Log Regression(275/999): loss=0.5767727463348897\n",
      "Log Regression(276/999): loss=0.5766411473854308\n",
      "Log Regression(277/999): loss=0.576331593118259\n",
      "Log Regression(278/999): loss=0.5762014217082365\n",
      "Log Regression(279/999): loss=0.5760497375336838\n",
      "Log Regression(280/999): loss=0.5758006528801549\n",
      "Log Regression(281/999): loss=0.5756583084531668\n",
      "Log Regression(282/999): loss=0.5780378321828585\n",
      "Log Regression(283/999): loss=0.580755836807886\n",
      "Log Regression(284/999): loss=0.5827080192763365\n",
      "Log Regression(285/999): loss=0.5777837602489725\n",
      "Log Regression(286/999): loss=0.5756513945001545\n",
      "Log Regression(287/999): loss=0.5750696408449344\n",
      "Log Regression(288/999): loss=0.5750516086683599\n",
      "Log Regression(289/999): loss=0.5754248631408615\n",
      "Log Regression(290/999): loss=0.5746035095302598\n",
      "Log Regression(291/999): loss=0.5746080608497086\n",
      "Log Regression(292/999): loss=0.5747790212674933\n",
      "Log Regression(293/999): loss=0.5743667582897879\n",
      "Log Regression(294/999): loss=0.5761023568737159\n",
      "Log Regression(295/999): loss=0.5749026463107485\n",
      "Log Regression(296/999): loss=0.5754084232288953\n",
      "Log Regression(297/999): loss=0.5770827057633086\n",
      "Log Regression(298/999): loss=0.5762048209193656\n",
      "Log Regression(299/999): loss=0.5750366306619215\n",
      "Log Regression(300/999): loss=0.574096854542756\n",
      "Log Regression(301/999): loss=0.574092374296004\n",
      "Log Regression(302/999): loss=0.5817802201030443\n",
      "Log Regression(303/999): loss=0.5810484318348033\n",
      "Log Regression(304/999): loss=0.586794833583011\n",
      "Log Regression(305/999): loss=0.5792993748802182\n",
      "Log Regression(306/999): loss=0.5855818936533271\n",
      "Log Regression(307/999): loss=0.5784432179090142\n",
      "Log Regression(308/999): loss=0.5747126806933367\n",
      "Log Regression(309/999): loss=0.5738137964739187\n",
      "Log Regression(310/999): loss=0.5747150719069811\n",
      "Log Regression(311/999): loss=0.5728901948867238\n",
      "Log Regression(312/999): loss=0.5725239888991873\n",
      "Log Regression(313/999): loss=0.5725754690232525\n",
      "Log Regression(314/999): loss=0.5724487490853057\n",
      "Log Regression(315/999): loss=0.5737268236122568\n",
      "Log Regression(316/999): loss=0.574652753483545\n",
      "Log Regression(317/999): loss=0.5721245863965115\n",
      "Log Regression(318/999): loss=0.5721746562313366\n",
      "Log Regression(319/999): loss=0.5737172821710207\n",
      "Log Regression(320/999): loss=0.5743398909560867\n",
      "Log Regression(321/999): loss=0.5805387364639372\n",
      "Log Regression(322/999): loss=0.5720508283890465\n",
      "Log Regression(323/999): loss=0.5714618930770189\n",
      "Log Regression(324/999): loss=0.5712920972478905\n",
      "Log Regression(325/999): loss=0.5714878736291296\n",
      "Log Regression(326/999): loss=0.5712524250921546\n",
      "Log Regression(327/999): loss=0.5737455618266247\n",
      "Log Regression(328/999): loss=0.5723128172164788\n",
      "Log Regression(329/999): loss=0.5710141948765279\n",
      "Log Regression(330/999): loss=0.574946008628019\n",
      "Log Regression(331/999): loss=0.5741964132733556\n",
      "Log Regression(332/999): loss=0.5766781905920193\n",
      "Log Regression(333/999): loss=0.5707013360491989\n",
      "Log Regression(334/999): loss=0.5709673278492463\n",
      "Log Regression(335/999): loss=0.5704454780800652\n",
      "Log Regression(336/999): loss=0.5708075393306787\n",
      "Log Regression(337/999): loss=0.5704340506735106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(338/999): loss=0.5701382928882396\n",
      "Log Regression(339/999): loss=0.5701251333169712\n",
      "Log Regression(340/999): loss=0.5707901798363242\n",
      "Log Regression(341/999): loss=0.5708013026778506\n",
      "Log Regression(342/999): loss=0.5717623213177188\n",
      "Log Regression(343/999): loss=0.5706446069651603\n",
      "Log Regression(344/999): loss=0.573073151068592\n",
      "Log Regression(345/999): loss=0.5779157393241955\n",
      "Log Regression(346/999): loss=0.5703947314615396\n",
      "Log Regression(347/999): loss=0.5698723167569223\n",
      "Log Regression(348/999): loss=0.5743860894859782\n",
      "Log Regression(349/999): loss=0.5719225424657641\n",
      "Log Regression(350/999): loss=0.5722172775796193\n",
      "Log Regression(351/999): loss=0.5767199628208879\n",
      "Log Regression(352/999): loss=0.5700351359791782\n",
      "Log Regression(353/999): loss=0.5714949778286968\n",
      "Log Regression(354/999): loss=0.5690895144688847\n",
      "Log Regression(355/999): loss=0.5691860236518651\n",
      "Log Regression(356/999): loss=0.5699299546430547\n",
      "Log Regression(357/999): loss=0.5689672542222726\n",
      "Log Regression(358/999): loss=0.5693715791526627\n",
      "Log Regression(359/999): loss=0.5699690028871618\n",
      "Log Regression(360/999): loss=0.5694213202563189\n",
      "Log Regression(361/999): loss=0.5694269783208171\n",
      "Log Regression(362/999): loss=0.5751241673101548\n",
      "Log Regression(363/999): loss=0.5708584682596581\n",
      "Log Regression(364/999): loss=0.5698767871663133\n",
      "Log Regression(365/999): loss=0.5682582531944291\n",
      "Log Regression(366/999): loss=0.5686525492632583\n",
      "Log Regression(367/999): loss=0.5681074276909839\n",
      "Log Regression(368/999): loss=0.5683545887947368\n",
      "Log Regression(369/999): loss=0.5694488255853399\n",
      "Log Regression(370/999): loss=0.5682637011602021\n",
      "Log Regression(371/999): loss=0.5691881707970715\n",
      "Log Regression(372/999): loss=0.5681120611640579\n",
      "Log Regression(373/999): loss=0.5682154731144561\n",
      "Log Regression(374/999): loss=0.571110864521577\n",
      "Log Regression(375/999): loss=0.5743432290642828\n",
      "Log Regression(376/999): loss=0.5732884343702237\n",
      "Log Regression(377/999): loss=0.5736339471481947\n",
      "Log Regression(378/999): loss=0.5790393058718891\n",
      "Log Regression(379/999): loss=0.5683308042570485\n",
      "Log Regression(380/999): loss=0.567828873222318\n",
      "Log Regression(381/999): loss=0.5678865468196196\n",
      "Log Regression(382/999): loss=0.5705201445545117\n",
      "Log Regression(383/999): loss=0.5704887968607313\n",
      "Log Regression(384/999): loss=0.5685207116645077\n",
      "Log Regression(385/999): loss=0.5699927988253588\n",
      "Log Regression(386/999): loss=0.5718255931360704\n",
      "Log Regression(387/999): loss=0.5694303304541078\n",
      "Log Regression(388/999): loss=0.5710031734190251\n",
      "Log Regression(389/999): loss=0.572087253703417\n",
      "Log Regression(390/999): loss=0.5691075516285494\n",
      "Log Regression(391/999): loss=0.5679161067158947\n",
      "Log Regression(392/999): loss=0.5665300088124631\n",
      "Log Regression(393/999): loss=0.5698058164574186\n",
      "Log Regression(394/999): loss=0.566558295537322\n",
      "Log Regression(395/999): loss=0.5675964309113648\n",
      "Log Regression(396/999): loss=0.5725121416029675\n",
      "Log Regression(397/999): loss=0.5675628757252449\n",
      "Log Regression(398/999): loss=0.5661475981703755\n",
      "Log Regression(399/999): loss=0.5700204773796946\n",
      "Log Regression(400/999): loss=0.5690469136900375\n",
      "Log Regression(401/999): loss=0.566515091898361\n",
      "Log Regression(402/999): loss=0.5715980738061054\n",
      "Log Regression(403/999): loss=0.5711415370893989\n",
      "Log Regression(404/999): loss=0.565789927975955\n",
      "Log Regression(405/999): loss=0.5657786342971307\n",
      "Log Regression(406/999): loss=0.5649632985200667\n",
      "Log Regression(407/999): loss=0.5649020749930241\n",
      "Log Regression(408/999): loss=0.5677067761479683\n",
      "Log Regression(409/999): loss=0.5719817617304778\n",
      "Log Regression(410/999): loss=0.5661199935482859\n",
      "Log Regression(411/999): loss=0.5646278066856112\n",
      "Log Regression(412/999): loss=0.5654086072632911\n",
      "Log Regression(413/999): loss=0.564363202305347\n",
      "Log Regression(414/999): loss=0.5643263666587078\n",
      "Log Regression(415/999): loss=0.5643737310248932\n",
      "Log Regression(416/999): loss=0.5645801751692597\n",
      "Log Regression(417/999): loss=0.5642827551834972\n",
      "Log Regression(418/999): loss=0.5642881801441255\n",
      "Log Regression(419/999): loss=0.5653808620329744\n",
      "Log Regression(420/999): loss=0.5644272246850458\n",
      "Log Regression(421/999): loss=0.5642581124679759\n",
      "Log Regression(422/999): loss=0.5641293159381965\n",
      "Log Regression(423/999): loss=0.5719940117728769\n",
      "Log Regression(424/999): loss=0.5659264236359371\n",
      "Log Regression(425/999): loss=0.5713595272695137\n",
      "Log Regression(426/999): loss=0.5644075440042137\n",
      "Log Regression(427/999): loss=0.5637986956265315\n",
      "Log Regression(428/999): loss=0.5649777337288809\n",
      "Log Regression(429/999): loss=0.5683282874176133\n",
      "Log Regression(430/999): loss=0.5648518144285972\n",
      "Log Regression(431/999): loss=0.5635631161079703\n",
      "Log Regression(432/999): loss=0.5643919812149932\n",
      "Log Regression(433/999): loss=0.5636452125513356\n",
      "Log Regression(434/999): loss=0.563910136125782\n",
      "Log Regression(435/999): loss=0.5679798712615253\n",
      "Log Regression(436/999): loss=0.5632526749087332\n",
      "Log Regression(437/999): loss=0.5634652762560504\n",
      "Log Regression(438/999): loss=0.5632458274446526\n",
      "Log Regression(439/999): loss=0.5691531010640669\n",
      "Log Regression(440/999): loss=0.5636402382341419\n",
      "Log Regression(441/999): loss=0.5642377024327767\n",
      "Log Regression(442/999): loss=0.566492144035065\n",
      "Log Regression(443/999): loss=0.5654040742937552\n",
      "Log Regression(444/999): loss=0.5700922557258142\n",
      "Log Regression(445/999): loss=0.5697550181564536\n",
      "Log Regression(446/999): loss=0.5697458404680323\n",
      "Log Regression(447/999): loss=0.5671250637878577\n",
      "Log Regression(448/999): loss=0.5634143006270617\n",
      "Log Regression(449/999): loss=0.5659544991723628\n",
      "Log Regression(450/999): loss=0.562728052333902\n",
      "Log Regression(451/999): loss=0.5627942281373471\n",
      "Log Regression(452/999): loss=0.5621624834448962\n",
      "Log Regression(453/999): loss=0.5638739223485298\n",
      "Log Regression(454/999): loss=0.5620835284769834\n",
      "Log Regression(455/999): loss=0.5659201400929343\n",
      "Log Regression(456/999): loss=0.5619377238024658\n",
      "Log Regression(457/999): loss=0.5620292361342908\n",
      "Log Regression(458/999): loss=0.5617504856148313\n",
      "Log Regression(459/999): loss=0.5619372220930559\n",
      "Log Regression(460/999): loss=0.5624144149670176\n",
      "Log Regression(461/999): loss=0.5614728314973169\n",
      "Log Regression(462/999): loss=0.5658668276646025\n",
      "Log Regression(463/999): loss=0.5614625080911498\n",
      "Log Regression(464/999): loss=0.5614121092625525\n",
      "Log Regression(465/999): loss=0.5630719899486489\n",
      "Log Regression(466/999): loss=0.563101137893247\n",
      "Log Regression(467/999): loss=0.5614729010035495\n",
      "Log Regression(468/999): loss=0.5623190259346941\n",
      "Log Regression(469/999): loss=0.5618565418531263\n",
      "Log Regression(470/999): loss=0.5628244416835956\n",
      "Log Regression(471/999): loss=0.5664120625424405\n",
      "Log Regression(472/999): loss=0.5671719491044753\n",
      "Log Regression(473/999): loss=0.5658551933811496\n",
      "Log Regression(474/999): loss=0.5664761961250686\n",
      "Log Regression(475/999): loss=0.5706814041620889\n",
      "Log Regression(476/999): loss=0.571945128397815\n",
      "Log Regression(477/999): loss=0.5720751321319554\n",
      "Log Regression(478/999): loss=0.5735883647445376\n",
      "Log Regression(479/999): loss=0.5647804701602686\n",
      "Log Regression(480/999): loss=0.562477835770479\n",
      "Log Regression(481/999): loss=0.5614414806913932\n",
      "Log Regression(482/999): loss=0.562484348110354\n",
      "Log Regression(483/999): loss=0.5639168718216091\n",
      "Log Regression(484/999): loss=0.5676874884377655\n",
      "Log Regression(485/999): loss=0.5643175164660098\n",
      "Log Regression(486/999): loss=0.5657034658988659\n",
      "Log Regression(487/999): loss=0.565861891083804\n",
      "Log Regression(488/999): loss=0.5610606477277508\n",
      "Log Regression(489/999): loss=0.5601431234378724\n",
      "Log Regression(490/999): loss=0.5603119926908672\n",
      "Log Regression(491/999): loss=0.5608295883127037\n",
      "Log Regression(492/999): loss=0.5608542798802701\n",
      "Log Regression(493/999): loss=0.5611264645156948\n",
      "Log Regression(494/999): loss=0.5609849491887269\n",
      "Log Regression(495/999): loss=0.5598167287080773\n",
      "Log Regression(496/999): loss=0.5618829185550223\n",
      "Log Regression(497/999): loss=0.5603405132892535\n",
      "Log Regression(498/999): loss=0.5602459820488367\n",
      "Log Regression(499/999): loss=0.5604909262360616\n",
      "Log Regression(500/999): loss=0.5606159477810462\n",
      "Log Regression(501/999): loss=0.5652411952113733\n",
      "Log Regression(502/999): loss=0.5599282843628205\n",
      "Log Regression(503/999): loss=0.5608461352910793\n",
      "Log Regression(504/999): loss=0.5612766976781003\n",
      "Log Regression(505/999): loss=0.5621893859375694\n",
      "Log Regression(506/999): loss=0.5605515852810486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(507/999): loss=0.5607570180991815\n",
      "Log Regression(508/999): loss=0.5619392158802953\n",
      "Log Regression(509/999): loss=0.5627063138593864\n",
      "Log Regression(510/999): loss=0.5643456878062221\n",
      "Log Regression(511/999): loss=0.5604185211714874\n",
      "Log Regression(512/999): loss=0.5623241880799337\n",
      "Log Regression(513/999): loss=0.5591746220674955\n",
      "Log Regression(514/999): loss=0.5622710923512323\n",
      "Log Regression(515/999): loss=0.5584849898664467\n",
      "Log Regression(516/999): loss=0.5589054217593626\n",
      "Log Regression(517/999): loss=0.5592891110398046\n",
      "Log Regression(518/999): loss=0.5582230310448656\n",
      "Log Regression(519/999): loss=0.5582808437024969\n",
      "Log Regression(520/999): loss=0.5585401921707166\n",
      "Log Regression(521/999): loss=0.5590105769651184\n",
      "Log Regression(522/999): loss=0.5587670326055649\n",
      "Log Regression(523/999): loss=0.5623199732221874\n",
      "Log Regression(524/999): loss=0.5580024299331711\n",
      "Log Regression(525/999): loss=0.5588581635147677\n",
      "Log Regression(526/999): loss=0.5579811489792861\n",
      "Log Regression(527/999): loss=0.5581231455177794\n",
      "Log Regression(528/999): loss=0.5588567130800514\n",
      "Log Regression(529/999): loss=0.5585970483152172\n",
      "Log Regression(530/999): loss=0.5598538891537201\n",
      "Log Regression(531/999): loss=0.5576975397621622\n",
      "Log Regression(532/999): loss=0.5593397281820898\n",
      "Log Regression(533/999): loss=0.5639663072862507\n",
      "Log Regression(534/999): loss=0.5628896788138225\n",
      "Log Regression(535/999): loss=0.5593532205178716\n",
      "Log Regression(536/999): loss=0.5573755652213248\n",
      "Log Regression(537/999): loss=0.5580736703948365\n",
      "Log Regression(538/999): loss=0.5671958024797376\n",
      "Log Regression(539/999): loss=0.5602917692183005\n",
      "Log Regression(540/999): loss=0.5597222555277883\n",
      "Log Regression(541/999): loss=0.5579868189984039\n",
      "Log Regression(542/999): loss=0.5572475698310917\n",
      "Log Regression(543/999): loss=0.5571177372850575\n",
      "Log Regression(544/999): loss=0.5621758694719289\n",
      "Log Regression(545/999): loss=0.5591927177744417\n",
      "Log Regression(546/999): loss=0.5583792199250632\n",
      "Log Regression(547/999): loss=0.5568254640232694\n",
      "Log Regression(548/999): loss=0.5568545428064903\n",
      "Log Regression(549/999): loss=0.5573869536579157\n",
      "Log Regression(550/999): loss=0.5582461731413781\n",
      "Log Regression(551/999): loss=0.5584767732932623\n",
      "Log Regression(552/999): loss=0.557309486532683\n",
      "Log Regression(553/999): loss=0.5564779037940647\n",
      "Log Regression(554/999): loss=0.5636282356812226\n",
      "Log Regression(555/999): loss=0.563344252156615\n",
      "Log Regression(556/999): loss=0.5615814958308313\n",
      "Log Regression(557/999): loss=0.5564547308219893\n",
      "Log Regression(558/999): loss=0.5581798290989536\n",
      "Log Regression(559/999): loss=0.5562759014280734\n",
      "Log Regression(560/999): loss=0.556311090291703\n",
      "Log Regression(561/999): loss=0.5562174083706775\n",
      "Log Regression(562/999): loss=0.5578536764377181\n",
      "Log Regression(563/999): loss=0.556175625889753\n",
      "Log Regression(564/999): loss=0.556357106179899\n",
      "Log Regression(565/999): loss=0.5561784835625085\n",
      "Log Regression(566/999): loss=0.557740998823728\n",
      "Log Regression(567/999): loss=0.5564063132882981\n",
      "Log Regression(568/999): loss=0.5591891075223755\n",
      "Log Regression(569/999): loss=0.5693995704421513\n",
      "Log Regression(570/999): loss=0.5641675007891361\n",
      "Log Regression(571/999): loss=0.5621576028413704\n",
      "Log Regression(572/999): loss=0.5574896083294518\n",
      "Log Regression(573/999): loss=0.5556863642422554\n",
      "Log Regression(574/999): loss=0.5559578754347934\n",
      "Log Regression(575/999): loss=0.5559750510979357\n",
      "Log Regression(576/999): loss=0.5557485213057982\n",
      "Log Regression(577/999): loss=0.5556212143443306\n",
      "Log Regression(578/999): loss=0.5560483903677523\n",
      "Log Regression(579/999): loss=0.5580938718690251\n",
      "Log Regression(580/999): loss=0.5606197025082411\n",
      "Log Regression(581/999): loss=0.5552686613899765\n",
      "Log Regression(582/999): loss=0.5557375045857147\n",
      "Log Regression(583/999): loss=0.5557221414869513\n",
      "Log Regression(584/999): loss=0.5562020095079006\n",
      "Log Regression(585/999): loss=0.5565203130427097\n",
      "Log Regression(586/999): loss=0.5575429965530667\n",
      "Log Regression(587/999): loss=0.5584312765982816\n",
      "Log Regression(588/999): loss=0.5590282852345588\n",
      "Log Regression(589/999): loss=0.5553129277769592\n",
      "Log Regression(590/999): loss=0.5551576496899445\n",
      "Log Regression(591/999): loss=0.5551380034554836\n",
      "Log Regression(592/999): loss=0.555242059573398\n",
      "Log Regression(593/999): loss=0.5623268682974735\n",
      "Log Regression(594/999): loss=0.555479805660239\n",
      "Log Regression(595/999): loss=0.5548410930826969\n",
      "Log Regression(596/999): loss=0.5558148240135306\n",
      "Log Regression(597/999): loss=0.5560016262348093\n",
      "Log Regression(598/999): loss=0.5547133350099654\n",
      "Log Regression(599/999): loss=0.5546155374745678\n",
      "Log Regression(600/999): loss=0.5550936203556088\n",
      "Log Regression(601/999): loss=0.5563184989122192\n",
      "Log Regression(602/999): loss=0.554899449873231\n",
      "Log Regression(603/999): loss=0.5544123054834266\n",
      "Log Regression(604/999): loss=0.5655971052958588\n",
      "Log Regression(605/999): loss=0.554393989611799\n",
      "Log Regression(606/999): loss=0.5548525892446139\n",
      "Log Regression(607/999): loss=0.5552215619005667\n",
      "Log Regression(608/999): loss=0.5544207588647956\n",
      "Log Regression(609/999): loss=0.5602911753794761\n",
      "Log Regression(610/999): loss=0.5595924368837263\n",
      "Log Regression(611/999): loss=0.5559211024827563\n",
      "Log Regression(612/999): loss=0.5544163198986879\n",
      "Log Regression(613/999): loss=0.5542160028001906\n",
      "Log Regression(614/999): loss=0.5553240154738628\n",
      "Log Regression(615/999): loss=0.554169280737221\n",
      "Log Regression(616/999): loss=0.554073411840654\n",
      "Log Regression(617/999): loss=0.5540883255167663\n",
      "Log Regression(618/999): loss=0.5540836311840569\n",
      "Log Regression(619/999): loss=0.5558065815081086\n",
      "Log Regression(620/999): loss=0.5538228356572769\n",
      "Log Regression(621/999): loss=0.5565147663696444\n",
      "Log Regression(622/999): loss=0.5566010386232226\n",
      "Log Regression(623/999): loss=0.5539738355323237\n",
      "Log Regression(624/999): loss=0.5540613756788351\n",
      "Log Regression(625/999): loss=0.5538190571305989\n",
      "Log Regression(626/999): loss=0.5538123646459078\n",
      "Log Regression(627/999): loss=0.5535496246716608\n",
      "Log Regression(628/999): loss=0.554807783801149\n",
      "Log Regression(629/999): loss=0.5535260358282111\n",
      "Log Regression(630/999): loss=0.5535080671238786\n",
      "Log Regression(631/999): loss=0.5551180373197533\n",
      "Log Regression(632/999): loss=0.5605549779592571\n",
      "Log Regression(633/999): loss=0.5552910224549616\n",
      "Log Regression(634/999): loss=0.553908362981706\n",
      "Log Regression(635/999): loss=0.5544451401496557\n",
      "Log Regression(636/999): loss=0.5609840650966983\n",
      "Log Regression(637/999): loss=0.5580176753378348\n",
      "Log Regression(638/999): loss=0.5542239599290348\n",
      "Log Regression(639/999): loss=0.5540874138081224\n",
      "Log Regression(640/999): loss=0.5541028392961158\n",
      "Log Regression(641/999): loss=0.5530369331990733\n",
      "Log Regression(642/999): loss=0.5533606288157391\n",
      "Log Regression(643/999): loss=0.5531773802541902\n",
      "Log Regression(644/999): loss=0.552906726036785\n",
      "Log Regression(645/999): loss=0.5533061182727689\n",
      "Log Regression(646/999): loss=0.5540772372481926\n",
      "Log Regression(647/999): loss=0.5530380548329273\n",
      "Log Regression(648/999): loss=0.5538106595466514\n",
      "Log Regression(649/999): loss=0.5585081126806941\n",
      "Log Regression(650/999): loss=0.5551662793179108\n",
      "Log Regression(651/999): loss=0.5559593092930067\n",
      "Log Regression(652/999): loss=0.5561130853965859\n",
      "Log Regression(653/999): loss=0.5580555902454181\n",
      "Log Regression(654/999): loss=0.5588761703870457\n",
      "Log Regression(655/999): loss=0.5529492603382034\n",
      "Log Regression(656/999): loss=0.5557040300961059\n",
      "Log Regression(657/999): loss=0.5525521466329146\n",
      "Log Regression(658/999): loss=0.552834584412157\n",
      "Log Regression(659/999): loss=0.5523357571404451\n",
      "Log Regression(660/999): loss=0.5568262169160906\n",
      "Log Regression(661/999): loss=0.5523738588905363\n",
      "Log Regression(662/999): loss=0.5522372913612688\n",
      "Log Regression(663/999): loss=0.5522860378448937\n",
      "Log Regression(664/999): loss=0.5537078260408127\n",
      "Log Regression(665/999): loss=0.5523530812854079\n",
      "Log Regression(666/999): loss=0.552275233498199\n",
      "Log Regression(667/999): loss=0.5522468948906337\n",
      "Log Regression(668/999): loss=0.5524864977251083\n",
      "Log Regression(669/999): loss=0.5553557569908113\n",
      "Log Regression(670/999): loss=0.5519414287448263\n",
      "Log Regression(671/999): loss=0.5536588405721489\n",
      "Log Regression(672/999): loss=0.5542468527647297\n",
      "Log Regression(673/999): loss=0.5542738873562698\n",
      "Log Regression(674/999): loss=0.556566135222316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(675/999): loss=0.5527336636447413\n",
      "Log Regression(676/999): loss=0.5552969173125292\n",
      "Log Regression(677/999): loss=0.5542067800893327\n",
      "Log Regression(678/999): loss=0.5543601307347544\n",
      "Log Regression(679/999): loss=0.5535750990547301\n",
      "Log Regression(680/999): loss=0.5523779110978952\n",
      "Log Regression(681/999): loss=0.5515899762118474\n",
      "Log Regression(682/999): loss=0.5518208068346944\n",
      "Log Regression(683/999): loss=0.5514279434698005\n",
      "Log Regression(684/999): loss=0.5550626741722032\n",
      "Log Regression(685/999): loss=0.5557215095598765\n",
      "Log Regression(686/999): loss=0.5542149533567488\n",
      "Log Regression(687/999): loss=0.5532555842681216\n",
      "Log Regression(688/999): loss=0.5529341945975622\n",
      "Log Regression(689/999): loss=0.5516851372529357\n",
      "Log Regression(690/999): loss=0.5538954964746738\n",
      "Log Regression(691/999): loss=0.5553461310563369\n",
      "Log Regression(692/999): loss=0.5511288620602012\n",
      "Log Regression(693/999): loss=0.5511081159670322\n",
      "Log Regression(694/999): loss=0.551206164490814\n",
      "Log Regression(695/999): loss=0.5512453467288346\n",
      "Log Regression(696/999): loss=0.551159145531334\n",
      "Log Regression(697/999): loss=0.5570886003357058\n",
      "Log Regression(698/999): loss=0.5517215635098753\n",
      "Log Regression(699/999): loss=0.5509818138843027\n",
      "Log Regression(700/999): loss=0.5534097778651921\n",
      "Log Regression(701/999): loss=0.5509463178610137\n",
      "Log Regression(702/999): loss=0.5509643935278983\n",
      "Log Regression(703/999): loss=0.5508112913202082\n",
      "Log Regression(704/999): loss=0.5509285560422098\n",
      "Log Regression(705/999): loss=0.5572274088728266\n",
      "Log Regression(706/999): loss=0.5520243500221501\n",
      "Log Regression(707/999): loss=0.5528222339474116\n",
      "Log Regression(708/999): loss=0.5507179765445601\n",
      "Log Regression(709/999): loss=0.5524235604867039\n",
      "Log Regression(710/999): loss=0.5510110884629833\n",
      "Log Regression(711/999): loss=0.55069185463825\n",
      "Log Regression(712/999): loss=0.5508320112811586\n",
      "Log Regression(713/999): loss=0.551890599839081\n",
      "Log Regression(714/999): loss=0.5504882898528287\n",
      "Log Regression(715/999): loss=0.5507484876858404\n",
      "Log Regression(716/999): loss=0.5543573358122228\n",
      "Log Regression(717/999): loss=0.5551588817455542\n",
      "Log Regression(718/999): loss=0.5509366424582212\n",
      "Log Regression(719/999): loss=0.5504625897393616\n",
      "Log Regression(720/999): loss=0.5503922014116882\n",
      "Log Regression(721/999): loss=0.5502941619806614\n",
      "Log Regression(722/999): loss=0.5515928569037138\n",
      "Log Regression(723/999): loss=0.5533036310889493\n",
      "Log Regression(724/999): loss=0.551038676412669\n",
      "Log Regression(725/999): loss=0.5507054971111179\n",
      "Log Regression(726/999): loss=0.5504006539963305\n",
      "Log Regression(727/999): loss=0.5505313799326318\n",
      "Log Regression(728/999): loss=0.5507947296218618\n",
      "Log Regression(729/999): loss=0.5512096850249817\n",
      "Log Regression(730/999): loss=0.551669955524526\n",
      "Log Regression(731/999): loss=0.5520992426683784\n",
      "Log Regression(732/999): loss=0.5504879865157619\n",
      "Log Regression(733/999): loss=0.5513053457370373\n",
      "Log Regression(734/999): loss=0.5503927048831341\n",
      "Log Regression(735/999): loss=0.5504434933552079\n",
      "Log Regression(736/999): loss=0.5530745300442728\n",
      "Log Regression(737/999): loss=0.5510953135661284\n",
      "Log Regression(738/999): loss=0.5504309746118382\n",
      "Log Regression(739/999): loss=0.5499407304320065\n",
      "Log Regression(740/999): loss=0.5522755582579252\n",
      "Log Regression(741/999): loss=0.5540832660050165\n",
      "Log Regression(742/999): loss=0.5535134585359373\n",
      "Log Regression(743/999): loss=0.5508038672246113\n",
      "Log Regression(744/999): loss=0.5502997571797505\n",
      "Log Regression(745/999): loss=0.5510229135953317\n",
      "Log Regression(746/999): loss=0.5528605283763542\n",
      "Log Regression(747/999): loss=0.5527466657388924\n",
      "Log Regression(748/999): loss=0.5555411842080094\n",
      "Log Regression(749/999): loss=0.5596360317670097\n",
      "Log Regression(750/999): loss=0.5656055052470378\n",
      "Log Regression(751/999): loss=0.5546113749280472\n",
      "Log Regression(752/999): loss=0.5540654695853298\n",
      "Log Regression(753/999): loss=0.5557922826131432\n",
      "Log Regression(754/999): loss=0.5564271995674026\n",
      "Log Regression(755/999): loss=0.5514756976879285\n",
      "Log Regression(756/999): loss=0.5498554652983866\n",
      "Log Regression(757/999): loss=0.5522819437967114\n",
      "Log Regression(758/999): loss=0.5500705826723565\n",
      "Log Regression(759/999): loss=0.5500955440489893\n",
      "Log Regression(760/999): loss=0.5498552833863655\n",
      "Log Regression(761/999): loss=0.5498127138407359\n",
      "Log Regression(762/999): loss=0.5495151328639384\n",
      "Log Regression(763/999): loss=0.5497243656520595\n",
      "Log Regression(764/999): loss=0.5519670052513324\n",
      "Log Regression(765/999): loss=0.5505383594559451\n",
      "Log Regression(766/999): loss=0.549756145410093\n",
      "Log Regression(767/999): loss=0.5498774022533887\n",
      "Log Regression(768/999): loss=0.5514118888857809\n",
      "Log Regression(769/999): loss=0.5534043946849165\n",
      "Log Regression(770/999): loss=0.5492775369917713\n",
      "Log Regression(771/999): loss=0.5497291839809836\n",
      "Log Regression(772/999): loss=0.5493640950319514\n",
      "Log Regression(773/999): loss=0.5505369977219234\n",
      "Log Regression(774/999): loss=0.5494271391416872\n",
      "Log Regression(775/999): loss=0.5494035821304992\n",
      "Log Regression(776/999): loss=0.5495066764965916\n",
      "Log Regression(777/999): loss=0.5498046856855267\n",
      "Log Regression(778/999): loss=0.5499522708065484\n",
      "Log Regression(779/999): loss=0.5492419317943211\n",
      "Log Regression(780/999): loss=0.549441327486756\n",
      "Log Regression(781/999): loss=0.5496575272088506\n",
      "Log Regression(782/999): loss=0.5489870677784774\n",
      "Log Regression(783/999): loss=0.554752704369852\n",
      "Log Regression(784/999): loss=0.5512272388361469\n",
      "Log Regression(785/999): loss=0.5500113454268365\n",
      "Log Regression(786/999): loss=0.5490682406234144\n",
      "Log Regression(787/999): loss=0.5494845116858966\n",
      "Log Regression(788/999): loss=0.5543258061875733\n",
      "Log Regression(789/999): loss=0.5492506736541358\n",
      "Log Regression(790/999): loss=0.5496669510016885\n",
      "Log Regression(791/999): loss=0.5492704204240157\n",
      "Log Regression(792/999): loss=0.5499983305137849\n",
      "Log Regression(793/999): loss=0.548905767656041\n",
      "Log Regression(794/999): loss=0.5487756588612125\n",
      "Log Regression(795/999): loss=0.5490222826877857\n",
      "Log Regression(796/999): loss=0.5490861282809933\n",
      "Log Regression(797/999): loss=0.5530626289353513\n",
      "Log Regression(798/999): loss=0.5492553589516274\n",
      "Log Regression(799/999): loss=0.5532116172673391\n",
      "Log Regression(800/999): loss=0.5491048621433554\n",
      "Log Regression(801/999): loss=0.5498359859894513\n",
      "Log Regression(802/999): loss=0.5510658671886847\n",
      "Log Regression(803/999): loss=0.5512595971618465\n",
      "Log Regression(804/999): loss=0.5509132506434337\n",
      "Log Regression(805/999): loss=0.5560296480273127\n",
      "Log Regression(806/999): loss=0.5547839050780632\n",
      "Log Regression(807/999): loss=0.5564615562043211\n",
      "Log Regression(808/999): loss=0.5510176969281069\n",
      "Log Regression(809/999): loss=0.5512178580778285\n",
      "Log Regression(810/999): loss=0.5505927681614846\n",
      "Log Regression(811/999): loss=0.5511797440649941\n",
      "Log Regression(812/999): loss=0.5505064812595039\n",
      "Log Regression(813/999): loss=0.5490473482174146\n",
      "Log Regression(814/999): loss=0.5481255864309621\n",
      "Log Regression(815/999): loss=0.5531228133308148\n",
      "Log Regression(816/999): loss=0.5487922330156813\n",
      "Log Regression(817/999): loss=0.550553814113375\n",
      "Log Regression(818/999): loss=0.5528495440106737\n",
      "Log Regression(819/999): loss=0.548335102663597\n",
      "Log Regression(820/999): loss=0.5481235816755429\n",
      "Log Regression(821/999): loss=0.548062423787344\n",
      "Log Regression(822/999): loss=0.5482548279296953\n",
      "Log Regression(823/999): loss=0.5477867044718842\n",
      "Log Regression(824/999): loss=0.5480214014128106\n",
      "Log Regression(825/999): loss=0.5505798182074911\n",
      "Log Regression(826/999): loss=0.5499350823086746\n",
      "Log Regression(827/999): loss=0.5531045238332711\n",
      "Log Regression(828/999): loss=0.5493225433909986\n",
      "Log Regression(829/999): loss=0.548905903186744\n",
      "Log Regression(830/999): loss=0.5485310652252876\n",
      "Log Regression(831/999): loss=0.5482502383246024\n",
      "Log Regression(832/999): loss=0.5498705868468916\n",
      "Log Regression(833/999): loss=0.5495775414742528\n",
      "Log Regression(834/999): loss=0.5479107193347832\n",
      "Log Regression(835/999): loss=0.5478582717445362\n",
      "Log Regression(836/999): loss=0.5501287420686758\n",
      "Log Regression(837/999): loss=0.5508872732132197\n",
      "Log Regression(838/999): loss=0.54855603492351\n",
      "Log Regression(839/999): loss=0.5476557119351213\n",
      "Log Regression(840/999): loss=0.5475364423620692\n",
      "Log Regression(841/999): loss=0.5492874907967071\n",
      "Log Regression(842/999): loss=0.5494195654171684\n",
      "Log Regression(843/999): loss=0.5481958421821339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(844/999): loss=0.5478433564367267\n",
      "Log Regression(845/999): loss=0.5478944219436075\n",
      "Log Regression(846/999): loss=0.5501566725818431\n",
      "Log Regression(847/999): loss=0.547686288726132\n",
      "Log Regression(848/999): loss=0.5474699173616534\n",
      "Log Regression(849/999): loss=0.5473236431251356\n",
      "Log Regression(850/999): loss=0.5478893879304243\n",
      "Log Regression(851/999): loss=0.5476404785069625\n",
      "Log Regression(852/999): loss=0.5565461030281751\n",
      "Log Regression(853/999): loss=0.5501860171913776\n",
      "Log Regression(854/999): loss=0.5513615320212831\n",
      "Log Regression(855/999): loss=0.5474317477158329\n",
      "Log Regression(856/999): loss=0.547713117918195\n",
      "Log Regression(857/999): loss=0.5517380539693602\n",
      "Log Regression(858/999): loss=0.5489450444664251\n",
      "Log Regression(859/999): loss=0.5503649098888881\n",
      "Log Regression(860/999): loss=0.5473155699780443\n",
      "Log Regression(861/999): loss=0.5512164616506866\n",
      "Log Regression(862/999): loss=0.5487554432223954\n",
      "Log Regression(863/999): loss=0.5475626613897722\n",
      "Log Regression(864/999): loss=0.5473591810125116\n",
      "Log Regression(865/999): loss=0.5480899605080102\n",
      "Log Regression(866/999): loss=0.550955654430132\n",
      "Log Regression(867/999): loss=0.5477559304679054\n",
      "Log Regression(868/999): loss=0.5480336234840157\n",
      "Log Regression(869/999): loss=0.5479418527392891\n",
      "Log Regression(870/999): loss=0.5484028933958555\n",
      "Log Regression(871/999): loss=0.5471316396022714\n",
      "Log Regression(872/999): loss=0.5472433835075143\n",
      "Log Regression(873/999): loss=0.5471201919367272\n",
      "Log Regression(874/999): loss=0.5470692977993836\n",
      "Log Regression(875/999): loss=0.5479582325419936\n",
      "Log Regression(876/999): loss=0.5481998577167028\n",
      "Log Regression(877/999): loss=0.5469146203337101\n",
      "Log Regression(878/999): loss=0.547097606425423\n",
      "Log Regression(879/999): loss=0.5482741303428482\n",
      "Log Regression(880/999): loss=0.5468687048259607\n",
      "Log Regression(881/999): loss=0.5467402680942646\n",
      "Log Regression(882/999): loss=0.5472380091137976\n",
      "Log Regression(883/999): loss=0.5471112847493051\n",
      "Log Regression(884/999): loss=0.5470260491541242\n",
      "Log Regression(885/999): loss=0.5487674301589947\n",
      "Log Regression(886/999): loss=0.54685907865411\n",
      "Log Regression(887/999): loss=0.5465332556512863\n",
      "Log Regression(888/999): loss=0.547208767990028\n",
      "Log Regression(889/999): loss=0.5465254397631841\n",
      "Log Regression(890/999): loss=0.5468410943059475\n",
      "Log Regression(891/999): loss=0.5466020524737778\n",
      "Log Regression(892/999): loss=0.5467082256660658\n",
      "Log Regression(893/999): loss=0.5482361548089358\n",
      "Log Regression(894/999): loss=0.5464729024730708\n",
      "Log Regression(895/999): loss=0.5475350630493047\n",
      "Log Regression(896/999): loss=0.5466553921012178\n",
      "Log Regression(897/999): loss=0.5468948951905541\n",
      "Log Regression(898/999): loss=0.5486693767959702\n",
      "Log Regression(899/999): loss=0.5522857774976588\n",
      "Log Regression(900/999): loss=0.5485689945509772\n",
      "Log Regression(901/999): loss=0.5488251090681657\n",
      "Log Regression(902/999): loss=0.5470014750917259\n",
      "Log Regression(903/999): loss=0.5466371167680978\n",
      "Log Regression(904/999): loss=0.5463245990220765\n",
      "Log Regression(905/999): loss=0.5463947427732985\n",
      "Log Regression(906/999): loss=0.5467129736264495\n",
      "Log Regression(907/999): loss=0.5470544357923027\n",
      "Log Regression(908/999): loss=0.5468856981610005\n",
      "Log Regression(909/999): loss=0.5462017691508796\n",
      "Log Regression(910/999): loss=0.5461726442721824\n",
      "Log Regression(911/999): loss=0.5492689909454883\n",
      "Log Regression(912/999): loss=0.5494088578880022\n",
      "Log Regression(913/999): loss=0.5478310104062549\n",
      "Log Regression(914/999): loss=0.5461929839370178\n",
      "Log Regression(915/999): loss=0.5461539803336469\n",
      "Log Regression(916/999): loss=0.5462104568397463\n",
      "Log Regression(917/999): loss=0.5489758197643703\n",
      "Log Regression(918/999): loss=0.5477488714856577\n",
      "Log Regression(919/999): loss=0.5461088645968283\n",
      "Log Regression(920/999): loss=0.5477060112747612\n",
      "Log Regression(921/999): loss=0.5461256247396581\n",
      "Log Regression(922/999): loss=0.5466858618009997\n",
      "Log Regression(923/999): loss=0.5468990407081046\n",
      "Log Regression(924/999): loss=0.54735911713821\n",
      "Log Regression(925/999): loss=0.5461985400467547\n",
      "Log Regression(926/999): loss=0.5483078466997727\n",
      "Log Regression(927/999): loss=0.5502178226287356\n",
      "Log Regression(928/999): loss=0.546991504434501\n",
      "Log Regression(929/999): loss=0.5502398986420151\n",
      "Log Regression(930/999): loss=0.5464499762997782\n",
      "Log Regression(931/999): loss=0.5466043711847786\n",
      "Log Regression(932/999): loss=0.5522088281081966\n",
      "Log Regression(933/999): loss=0.5493434395171697\n",
      "Log Regression(934/999): loss=0.5481182736989306\n",
      "Log Regression(935/999): loss=0.5508664507354472\n",
      "Log Regression(936/999): loss=0.546981985109628\n",
      "Log Regression(937/999): loss=0.547595105637965\n",
      "Log Regression(938/999): loss=0.5505556618224333\n",
      "Log Regression(939/999): loss=0.5476017746899301\n",
      "Log Regression(940/999): loss=0.5458219126056703\n",
      "Log Regression(941/999): loss=0.5460742774504118\n",
      "Log Regression(942/999): loss=0.5483314418525694\n",
      "Log Regression(943/999): loss=0.5475197906652668\n",
      "Log Regression(944/999): loss=0.5461611334572283\n",
      "Log Regression(945/999): loss=0.5486013531119991\n",
      "Log Regression(946/999): loss=0.5457512708475376\n",
      "Log Regression(947/999): loss=0.5463599798629891\n",
      "Log Regression(948/999): loss=0.550475255413229\n",
      "Log Regression(949/999): loss=0.5459091854815098\n",
      "Log Regression(950/999): loss=0.5456490446930432\n",
      "Log Regression(951/999): loss=0.5457139322879392\n",
      "Log Regression(952/999): loss=0.5475283529671267\n",
      "Log Regression(953/999): loss=0.5478294988306281\n",
      "Log Regression(954/999): loss=0.5588313165293969\n",
      "Log Regression(955/999): loss=0.5469333871127187\n",
      "Log Regression(956/999): loss=0.5504891672558311\n",
      "Log Regression(957/999): loss=0.5464755240851777\n",
      "Log Regression(958/999): loss=0.5466330289415904\n",
      "Log Regression(959/999): loss=0.5510809876485012\n",
      "Log Regression(960/999): loss=0.5558356948433246\n",
      "Log Regression(961/999): loss=0.5531641624951077\n",
      "Log Regression(962/999): loss=0.5504754036949486\n",
      "Log Regression(963/999): loss=0.5502384926286857\n",
      "Log Regression(964/999): loss=0.5488975879587142\n",
      "Log Regression(965/999): loss=0.5456488401604883\n",
      "Log Regression(966/999): loss=0.5463127262975548\n",
      "Log Regression(967/999): loss=0.5468223189730367\n",
      "Log Regression(968/999): loss=0.5468401021328099\n",
      "Log Regression(969/999): loss=0.5456778754343758\n",
      "Log Regression(970/999): loss=0.5475219564910779\n",
      "Log Regression(971/999): loss=0.5455250270523718\n",
      "Log Regression(972/999): loss=0.5468146467100874\n",
      "Log Regression(973/999): loss=0.5505165862984879\n",
      "Log Regression(974/999): loss=0.5512527551882793\n",
      "Log Regression(975/999): loss=0.5494586790092352\n",
      "Log Regression(976/999): loss=0.5457382621217567\n",
      "Log Regression(977/999): loss=0.5453531336583625\n",
      "Log Regression(978/999): loss=0.5471436031866767\n",
      "Log Regression(979/999): loss=0.546434845974107\n",
      "Log Regression(980/999): loss=0.5462187345522618\n",
      "Log Regression(981/999): loss=0.5451823596280032\n",
      "Log Regression(982/999): loss=0.5451893300479458\n",
      "Log Regression(983/999): loss=0.5481740782799166\n",
      "Log Regression(984/999): loss=0.545893149676447\n",
      "Log Regression(985/999): loss=0.5478334352428101\n",
      "Log Regression(986/999): loss=0.5451796711558302\n",
      "Log Regression(987/999): loss=0.5454834889919206\n",
      "Log Regression(988/999): loss=0.5462762118442109\n",
      "Log Regression(989/999): loss=0.5453553687304268\n",
      "Log Regression(990/999): loss=0.548228972700524\n",
      "Log Regression(991/999): loss=0.5515108865415832\n",
      "Log Regression(992/999): loss=0.5555005827978005\n",
      "Log Regression(993/999): loss=0.552253307826009\n",
      "Log Regression(994/999): loss=0.5485792377749481\n",
      "Log Regression(995/999): loss=0.5455301065238628\n",
      "Log Regression(996/999): loss=0.5458273607980383\n",
      "Log Regression(997/999): loss=0.5477656360153292\n",
      "Log Regression(998/999): loss=0.5461441951088272\n",
      "Log Regression(999/999): loss=0.545504427571495\n"
     ]
    }
   ],
   "source": [
    "w, mse = reg_logistic_regression(y_train_lr, tX_train, lambda_, initial_w, max_iters, gamma, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.714205"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict_labels(w, tX_train)\n",
    "np.mean(y_train.reshape(-1,1)==y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71508"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict_labels(w, tX_val)\n",
    "np.mean(y_val.reshape(-1,1)==y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
