{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_processor as dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = np.arange(len(y))\n",
    "np.random.shuffle(ind)\n",
    "ind_train = ind[:int(len(ind)*0.9)]\n",
    "ind_test = ind[int(len(ind)*0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_train = tX[ind_train]\n",
    "y_train = y[ind_train]\n",
    "\n",
    "tX_val = tX[ind_test]\n",
    "y_val = y[ind_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_train_f, filler = dp.fill_nan(tX_train, nan_value=-999, method='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_val_f, _ = dp.fill_nan(tX_val, nan_value=-999, method='use_filler', filler=filler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((225000, 30), (25000, 30))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX_train.shape,tX_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill NaN. Apply LS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, mse = least_squares(y_train, tX_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1. -1.  1. ...  1. -1. -1.]\n",
      "(225000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7404266666666667"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict_labels(w, tX_train)\n",
    "print(y_pred)\n",
    "print(y_pred.shape)\n",
    "np.mean(y_train==y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.  1.  1. ... -1. -1. -1.]\n",
      "(25000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.74284"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict_labels(w, tX_val)\n",
    "print(y_pred)\n",
    "print(y_pred.shape)\n",
    "np.mean(y_val==y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001: MSE_TRAIN=0.34383321387881927 TRAIN=0.7404311111111112 VAL=0.74284\n",
      "0.001: MSE_TRAIN=0.34383321491499214 TRAIN=0.7404355555555555 VAL=0.74284\n",
      "0.01: MSE_TRAIN=0.3438332573871147 TRAIN=0.7404444444444445 VAL=0.7428\n",
      "0.1: MSE_TRAIN=0.3438334437031598 TRAIN=0.7404088888888889 VAL=0.74268\n",
      "1: MSE_TRAIN=0.34383351744321655 TRAIN=0.7404044444444444 VAL=0.74268\n",
      "10.0: MSE_TRAIN=0.3438335333866524 TRAIN=0.7403866666666666 VAL=0.74268\n",
      "100.0: MSE_TRAIN=0.34383418712302466 TRAIN=0.7403511111111111 VAL=0.74276\n",
      "1000.0: MSE_TRAIN=0.3438868882829171 TRAIN=0.7398933333333333 VAL=0.7418\n",
      "10000.0: MSE_TRAIN=0.3455044343817568 TRAIN=0.7372133333333334 VAL=0.73792\n"
     ]
    }
   ],
   "source": [
    "for l in [1e-4, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4]:\n",
    "    w, mse = ridge_regression(y_train, tX_train, l)\n",
    "    y_pred_train = predict_labels(w, tX_train)\n",
    "    y_pred_val = predict_labels(w, tX_val)\n",
    "    print(str(l)+\": MSE_TRAIN=\"+str(mse)+ \" TRAIN=\"+str(np.mean(y_train==y_pred_train))+\" VAL=\"+str(np.mean(y_val==y_pred_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Squares GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros(tX.shape[1])#np.random.normal(loc=0, scale=1, size=(tX.shape[1],1))\n",
    "max_iters = 1000\n",
    "gamma = 3e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/999): loss=0.5000000000000003\n",
      "Gradient Descent(1/999): loss=0.47514767720269957\n",
      "Gradient Descent(2/999): loss=0.4723188235681864\n",
      "Gradient Descent(3/999): loss=0.4698163342630682\n",
      "Gradient Descent(4/999): loss=0.4674265566178349\n",
      "Gradient Descent(5/999): loss=0.46514164772825245\n",
      "Gradient Descent(6/999): loss=0.4629560507024571\n",
      "Gradient Descent(7/999): loss=0.4608645305822573\n",
      "Gradient Descent(8/999): loss=0.45886213900965495\n",
      "Gradient Descent(9/999): loss=0.45694419825845206\n",
      "Gradient Descent(10/999): loss=0.45510628631722694\n",
      "Gradient Descent(11/999): loss=0.4533442227997835\n",
      "Gradient Descent(12/999): loss=0.45165405563508226\n",
      "Gradient Descent(13/999): loss=0.4500320484930448\n",
      "Gradient Descent(14/999): loss=0.4484746689051728\n",
      "Gradient Descent(15/999): loss=0.4469785770412906\n",
      "Gradient Descent(16/999): loss=0.44554061510593923\n",
      "Gradient Descent(17/999): loss=0.4441577973200208\n",
      "Gradient Descent(18/999): loss=0.44282730045524227\n",
      "Gradient Descent(19/999): loss=0.44154645489074\n",
      "Gradient Descent(20/999): loss=0.44031273616297534\n",
      "Gradient Descent(21/999): loss=0.4391237569816263\n",
      "Gradient Descent(22/999): loss=0.43797725968570755\n",
      "Gradient Descent(23/999): loss=0.43687110911559857\n",
      "Gradient Descent(24/999): loss=0.4358032858780057\n",
      "Gradient Descent(25/999): loss=0.4347718799821655\n",
      "Gradient Descent(26/999): loss=0.43377508482679294\n",
      "Gradient Descent(27/999): loss=0.4328111915184234\n",
      "Gradient Descent(28/999): loss=0.4318785835028596\n",
      "Gradient Descent(29/999): loss=0.43097573149245133\n",
      "Gradient Descent(30/999): loss=0.4301011886728928\n",
      "Gradient Descent(31/999): loss=0.42925358617411685\n",
      "Gradient Descent(32/999): loss=0.4284316287907271\n",
      "Gradient Descent(33/999): loss=0.4276340909382027\n",
      "Gradient Descent(34/999): loss=0.42685981283187957\n",
      "Gradient Descent(35/999): loss=0.42610769687642247\n",
      "Gradient Descent(36/999): loss=0.4253767042541843\n",
      "Gradient Descent(37/999): loss=0.42466585170148724\n",
      "Gradient Descent(38/999): loss=0.4239742084624667\n",
      "Gradient Descent(39/999): loss=0.42330089341068766\n",
      "Gradient Descent(40/999): loss=0.4226450723292887\n",
      "Gradient Descent(41/999): loss=0.42200595534091023\n",
      "Gradient Descent(42/999): loss=0.4213827944791551\n",
      "Gradient Descent(43/999): loss=0.42077488139377617\n",
      "Gradient Descent(44/999): loss=0.4201815451822238\n",
      "Gradient Descent(45/999): loss=0.4196021503405844\n",
      "Gradient Descent(46/999): loss=0.4190360948273326\n",
      "Gradient Descent(47/999): loss=0.4184828082336778\n",
      "Gradient Descent(48/999): loss=0.41794175005462875\n",
      "Gradient Descent(49/999): loss=0.41741240805522917\n",
      "Gradient Descent(50/999): loss=0.41689429672671274\n",
      "Gradient Descent(51/999): loss=0.4163869558276283\n",
      "Gradient Descent(52/999): loss=0.4158899490052479\n",
      "Gradient Descent(53/999): loss=0.41540286249283515\n",
      "Gradient Descent(54/999): loss=0.41492530387859133\n",
      "Gradient Descent(55/999): loss=0.4144569009423325\n",
      "Gradient Descent(56/999): loss=0.4139973005561613\n",
      "Gradient Descent(57/999): loss=0.41354616764560825\n",
      "Gradient Descent(58/999): loss=0.41310318420791237\n",
      "Gradient Descent(59/999): loss=0.4126680483842875\n",
      "Gradient Descent(60/999): loss=0.41224047358320576\n",
      "Gradient Descent(61/999): loss=0.41182018765188144\n",
      "Gradient Descent(62/999): loss=0.4114069320933015\n",
      "Gradient Descent(63/999): loss=0.41100046132629436\n",
      "Gradient Descent(64/999): loss=0.4106005419862641\n",
      "Gradient Descent(65/999): loss=0.41020695226434856\n",
      "Gradient Descent(66/999): loss=0.4098194812828864\n",
      "Gradient Descent(67/999): loss=0.409437928505192\n",
      "Gradient Descent(68/999): loss=0.40906210317774677\n",
      "Gradient Descent(69/999): loss=0.4086918238030226\n",
      "Gradient Descent(70/999): loss=0.4083269176412503\n",
      "Gradient Descent(71/999): loss=0.4079672202395353\n",
      "Gradient Descent(72/999): loss=0.40761257498681847\n",
      "Gradient Descent(73/999): loss=0.40726283269325564\n",
      "Gradient Descent(74/999): loss=0.406917851192671\n",
      "Gradient Descent(75/999): loss=0.40657749496681506\n",
      "Gradient Descent(76/999): loss=0.4062416347902233\n",
      "Gradient Descent(77/999): loss=0.40591014739454523\n",
      "Gradient Descent(78/999): loss=0.4055829151512675\n",
      "Gradient Descent(79/999): loss=0.40525982577181896\n",
      "Gradient Descent(80/999): loss=0.4049407720241037\n",
      "Gradient Descent(81/999): loss=0.40462565146455315\n",
      "Gradient Descent(82/999): loss=0.40431436618484473\n",
      "Gradient Descent(83/999): loss=0.40400682257248033\n",
      "Gradient Descent(84/999): loss=0.4037029310844597\n",
      "Gradient Descent(85/999): loss=0.4034026060333301\n",
      "Gradient Descent(86/999): loss=0.40310576538492776\n",
      "Gradient Descent(87/999): loss=0.4028123305671719\n",
      "Gradient Descent(88/999): loss=0.40252222628929846\n",
      "Gradient Descent(89/999): loss=0.40223538037096235\n",
      "Gradient Descent(90/999): loss=0.401951723580664\n",
      "Gradient Descent(91/999): loss=0.4016711894829858\n",
      "Gradient Descent(92/999): loss=0.40139371429415716\n",
      "Gradient Descent(93/999): loss=0.4011192367454852\n",
      "Gradient Descent(94/999): loss=0.4008476979542232\n",
      "Gradient Descent(95/999): loss=0.40057904130146443\n",
      "Gradient Descent(96/999): loss=0.400313212316676\n",
      "Gradient Descent(97/999): loss=0.40005015856850773\n",
      "Gradient Descent(98/999): loss=0.3997898295615313\n",
      "Gradient Descent(99/999): loss=0.3995321766385824\n",
      "Gradient Descent(100/999): loss=0.3992771528883994\n",
      "Gradient Descent(101/999): loss=0.3990247130582663\n",
      "Gradient Descent(102/999): loss=0.39877481347138577\n",
      "Gradient Descent(103/999): loss=0.398527411948721\n",
      "Gradient Descent(104/999): loss=0.39828246773506304\n",
      "Gradient Descent(105/999): loss=0.3980399414290891\n",
      "Gradient Descent(106/999): loss=0.3977997949171949\n",
      "Gradient Descent(107/999): loss=0.3975619913108917\n",
      "Gradient Descent(108/999): loss=0.39732649488757393\n",
      "Gradient Descent(109/999): loss=0.3970932710344709\n",
      "Gradient Descent(110/999): loss=0.396862286195611\n",
      "Gradient Descent(111/999): loss=0.3966335078216271\n",
      "Gradient Descent(112/999): loss=0.39640690432225534\n",
      "Gradient Descent(113/999): loss=0.39618244502137157\n",
      "Gradient Descent(114/999): loss=0.3959601001144323\n",
      "Gradient Descent(115/999): loss=0.39573984062818524\n",
      "Gradient Descent(116/999): loss=0.3955216383825258\n",
      "Gradient Descent(117/999): loss=0.39530546595438265\n",
      "Gradient Descent(118/999): loss=0.3950912966435201\n",
      "Gradient Descent(119/999): loss=0.3948791044401531\n",
      "Gradient Descent(120/999): loss=0.39466886399427537\n",
      "Gradient Descent(121/999): loss=0.3944605505866072\n",
      "Gradient Descent(122/999): loss=0.39425414010107396\n",
      "Gradient Descent(123/999): loss=0.3940496089987318\n",
      "Gradient Descent(124/999): loss=0.3938469342930616\n",
      "Gradient Descent(125/999): loss=0.3936460935265561\n",
      "Gradient Descent(126/999): loss=0.3934470647485297\n",
      "Gradient Descent(127/999): loss=0.39324982649408496\n",
      "Gradient Descent(128/999): loss=0.3930543577641708\n",
      "Gradient Descent(129/999): loss=0.392860638006675\n",
      "Gradient Descent(130/999): loss=0.3926686470984934\n",
      "Gradient Descent(131/999): loss=0.3924783653285232\n",
      "Gradient Descent(132/999): loss=0.3922897733815292\n",
      "Gradient Descent(133/999): loss=0.3921028523228357\n",
      "Gradient Descent(134/999): loss=0.3919175835838015\n",
      "Gradient Descent(135/999): loss=0.391733948948031\n",
      "Gradient Descent(136/999): loss=0.3915519305382846\n",
      "Gradient Descent(137/999): loss=0.39137151080405175\n",
      "Gradient Descent(138/999): loss=0.39119267250974454\n",
      "Gradient Descent(139/999): loss=0.39101539872348756\n",
      "Gradient Descent(140/999): loss=0.3908396728064612\n",
      "Gradient Descent(141/999): loss=0.3906654784027781\n",
      "Gradient Descent(142/999): loss=0.39049279942985793\n",
      "Gradient Descent(143/999): loss=0.3903216200692745\n",
      "Gradient Descent(144/999): loss=0.39015192475805144\n",
      "Gradient Descent(145/999): loss=0.389983698180383\n",
      "Gradient Descent(146/999): loss=0.3898169252597513\n",
      "Gradient Descent(147/999): loss=0.3896515911514273\n",
      "Gradient Descent(148/999): loss=0.389487681235328\n",
      "Gradient Descent(149/999): loss=0.3893251811092136\n",
      "Gradient Descent(150/999): loss=0.3891640765822063\n",
      "Gradient Descent(151/999): loss=0.3890043536686132\n",
      "Gradient Descent(152/999): loss=0.38884599858203744\n",
      "Gradient Descent(153/999): loss=0.3886889977297611\n",
      "Gradient Descent(154/999): loss=0.38853333770738707\n",
      "Gradient Descent(155/999): loss=0.3883790052937244\n",
      "Gradient Descent(156/999): loss=0.38822598744590625\n",
      "Gradient Descent(157/999): loss=0.3880742712947254\n",
      "Gradient Descent(158/999): loss=0.3879238441401779\n",
      "Gradient Descent(159/999): loss=0.38777469344720444\n",
      "Gradient Descent(160/999): loss=0.3876268068416159\n",
      "Gradient Descent(161/999): loss=0.38748017210619634\n",
      "Gradient Descent(162/999): loss=0.38733477717697384\n",
      "Gradient Descent(163/999): loss=0.3871906101396484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(164/999): loss=0.38704765922617085\n",
      "Gradient Descent(165/999): loss=0.3869059128114636\n",
      "Gradient Descent(166/999): loss=0.386765359410277\n",
      "Gradient Descent(167/999): loss=0.38662598767417256\n",
      "Gradient Descent(168/999): loss=0.3864877863886289\n",
      "Gradient Descent(169/999): loss=0.386350744470261\n",
      "Gradient Descent(170/999): loss=0.38621485096415037\n",
      "Gradient Descent(171/999): loss=0.38608009504127805\n",
      "Gradient Descent(172/999): loss=0.38594646599605514\n",
      "Gradient Descent(173/999): loss=0.3858139532439485\n",
      "Gradient Descent(174/999): loss=0.3856825463191928\n",
      "Gradient Descent(175/999): loss=0.3855522348725884\n",
      "Gradient Descent(176/999): loss=0.3854230086693781\n",
      "Gradient Descent(177/999): loss=0.38529485758720106\n",
      "Gradient Descent(178/999): loss=0.38516777161411636\n",
      "Gradient Descent(179/999): loss=0.38504174084669796\n",
      "Gradient Descent(180/999): loss=0.38491675548819293\n",
      "Gradient Descent(181/999): loss=0.3847928058467415\n",
      "Gradient Descent(182/999): loss=0.384669882333658\n",
      "Gradient Descent(183/999): loss=0.38454797546176567\n",
      "Gradient Descent(184/999): loss=0.3844270758437854\n",
      "Gradient Descent(185/999): loss=0.3843071741907756\n",
      "Gradient Descent(186/999): loss=0.3841882613106208\n",
      "Gradient Descent(187/999): loss=0.38407032810656494\n",
      "Gradient Descent(188/999): loss=0.3839533655757897\n",
      "Gradient Descent(189/999): loss=0.3838373648080344\n",
      "Gradient Descent(190/999): loss=0.38372231698425485\n",
      "Gradient Descent(191/999): loss=0.3836082133753223\n",
      "Gradient Descent(192/999): loss=0.38349504534075574\n",
      "Gradient Descent(193/999): loss=0.3833828043274913\n",
      "Gradient Descent(194/999): loss=0.38327148186868365\n",
      "Gradient Descent(195/999): loss=0.38316106958253887\n",
      "Gradient Descent(196/999): loss=0.38305155917117817\n",
      "Gradient Descent(197/999): loss=0.38294294241952903\n",
      "Gradient Descent(198/999): loss=0.3828352111942456\n",
      "Gradient Descent(199/999): loss=0.38272835744265454\n",
      "Gradient Descent(200/999): loss=0.3826223731917264\n",
      "Gradient Descent(201/999): loss=0.38251725054707003\n",
      "Gradient Descent(202/999): loss=0.38241298169195276\n",
      "Gradient Descent(203/999): loss=0.38230955888633966\n",
      "Gradient Descent(204/999): loss=0.38220697446595653\n",
      "Gradient Descent(205/999): loss=0.3821052208413717\n",
      "Gradient Descent(206/999): loss=0.38200429049709855\n",
      "Gradient Descent(207/999): loss=0.38190417599071563\n",
      "Gradient Descent(208/999): loss=0.38180486995200646\n",
      "Gradient Descent(209/999): loss=0.38170636508211514\n",
      "Gradient Descent(210/999): loss=0.38160865415271916\n",
      "Gradient Descent(211/999): loss=0.3815117300052192\n",
      "Gradient Descent(212/999): loss=0.3814155855499428\n",
      "Gradient Descent(213/999): loss=0.381320213765364\n",
      "Gradient Descent(214/999): loss=0.38122560769733765\n",
      "Gradient Descent(215/999): loss=0.3811317604583461\n",
      "Gradient Descent(216/999): loss=0.3810386652267609\n",
      "Gradient Descent(217/999): loss=0.3809463152461159\n",
      "Gradient Descent(218/999): loss=0.38085470382439435\n",
      "Gradient Descent(219/999): loss=0.38076382433332684\n",
      "Gradient Descent(220/999): loss=0.38067367020770126\n",
      "Gradient Descent(221/999): loss=0.38058423494468385\n",
      "Gradient Descent(222/999): loss=0.38049551210315175\n",
      "Gradient Descent(223/999): loss=0.38040749530303547\n",
      "Gradient Descent(224/999): loss=0.38032017822467146\n",
      "Gradient Descent(225/999): loss=0.38023355460816527\n",
      "Gradient Descent(226/999): loss=0.38014761825276405\n",
      "Gradient Descent(227/999): loss=0.3800623630162377\n",
      "Gradient Descent(228/999): loss=0.37997778281427047\n",
      "Gradient Descent(229/999): loss=0.3798938716198599\n",
      "Gradient Descent(230/999): loss=0.37981062346272515\n",
      "Gradient Descent(231/999): loss=0.3797280324287235\n",
      "Gradient Descent(232/999): loss=0.3796460926592749\n",
      "Gradient Descent(233/999): loss=0.379564798350795\n",
      "Gradient Descent(234/999): loss=0.3794841437541337\n",
      "Gradient Descent(235/999): loss=0.3794041231740246\n",
      "Gradient Descent(236/999): loss=0.37932473096853886\n",
      "Gradient Descent(237/999): loss=0.3792459615485472\n",
      "Gradient Descent(238/999): loss=0.3791678093771894\n",
      "Gradient Descent(239/999): loss=0.3790902689693498\n",
      "Gradient Descent(240/999): loss=0.3790133348911394\n",
      "Gradient Descent(241/999): loss=0.37893700175938516\n",
      "Gradient Descent(242/999): loss=0.3788612642411248\n",
      "Gradient Descent(243/999): loss=0.3787861170531086\n",
      "Gradient Descent(244/999): loss=0.3787115549613063\n",
      "Gradient Descent(245/999): loss=0.37863757278042093\n",
      "Gradient Descent(246/999): loss=0.37856416537340787\n",
      "Gradient Descent(247/999): loss=0.3784913276509995\n",
      "Gradient Descent(248/999): loss=0.3784190545712356\n",
      "Gradient Descent(249/999): loss=0.37834734113900026\n",
      "Gradient Descent(250/999): loss=0.37827618240556116\n",
      "Gradient Descent(251/999): loss=0.3782055734681185\n",
      "Gradient Descent(252/999): loss=0.3781355094693545\n",
      "Gradient Descent(253/999): loss=0.37806598559699217\n",
      "Gradient Descent(254/999): loss=0.37799699708335516\n",
      "Gradient Descent(255/999): loss=0.37792853920493574\n",
      "Gradient Descent(256/999): loss=0.37786060728196613\n",
      "Gradient Descent(257/999): loss=0.377793196677994\n",
      "Gradient Descent(258/999): loss=0.37772630279946384\n",
      "Gradient Descent(259/999): loss=0.3776599210953025\n",
      "Gradient Descent(260/999): loss=0.3775940470565083\n",
      "Gradient Descent(261/999): loss=0.37752867621574654\n",
      "Gradient Descent(262/999): loss=0.3774638041469471\n",
      "Gradient Descent(263/999): loss=0.37739942646490815\n",
      "Gradient Descent(264/999): loss=0.3773355388249028\n",
      "Gradient Descent(265/999): loss=0.37727213692229117\n",
      "Gradient Descent(266/999): loss=0.3772092164921349\n",
      "Gradient Descent(267/999): loss=0.37714677330881763\n",
      "Gradient Descent(268/999): loss=0.3770848031856677\n",
      "Gradient Descent(269/999): loss=0.37702330197458644\n",
      "Gradient Descent(270/999): loss=0.376962265565678\n",
      "Gradient Descent(271/999): loss=0.3769016898868864\n",
      "Gradient Descent(272/999): loss=0.3768415709036326\n",
      "Gradient Descent(273/999): loss=0.37678190461845823\n",
      "Gradient Descent(274/999): loss=0.37672268707067114\n",
      "Gradient Descent(275/999): loss=0.3766639143359959\n",
      "Gradient Descent(276/999): loss=0.3766055825262266\n",
      "Gradient Descent(277/999): loss=0.37654768778888426\n",
      "Gradient Descent(278/999): loss=0.3764902263068769\n",
      "Gradient Descent(279/999): loss=0.37643319429816363\n",
      "Gradient Descent(280/999): loss=0.37637658801542223\n",
      "Gradient Descent(281/999): loss=0.37632040374571885\n",
      "Gradient Descent(282/999): loss=0.37626463781018277\n",
      "Gradient Descent(283/999): loss=0.3762092865636833\n",
      "Gradient Descent(284/999): loss=0.37615434639450973\n",
      "Gradient Descent(285/999): loss=0.3760998137240558\n",
      "Gradient Descent(286/999): loss=0.3760456850065058\n",
      "Gradient Descent(287/999): loss=0.37599195672852476\n",
      "Gradient Descent(288/999): loss=0.37593862540895123\n",
      "Gradient Descent(289/999): loss=0.37588568759849306\n",
      "Gradient Descent(290/999): loss=0.37583313987942685\n",
      "Gradient Descent(291/999): loss=0.3757809788652999\n",
      "Gradient Descent(292/999): loss=0.37572920120063424\n",
      "Gradient Descent(293/999): loss=0.3756778035606359\n",
      "Gradient Descent(294/999): loss=0.3756267826509043\n",
      "Gradient Descent(295/999): loss=0.37557613520714644\n",
      "Gradient Descent(296/999): loss=0.37552585799489313\n",
      "Gradient Descent(297/999): loss=0.3754759478092182\n",
      "Gradient Descent(298/999): loss=0.37542640147445944\n",
      "Gradient Descent(299/999): loss=0.37537721584394507\n",
      "Gradient Descent(300/999): loss=0.3753283877997192\n",
      "Gradient Descent(301/999): loss=0.37527991425227275\n",
      "Gradient Descent(302/999): loss=0.37523179214027585\n",
      "Gradient Descent(303/999): loss=0.37518401843031235\n",
      "Gradient Descent(304/999): loss=0.3751365901166183\n",
      "Gradient Descent(305/999): loss=0.37508950422082277\n",
      "Gradient Descent(306/999): loss=0.37504275779168894\n",
      "Gradient Descent(307/999): loss=0.374996347904861\n",
      "Gradient Descent(308/999): loss=0.3749502716626112\n",
      "Gradient Descent(309/999): loss=0.37490452619359\n",
      "Gradient Descent(310/999): loss=0.3748591086525793\n",
      "Gradient Descent(311/999): loss=0.37481401622024657\n",
      "Gradient Descent(312/999): loss=0.3747692461029032\n",
      "Gradient Descent(313/999): loss=0.3747247955322635\n",
      "Gradient Descent(314/999): loss=0.37468066176520703\n",
      "Gradient Descent(315/999): loss=0.3746368420835432\n",
      "Gradient Descent(316/999): loss=0.37459333379377735\n",
      "Gradient Descent(317/999): loss=0.3745501342268801\n",
      "Gradient Descent(318/999): loss=0.37450724073805797\n",
      "Gradient Descent(319/999): loss=0.37446465070652735\n",
      "Gradient Descent(320/999): loss=0.37442236153528935\n",
      "Gradient Descent(321/999): loss=0.37438037065090746\n",
      "Gradient Descent(322/999): loss=0.3743386755032878\n",
      "Gradient Descent(323/999): loss=0.37429727356546094\n",
      "Gradient Descent(324/999): loss=0.37425616233336495\n",
      "Gradient Descent(325/999): loss=0.3742153393256326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(326/999): loss=0.37417480208337905\n",
      "Gradient Descent(327/999): loss=0.3741345481699916\n",
      "Gradient Descent(328/999): loss=0.3740945751709227\n",
      "Gradient Descent(329/999): loss=0.3740548806934829\n",
      "Gradient Descent(330/999): loss=0.3740154623666381\n",
      "Gradient Descent(331/999): loss=0.37397631784080665\n",
      "Gradient Descent(332/999): loss=0.37393744478765994\n",
      "Gradient Descent(333/999): loss=0.3738988408999242\n",
      "Gradient Descent(334/999): loss=0.3738605038911842\n",
      "Gradient Descent(335/999): loss=0.3738224314956887\n",
      "Gradient Descent(336/999): loss=0.37378462146815833\n",
      "Gradient Descent(337/999): loss=0.373747071583595\n",
      "Gradient Descent(338/999): loss=0.3737097796370922\n",
      "Gradient Descent(339/999): loss=0.373672743443649\n",
      "Gradient Descent(340/999): loss=0.37363596083798484\n",
      "Gradient Descent(341/999): loss=0.3735994296743547\n",
      "Gradient Descent(342/999): loss=0.3735631478263694\n",
      "Gradient Descent(343/999): loss=0.3735271131868135\n",
      "Gradient Descent(344/999): loss=0.37349132366746873\n",
      "Gradient Descent(345/999): loss=0.3734557771989363\n",
      "Gradient Descent(346/999): loss=0.3734204717304625\n",
      "Gradient Descent(347/999): loss=0.3733854052297655\n",
      "Gradient Descent(348/999): loss=0.3733505756828632\n",
      "Gradient Descent(349/999): loss=0.37331598109390424\n",
      "Gradient Descent(350/999): loss=0.3732816194849984\n",
      "Gradient Descent(351/999): loss=0.3732474888960513\n",
      "Gradient Descent(352/999): loss=0.3732135873845979\n",
      "Gradient Descent(353/999): loss=0.3731799130256396\n",
      "Gradient Descent(354/999): loss=0.37314646391148193\n",
      "Gradient Descent(355/999): loss=0.37311323815157466\n",
      "Gradient Descent(356/999): loss=0.3730802338723521\n",
      "Gradient Descent(357/999): loss=0.37304744921707567\n",
      "Gradient Descent(358/999): loss=0.37301488234567837\n",
      "Gradient Descent(359/999): loss=0.37298253143461\n",
      "Gradient Descent(360/999): loss=0.3729503946766844\n",
      "Gradient Descent(361/999): loss=0.37291847028092723\n",
      "Gradient Descent(362/999): loss=0.37288675647242653\n",
      "Gradient Descent(363/999): loss=0.3728552514921839\n",
      "Gradient Descent(364/999): loss=0.3728239535969664\n",
      "Gradient Descent(365/999): loss=0.37279286105916193\n",
      "Gradient Descent(366/999): loss=0.3727619721666336\n",
      "Gradient Descent(367/999): loss=0.372731285222577\n",
      "Gradient Descent(368/999): loss=0.37270079854537874\n",
      "Gradient Descent(369/999): loss=0.37267051046847505\n",
      "Gradient Descent(370/999): loss=0.37264041934021347\n",
      "Gradient Descent(371/999): loss=0.37261052352371477\n",
      "Gradient Descent(372/999): loss=0.3725808213967365\n",
      "Gradient Descent(373/999): loss=0.3725513113515377\n",
      "Gradient Descent(374/999): loss=0.3725219917947443\n",
      "Gradient Descent(375/999): loss=0.372492861147218\n",
      "Gradient Descent(376/999): loss=0.37246391784392335\n",
      "Gradient Descent(377/999): loss=0.372435160333798\n",
      "Gradient Descent(378/999): loss=0.3724065870796244\n",
      "Gradient Descent(379/999): loss=0.3723781965579009\n",
      "Gradient Descent(380/999): loss=0.3723499872587162\n",
      "Gradient Descent(381/999): loss=0.3723219576856233\n",
      "Gradient Descent(382/999): loss=0.37229410635551624\n",
      "Gradient Descent(383/999): loss=0.372266431798506\n",
      "Gradient Descent(384/999): loss=0.37223893255779955\n",
      "Gradient Descent(385/999): loss=0.3722116071895785\n",
      "Gradient Descent(386/999): loss=0.3721844542628802\n",
      "Gradient Descent(387/999): loss=0.37215747235947894\n",
      "Gradient Descent(388/999): loss=0.372130660073769\n",
      "Gradient Descent(389/999): loss=0.3721040160126475\n",
      "Gradient Descent(390/999): loss=0.3720775387954004\n",
      "Gradient Descent(391/999): loss=0.3720512270535887\n",
      "Gradient Descent(392/999): loss=0.37202507943093377\n",
      "Gradient Descent(393/999): loss=0.37199909458320757\n",
      "Gradient Descent(394/999): loss=0.3719732711781202\n",
      "Gradient Descent(395/999): loss=0.3719476078952109\n",
      "Gradient Descent(396/999): loss=0.37192210342573956\n",
      "Gradient Descent(397/999): loss=0.3718967564725784\n",
      "Gradient Descent(398/999): loss=0.37187156575010516\n",
      "Gradient Descent(399/999): loss=0.3718465299840984\n",
      "Gradient Descent(400/999): loss=0.3718216479116314\n",
      "Gradient Descent(401/999): loss=0.3717969182809697\n",
      "Gradient Descent(402/999): loss=0.3717723398514673\n",
      "Gradient Descent(403/999): loss=0.3717479113934658\n",
      "Gradient Descent(404/999): loss=0.3717236316881928\n",
      "Gradient Descent(405/999): loss=0.3716994995276626\n",
      "Gradient Descent(406/999): loss=0.3716755137145767\n",
      "Gradient Descent(407/999): loss=0.3716516730622265\n",
      "Gradient Descent(408/999): loss=0.3716279763943956\n",
      "Gradient Descent(409/999): loss=0.37160442254526366\n",
      "Gradient Descent(410/999): loss=0.37158101035931157\n",
      "Gradient Descent(411/999): loss=0.3715577386912264\n",
      "Gradient Descent(412/999): loss=0.37153460640580854\n",
      "Gradient Descent(413/999): loss=0.37151161237787894\n",
      "Gradient Descent(414/999): loss=0.37148875549218674\n",
      "Gradient Descent(415/999): loss=0.3714660346433193\n",
      "Gradient Descent(416/999): loss=0.37144344873561136\n",
      "Gradient Descent(417/999): loss=0.37142099668305656\n",
      "Gradient Descent(418/999): loss=0.3713986774092184\n",
      "Gradient Descent(419/999): loss=0.37137648984714305\n",
      "Gradient Descent(420/999): loss=0.37135443293927284\n",
      "Gradient Descent(421/999): loss=0.3713325056373598\n",
      "Gradient Descent(422/999): loss=0.37131070690238105\n",
      "Gradient Descent(423/999): loss=0.3712890357044546\n",
      "Gradient Descent(424/999): loss=0.37126749102275447\n",
      "Gradient Descent(425/999): loss=0.3712460718454307\n",
      "Gradient Descent(426/999): loss=0.3712247771695241\n",
      "Gradient Descent(427/999): loss=0.3712036060008881\n",
      "Gradient Descent(428/999): loss=0.371182557354106\n",
      "Gradient Descent(429/999): loss=0.37116163025241244\n",
      "Gradient Descent(430/999): loss=0.3711408237276143\n",
      "Gradient Descent(431/999): loss=0.37112013682001266\n",
      "Gradient Descent(432/999): loss=0.3710995685783247\n",
      "Gradient Descent(433/999): loss=0.3710791180596076\n",
      "Gradient Descent(434/999): loss=0.3710587843291821\n",
      "Gradient Descent(435/999): loss=0.3710385664605576\n",
      "Gradient Descent(436/999): loss=0.3710184635353575\n",
      "Gradient Descent(437/999): loss=0.37099847464324465\n",
      "Gradient Descent(438/999): loss=0.3709785988818487\n",
      "Gradient Descent(439/999): loss=0.37095883535669355\n",
      "Gradient Descent(440/999): loss=0.3709391831811255\n",
      "Gradient Descent(441/999): loss=0.37091964147624173\n",
      "Gradient Descent(442/999): loss=0.37090020937082\n",
      "Gradient Descent(443/999): loss=0.37088088600124886\n",
      "Gradient Descent(444/999): loss=0.3708616705114581\n",
      "Gradient Descent(445/999): loss=0.3708425620528505\n",
      "Gradient Descent(446/999): loss=0.37082355978423404\n",
      "Gradient Descent(447/999): loss=0.3708046628717541\n",
      "Gradient Descent(448/999): loss=0.3707858704888271\n",
      "Gradient Descent(449/999): loss=0.37076718181607404\n",
      "Gradient Descent(450/999): loss=0.3707485960412562\n",
      "Gradient Descent(451/999): loss=0.3707301123592088\n",
      "Gradient Descent(452/999): loss=0.3707117299717776\n",
      "Gradient Descent(453/999): loss=0.3706934480877553\n",
      "Gradient Descent(454/999): loss=0.3706752659228185\n",
      "Gradient Descent(455/999): loss=0.3706571826994647\n",
      "Gradient Descent(456/999): loss=0.3706391976469512\n",
      "Gradient Descent(457/999): loss=0.37062131000123316\n",
      "Gradient Descent(458/999): loss=0.3706035190049034\n",
      "Gradient Descent(459/999): loss=0.3705858239071319\n",
      "Gradient Descent(460/999): loss=0.3705682239636063\n",
      "Gradient Descent(461/999): loss=0.3705507184364728\n",
      "Gradient Descent(462/999): loss=0.3705333065942779\n",
      "Gradient Descent(463/999): loss=0.3705159877119102\n",
      "Gradient Descent(464/999): loss=0.3704987610705429\n",
      "Gradient Descent(465/999): loss=0.370481625957577\n",
      "Gradient Descent(466/999): loss=0.3704645816665855\n",
      "Gradient Descent(467/999): loss=0.3704476274972567\n",
      "Gradient Descent(468/999): loss=0.37043076275533904\n",
      "Gradient Descent(469/999): loss=0.37041398675258713\n",
      "Gradient Descent(470/999): loss=0.3703972988067065\n",
      "Gradient Descent(471/999): loss=0.37038069824130027\n",
      "Gradient Descent(472/999): loss=0.37036418438581553\n",
      "Gradient Descent(473/999): loss=0.3703477565754913\n",
      "Gradient Descent(474/999): loss=0.370331414151305\n",
      "Gradient Descent(475/999): loss=0.37031515645992197\n",
      "Gradient Descent(476/999): loss=0.3702989828536433\n",
      "Gradient Descent(477/999): loss=0.37028289269035486\n",
      "Gradient Descent(478/999): loss=0.3702668853334777\n",
      "Gradient Descent(479/999): loss=0.3702509601519172\n",
      "Gradient Descent(480/999): loss=0.37023511652001423\n",
      "Gradient Descent(481/999): loss=0.37021935381749593\n",
      "Gradient Descent(482/999): loss=0.3702036714294274\n",
      "Gradient Descent(483/999): loss=0.37018806874616283\n",
      "Gradient Descent(484/999): loss=0.3701725451632991\n",
      "Gradient Descent(485/999): loss=0.3701571000816271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(486/999): loss=0.37014173290708646\n",
      "Gradient Descent(487/999): loss=0.3701264430507177\n",
      "Gradient Descent(488/999): loss=0.37011122992861756\n",
      "Gradient Descent(489/999): loss=0.3700960929618928\n",
      "Gradient Descent(490/999): loss=0.37008103157661537\n",
      "Gradient Descent(491/999): loss=0.3700660452037776\n",
      "Gradient Descent(492/999): loss=0.37005113327924855\n",
      "Gradient Descent(493/999): loss=0.37003629524372883\n",
      "Gradient Descent(494/999): loss=0.37002153054270864\n",
      "Gradient Descent(495/999): loss=0.3700068386264241\n",
      "Gradient Descent(496/999): loss=0.3699922189498144\n",
      "Gradient Descent(497/999): loss=0.3699776709724799\n",
      "Gradient Descent(498/999): loss=0.36996319415864004\n",
      "Gradient Descent(499/999): loss=0.3699487879770923\n",
      "Gradient Descent(500/999): loss=0.36993445190117064\n",
      "Gradient Descent(501/999): loss=0.36992018540870497\n",
      "Gradient Descent(502/999): loss=0.3699059879819809\n",
      "Gradient Descent(503/999): loss=0.3698918591076998\n",
      "Gradient Descent(504/999): loss=0.36987779827693884\n",
      "Gradient Descent(505/999): loss=0.3698638049851124\n",
      "Gradient Descent(506/999): loss=0.36984987873193287\n",
      "Gradient Descent(507/999): loss=0.36983601902137164\n",
      "Gradient Descent(508/999): loss=0.36982222536162207\n",
      "Gradient Descent(509/999): loss=0.36980849726506076\n",
      "Gradient Descent(510/999): loss=0.3697948342482105\n",
      "Gradient Descent(511/999): loss=0.36978123583170314\n",
      "Gradient Descent(512/999): loss=0.36976770154024263\n",
      "Gradient Descent(513/999): loss=0.3697542309025687\n",
      "Gradient Descent(514/999): loss=0.36974082345142106\n",
      "Gradient Descent(515/999): loss=0.3697274787235032\n",
      "Gradient Descent(516/999): loss=0.36971419625944724\n",
      "Gradient Descent(517/999): loss=0.3697009756037785\n",
      "Gradient Descent(518/999): loss=0.3696878163048813\n",
      "Gradient Descent(519/999): loss=0.36967471791496376\n",
      "Gradient Descent(520/999): loss=0.3696616799900246\n",
      "Gradient Descent(521/999): loss=0.36964870208981787\n",
      "Gradient Descent(522/999): loss=0.3696357837778213\n",
      "Gradient Descent(523/999): loss=0.36962292462120155\n",
      "Gradient Descent(524/999): loss=0.3696101241907816\n",
      "Gradient Descent(525/999): loss=0.3695973820610092\n",
      "Gradient Descent(526/999): loss=0.36958469780992304\n",
      "Gradient Descent(527/999): loss=0.36957207101912165\n",
      "Gradient Descent(528/999): loss=0.36955950127373155\n",
      "Gradient Descent(529/999): loss=0.36954698816237586\n",
      "Gradient Descent(530/999): loss=0.3695345312771429\n",
      "Gradient Descent(531/999): loss=0.36952213021355534\n",
      "Gradient Descent(532/999): loss=0.36950978457054023\n",
      "Gradient Descent(533/999): loss=0.369497493950398\n",
      "Gradient Descent(534/999): loss=0.3694852579587728\n",
      "Gradient Descent(535/999): loss=0.36947307620462244\n",
      "Gradient Descent(536/999): loss=0.3694609483001895\n",
      "Gradient Descent(537/999): loss=0.36944887386097175\n",
      "Gradient Descent(538/999): loss=0.3694368525056929\n",
      "Gradient Descent(539/999): loss=0.36942488385627487\n",
      "Gradient Descent(540/999): loss=0.3694129675378085\n",
      "Gradient Descent(541/999): loss=0.3694011031785257\n",
      "Gradient Descent(542/999): loss=0.36938929040977164\n",
      "Gradient Descent(543/999): loss=0.3693775288659774\n",
      "Gradient Descent(544/999): loss=0.3693658181846319\n",
      "Gradient Descent(545/999): loss=0.3693541580062552\n",
      "Gradient Descent(546/999): loss=0.3693425479743717\n",
      "Gradient Descent(547/999): loss=0.36933098773548334\n",
      "Gradient Descent(548/999): loss=0.3693194769390433\n",
      "Gradient Descent(549/999): loss=0.3693080152374298\n",
      "Gradient Descent(550/999): loss=0.36929660228592\n",
      "Gradient Descent(551/999): loss=0.3692852377426647\n",
      "Gradient Descent(552/999): loss=0.36927392126866293\n",
      "Gradient Descent(553/999): loss=0.3692626525277362\n",
      "Gradient Descent(554/999): loss=0.3692514311865038\n",
      "Gradient Descent(555/999): loss=0.3692402569143586\n",
      "Gradient Descent(556/999): loss=0.36922912938344143\n",
      "Gradient Descent(557/999): loss=0.36921804826861804\n",
      "Gradient Descent(558/999): loss=0.3692070132474539\n",
      "Gradient Descent(559/999): loss=0.3691960240001909\n",
      "Gradient Descent(560/999): loss=0.36918508020972374\n",
      "Gradient Descent(561/999): loss=0.3691741815615765\n",
      "Gradient Descent(562/999): loss=0.36916332774387894\n",
      "Gradient Descent(563/999): loss=0.36915251844734426\n",
      "Gradient Descent(564/999): loss=0.36914175336524574\n",
      "Gradient Descent(565/999): loss=0.3691310321933943\n",
      "Gradient Descent(566/999): loss=0.36912035463011655\n",
      "Gradient Descent(567/999): loss=0.36910972037623146\n",
      "Gradient Descent(568/999): loss=0.3690991291350302\n",
      "Gradient Descent(569/999): loss=0.36908858061225236\n",
      "Gradient Descent(570/999): loss=0.36907807451606606\n",
      "Gradient Descent(571/999): loss=0.36906761055704534\n",
      "Gradient Descent(572/999): loss=0.3690571884481498\n",
      "Gradient Descent(573/999): loss=0.3690468079047033\n",
      "Gradient Descent(574/999): loss=0.36903646864437295\n",
      "Gradient Descent(575/999): loss=0.3690261703871489\n",
      "Gradient Descent(576/999): loss=0.3690159128553235\n",
      "Gradient Descent(577/999): loss=0.36900569577347175\n",
      "Gradient Descent(578/999): loss=0.36899551886843024\n",
      "Gradient Descent(579/999): loss=0.36898538186927826\n",
      "Gradient Descent(580/999): loss=0.3689752845073172\n",
      "Gradient Descent(581/999): loss=0.36896522651605185\n",
      "Gradient Descent(582/999): loss=0.3689552076311706\n",
      "Gradient Descent(583/999): loss=0.3689452275905266\n",
      "Gradient Descent(584/999): loss=0.3689352861341182\n",
      "Gradient Descent(585/999): loss=0.36892538300407046\n",
      "Gradient Descent(586/999): loss=0.36891551794461724\n",
      "Gradient Descent(587/999): loss=0.3689056907020811\n",
      "Gradient Descent(588/999): loss=0.3688959010248566\n",
      "Gradient Descent(589/999): loss=0.3688861486633908\n",
      "Gradient Descent(590/999): loss=0.36887643337016696\n",
      "Gradient Descent(591/999): loss=0.3688667548996847\n",
      "Gradient Descent(592/999): loss=0.36885711300844404\n",
      "Gradient Descent(593/999): loss=0.36884750745492667\n",
      "Gradient Descent(594/999): loss=0.36883793799957976\n",
      "Gradient Descent(595/999): loss=0.3688284044047979\n",
      "Gradient Descent(596/999): loss=0.36881890643490606\n",
      "Gradient Descent(597/999): loss=0.3688094438561434\n",
      "Gradient Descent(598/999): loss=0.36880001643664656\n",
      "Gradient Descent(599/999): loss=0.3687906239464318\n",
      "Gradient Descent(600/999): loss=0.3687812661573802\n",
      "Gradient Descent(601/999): loss=0.3687719428432203\n",
      "Gradient Descent(602/999): loss=0.36876265377951245\n",
      "Gradient Descent(603/999): loss=0.36875339874363305\n",
      "Gradient Descent(604/999): loss=0.36874417751475785\n",
      "Gradient Descent(605/999): loss=0.3687349898738473\n",
      "Gradient Descent(606/999): loss=0.3687258356036304\n",
      "Gradient Descent(607/999): loss=0.36871671448858934\n",
      "Gradient Descent(608/999): loss=0.36870762631494425\n",
      "Gradient Descent(609/999): loss=0.36869857087063856\n",
      "Gradient Descent(610/999): loss=0.36868954794532266\n",
      "Gradient Descent(611/999): loss=0.36868055733034133\n",
      "Gradient Descent(612/999): loss=0.3686715988187162\n",
      "Gradient Descent(613/999): loss=0.3686626722051336\n",
      "Gradient Descent(614/999): loss=0.36865377728592874\n",
      "Gradient Descent(615/999): loss=0.3686449138590718\n",
      "Gradient Descent(616/999): loss=0.3686360817241534\n",
      "Gradient Descent(617/999): loss=0.3686272806823715\n",
      "Gradient Descent(618/999): loss=0.3686185105365158\n",
      "Gradient Descent(619/999): loss=0.36860977109095555\n",
      "Gradient Descent(620/999): loss=0.36860106215162497\n",
      "Gradient Descent(621/999): loss=0.3685923835260099\n",
      "Gradient Descent(622/999): loss=0.3685837350231344\n",
      "Gradient Descent(623/999): loss=0.3685751164535471\n",
      "Gradient Descent(624/999): loss=0.3685665276293083\n",
      "Gradient Descent(625/999): loss=0.36855796836397714\n",
      "Gradient Descent(626/999): loss=0.368549438472598\n",
      "Gradient Descent(627/999): loss=0.36854093777168784\n",
      "Gradient Descent(628/999): loss=0.3685324660792237\n",
      "Gradient Descent(629/999): loss=0.36852402321463007\n",
      "Gradient Descent(630/999): loss=0.3685156089987661\n",
      "Gradient Descent(631/999): loss=0.3685072232539132\n",
      "Gradient Descent(632/999): loss=0.36849886580376323\n",
      "Gradient Descent(633/999): loss=0.3684905364734056\n",
      "Gradient Descent(634/999): loss=0.36848223508931577\n",
      "Gradient Descent(635/999): loss=0.36847396147934286\n",
      "Gradient Descent(636/999): loss=0.36846571547269846\n",
      "Gradient Descent(637/999): loss=0.3684574968999441\n",
      "Gradient Descent(638/999): loss=0.36844930559297995\n",
      "Gradient Descent(639/999): loss=0.3684411413850335\n",
      "Gradient Descent(640/999): loss=0.3684330041106476\n",
      "Gradient Descent(641/999): loss=0.3684248936056696\n",
      "Gradient Descent(642/999): loss=0.36841680970724\n",
      "Gradient Descent(643/999): loss=0.36840875225378117\n",
      "Gradient Descent(644/999): loss=0.36840072108498645\n",
      "Gradient Descent(645/999): loss=0.3683927160418092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(646/999): loss=0.36838473696645174\n",
      "Gradient Descent(647/999): loss=0.3683767837023548\n",
      "Gradient Descent(648/999): loss=0.36836885609418757\n",
      "Gradient Descent(649/999): loss=0.36836095398783547\n",
      "Gradient Descent(650/999): loss=0.3683530772303914\n",
      "Gradient Descent(651/999): loss=0.36834522567014444\n",
      "Gradient Descent(652/999): loss=0.3683373991565697\n",
      "Gradient Descent(653/999): loss=0.3683295975403185\n",
      "Gradient Descent(654/999): loss=0.36832182067320807\n",
      "Gradient Descent(655/999): loss=0.3683140684082111\n",
      "Gradient Descent(656/999): loss=0.3683063405994469\n",
      "Gradient Descent(657/999): loss=0.36829863710217026\n",
      "Gradient Descent(658/999): loss=0.3682909577727631\n",
      "Gradient Descent(659/999): loss=0.3682833024687236\n",
      "Gradient Descent(660/999): loss=0.3682756710486576\n",
      "Gradient Descent(661/999): loss=0.36826806337226836\n",
      "Gradient Descent(662/999): loss=0.3682604793003478\n",
      "Gradient Descent(663/999): loss=0.3682529186947665\n",
      "Gradient Descent(664/999): loss=0.36824538141846536\n",
      "Gradient Descent(665/999): loss=0.36823786733544595\n",
      "Gradient Descent(666/999): loss=0.36823037631076116\n",
      "Gradient Descent(667/999): loss=0.3682229082105069\n",
      "Gradient Descent(668/999): loss=0.36821546290181284\n",
      "Gradient Descent(669/999): loss=0.36820804025283344\n",
      "Gradient Descent(670/999): loss=0.3682006401327394\n",
      "Gradient Descent(671/999): loss=0.3681932624117092\n",
      "Gradient Descent(672/999): loss=0.3681859069609201\n",
      "Gradient Descent(673/999): loss=0.36817857365254\n",
      "Gradient Descent(674/999): loss=0.3681712623597188\n",
      "Gradient Descent(675/999): loss=0.36816397295658\n",
      "Gradient Descent(676/999): loss=0.36815670531821276\n",
      "Gradient Descent(677/999): loss=0.3681494593206629\n",
      "Gradient Descent(678/999): loss=0.3681422348409258\n",
      "Gradient Descent(679/999): loss=0.3681350317569379\n",
      "Gradient Descent(680/999): loss=0.36812784994756825\n",
      "Gradient Descent(681/999): loss=0.36812068929261127\n",
      "Gradient Descent(682/999): loss=0.36811354967277826\n",
      "Gradient Descent(683/999): loss=0.36810643096969065\n",
      "Gradient Descent(684/999): loss=0.36809933306587084\n",
      "Gradient Descent(685/999): loss=0.3680922558447356\n",
      "Gradient Descent(686/999): loss=0.3680851991905886\n",
      "Gradient Descent(687/999): loss=0.36807816298861196\n",
      "Gradient Descent(688/999): loss=0.3680711471248598\n",
      "Gradient Descent(689/999): loss=0.3680641514862498\n",
      "Gradient Descent(690/999): loss=0.3680571759605575\n",
      "Gradient Descent(691/999): loss=0.36805022043640706\n",
      "Gradient Descent(692/999): loss=0.3680432848032656\n",
      "Gradient Descent(693/999): loss=0.3680363689514357\n",
      "Gradient Descent(694/999): loss=0.36802947277204784\n",
      "Gradient Descent(695/999): loss=0.36802259615705396\n",
      "Gradient Descent(696/999): loss=0.36801573899922024\n",
      "Gradient Descent(697/999): loss=0.3680089011921205\n",
      "Gradient Descent(698/999): loss=0.36800208263012907\n",
      "Gradient Descent(699/999): loss=0.36799528320841435\n",
      "Gradient Descent(700/999): loss=0.36798850282293183\n",
      "Gradient Descent(701/999): loss=0.3679817413704176\n",
      "Gradient Descent(702/999): loss=0.3679749987483819\n",
      "Gradient Descent(703/999): loss=0.36796827485510214\n",
      "Gradient Descent(704/999): loss=0.36796156958961707\n",
      "Gradient Descent(705/999): loss=0.36795488285172\n",
      "Gradient Descent(706/999): loss=0.36794821454195287\n",
      "Gradient Descent(707/999): loss=0.367941564561599\n",
      "Gradient Descent(708/999): loss=0.36793493281267786\n",
      "Gradient Descent(709/999): loss=0.3679283191979384\n",
      "Gradient Descent(710/999): loss=0.36792172362085324\n",
      "Gradient Descent(711/999): loss=0.3679151459856125\n",
      "Gradient Descent(712/999): loss=0.36790858619711714\n",
      "Gradient Descent(713/999): loss=0.36790204416097433\n",
      "Gradient Descent(714/999): loss=0.3678955197834901\n",
      "Gradient Descent(715/999): loss=0.3678890129716642\n",
      "Gradient Descent(716/999): loss=0.3678825236331849\n",
      "Gradient Descent(717/999): loss=0.3678760516764219\n",
      "Gradient Descent(718/999): loss=0.36786959701042127\n",
      "Gradient Descent(719/999): loss=0.3678631595449006\n",
      "Gradient Descent(720/999): loss=0.36785673919024175\n",
      "Gradient Descent(721/999): loss=0.3678503358574866\n",
      "Gradient Descent(722/999): loss=0.36784394945833093\n",
      "Gradient Descent(723/999): loss=0.36783757990511945\n",
      "Gradient Descent(724/999): loss=0.3678312271108395\n",
      "Gradient Descent(725/999): loss=0.36782489098911725\n",
      "Gradient Descent(726/999): loss=0.36781857145421\n",
      "Gradient Descent(727/999): loss=0.36781226842100356\n",
      "Gradient Descent(728/999): loss=0.3678059818050053\n",
      "Gradient Descent(729/999): loss=0.36779971152233937\n",
      "Gradient Descent(730/999): loss=0.3677934574897417\n",
      "Gradient Descent(731/999): loss=0.3677872196245552\n",
      "Gradient Descent(732/999): loss=0.3677809978447235\n",
      "Gradient Descent(733/999): loss=0.367774792068788\n",
      "Gradient Descent(734/999): loss=0.3677686022158807\n",
      "Gradient Descent(735/999): loss=0.3677624282057211\n",
      "Gradient Descent(736/999): loss=0.3677562699586103\n",
      "Gradient Descent(737/999): loss=0.3677501273954263\n",
      "Gradient Descent(738/999): loss=0.36774400043761934\n",
      "Gradient Descent(739/999): loss=0.3677378890072075\n",
      "Gradient Descent(740/999): loss=0.3677317930267713\n",
      "Gradient Descent(741/999): loss=0.36772571241945\n",
      "Gradient Descent(742/999): loss=0.3677196471089356\n",
      "Gradient Descent(743/999): loss=0.3677135970194697\n",
      "Gradient Descent(744/999): loss=0.36770756207583793\n",
      "Gradient Descent(745/999): loss=0.3677015422033663\n",
      "Gradient Descent(746/999): loss=0.3676955373279159\n",
      "Gradient Descent(747/999): loss=0.3676895473758786\n",
      "Gradient Descent(748/999): loss=0.3676835722741738\n",
      "Gradient Descent(749/999): loss=0.36767761195024273\n",
      "Gradient Descent(750/999): loss=0.36767166633204434\n",
      "Gradient Descent(751/999): loss=0.36766573534805197\n",
      "Gradient Descent(752/999): loss=0.36765981892724814\n",
      "Gradient Descent(753/999): loss=0.3676539169991206\n",
      "Gradient Descent(754/999): loss=0.3676480294936582\n",
      "Gradient Descent(755/999): loss=0.3676421563413476\n",
      "Gradient Descent(756/999): loss=0.3676362974731673\n",
      "Gradient Descent(757/999): loss=0.3676304528205856\n",
      "Gradient Descent(758/999): loss=0.3676246223155554\n",
      "Gradient Descent(759/999): loss=0.3676188058905104\n",
      "Gradient Descent(760/999): loss=0.3676130034783614\n",
      "Gradient Descent(761/999): loss=0.3676072150124924\n",
      "Gradient Descent(762/999): loss=0.36760144042675613\n",
      "Gradient Descent(763/999): loss=0.36759567965547113\n",
      "Gradient Descent(764/999): loss=0.3675899326334178\n",
      "Gradient Descent(765/999): loss=0.36758419929583336\n",
      "Gradient Descent(766/999): loss=0.3675784795784095\n",
      "Gradient Descent(767/999): loss=0.36757277341728845\n",
      "Gradient Descent(768/999): loss=0.367567080749059\n",
      "Gradient Descent(769/999): loss=0.3675614015107527\n",
      "Gradient Descent(770/999): loss=0.3675557356398404\n",
      "Gradient Descent(771/999): loss=0.3675500830742292\n",
      "Gradient Descent(772/999): loss=0.3675444437522581\n",
      "Gradient Descent(773/999): loss=0.3675388176126947\n",
      "Gradient Descent(774/999): loss=0.3675332045947321\n",
      "Gradient Descent(775/999): loss=0.3675276046379852\n",
      "Gradient Descent(776/999): loss=0.36752201768248677\n",
      "Gradient Descent(777/999): loss=0.367516443668685\n",
      "Gradient Descent(778/999): loss=0.3675108825374395\n",
      "Gradient Descent(779/999): loss=0.36750533423001813\n",
      "Gradient Descent(780/999): loss=0.3674997986880933\n",
      "Gradient Descent(781/999): loss=0.3674942758537394\n",
      "Gradient Descent(782/999): loss=0.36748876566942923\n",
      "Gradient Descent(783/999): loss=0.36748326807803033\n",
      "Gradient Descent(784/999): loss=0.3674777830228026\n",
      "Gradient Descent(785/999): loss=0.3674723104473945\n",
      "Gradient Descent(786/999): loss=0.36746685029584003\n",
      "Gradient Descent(787/999): loss=0.3674614025125557\n",
      "Gradient Descent(788/999): loss=0.3674559670423376\n",
      "Gradient Descent(789/999): loss=0.3674505438303581\n",
      "Gradient Descent(790/999): loss=0.3674451328221625\n",
      "Gradient Descent(791/999): loss=0.36743973396366686\n",
      "Gradient Descent(792/999): loss=0.36743434720115437\n",
      "Gradient Descent(793/999): loss=0.3674289724812725\n",
      "Gradient Descent(794/999): loss=0.3674236097510301\n",
      "Gradient Descent(795/999): loss=0.3674182589577947\n",
      "Gradient Descent(796/999): loss=0.36741292004928927\n",
      "Gradient Descent(797/999): loss=0.36740759297358977\n",
      "Gradient Descent(798/999): loss=0.36740227767912137\n",
      "Gradient Descent(799/999): loss=0.3673969741146573\n",
      "Gradient Descent(800/999): loss=0.36739168222931456\n",
      "Gradient Descent(801/999): loss=0.36738640197255157\n",
      "Gradient Descent(802/999): loss=0.36738113329416605\n",
      "Gradient Descent(803/999): loss=0.36737587614429146\n",
      "Gradient Descent(804/999): loss=0.3673706304733946\n",
      "Gradient Descent(805/999): loss=0.3673653962322734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(806/999): loss=0.3673601733720534\n",
      "Gradient Descent(807/999): loss=0.3673549618441861\n",
      "Gradient Descent(808/999): loss=0.36734976160044536\n",
      "Gradient Descent(809/999): loss=0.36734457259292586\n",
      "Gradient Descent(810/999): loss=0.3673393947740395\n",
      "Gradient Descent(811/999): loss=0.36733422809651384\n",
      "Gradient Descent(812/999): loss=0.3673290725133885\n",
      "Gradient Descent(813/999): loss=0.3673239279780141\n",
      "Gradient Descent(814/999): loss=0.3673187944440483\n",
      "Gradient Descent(815/999): loss=0.3673136718654545\n",
      "Gradient Descent(816/999): loss=0.3673085601964984\n",
      "Gradient Descent(817/999): loss=0.3673034593917468\n",
      "Gradient Descent(818/999): loss=0.3672983694060641\n",
      "Gradient Descent(819/999): loss=0.36729329019461054\n",
      "Gradient Descent(820/999): loss=0.3672882217128398\n",
      "Gradient Descent(821/999): loss=0.3672831639164961\n",
      "Gradient Descent(822/999): loss=0.3672781167616131\n",
      "Gradient Descent(823/999): loss=0.3672730802045103\n",
      "Gradient Descent(824/999): loss=0.3672680542017914\n",
      "Gradient Descent(825/999): loss=0.36726303871034255\n",
      "Gradient Descent(826/999): loss=0.3672580336873289\n",
      "Gradient Descent(827/999): loss=0.3672530390901934\n",
      "Gradient Descent(828/999): loss=0.3672480548766543\n",
      "Gradient Descent(829/999): loss=0.3672430810047028\n",
      "Gradient Descent(830/999): loss=0.3672381174326012\n",
      "Gradient Descent(831/999): loss=0.3672331641188805\n",
      "Gradient Descent(832/999): loss=0.36722822102233865\n",
      "Gradient Descent(833/999): loss=0.36722328810203775\n",
      "Gradient Descent(834/999): loss=0.3672183653173029\n",
      "Gradient Descent(835/999): loss=0.3672134526277194\n",
      "Gradient Descent(836/999): loss=0.3672085499931312\n",
      "Gradient Descent(837/999): loss=0.3672036573736381\n",
      "Gradient Descent(838/999): loss=0.36719877472959467\n",
      "Gradient Descent(839/999): loss=0.36719390202160807\n",
      "Gradient Descent(840/999): loss=0.36718903921053553\n",
      "Gradient Descent(841/999): loss=0.36718418625748267\n",
      "Gradient Descent(842/999): loss=0.36717934312380185\n",
      "Gradient Descent(843/999): loss=0.3671745097710902\n",
      "Gradient Descent(844/999): loss=0.3671696861611866\n",
      "Gradient Descent(845/999): loss=0.3671648722561716\n",
      "Gradient Descent(846/999): loss=0.3671600680183646\n",
      "Gradient Descent(847/999): loss=0.3671552734103215\n",
      "Gradient Descent(848/999): loss=0.36715048839483366\n",
      "Gradient Descent(849/999): loss=0.3671457129349259\n",
      "Gradient Descent(850/999): loss=0.3671409469938543\n",
      "Gradient Descent(851/999): loss=0.3671361905351052\n",
      "Gradient Descent(852/999): loss=0.3671314435223922\n",
      "Gradient Descent(853/999): loss=0.3671267059196556\n",
      "Gradient Descent(854/999): loss=0.3671219776910598\n",
      "Gradient Descent(855/999): loss=0.367117258800992\n",
      "Gradient Descent(856/999): loss=0.36711254921406056\n",
      "Gradient Descent(857/999): loss=0.36710784889509285\n",
      "Gradient Descent(858/999): loss=0.3671031578091336\n",
      "Gradient Descent(859/999): loss=0.36709847592144396\n",
      "Gradient Descent(860/999): loss=0.3670938031974988\n",
      "Gradient Descent(861/999): loss=0.36708913960298584\n",
      "Gradient Descent(862/999): loss=0.36708448510380337\n",
      "Gradient Descent(863/999): loss=0.36707983966605945\n",
      "Gradient Descent(864/999): loss=0.3670752032560694\n",
      "Gradient Descent(865/999): loss=0.3670705758403547\n",
      "Gradient Descent(866/999): loss=0.3670659573856414\n",
      "Gradient Descent(867/999): loss=0.36706134785885847\n",
      "Gradient Descent(868/999): loss=0.36705674722713616\n",
      "Gradient Descent(869/999): loss=0.3670521554578045\n",
      "Gradient Descent(870/999): loss=0.3670475725183918\n",
      "Gradient Descent(871/999): loss=0.3670429983766234\n",
      "Gradient Descent(872/999): loss=0.3670384330004195\n",
      "Gradient Descent(873/999): loss=0.36703387635789414\n",
      "Gradient Descent(874/999): loss=0.36702932841735403\n",
      "Gradient Descent(875/999): loss=0.36702478914729614\n",
      "Gradient Descent(876/999): loss=0.3670202585164069\n",
      "Gradient Descent(877/999): loss=0.367015736493561\n",
      "Gradient Descent(878/999): loss=0.3670112230478193\n",
      "Gradient Descent(879/999): loss=0.36700671814842756\n",
      "Gradient Descent(880/999): loss=0.36700222176481573\n",
      "Gradient Descent(881/999): loss=0.36699773386659545\n",
      "Gradient Descent(882/999): loss=0.36699325442355935\n",
      "Gradient Descent(883/999): loss=0.36698878340568\n",
      "Gradient Descent(884/999): loss=0.3669843207831073\n",
      "Gradient Descent(885/999): loss=0.36697986652616843\n",
      "Gradient Descent(886/999): loss=0.3669754206053658\n",
      "Gradient Descent(887/999): loss=0.36697098299137637\n",
      "Gradient Descent(888/999): loss=0.36696655365504943\n",
      "Gradient Descent(889/999): loss=0.36696213256740556\n",
      "Gradient Descent(890/999): loss=0.3669577196996361\n",
      "Gradient Descent(891/999): loss=0.36695331502310086\n",
      "Gradient Descent(892/999): loss=0.36694891850932765\n",
      "Gradient Descent(893/999): loss=0.36694453013001027\n",
      "Gradient Descent(894/999): loss=0.3669401498570078\n",
      "Gradient Descent(895/999): loss=0.36693577766234287\n",
      "Gradient Descent(896/999): loss=0.3669314135182017\n",
      "Gradient Descent(897/999): loss=0.3669270573969307\n",
      "Gradient Descent(898/999): loss=0.3669227092710371\n",
      "Gradient Descent(899/999): loss=0.36691836911318737\n",
      "Gradient Descent(900/999): loss=0.3669140368962052\n",
      "Gradient Descent(901/999): loss=0.3669097125930714\n",
      "Gradient Descent(902/999): loss=0.3669053961769219\n",
      "Gradient Descent(903/999): loss=0.36690108762104706\n",
      "Gradient Descent(904/999): loss=0.36689678689889055\n",
      "Gradient Descent(905/999): loss=0.36689249398404766\n",
      "Gradient Descent(906/999): loss=0.3668882088502648\n",
      "Gradient Descent(907/999): loss=0.36688393147143833\n",
      "Gradient Descent(908/999): loss=0.3668796618216126\n",
      "Gradient Descent(909/999): loss=0.3668753998749798\n",
      "Gradient Descent(910/999): loss=0.366871145605879\n",
      "Gradient Descent(911/999): loss=0.36686689898879415\n",
      "Gradient Descent(912/999): loss=0.36686265999835327\n",
      "Gradient Descent(913/999): loss=0.3668584286093279\n",
      "Gradient Descent(914/999): loss=0.36685420479663167\n",
      "Gradient Descent(915/999): loss=0.3668499885353192\n",
      "Gradient Descent(916/999): loss=0.36684577980058536\n",
      "Gradient Descent(917/999): loss=0.36684157856776356\n",
      "Gradient Descent(918/999): loss=0.3668373848123259\n",
      "Gradient Descent(919/999): loss=0.36683319850988044\n",
      "Gradient Descent(920/999): loss=0.3668290196361723\n",
      "Gradient Descent(921/999): loss=0.3668248481670806\n",
      "Gradient Descent(922/999): loss=0.3668206840786191\n",
      "Gradient Descent(923/999): loss=0.36681652734693415\n",
      "Gradient Descent(924/999): loss=0.36681237794830424\n",
      "Gradient Descent(925/999): loss=0.3668082358591387\n",
      "Gradient Descent(926/999): loss=0.36680410105597727\n",
      "Gradient Descent(927/999): loss=0.3667999735154883\n",
      "Gradient Descent(928/999): loss=0.3667958532144689\n",
      "Gradient Descent(929/999): loss=0.3667917401298433\n",
      "Gradient Descent(930/999): loss=0.36678763423866123\n",
      "Gradient Descent(931/999): loss=0.3667835355180989\n",
      "Gradient Descent(932/999): loss=0.3667794439454564\n",
      "Gradient Descent(933/999): loss=0.36677535949815765\n",
      "Gradient Descent(934/999): loss=0.3667712821537489\n",
      "Gradient Descent(935/999): loss=0.3667672118898984\n",
      "Gradient Descent(936/999): loss=0.3667631486843952\n",
      "Gradient Descent(937/999): loss=0.3667590925151485\n",
      "Gradient Descent(938/999): loss=0.36675504336018666\n",
      "Gradient Descent(939/999): loss=0.36675100119765586\n",
      "Gradient Descent(940/999): loss=0.3667469660058202\n",
      "Gradient Descent(941/999): loss=0.36674293776306044\n",
      "Gradient Descent(942/999): loss=0.3667389164478726\n",
      "Gradient Descent(943/999): loss=0.3667349020388678\n",
      "Gradient Descent(944/999): loss=0.36673089451477137\n",
      "Gradient Descent(945/999): loss=0.36672689385442153\n",
      "Gradient Descent(946/999): loss=0.36672290003676977\n",
      "Gradient Descent(947/999): loss=0.3667189130408781\n",
      "Gradient Descent(948/999): loss=0.36671493284592005\n",
      "Gradient Descent(949/999): loss=0.3667109594311788\n",
      "Gradient Descent(950/999): loss=0.3667069927760471\n",
      "Gradient Descent(951/999): loss=0.3667030328600263\n",
      "Gradient Descent(952/999): loss=0.3666990796627247\n",
      "Gradient Descent(953/999): loss=0.36669513316385827\n",
      "Gradient Descent(954/999): loss=0.36669119334324884\n",
      "Gradient Descent(955/999): loss=0.36668726018082365\n",
      "Gradient Descent(956/999): loss=0.36668333365661443\n",
      "Gradient Descent(957/999): loss=0.36667941375075686\n",
      "Gradient Descent(958/999): loss=0.3666755004434901\n",
      "Gradient Descent(959/999): loss=0.3666715937151553\n",
      "Gradient Descent(960/999): loss=0.3666676935461959\n",
      "Gradient Descent(961/999): loss=0.3666637999171554\n",
      "Gradient Descent(962/999): loss=0.36665991280867855\n",
      "Gradient Descent(963/999): loss=0.3666560322015089\n",
      "Gradient Descent(964/999): loss=0.36665215807648927\n",
      "Gradient Descent(965/999): loss=0.3666482904145606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(966/999): loss=0.3666444291967611\n",
      "Gradient Descent(967/999): loss=0.3666405744042259\n",
      "Gradient Descent(968/999): loss=0.3666367260181864\n",
      "Gradient Descent(969/999): loss=0.36663288401996885\n",
      "Gradient Descent(970/999): loss=0.366629048390995\n",
      "Gradient Descent(971/999): loss=0.36662521911277995\n",
      "Gradient Descent(972/999): loss=0.36662139616693284\n",
      "Gradient Descent(973/999): loss=0.3666175795351552\n",
      "Gradient Descent(974/999): loss=0.36661376919924094\n",
      "Gradient Descent(975/999): loss=0.366609965141075\n",
      "Gradient Descent(976/999): loss=0.3666061673426339\n",
      "Gradient Descent(977/999): loss=0.3666023757859838\n",
      "Gradient Descent(978/999): loss=0.36659859045328036\n",
      "Gradient Descent(979/999): loss=0.36659481132676863\n",
      "Gradient Descent(980/999): loss=0.36659103838878176\n",
      "Gradient Descent(981/999): loss=0.3665872716217407\n",
      "Gradient Descent(982/999): loss=0.36658351100815345\n",
      "Gradient Descent(983/999): loss=0.3665797565306145\n",
      "Gradient Descent(984/999): loss=0.3665760081718043\n",
      "Gradient Descent(985/999): loss=0.3665722659144887\n",
      "Gradient Descent(986/999): loss=0.3665685297415181\n",
      "Gradient Descent(987/999): loss=0.36656479963582744\n",
      "Gradient Descent(988/999): loss=0.3665610755804347\n",
      "Gradient Descent(989/999): loss=0.366557357558441\n",
      "Gradient Descent(990/999): loss=0.3665536455530303\n",
      "Gradient Descent(991/999): loss=0.3665499395474679\n",
      "Gradient Descent(992/999): loss=0.3665462395251008\n",
      "Gradient Descent(993/999): loss=0.36654254546935633\n",
      "Gradient Descent(994/999): loss=0.3665388573637422\n",
      "Gradient Descent(995/999): loss=0.36653517519184586\n",
      "Gradient Descent(996/999): loss=0.36653149893733394\n",
      "Gradient Descent(997/999): loss=0.36652782858395133\n",
      "Gradient Descent(998/999): loss=0.3665241641155211\n",
      "Gradient Descent(999/999): loss=0.36652050551594373\n"
     ]
    }
   ],
   "source": [
    "w, mse = least_squares_GD(y_train, tX_train, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7219155555555555"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict_labels(w, tX_train)\n",
    "np.mean(y_train.reshape(-1,1)==y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72184"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict_labels(w, tX_val)\n",
    "np.mean(y_val.reshape(-1,1)==y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Squares SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros(tX.shape[1])#np.random.normal(loc=0, scale=1, size=(tX.shape[1],1))\n",
    "max_iters = 5000\n",
    "gamma = 3e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/999): loss=0.4978445564319043\n",
      "Gradient Descent(1/999): loss=0.49123923762061833\n",
      "Gradient Descent(2/999): loss=0.4880131989135347\n",
      "Gradient Descent(3/999): loss=0.4845408339254324\n",
      "Gradient Descent(4/999): loss=0.4865696244391807\n",
      "Gradient Descent(5/999): loss=0.482804517193521\n",
      "Gradient Descent(6/999): loss=0.48129068967045385\n",
      "Gradient Descent(7/999): loss=0.4812130371856222\n",
      "Gradient Descent(8/999): loss=0.48232716417095733\n",
      "Gradient Descent(9/999): loss=0.4825403986654286\n",
      "Gradient Descent(10/999): loss=0.4812578237833007\n",
      "Gradient Descent(11/999): loss=0.4793890881185568\n",
      "Gradient Descent(12/999): loss=0.4778084057884684\n",
      "Gradient Descent(13/999): loss=0.4750920367974531\n",
      "Gradient Descent(14/999): loss=0.4758001275271161\n",
      "Gradient Descent(15/999): loss=0.47444480301903796\n",
      "Gradient Descent(16/999): loss=0.4738897010827102\n",
      "Gradient Descent(17/999): loss=0.47340607204806545\n",
      "Gradient Descent(18/999): loss=0.4733425343451806\n",
      "Gradient Descent(19/999): loss=0.47345163269696033\n",
      "Gradient Descent(20/999): loss=0.47290695759914236\n",
      "Gradient Descent(21/999): loss=0.47255634284234077\n",
      "Gradient Descent(22/999): loss=0.4719716198022061\n",
      "Gradient Descent(23/999): loss=0.4714204184225932\n",
      "Gradient Descent(24/999): loss=0.471133737702898\n",
      "Gradient Descent(25/999): loss=0.47082514080116494\n",
      "Gradient Descent(26/999): loss=0.4708862729771384\n",
      "Gradient Descent(27/999): loss=0.4699154639091567\n",
      "Gradient Descent(28/999): loss=0.4694480083326204\n",
      "Gradient Descent(29/999): loss=0.46923364155046915\n",
      "Gradient Descent(30/999): loss=0.46894434539500174\n",
      "Gradient Descent(31/999): loss=0.4687033191515611\n",
      "Gradient Descent(32/999): loss=0.4685554364048271\n",
      "Gradient Descent(33/999): loss=0.4684990575511305\n",
      "Gradient Descent(34/999): loss=0.46827952564946146\n",
      "Gradient Descent(35/999): loss=0.46806366971284163\n",
      "Gradient Descent(36/999): loss=0.4678247935046933\n",
      "Gradient Descent(37/999): loss=0.4676955296633225\n",
      "Gradient Descent(38/999): loss=0.4673323467572083\n",
      "Gradient Descent(39/999): loss=0.4673413635667016\n",
      "Gradient Descent(40/999): loss=0.46703945010947623\n",
      "Gradient Descent(41/999): loss=0.4668993116780129\n",
      "Gradient Descent(42/999): loss=0.4667721377574088\n",
      "Gradient Descent(43/999): loss=0.4671103762440999\n",
      "Gradient Descent(44/999): loss=0.4670332088446792\n",
      "Gradient Descent(45/999): loss=0.46706963097549453\n",
      "Gradient Descent(46/999): loss=0.467209739054033\n",
      "Gradient Descent(47/999): loss=0.467845815415532\n",
      "Gradient Descent(48/999): loss=0.4679255495139672\n",
      "Gradient Descent(49/999): loss=0.4667153379571502\n",
      "Gradient Descent(50/999): loss=0.46583517017619025\n",
      "Gradient Descent(51/999): loss=0.46580683239533527\n",
      "Gradient Descent(52/999): loss=0.465042910190008\n",
      "Gradient Descent(53/999): loss=0.4646431617351284\n",
      "Gradient Descent(54/999): loss=0.464400390801461\n",
      "Gradient Descent(55/999): loss=0.46388398089257965\n",
      "Gradient Descent(56/999): loss=0.463595213448514\n",
      "Gradient Descent(57/999): loss=0.46345012167225175\n",
      "Gradient Descent(58/999): loss=0.4634135372209672\n",
      "Gradient Descent(59/999): loss=0.4631052409886764\n",
      "Gradient Descent(60/999): loss=0.4629516016067651\n",
      "Gradient Descent(61/999): loss=0.46280607651124556\n",
      "Gradient Descent(62/999): loss=0.4626268126330189\n",
      "Gradient Descent(63/999): loss=0.4625989350854609\n",
      "Gradient Descent(64/999): loss=0.4630153065267691\n",
      "Gradient Descent(65/999): loss=0.46247141765399935\n",
      "Gradient Descent(66/999): loss=0.4630166176200995\n",
      "Gradient Descent(67/999): loss=0.4634337292784185\n",
      "Gradient Descent(68/999): loss=0.46287228938228053\n",
      "Gradient Descent(69/999): loss=0.463509404529731\n",
      "Gradient Descent(70/999): loss=0.46275490089194127\n",
      "Gradient Descent(71/999): loss=0.46148167905062787\n",
      "Gradient Descent(72/999): loss=0.4612809032646238\n",
      "Gradient Descent(73/999): loss=0.4620689467647663\n",
      "Gradient Descent(74/999): loss=0.46218452749872635\n",
      "Gradient Descent(75/999): loss=0.46161266265882095\n",
      "Gradient Descent(76/999): loss=0.4610398778249127\n",
      "Gradient Descent(77/999): loss=0.460086265750447\n",
      "Gradient Descent(78/999): loss=0.4595390663624854\n",
      "Gradient Descent(79/999): loss=0.4594019356816811\n",
      "Gradient Descent(80/999): loss=0.45903475667626636\n",
      "Gradient Descent(81/999): loss=0.45883639321286085\n",
      "Gradient Descent(82/999): loss=0.4586452958390403\n",
      "Gradient Descent(83/999): loss=0.45846325808816907\n",
      "Gradient Descent(84/999): loss=0.45847193312543305\n",
      "Gradient Descent(85/999): loss=0.4579347493866558\n",
      "Gradient Descent(86/999): loss=0.45779441538675075\n",
      "Gradient Descent(87/999): loss=0.457510051986463\n",
      "Gradient Descent(88/999): loss=0.45732451511576433\n",
      "Gradient Descent(89/999): loss=0.45708810510388587\n",
      "Gradient Descent(90/999): loss=0.4569665521709717\n",
      "Gradient Descent(91/999): loss=0.45739604830071134\n",
      "Gradient Descent(92/999): loss=0.4581912380321968\n",
      "Gradient Descent(93/999): loss=0.4570354885407032\n",
      "Gradient Descent(94/999): loss=0.45668886650583584\n",
      "Gradient Descent(95/999): loss=0.4563938440827609\n",
      "Gradient Descent(96/999): loss=0.45601771377051714\n",
      "Gradient Descent(97/999): loss=0.45586960008675786\n",
      "Gradient Descent(98/999): loss=0.4556586534721917\n",
      "Gradient Descent(99/999): loss=0.4554809931407441\n",
      "Gradient Descent(100/999): loss=0.4556603083142728\n",
      "Gradient Descent(101/999): loss=0.45526563224542144\n",
      "Gradient Descent(102/999): loss=0.4549794748995031\n",
      "Gradient Descent(103/999): loss=0.4548423036698165\n",
      "Gradient Descent(104/999): loss=0.4545023326071126\n",
      "Gradient Descent(105/999): loss=0.454241176085902\n",
      "Gradient Descent(106/999): loss=0.4540142623919829\n",
      "Gradient Descent(107/999): loss=0.45392780416832457\n",
      "Gradient Descent(108/999): loss=0.4541501596250837\n",
      "Gradient Descent(109/999): loss=0.45380420012299233\n",
      "Gradient Descent(110/999): loss=0.45394283719562767\n",
      "Gradient Descent(111/999): loss=0.45362313007144295\n",
      "Gradient Descent(112/999): loss=0.4532467940991028\n",
      "Gradient Descent(113/999): loss=0.4529245701031568\n",
      "Gradient Descent(114/999): loss=0.452868851326508\n",
      "Gradient Descent(115/999): loss=0.45252502441346476\n",
      "Gradient Descent(116/999): loss=0.4526079776374259\n",
      "Gradient Descent(117/999): loss=0.4526403737018846\n",
      "Gradient Descent(118/999): loss=0.4529290826678366\n",
      "Gradient Descent(119/999): loss=0.4525359544081379\n",
      "Gradient Descent(120/999): loss=0.4519654480256863\n",
      "Gradient Descent(121/999): loss=0.4526141828710783\n",
      "Gradient Descent(122/999): loss=0.4522581014872416\n",
      "Gradient Descent(123/999): loss=0.45194657992072335\n",
      "Gradient Descent(124/999): loss=0.45209727614294065\n",
      "Gradient Descent(125/999): loss=0.4514424974342194\n",
      "Gradient Descent(126/999): loss=0.4510565507442195\n",
      "Gradient Descent(127/999): loss=0.4508162350069529\n",
      "Gradient Descent(128/999): loss=0.4506508152037354\n",
      "Gradient Descent(129/999): loss=0.4502491072171185\n",
      "Gradient Descent(130/999): loss=0.45044232785890614\n",
      "Gradient Descent(131/999): loss=0.45088182048258\n",
      "Gradient Descent(132/999): loss=0.4506410710981693\n",
      "Gradient Descent(133/999): loss=0.4506606208850966\n",
      "Gradient Descent(134/999): loss=0.4509198479012757\n",
      "Gradient Descent(135/999): loss=0.4501949868793718\n",
      "Gradient Descent(136/999): loss=0.4503062932040171\n",
      "Gradient Descent(137/999): loss=0.44931911949099274\n",
      "Gradient Descent(138/999): loss=0.44933341036395263\n",
      "Gradient Descent(139/999): loss=0.44911479122019865\n",
      "Gradient Descent(140/999): loss=0.44915316691049084\n",
      "Gradient Descent(141/999): loss=0.44916300307114543\n",
      "Gradient Descent(142/999): loss=0.44891404127915013\n",
      "Gradient Descent(143/999): loss=0.4485504926370205\n",
      "Gradient Descent(144/999): loss=0.44824117225706345\n",
      "Gradient Descent(145/999): loss=0.44772205717774916\n",
      "Gradient Descent(146/999): loss=0.4476467971962212\n",
      "Gradient Descent(147/999): loss=0.44747082412870903\n",
      "Gradient Descent(148/999): loss=0.4472909881509993\n",
      "Gradient Descent(149/999): loss=0.44712445552167135\n",
      "Gradient Descent(150/999): loss=0.44715148361652707\n",
      "Gradient Descent(151/999): loss=0.44679092191149805\n",
      "Gradient Descent(152/999): loss=0.44674242469091396\n",
      "Gradient Descent(153/999): loss=0.44662811278646103\n",
      "Gradient Descent(154/999): loss=0.4464816089212643\n",
      "Gradient Descent(155/999): loss=0.4464985200792186\n",
      "Gradient Descent(156/999): loss=0.4462979609983535\n",
      "Gradient Descent(157/999): loss=0.4460108324712086\n",
      "Gradient Descent(158/999): loss=0.44589222982061033\n",
      "Gradient Descent(159/999): loss=0.44585812167399336\n",
      "Gradient Descent(160/999): loss=0.44577690077095544\n",
      "Gradient Descent(161/999): loss=0.44525334285762663\n",
      "Gradient Descent(162/999): loss=0.4451096009539626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(163/999): loss=0.44479296097341187\n",
      "Gradient Descent(164/999): loss=0.4450521803310013\n",
      "Gradient Descent(165/999): loss=0.44483408582899486\n",
      "Gradient Descent(166/999): loss=0.4450814962856125\n",
      "Gradient Descent(167/999): loss=0.4453018177058671\n",
      "Gradient Descent(168/999): loss=0.4447993223783432\n",
      "Gradient Descent(169/999): loss=0.44486231572298407\n",
      "Gradient Descent(170/999): loss=0.44440007150573485\n",
      "Gradient Descent(171/999): loss=0.44472924434548655\n",
      "Gradient Descent(172/999): loss=0.4447099156371695\n",
      "Gradient Descent(173/999): loss=0.4438578428253266\n",
      "Gradient Descent(174/999): loss=0.4435315866956234\n",
      "Gradient Descent(175/999): loss=0.44369885213643306\n",
      "Gradient Descent(176/999): loss=0.443308257692218\n",
      "Gradient Descent(177/999): loss=0.44300010268246703\n",
      "Gradient Descent(178/999): loss=0.44285566323550085\n",
      "Gradient Descent(179/999): loss=0.4426808287296232\n",
      "Gradient Descent(180/999): loss=0.44255065627128665\n",
      "Gradient Descent(181/999): loss=0.4427642856489929\n",
      "Gradient Descent(182/999): loss=0.44266579912400156\n",
      "Gradient Descent(183/999): loss=0.44263024051715516\n",
      "Gradient Descent(184/999): loss=0.44241387495734796\n",
      "Gradient Descent(185/999): loss=0.4423113133476265\n",
      "Gradient Descent(186/999): loss=0.44251802754213015\n",
      "Gradient Descent(187/999): loss=0.44234112268919556\n",
      "Gradient Descent(188/999): loss=0.4426730705766474\n",
      "Gradient Descent(189/999): loss=0.44214467030804694\n",
      "Gradient Descent(190/999): loss=0.4417414404397522\n",
      "Gradient Descent(191/999): loss=0.4413747003964426\n",
      "Gradient Descent(192/999): loss=0.441129522516274\n",
      "Gradient Descent(193/999): loss=0.4409920937538925\n",
      "Gradient Descent(194/999): loss=0.4407766033896517\n",
      "Gradient Descent(195/999): loss=0.44056780095741965\n",
      "Gradient Descent(196/999): loss=0.44065563063528396\n",
      "Gradient Descent(197/999): loss=0.4403559530449217\n",
      "Gradient Descent(198/999): loss=0.44015381124139696\n",
      "Gradient Descent(199/999): loss=0.4402733830494377\n",
      "Gradient Descent(200/999): loss=0.43997721334309353\n",
      "Gradient Descent(201/999): loss=0.43980714564120066\n",
      "Gradient Descent(202/999): loss=0.4397024557781551\n",
      "Gradient Descent(203/999): loss=0.4395182142717852\n",
      "Gradient Descent(204/999): loss=0.4393643506442928\n",
      "Gradient Descent(205/999): loss=0.43931995022864534\n",
      "Gradient Descent(206/999): loss=0.4393940795666321\n",
      "Gradient Descent(207/999): loss=0.43929686197701545\n",
      "Gradient Descent(208/999): loss=0.43905215833301603\n",
      "Gradient Descent(209/999): loss=0.43900708434030916\n",
      "Gradient Descent(210/999): loss=0.4392047481173299\n",
      "Gradient Descent(211/999): loss=0.43888237663226654\n",
      "Gradient Descent(212/999): loss=0.4386848692550268\n",
      "Gradient Descent(213/999): loss=0.438594320475972\n",
      "Gradient Descent(214/999): loss=0.4385314170203267\n",
      "Gradient Descent(215/999): loss=0.43844142349858445\n",
      "Gradient Descent(216/999): loss=0.43849472912463455\n",
      "Gradient Descent(217/999): loss=0.4385080700255276\n",
      "Gradient Descent(218/999): loss=0.4381307869688676\n",
      "Gradient Descent(219/999): loss=0.4381095438890168\n",
      "Gradient Descent(220/999): loss=0.43809793490435295\n",
      "Gradient Descent(221/999): loss=0.4379574098941761\n",
      "Gradient Descent(222/999): loss=0.4377348141013609\n",
      "Gradient Descent(223/999): loss=0.43756408817481746\n",
      "Gradient Descent(224/999): loss=0.437631802732865\n",
      "Gradient Descent(225/999): loss=0.4373723677813838\n",
      "Gradient Descent(226/999): loss=0.43725287065684576\n",
      "Gradient Descent(227/999): loss=0.4371373336088593\n",
      "Gradient Descent(228/999): loss=0.43714827689065894\n",
      "Gradient Descent(229/999): loss=0.43700318517304837\n",
      "Gradient Descent(230/999): loss=0.43689455208776334\n",
      "Gradient Descent(231/999): loss=0.4368562236432135\n",
      "Gradient Descent(232/999): loss=0.43681392042178546\n",
      "Gradient Descent(233/999): loss=0.43701928127441664\n",
      "Gradient Descent(234/999): loss=0.4367993071060096\n",
      "Gradient Descent(235/999): loss=0.4365522500356253\n",
      "Gradient Descent(236/999): loss=0.43639240435729815\n",
      "Gradient Descent(237/999): loss=0.4363324777623843\n",
      "Gradient Descent(238/999): loss=0.43620510980155586\n",
      "Gradient Descent(239/999): loss=0.43606358032621945\n",
      "Gradient Descent(240/999): loss=0.4363140262458733\n",
      "Gradient Descent(241/999): loss=0.4357954077345989\n",
      "Gradient Descent(242/999): loss=0.435629597580029\n",
      "Gradient Descent(243/999): loss=0.43566753438969785\n",
      "Gradient Descent(244/999): loss=0.43536973683778685\n",
      "Gradient Descent(245/999): loss=0.43523598982060013\n",
      "Gradient Descent(246/999): loss=0.4350755162150398\n",
      "Gradient Descent(247/999): loss=0.4349478862323907\n",
      "Gradient Descent(248/999): loss=0.43481415482772884\n",
      "Gradient Descent(249/999): loss=0.43473972141994344\n",
      "Gradient Descent(250/999): loss=0.43476729913148693\n",
      "Gradient Descent(251/999): loss=0.4346036274176066\n",
      "Gradient Descent(252/999): loss=0.4344749221892148\n",
      "Gradient Descent(253/999): loss=0.4344120614735768\n",
      "Gradient Descent(254/999): loss=0.4342958590828063\n",
      "Gradient Descent(255/999): loss=0.43421980388006093\n",
      "Gradient Descent(256/999): loss=0.43406022647013165\n",
      "Gradient Descent(257/999): loss=0.43397632157474575\n",
      "Gradient Descent(258/999): loss=0.4339369176533613\n",
      "Gradient Descent(259/999): loss=0.43379423156294317\n",
      "Gradient Descent(260/999): loss=0.4335884357481559\n",
      "Gradient Descent(261/999): loss=0.43357622441988114\n",
      "Gradient Descent(262/999): loss=0.4336353139255977\n",
      "Gradient Descent(263/999): loss=0.43363554567741236\n",
      "Gradient Descent(264/999): loss=0.433558458356916\n",
      "Gradient Descent(265/999): loss=0.43377290584750255\n",
      "Gradient Descent(266/999): loss=0.43310405680933395\n",
      "Gradient Descent(267/999): loss=0.43301839424885674\n",
      "Gradient Descent(268/999): loss=0.43293222678910737\n",
      "Gradient Descent(269/999): loss=0.43282186130731337\n",
      "Gradient Descent(270/999): loss=0.43278988485960385\n",
      "Gradient Descent(271/999): loss=0.4326329746545167\n",
      "Gradient Descent(272/999): loss=0.4326077829777607\n",
      "Gradient Descent(273/999): loss=0.4325147821531183\n",
      "Gradient Descent(274/999): loss=0.43246559470840434\n",
      "Gradient Descent(275/999): loss=0.43254312920609667\n",
      "Gradient Descent(276/999): loss=0.43258566349026684\n",
      "Gradient Descent(277/999): loss=0.4324347538494128\n",
      "Gradient Descent(278/999): loss=0.4321383465806456\n",
      "Gradient Descent(279/999): loss=0.4320129288884941\n",
      "Gradient Descent(280/999): loss=0.4318901985625639\n",
      "Gradient Descent(281/999): loss=0.43182149046090573\n",
      "Gradient Descent(282/999): loss=0.431729368177451\n",
      "Gradient Descent(283/999): loss=0.43165537053204206\n",
      "Gradient Descent(284/999): loss=0.43152130691409946\n",
      "Gradient Descent(285/999): loss=0.43140108311937697\n",
      "Gradient Descent(286/999): loss=0.4313729632002175\n",
      "Gradient Descent(287/999): loss=0.4314519520220319\n",
      "Gradient Descent(288/999): loss=0.4312058999283623\n",
      "Gradient Descent(289/999): loss=0.43116643520320097\n",
      "Gradient Descent(290/999): loss=0.43101836377878305\n",
      "Gradient Descent(291/999): loss=0.43089239895434717\n",
      "Gradient Descent(292/999): loss=0.4307767862630283\n",
      "Gradient Descent(293/999): loss=0.43091732616670847\n",
      "Gradient Descent(294/999): loss=0.43086086517114847\n",
      "Gradient Descent(295/999): loss=0.43049949762183215\n",
      "Gradient Descent(296/999): loss=0.4302864616468542\n",
      "Gradient Descent(297/999): loss=0.43013970945207297\n",
      "Gradient Descent(298/999): loss=0.43007662503701743\n",
      "Gradient Descent(299/999): loss=0.4300095088342779\n",
      "Gradient Descent(300/999): loss=0.4300008633555352\n",
      "Gradient Descent(301/999): loss=0.4301733028016119\n",
      "Gradient Descent(302/999): loss=0.42985080327824454\n",
      "Gradient Descent(303/999): loss=0.42981802332978153\n",
      "Gradient Descent(304/999): loss=0.42967079515542633\n",
      "Gradient Descent(305/999): loss=0.42938212302279855\n",
      "Gradient Descent(306/999): loss=0.4292457762586784\n",
      "Gradient Descent(307/999): loss=0.4292152740752582\n",
      "Gradient Descent(308/999): loss=0.4295760039638427\n",
      "Gradient Descent(309/999): loss=0.429365384075156\n",
      "Gradient Descent(310/999): loss=0.4294932091729923\n",
      "Gradient Descent(311/999): loss=0.42913587664078645\n",
      "Gradient Descent(312/999): loss=0.4299202123752144\n",
      "Gradient Descent(313/999): loss=0.42952503374196616\n",
      "Gradient Descent(314/999): loss=0.4296101742900926\n",
      "Gradient Descent(315/999): loss=0.4291859259634236\n",
      "Gradient Descent(316/999): loss=0.4285056381987604\n",
      "Gradient Descent(317/999): loss=0.42870390234846795\n",
      "Gradient Descent(318/999): loss=0.42903848615249923\n",
      "Gradient Descent(319/999): loss=0.42919009763175925\n",
      "Gradient Descent(320/999): loss=0.4286442296581669\n",
      "Gradient Descent(321/999): loss=0.4283041944938039\n",
      "Gradient Descent(322/999): loss=0.42865123036348574\n",
      "Gradient Descent(323/999): loss=0.42823304712504956\n",
      "Gradient Descent(324/999): loss=0.4280354226128468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(325/999): loss=0.4278609897521303\n",
      "Gradient Descent(326/999): loss=0.42779406528120495\n",
      "Gradient Descent(327/999): loss=0.4276754421324031\n",
      "Gradient Descent(328/999): loss=0.4276411017242782\n",
      "Gradient Descent(329/999): loss=0.4276320191109741\n",
      "Gradient Descent(330/999): loss=0.42761395913962785\n",
      "Gradient Descent(331/999): loss=0.42745753284331794\n",
      "Gradient Descent(332/999): loss=0.42774808290928823\n",
      "Gradient Descent(333/999): loss=0.427788650592864\n",
      "Gradient Descent(334/999): loss=0.4271120367898014\n",
      "Gradient Descent(335/999): loss=0.4270699318739413\n",
      "Gradient Descent(336/999): loss=0.427236418410412\n",
      "Gradient Descent(337/999): loss=0.4267612694181999\n",
      "Gradient Descent(338/999): loss=0.426704476657892\n",
      "Gradient Descent(339/999): loss=0.42671167884985506\n",
      "Gradient Descent(340/999): loss=0.42656256350692234\n",
      "Gradient Descent(341/999): loss=0.4266029614300968\n",
      "Gradient Descent(342/999): loss=0.4263973210817566\n",
      "Gradient Descent(343/999): loss=0.4263075124421799\n",
      "Gradient Descent(344/999): loss=0.426219492213541\n",
      "Gradient Descent(345/999): loss=0.4261481904398483\n",
      "Gradient Descent(346/999): loss=0.42600518884648986\n",
      "Gradient Descent(347/999): loss=0.4260019060130009\n",
      "Gradient Descent(348/999): loss=0.42591753460037657\n",
      "Gradient Descent(349/999): loss=0.42576730026387755\n",
      "Gradient Descent(350/999): loss=0.42572894047747295\n",
      "Gradient Descent(351/999): loss=0.4256602420897141\n",
      "Gradient Descent(352/999): loss=0.4256610537617265\n",
      "Gradient Descent(353/999): loss=0.42559358788160795\n",
      "Gradient Descent(354/999): loss=0.42585297258889954\n",
      "Gradient Descent(355/999): loss=0.4258345170743291\n",
      "Gradient Descent(356/999): loss=0.42576560625815174\n",
      "Gradient Descent(357/999): loss=0.42542446299525494\n",
      "Gradient Descent(358/999): loss=0.42589278732229074\n",
      "Gradient Descent(359/999): loss=0.425376274245315\n",
      "Gradient Descent(360/999): loss=0.42549102180087406\n",
      "Gradient Descent(361/999): loss=0.425668628516891\n",
      "Gradient Descent(362/999): loss=0.4259294226711321\n",
      "Gradient Descent(363/999): loss=0.4257837668393762\n",
      "Gradient Descent(364/999): loss=0.4258114300586656\n",
      "Gradient Descent(365/999): loss=0.4269079255663003\n",
      "Gradient Descent(366/999): loss=0.4263113790521518\n",
      "Gradient Descent(367/999): loss=0.4261229270810943\n",
      "Gradient Descent(368/999): loss=0.4265458891617961\n",
      "Gradient Descent(369/999): loss=0.4257864109400308\n",
      "Gradient Descent(370/999): loss=0.4249673779145391\n",
      "Gradient Descent(371/999): loss=0.4252257491642855\n",
      "Gradient Descent(372/999): loss=0.42477578017392487\n",
      "Gradient Descent(373/999): loss=0.4246044435733834\n",
      "Gradient Descent(374/999): loss=0.42516616100869076\n",
      "Gradient Descent(375/999): loss=0.4246935743411406\n",
      "Gradient Descent(376/999): loss=0.42467059554181524\n",
      "Gradient Descent(377/999): loss=0.4240624964521547\n",
      "Gradient Descent(378/999): loss=0.4238770930436347\n",
      "Gradient Descent(379/999): loss=0.42393512988694565\n",
      "Gradient Descent(380/999): loss=0.4238459694788167\n",
      "Gradient Descent(381/999): loss=0.42429339025289936\n",
      "Gradient Descent(382/999): loss=0.4242194047285764\n",
      "Gradient Descent(383/999): loss=0.4238408495218758\n",
      "Gradient Descent(384/999): loss=0.42435248027507094\n",
      "Gradient Descent(385/999): loss=0.42384249166168647\n",
      "Gradient Descent(386/999): loss=0.42394794820834314\n",
      "Gradient Descent(387/999): loss=0.4236121590845174\n",
      "Gradient Descent(388/999): loss=0.4235573200825438\n",
      "Gradient Descent(389/999): loss=0.42328673301539377\n",
      "Gradient Descent(390/999): loss=0.4230990591897362\n",
      "Gradient Descent(391/999): loss=0.4230047867531486\n",
      "Gradient Descent(392/999): loss=0.4229247024830935\n",
      "Gradient Descent(393/999): loss=0.4228642811234382\n",
      "Gradient Descent(394/999): loss=0.42291299310384456\n",
      "Gradient Descent(395/999): loss=0.42299393927056583\n",
      "Gradient Descent(396/999): loss=0.4228578145524541\n",
      "Gradient Descent(397/999): loss=0.42276510272853113\n",
      "Gradient Descent(398/999): loss=0.4225867288889931\n",
      "Gradient Descent(399/999): loss=0.4224944317291868\n",
      "Gradient Descent(400/999): loss=0.4224050321380583\n",
      "Gradient Descent(401/999): loss=0.4224284321001304\n",
      "Gradient Descent(402/999): loss=0.4223441754061196\n",
      "Gradient Descent(403/999): loss=0.42281036456577126\n",
      "Gradient Descent(404/999): loss=0.4225993858231266\n",
      "Gradient Descent(405/999): loss=0.42198860749769235\n",
      "Gradient Descent(406/999): loss=0.42187601461204377\n",
      "Gradient Descent(407/999): loss=0.4220632637435495\n",
      "Gradient Descent(408/999): loss=0.4218176218291626\n",
      "Gradient Descent(409/999): loss=0.4217677170550439\n",
      "Gradient Descent(410/999): loss=0.42159156797768577\n",
      "Gradient Descent(411/999): loss=0.42156502673390095\n",
      "Gradient Descent(412/999): loss=0.42150577781666043\n",
      "Gradient Descent(413/999): loss=0.4216503207524744\n",
      "Gradient Descent(414/999): loss=0.4216848641123594\n",
      "Gradient Descent(415/999): loss=0.42139227372916066\n",
      "Gradient Descent(416/999): loss=0.42126808250130227\n",
      "Gradient Descent(417/999): loss=0.42129882468507124\n",
      "Gradient Descent(418/999): loss=0.42112085559108187\n",
      "Gradient Descent(419/999): loss=0.42105647739254487\n",
      "Gradient Descent(420/999): loss=0.4210947031339948\n",
      "Gradient Descent(421/999): loss=0.4210516907607613\n",
      "Gradient Descent(422/999): loss=0.420893803800255\n",
      "Gradient Descent(423/999): loss=0.4208433533526907\n",
      "Gradient Descent(424/999): loss=0.42078211175445424\n",
      "Gradient Descent(425/999): loss=0.42074581745788525\n",
      "Gradient Descent(426/999): loss=0.4207257088647195\n",
      "Gradient Descent(427/999): loss=0.4209320522698444\n",
      "Gradient Descent(428/999): loss=0.42068832093650027\n",
      "Gradient Descent(429/999): loss=0.4205682923969189\n",
      "Gradient Descent(430/999): loss=0.4204822717544225\n",
      "Gradient Descent(431/999): loss=0.4206375784808433\n",
      "Gradient Descent(432/999): loss=0.42045606922498047\n",
      "Gradient Descent(433/999): loss=0.4204469742853985\n",
      "Gradient Descent(434/999): loss=0.42039109340243086\n",
      "Gradient Descent(435/999): loss=0.4203146057624375\n",
      "Gradient Descent(436/999): loss=0.42024294757534014\n",
      "Gradient Descent(437/999): loss=0.4202316246327125\n",
      "Gradient Descent(438/999): loss=0.42079157457281635\n",
      "Gradient Descent(439/999): loss=0.42030111566065526\n",
      "Gradient Descent(440/999): loss=0.4199821240073192\n",
      "Gradient Descent(441/999): loss=0.41991195282056976\n",
      "Gradient Descent(442/999): loss=0.41983747877532707\n",
      "Gradient Descent(443/999): loss=0.41977096845621537\n",
      "Gradient Descent(444/999): loss=0.41981008931433894\n",
      "Gradient Descent(445/999): loss=0.41973039422914754\n",
      "Gradient Descent(446/999): loss=0.4198992628442307\n",
      "Gradient Descent(447/999): loss=0.41982144551213635\n",
      "Gradient Descent(448/999): loss=0.42006797507571425\n",
      "Gradient Descent(449/999): loss=0.4197378752338443\n",
      "Gradient Descent(450/999): loss=0.41954023633755666\n",
      "Gradient Descent(451/999): loss=0.4196044916650453\n",
      "Gradient Descent(452/999): loss=0.4196478774936416\n",
      "Gradient Descent(453/999): loss=0.4194724420004253\n",
      "Gradient Descent(454/999): loss=0.4196045480285701\n",
      "Gradient Descent(455/999): loss=0.41938700106206367\n",
      "Gradient Descent(456/999): loss=0.4191972059302367\n",
      "Gradient Descent(457/999): loss=0.4191007968160574\n",
      "Gradient Descent(458/999): loss=0.41943347235788986\n",
      "Gradient Descent(459/999): loss=0.4192661338575193\n",
      "Gradient Descent(460/999): loss=0.41912122677057057\n",
      "Gradient Descent(461/999): loss=0.41948297433344856\n",
      "Gradient Descent(462/999): loss=0.4196535621173229\n",
      "Gradient Descent(463/999): loss=0.4188929761760039\n",
      "Gradient Descent(464/999): loss=0.4187617511580127\n",
      "Gradient Descent(465/999): loss=0.4186718769739174\n",
      "Gradient Descent(466/999): loss=0.4186095442252443\n",
      "Gradient Descent(467/999): loss=0.41869898156529317\n",
      "Gradient Descent(468/999): loss=0.4187167415178653\n",
      "Gradient Descent(469/999): loss=0.41852301335859543\n",
      "Gradient Descent(470/999): loss=0.41848015678929157\n",
      "Gradient Descent(471/999): loss=0.4184395653900498\n",
      "Gradient Descent(472/999): loss=0.4185774702554404\n",
      "Gradient Descent(473/999): loss=0.41865708929355067\n",
      "Gradient Descent(474/999): loss=0.4182584727213859\n",
      "Gradient Descent(475/999): loss=0.4182685259224025\n",
      "Gradient Descent(476/999): loss=0.4182410557352788\n",
      "Gradient Descent(477/999): loss=0.41815539226613224\n",
      "Gradient Descent(478/999): loss=0.4181207626115649\n",
      "Gradient Descent(479/999): loss=0.41800743328412215\n",
      "Gradient Descent(480/999): loss=0.4180665085619035\n",
      "Gradient Descent(481/999): loss=0.41814827739910587\n",
      "Gradient Descent(482/999): loss=0.4180849920330562\n",
      "Gradient Descent(483/999): loss=0.4179772566343976\n",
      "Gradient Descent(484/999): loss=0.4179474837806935\n",
      "Gradient Descent(485/999): loss=0.41785547105737164\n",
      "Gradient Descent(486/999): loss=0.4177612037174503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(487/999): loss=0.4177956887663558\n",
      "Gradient Descent(488/999): loss=0.41769621363244397\n",
      "Gradient Descent(489/999): loss=0.4179964946325642\n",
      "Gradient Descent(490/999): loss=0.4179759650692734\n",
      "Gradient Descent(491/999): loss=0.4178820222695865\n",
      "Gradient Descent(492/999): loss=0.4180289532707104\n",
      "Gradient Descent(493/999): loss=0.4183522268532387\n",
      "Gradient Descent(494/999): loss=0.41826402246754235\n",
      "Gradient Descent(495/999): loss=0.41765882827360834\n",
      "Gradient Descent(496/999): loss=0.4174765794940306\n",
      "Gradient Descent(497/999): loss=0.41753773504496255\n",
      "Gradient Descent(498/999): loss=0.4173037013342604\n",
      "Gradient Descent(499/999): loss=0.4172808343194396\n",
      "Gradient Descent(500/999): loss=0.4184548031033199\n",
      "Gradient Descent(501/999): loss=0.41820692025392964\n",
      "Gradient Descent(502/999): loss=0.41765841754852706\n",
      "Gradient Descent(503/999): loss=0.4175677639418083\n",
      "Gradient Descent(504/999): loss=0.417149716141366\n",
      "Gradient Descent(505/999): loss=0.4170116146030017\n",
      "Gradient Descent(506/999): loss=0.4168538382988895\n",
      "Gradient Descent(507/999): loss=0.4166824959146382\n",
      "Gradient Descent(508/999): loss=0.4166611569201151\n",
      "Gradient Descent(509/999): loss=0.4166208842985268\n",
      "Gradient Descent(510/999): loss=0.4164624262681466\n",
      "Gradient Descent(511/999): loss=0.41639537765810547\n",
      "Gradient Descent(512/999): loss=0.41650406585499966\n",
      "Gradient Descent(513/999): loss=0.4165160164081641\n",
      "Gradient Descent(514/999): loss=0.41642732413535\n",
      "Gradient Descent(515/999): loss=0.41620575077381355\n",
      "Gradient Descent(516/999): loss=0.41635566348274383\n",
      "Gradient Descent(517/999): loss=0.416212487757847\n",
      "Gradient Descent(518/999): loss=0.4164386565396298\n",
      "Gradient Descent(519/999): loss=0.4160226690390594\n",
      "Gradient Descent(520/999): loss=0.4160635785047541\n",
      "Gradient Descent(521/999): loss=0.415953582545532\n",
      "Gradient Descent(522/999): loss=0.4158559556059157\n",
      "Gradient Descent(523/999): loss=0.4158070558431953\n",
      "Gradient Descent(524/999): loss=0.41581152659829235\n",
      "Gradient Descent(525/999): loss=0.41587846367611364\n",
      "Gradient Descent(526/999): loss=0.4160175815633395\n",
      "Gradient Descent(527/999): loss=0.41571370924322315\n",
      "Gradient Descent(528/999): loss=0.41554945785992936\n",
      "Gradient Descent(529/999): loss=0.4155252686496721\n",
      "Gradient Descent(530/999): loss=0.4154106157830652\n",
      "Gradient Descent(531/999): loss=0.41537342077487643\n",
      "Gradient Descent(532/999): loss=0.41522363447315114\n",
      "Gradient Descent(533/999): loss=0.41515475726227336\n",
      "Gradient Descent(534/999): loss=0.41522618314901655\n",
      "Gradient Descent(535/999): loss=0.41513789347024604\n",
      "Gradient Descent(536/999): loss=0.41557365362448084\n",
      "Gradient Descent(537/999): loss=0.41495966766417297\n",
      "Gradient Descent(538/999): loss=0.41495788233436853\n",
      "Gradient Descent(539/999): loss=0.4149031859573862\n",
      "Gradient Descent(540/999): loss=0.41503211225908515\n",
      "Gradient Descent(541/999): loss=0.41488578871887893\n",
      "Gradient Descent(542/999): loss=0.4148130666129163\n",
      "Gradient Descent(543/999): loss=0.4146618286481094\n",
      "Gradient Descent(544/999): loss=0.4146427382795493\n",
      "Gradient Descent(545/999): loss=0.41477924081396333\n",
      "Gradient Descent(546/999): loss=0.4145433327062206\n",
      "Gradient Descent(547/999): loss=0.41442348902438003\n",
      "Gradient Descent(548/999): loss=0.4143123821363709\n",
      "Gradient Descent(549/999): loss=0.41429556082577135\n",
      "Gradient Descent(550/999): loss=0.4142401810065011\n",
      "Gradient Descent(551/999): loss=0.4141792426452719\n",
      "Gradient Descent(552/999): loss=0.4141332918492813\n",
      "Gradient Descent(553/999): loss=0.4140846260909725\n",
      "Gradient Descent(554/999): loss=0.4141119409734217\n",
      "Gradient Descent(555/999): loss=0.41399747645621654\n",
      "Gradient Descent(556/999): loss=0.41394775557141705\n",
      "Gradient Descent(557/999): loss=0.4139669953238367\n",
      "Gradient Descent(558/999): loss=0.41399584046796356\n",
      "Gradient Descent(559/999): loss=0.41386376972783706\n",
      "Gradient Descent(560/999): loss=0.413788327423512\n",
      "Gradient Descent(561/999): loss=0.4137754667044004\n",
      "Gradient Descent(562/999): loss=0.4137493427010808\n",
      "Gradient Descent(563/999): loss=0.41367681395794137\n",
      "Gradient Descent(564/999): loss=0.4136455548518288\n",
      "Gradient Descent(565/999): loss=0.41365085777705896\n",
      "Gradient Descent(566/999): loss=0.41361058623693525\n",
      "Gradient Descent(567/999): loss=0.41350360009948706\n",
      "Gradient Descent(568/999): loss=0.4134489963201988\n",
      "Gradient Descent(569/999): loss=0.4134198536418734\n",
      "Gradient Descent(570/999): loss=0.41334057589640577\n",
      "Gradient Descent(571/999): loss=0.4136649944349297\n",
      "Gradient Descent(572/999): loss=0.41365462245178714\n",
      "Gradient Descent(573/999): loss=0.41337343746445093\n",
      "Gradient Descent(574/999): loss=0.41318034940967585\n",
      "Gradient Descent(575/999): loss=0.41310679726944777\n",
      "Gradient Descent(576/999): loss=0.4130468282193088\n",
      "Gradient Descent(577/999): loss=0.412998337552181\n",
      "Gradient Descent(578/999): loss=0.4131754829270645\n",
      "Gradient Descent(579/999): loss=0.4131542273058056\n",
      "Gradient Descent(580/999): loss=0.4129599788651935\n",
      "Gradient Descent(581/999): loss=0.4132432360174915\n",
      "Gradient Descent(582/999): loss=0.41311612822415905\n",
      "Gradient Descent(583/999): loss=0.41301869276137143\n",
      "Gradient Descent(584/999): loss=0.41280623742060746\n",
      "Gradient Descent(585/999): loss=0.4127952931851515\n",
      "Gradient Descent(586/999): loss=0.41264286282022566\n",
      "Gradient Descent(587/999): loss=0.4126234042669985\n",
      "Gradient Descent(588/999): loss=0.4126691838305694\n",
      "Gradient Descent(589/999): loss=0.41252770716852694\n",
      "Gradient Descent(590/999): loss=0.41268053876187866\n",
      "Gradient Descent(591/999): loss=0.4125811860897569\n",
      "Gradient Descent(592/999): loss=0.4124220062026693\n",
      "Gradient Descent(593/999): loss=0.4126942687126772\n",
      "Gradient Descent(594/999): loss=0.4133428680057385\n",
      "Gradient Descent(595/999): loss=0.41338583004707136\n",
      "Gradient Descent(596/999): loss=0.4132281562082922\n",
      "Gradient Descent(597/999): loss=0.4129495498982477\n",
      "Gradient Descent(598/999): loss=0.4132742840431304\n",
      "Gradient Descent(599/999): loss=0.4125345199228493\n",
      "Gradient Descent(600/999): loss=0.4124143932205798\n",
      "Gradient Descent(601/999): loss=0.4121557125360465\n",
      "Gradient Descent(602/999): loss=0.41235348057507953\n",
      "Gradient Descent(603/999): loss=0.4124069533796096\n",
      "Gradient Descent(604/999): loss=0.4126878349137249\n",
      "Gradient Descent(605/999): loss=0.4128200180853948\n",
      "Gradient Descent(606/999): loss=0.4128671708258005\n",
      "Gradient Descent(607/999): loss=0.4127865995115272\n",
      "Gradient Descent(608/999): loss=0.4127493776072481\n",
      "Gradient Descent(609/999): loss=0.41313108964318956\n",
      "Gradient Descent(610/999): loss=0.4130111220043185\n",
      "Gradient Descent(611/999): loss=0.4132569642981369\n",
      "Gradient Descent(612/999): loss=0.41325773555092327\n",
      "Gradient Descent(613/999): loss=0.4127467310246033\n",
      "Gradient Descent(614/999): loss=0.4126829671572627\n",
      "Gradient Descent(615/999): loss=0.4121963421150002\n",
      "Gradient Descent(616/999): loss=0.4122387647103251\n",
      "Gradient Descent(617/999): loss=0.4119397914935132\n",
      "Gradient Descent(618/999): loss=0.41151225815164794\n",
      "Gradient Descent(619/999): loss=0.41141317821027085\n",
      "Gradient Descent(620/999): loss=0.4113334264414948\n",
      "Gradient Descent(621/999): loss=0.4111777703144768\n",
      "Gradient Descent(622/999): loss=0.4111397010310286\n",
      "Gradient Descent(623/999): loss=0.4111969346673078\n",
      "Gradient Descent(624/999): loss=0.4111388666504289\n",
      "Gradient Descent(625/999): loss=0.4112306310984299\n",
      "Gradient Descent(626/999): loss=0.41155048123221527\n",
      "Gradient Descent(627/999): loss=0.41128031945185073\n",
      "Gradient Descent(628/999): loss=0.4114970335231141\n",
      "Gradient Descent(629/999): loss=0.41128169973628065\n",
      "Gradient Descent(630/999): loss=0.4110766176812348\n",
      "Gradient Descent(631/999): loss=0.4109960269706618\n",
      "Gradient Descent(632/999): loss=0.4108341927138419\n",
      "Gradient Descent(633/999): loss=0.41077766297089785\n",
      "Gradient Descent(634/999): loss=0.41074881796812385\n",
      "Gradient Descent(635/999): loss=0.41071945666152604\n",
      "Gradient Descent(636/999): loss=0.41066456250285166\n",
      "Gradient Descent(637/999): loss=0.4106350095464998\n",
      "Gradient Descent(638/999): loss=0.4106827854438586\n",
      "Gradient Descent(639/999): loss=0.41049721076884343\n",
      "Gradient Descent(640/999): loss=0.41055114883472454\n",
      "Gradient Descent(641/999): loss=0.41043092857160923\n",
      "Gradient Descent(642/999): loss=0.4103974017428048\n",
      "Gradient Descent(643/999): loss=0.4103832687389135\n",
      "Gradient Descent(644/999): loss=0.4103346359640348\n",
      "Gradient Descent(645/999): loss=0.4103704992306072\n",
      "Gradient Descent(646/999): loss=0.4102910785783727\n",
      "Gradient Descent(647/999): loss=0.4101985999618157\n",
      "Gradient Descent(648/999): loss=0.4101757674452186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(649/999): loss=0.410148646797067\n",
      "Gradient Descent(650/999): loss=0.410062240962285\n",
      "Gradient Descent(651/999): loss=0.4099868485611347\n",
      "Gradient Descent(652/999): loss=0.40996956900807463\n",
      "Gradient Descent(653/999): loss=0.40997028183605916\n",
      "Gradient Descent(654/999): loss=0.40987434793461025\n",
      "Gradient Descent(655/999): loss=0.40990187924941207\n",
      "Gradient Descent(656/999): loss=0.40980453332226024\n",
      "Gradient Descent(657/999): loss=0.4096999101090274\n",
      "Gradient Descent(658/999): loss=0.40966767674698457\n",
      "Gradient Descent(659/999): loss=0.4096238413646568\n",
      "Gradient Descent(660/999): loss=0.4095764511922953\n",
      "Gradient Descent(661/999): loss=0.4095869760561112\n",
      "Gradient Descent(662/999): loss=0.40963625318995944\n",
      "Gradient Descent(663/999): loss=0.4094798084650021\n",
      "Gradient Descent(664/999): loss=0.40939146222628736\n",
      "Gradient Descent(665/999): loss=0.40974363312610707\n",
      "Gradient Descent(666/999): loss=0.409745612548248\n",
      "Gradient Descent(667/999): loss=0.4105115441205169\n",
      "Gradient Descent(668/999): loss=0.410145382173866\n",
      "Gradient Descent(669/999): loss=0.4102082198603383\n",
      "Gradient Descent(670/999): loss=0.40994649873793526\n",
      "Gradient Descent(671/999): loss=0.4106240451740131\n",
      "Gradient Descent(672/999): loss=0.41002679379238494\n",
      "Gradient Descent(673/999): loss=0.40934009473485017\n",
      "Gradient Descent(674/999): loss=0.4096042434747677\n",
      "Gradient Descent(675/999): loss=0.41019900916685864\n",
      "Gradient Descent(676/999): loss=0.40982899176750837\n",
      "Gradient Descent(677/999): loss=0.40974952778030377\n",
      "Gradient Descent(678/999): loss=0.41002316243061226\n",
      "Gradient Descent(679/999): loss=0.4115117079624093\n",
      "Gradient Descent(680/999): loss=0.4104910297557627\n",
      "Gradient Descent(681/999): loss=0.4094806185100477\n",
      "Gradient Descent(682/999): loss=0.40954121877315053\n",
      "Gradient Descent(683/999): loss=0.4097110762418203\n",
      "Gradient Descent(684/999): loss=0.40926800819576176\n",
      "Gradient Descent(685/999): loss=0.4092955800566662\n",
      "Gradient Descent(686/999): loss=0.40919818287936693\n",
      "Gradient Descent(687/999): loss=0.4096017543489093\n",
      "Gradient Descent(688/999): loss=0.41020094783209265\n",
      "Gradient Descent(689/999): loss=0.40941259528642476\n",
      "Gradient Descent(690/999): loss=0.40977969499996847\n",
      "Gradient Descent(691/999): loss=0.4096320279152268\n",
      "Gradient Descent(692/999): loss=0.409642595736889\n",
      "Gradient Descent(693/999): loss=0.4095624266452241\n",
      "Gradient Descent(694/999): loss=0.41016232542449205\n",
      "Gradient Descent(695/999): loss=0.41040814742252674\n",
      "Gradient Descent(696/999): loss=0.4091723919270635\n",
      "Gradient Descent(697/999): loss=0.409527683342771\n",
      "Gradient Descent(698/999): loss=0.40891896232941877\n",
      "Gradient Descent(699/999): loss=0.40919317703919533\n",
      "Gradient Descent(700/999): loss=0.4091102569240551\n",
      "Gradient Descent(701/999): loss=0.4088864219565489\n",
      "Gradient Descent(702/999): loss=0.40870128394553834\n",
      "Gradient Descent(703/999): loss=0.40845923538663503\n",
      "Gradient Descent(704/999): loss=0.4096565807577887\n",
      "Gradient Descent(705/999): loss=0.409458299306764\n",
      "Gradient Descent(706/999): loss=0.4088618178535289\n",
      "Gradient Descent(707/999): loss=0.40856463261002957\n",
      "Gradient Descent(708/999): loss=0.4089817278471778\n",
      "Gradient Descent(709/999): loss=0.4085027778372434\n",
      "Gradient Descent(710/999): loss=0.40861517837087175\n",
      "Gradient Descent(711/999): loss=0.40860996579761116\n",
      "Gradient Descent(712/999): loss=0.4085411656699289\n",
      "Gradient Descent(713/999): loss=0.4089099897852802\n",
      "Gradient Descent(714/999): loss=0.4095835164114429\n",
      "Gradient Descent(715/999): loss=0.4082189529904832\n",
      "Gradient Descent(716/999): loss=0.4083850587764835\n",
      "Gradient Descent(717/999): loss=0.4083229998223911\n",
      "Gradient Descent(718/999): loss=0.4077624809832645\n",
      "Gradient Descent(719/999): loss=0.4077246951172643\n",
      "Gradient Descent(720/999): loss=0.40770614946717576\n",
      "Gradient Descent(721/999): loss=0.4077415544268437\n",
      "Gradient Descent(722/999): loss=0.4079262658215339\n",
      "Gradient Descent(723/999): loss=0.40804067138518474\n",
      "Gradient Descent(724/999): loss=0.4077558294498997\n",
      "Gradient Descent(725/999): loss=0.4080238565258497\n",
      "Gradient Descent(726/999): loss=0.4076916838754622\n",
      "Gradient Descent(727/999): loss=0.40804135859212315\n",
      "Gradient Descent(728/999): loss=0.4082968692696695\n",
      "Gradient Descent(729/999): loss=0.40840361548023274\n",
      "Gradient Descent(730/999): loss=0.40838431053354396\n",
      "Gradient Descent(731/999): loss=0.4081397354640135\n",
      "Gradient Descent(732/999): loss=0.40824532192667334\n",
      "Gradient Descent(733/999): loss=0.4079203524787043\n",
      "Gradient Descent(734/999): loss=0.40801329142565956\n",
      "Gradient Descent(735/999): loss=0.4073473950213138\n",
      "Gradient Descent(736/999): loss=0.40718722494902865\n",
      "Gradient Descent(737/999): loss=0.4071710303727986\n",
      "Gradient Descent(738/999): loss=0.40711813787390055\n",
      "Gradient Descent(739/999): loss=0.40718582115189367\n",
      "Gradient Descent(740/999): loss=0.4072954728885853\n",
      "Gradient Descent(741/999): loss=0.40710180124159956\n",
      "Gradient Descent(742/999): loss=0.4071368921610407\n",
      "Gradient Descent(743/999): loss=0.40710354967047857\n",
      "Gradient Descent(744/999): loss=0.40682351388926963\n",
      "Gradient Descent(745/999): loss=0.40678276231669963\n",
      "Gradient Descent(746/999): loss=0.4067621461164926\n",
      "Gradient Descent(747/999): loss=0.4067439189452806\n",
      "Gradient Descent(748/999): loss=0.4068343291896187\n",
      "Gradient Descent(749/999): loss=0.4068742674023521\n",
      "Gradient Descent(750/999): loss=0.40728220859231673\n",
      "Gradient Descent(751/999): loss=0.4068121797989256\n",
      "Gradient Descent(752/999): loss=0.40702153472644553\n",
      "Gradient Descent(753/999): loss=0.4082195040873979\n",
      "Gradient Descent(754/999): loss=0.4074372301283644\n",
      "Gradient Descent(755/999): loss=0.40768233153408723\n",
      "Gradient Descent(756/999): loss=0.4077189984903705\n",
      "Gradient Descent(757/999): loss=0.4076937747627095\n",
      "Gradient Descent(758/999): loss=0.4079637934321867\n",
      "Gradient Descent(759/999): loss=0.407348687805409\n",
      "Gradient Descent(760/999): loss=0.40755408837407436\n",
      "Gradient Descent(761/999): loss=0.4079375835118441\n",
      "Gradient Descent(762/999): loss=0.4084984701984562\n",
      "Gradient Descent(763/999): loss=0.40704593057796645\n",
      "Gradient Descent(764/999): loss=0.40723983471876607\n",
      "Gradient Descent(765/999): loss=0.40705377854798014\n",
      "Gradient Descent(766/999): loss=0.40707543964904813\n",
      "Gradient Descent(767/999): loss=0.40647525163013787\n",
      "Gradient Descent(768/999): loss=0.4064738124720279\n",
      "Gradient Descent(769/999): loss=0.40655094914586204\n",
      "Gradient Descent(770/999): loss=0.40612009664628146\n",
      "Gradient Descent(771/999): loss=0.4060039967321247\n",
      "Gradient Descent(772/999): loss=0.4059191499332729\n",
      "Gradient Descent(773/999): loss=0.40587469153835676\n",
      "Gradient Descent(774/999): loss=0.4058320494139203\n",
      "Gradient Descent(775/999): loss=0.40586106531012067\n",
      "Gradient Descent(776/999): loss=0.40578449938150185\n",
      "Gradient Descent(777/999): loss=0.4057750867097832\n",
      "Gradient Descent(778/999): loss=0.4058092478815996\n",
      "Gradient Descent(779/999): loss=0.40572967836648177\n",
      "Gradient Descent(780/999): loss=0.40569660285164927\n",
      "Gradient Descent(781/999): loss=0.4061012256076458\n",
      "Gradient Descent(782/999): loss=0.4058321344044497\n",
      "Gradient Descent(783/999): loss=0.4057604795530823\n",
      "Gradient Descent(784/999): loss=0.40563738960655077\n",
      "Gradient Descent(785/999): loss=0.40560713491753225\n",
      "Gradient Descent(786/999): loss=0.4056009612891944\n",
      "Gradient Descent(787/999): loss=0.4057366090722744\n",
      "Gradient Descent(788/999): loss=0.4057557592925408\n",
      "Gradient Descent(789/999): loss=0.40552943979537165\n",
      "Gradient Descent(790/999): loss=0.4054561660882877\n",
      "Gradient Descent(791/999): loss=0.40550054752708725\n",
      "Gradient Descent(792/999): loss=0.4057064421390119\n",
      "Gradient Descent(793/999): loss=0.4055598436792744\n",
      "Gradient Descent(794/999): loss=0.4054170265186595\n",
      "Gradient Descent(795/999): loss=0.4054684084857471\n",
      "Gradient Descent(796/999): loss=0.405191527962186\n",
      "Gradient Descent(797/999): loss=0.40516760154291637\n",
      "Gradient Descent(798/999): loss=0.4051442151138092\n",
      "Gradient Descent(799/999): loss=0.4051685743508801\n",
      "Gradient Descent(800/999): loss=0.4051988782818515\n",
      "Gradient Descent(801/999): loss=0.40530123896595627\n",
      "Gradient Descent(802/999): loss=0.40508155178677985\n",
      "Gradient Descent(803/999): loss=0.4051611408007156\n",
      "Gradient Descent(804/999): loss=0.4050366707595973\n",
      "Gradient Descent(805/999): loss=0.4049256585081778\n",
      "Gradient Descent(806/999): loss=0.4048917658409152\n",
      "Gradient Descent(807/999): loss=0.4048815283618812\n",
      "Gradient Descent(808/999): loss=0.40499418035051715\n",
      "Gradient Descent(809/999): loss=0.40485277660213204\n",
      "Gradient Descent(810/999): loss=0.404823736725305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(811/999): loss=0.40475525729244444\n",
      "Gradient Descent(812/999): loss=0.4046635449411427\n",
      "Gradient Descent(813/999): loss=0.4046172779220924\n",
      "Gradient Descent(814/999): loss=0.4046749651045595\n",
      "Gradient Descent(815/999): loss=0.4046346785925506\n",
      "Gradient Descent(816/999): loss=0.4045372115585433\n",
      "Gradient Descent(817/999): loss=0.404492050851792\n",
      "Gradient Descent(818/999): loss=0.404507379660633\n",
      "Gradient Descent(819/999): loss=0.4047839027677783\n",
      "Gradient Descent(820/999): loss=0.40440659763661024\n",
      "Gradient Descent(821/999): loss=0.4043849523605578\n",
      "Gradient Descent(822/999): loss=0.4045285057510985\n",
      "Gradient Descent(823/999): loss=0.4044100862038184\n",
      "Gradient Descent(824/999): loss=0.4043327675691818\n",
      "Gradient Descent(825/999): loss=0.40441389293964053\n",
      "Gradient Descent(826/999): loss=0.4043913385939749\n",
      "Gradient Descent(827/999): loss=0.40453568992761013\n",
      "Gradient Descent(828/999): loss=0.4047707480440102\n",
      "Gradient Descent(829/999): loss=0.40470588051498857\n",
      "Gradient Descent(830/999): loss=0.40541017852366584\n",
      "Gradient Descent(831/999): loss=0.40612304522018267\n",
      "Gradient Descent(832/999): loss=0.40593900126192917\n",
      "Gradient Descent(833/999): loss=0.4054459401679128\n",
      "Gradient Descent(834/999): loss=0.4046935318407904\n",
      "Gradient Descent(835/999): loss=0.40425623508293623\n",
      "Gradient Descent(836/999): loss=0.4039935029066094\n",
      "Gradient Descent(837/999): loss=0.40396037948508456\n",
      "Gradient Descent(838/999): loss=0.40396662363802227\n",
      "Gradient Descent(839/999): loss=0.4039946977739436\n",
      "Gradient Descent(840/999): loss=0.4039181612144223\n",
      "Gradient Descent(841/999): loss=0.4038738003396878\n",
      "Gradient Descent(842/999): loss=0.40384467495682425\n",
      "Gradient Descent(843/999): loss=0.40383991443549877\n",
      "Gradient Descent(844/999): loss=0.4038809841956087\n",
      "Gradient Descent(845/999): loss=0.40402256056853486\n",
      "Gradient Descent(846/999): loss=0.4041373492800244\n",
      "Gradient Descent(847/999): loss=0.40382228751582483\n",
      "Gradient Descent(848/999): loss=0.40384692453528026\n",
      "Gradient Descent(849/999): loss=0.40389739753134224\n",
      "Gradient Descent(850/999): loss=0.4037033374888431\n",
      "Gradient Descent(851/999): loss=0.4035403637296793\n",
      "Gradient Descent(852/999): loss=0.40350048822770523\n",
      "Gradient Descent(853/999): loss=0.40353984355089234\n",
      "Gradient Descent(854/999): loss=0.4038887402601007\n",
      "Gradient Descent(855/999): loss=0.40358505509550774\n",
      "Gradient Descent(856/999): loss=0.4034845885377621\n",
      "Gradient Descent(857/999): loss=0.40450821003072723\n",
      "Gradient Descent(858/999): loss=0.4043033283564786\n",
      "Gradient Descent(859/999): loss=0.4038413424280202\n",
      "Gradient Descent(860/999): loss=0.4043005898590411\n",
      "Gradient Descent(861/999): loss=0.40428996711824594\n",
      "Gradient Descent(862/999): loss=0.4038185934476188\n",
      "Gradient Descent(863/999): loss=0.4035434463964753\n",
      "Gradient Descent(864/999): loss=0.40347951593357734\n",
      "Gradient Descent(865/999): loss=0.40334884639178553\n",
      "Gradient Descent(866/999): loss=0.40324533625899833\n",
      "Gradient Descent(867/999): loss=0.4032796925152363\n",
      "Gradient Descent(868/999): loss=0.4033897923978159\n",
      "Gradient Descent(869/999): loss=0.4031286502317643\n",
      "Gradient Descent(870/999): loss=0.40313674376190556\n",
      "Gradient Descent(871/999): loss=0.40310850118863994\n",
      "Gradient Descent(872/999): loss=0.4031269748177214\n",
      "Gradient Descent(873/999): loss=0.40315639092065514\n",
      "Gradient Descent(874/999): loss=0.4031675329497058\n",
      "Gradient Descent(875/999): loss=0.40293431133357965\n",
      "Gradient Descent(876/999): loss=0.4029036137894023\n",
      "Gradient Descent(877/999): loss=0.4028677884569854\n",
      "Gradient Descent(878/999): loss=0.4028302788948974\n",
      "Gradient Descent(879/999): loss=0.40282285701664255\n",
      "Gradient Descent(880/999): loss=0.4028024114230589\n",
      "Gradient Descent(881/999): loss=0.40292720538681087\n",
      "Gradient Descent(882/999): loss=0.40297748928492433\n",
      "Gradient Descent(883/999): loss=0.40284677204508984\n",
      "Gradient Descent(884/999): loss=0.4028490241550969\n",
      "Gradient Descent(885/999): loss=0.4027361185796886\n",
      "Gradient Descent(886/999): loss=0.4026205614402624\n",
      "Gradient Descent(887/999): loss=0.4026094457957201\n",
      "Gradient Descent(888/999): loss=0.40268528950523547\n",
      "Gradient Descent(889/999): loss=0.40264562847123914\n",
      "Gradient Descent(890/999): loss=0.4026892811854493\n",
      "Gradient Descent(891/999): loss=0.4030705157243349\n",
      "Gradient Descent(892/999): loss=0.40293339575879206\n",
      "Gradient Descent(893/999): loss=0.40256004805103573\n",
      "Gradient Descent(894/999): loss=0.4025038318703142\n",
      "Gradient Descent(895/999): loss=0.4024546554105922\n",
      "Gradient Descent(896/999): loss=0.4024626373213611\n",
      "Gradient Descent(897/999): loss=0.40244382691772285\n",
      "Gradient Descent(898/999): loss=0.4027143301697934\n",
      "Gradient Descent(899/999): loss=0.40267404083779595\n",
      "Gradient Descent(900/999): loss=0.40233660264930216\n",
      "Gradient Descent(901/999): loss=0.40247513115933425\n",
      "Gradient Descent(902/999): loss=0.40228230252104996\n",
      "Gradient Descent(903/999): loss=0.40229970877928156\n",
      "Gradient Descent(904/999): loss=0.40231727255424354\n",
      "Gradient Descent(905/999): loss=0.4021539205422594\n",
      "Gradient Descent(906/999): loss=0.40245443759930893\n",
      "Gradient Descent(907/999): loss=0.4026177116881835\n",
      "Gradient Descent(908/999): loss=0.40250376037370744\n",
      "Gradient Descent(909/999): loss=0.40243413861159183\n",
      "Gradient Descent(910/999): loss=0.4023238936857633\n",
      "Gradient Descent(911/999): loss=0.4021691025391681\n",
      "Gradient Descent(912/999): loss=0.40232701901892975\n",
      "Gradient Descent(913/999): loss=0.40202398752117907\n",
      "Gradient Descent(914/999): loss=0.40223798022098056\n",
      "Gradient Descent(915/999): loss=0.4027716313458678\n",
      "Gradient Descent(916/999): loss=0.4026299951375451\n",
      "Gradient Descent(917/999): loss=0.40297223646027125\n",
      "Gradient Descent(918/999): loss=0.40215461002716557\n",
      "Gradient Descent(919/999): loss=0.4018736115808545\n",
      "Gradient Descent(920/999): loss=0.40180368858498267\n",
      "Gradient Descent(921/999): loss=0.4016734471146583\n",
      "Gradient Descent(922/999): loss=0.40166686222840325\n",
      "Gradient Descent(923/999): loss=0.401584775215491\n",
      "Gradient Descent(924/999): loss=0.401642098460388\n",
      "Gradient Descent(925/999): loss=0.40151977925901594\n",
      "Gradient Descent(926/999): loss=0.4016553517815761\n",
      "Gradient Descent(927/999): loss=0.4016513888723099\n",
      "Gradient Descent(928/999): loss=0.4015562075345792\n",
      "Gradient Descent(929/999): loss=0.4014694213878545\n",
      "Gradient Descent(930/999): loss=0.4015527275581152\n",
      "Gradient Descent(931/999): loss=0.4014232730221056\n",
      "Gradient Descent(932/999): loss=0.4013733991877322\n",
      "Gradient Descent(933/999): loss=0.401379645688566\n",
      "Gradient Descent(934/999): loss=0.4013060216633465\n",
      "Gradient Descent(935/999): loss=0.4014486306914698\n",
      "Gradient Descent(936/999): loss=0.4012824358565212\n",
      "Gradient Descent(937/999): loss=0.40128040609263643\n",
      "Gradient Descent(938/999): loss=0.4012375147436706\n",
      "Gradient Descent(939/999): loss=0.40119106755371514\n",
      "Gradient Descent(940/999): loss=0.4011564353449616\n",
      "Gradient Descent(941/999): loss=0.4012799532221007\n",
      "Gradient Descent(942/999): loss=0.4011457542085956\n",
      "Gradient Descent(943/999): loss=0.4012751801210613\n",
      "Gradient Descent(944/999): loss=0.40106448676422607\n",
      "Gradient Descent(945/999): loss=0.40105057398211624\n",
      "Gradient Descent(946/999): loss=0.40106657061373596\n",
      "Gradient Descent(947/999): loss=0.40106790616431415\n",
      "Gradient Descent(948/999): loss=0.40094294554519727\n",
      "Gradient Descent(949/999): loss=0.4009262590117108\n",
      "Gradient Descent(950/999): loss=0.4009472102451196\n",
      "Gradient Descent(951/999): loss=0.4009106949241998\n",
      "Gradient Descent(952/999): loss=0.40089505898222405\n",
      "Gradient Descent(953/999): loss=0.40102151202746134\n",
      "Gradient Descent(954/999): loss=0.4009464825271043\n",
      "Gradient Descent(955/999): loss=0.40101428601646466\n",
      "Gradient Descent(956/999): loss=0.4008993329355697\n",
      "Gradient Descent(957/999): loss=0.40074128195339676\n",
      "Gradient Descent(958/999): loss=0.400785552755708\n",
      "Gradient Descent(959/999): loss=0.4007433336706329\n",
      "Gradient Descent(960/999): loss=0.4007071734142863\n",
      "Gradient Descent(961/999): loss=0.40076422304881854\n",
      "Gradient Descent(962/999): loss=0.4006342960902234\n",
      "Gradient Descent(963/999): loss=0.40080918918852354\n",
      "Gradient Descent(964/999): loss=0.40084995308440224\n",
      "Gradient Descent(965/999): loss=0.40069090810598895\n",
      "Gradient Descent(966/999): loss=0.40064034672823634\n",
      "Gradient Descent(967/999): loss=0.40077078028725754\n",
      "Gradient Descent(968/999): loss=0.400959739717342\n",
      "Gradient Descent(969/999): loss=0.40049955015490435\n",
      "Gradient Descent(970/999): loss=0.40045102111077757\n",
      "Gradient Descent(971/999): loss=0.4004093203399401\n",
      "Gradient Descent(972/999): loss=0.4003836248547666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(973/999): loss=0.40032485522469\n",
      "Gradient Descent(974/999): loss=0.4004700702046661\n",
      "Gradient Descent(975/999): loss=0.4002649992546339\n",
      "Gradient Descent(976/999): loss=0.4002024784358968\n",
      "Gradient Descent(977/999): loss=0.4005074822956666\n",
      "Gradient Descent(978/999): loss=0.4001717596099878\n",
      "Gradient Descent(979/999): loss=0.4001173744643487\n",
      "Gradient Descent(980/999): loss=0.40020159142598005\n",
      "Gradient Descent(981/999): loss=0.4000763310875739\n",
      "Gradient Descent(982/999): loss=0.400128355062376\n",
      "Gradient Descent(983/999): loss=0.4000477518601404\n",
      "Gradient Descent(984/999): loss=0.4000102611611286\n",
      "Gradient Descent(985/999): loss=0.3999947193680409\n",
      "Gradient Descent(986/999): loss=0.3999846126166332\n",
      "Gradient Descent(987/999): loss=0.39994242982539996\n",
      "Gradient Descent(988/999): loss=0.3999495622167426\n",
      "Gradient Descent(989/999): loss=0.39993773167350893\n",
      "Gradient Descent(990/999): loss=0.39997973309878604\n",
      "Gradient Descent(991/999): loss=0.39987431870917145\n",
      "Gradient Descent(992/999): loss=0.39986852994259353\n",
      "Gradient Descent(993/999): loss=0.3998798981813895\n",
      "Gradient Descent(994/999): loss=0.3999389584185791\n",
      "Gradient Descent(995/999): loss=0.39992666779476593\n",
      "Gradient Descent(996/999): loss=0.3997665898676055\n",
      "Gradient Descent(997/999): loss=0.39999293189161916\n",
      "Gradient Descent(998/999): loss=0.3999362148668711\n",
      "Gradient Descent(999/999): loss=0.39980683443192233\n"
     ]
    }
   ],
   "source": [
    "w, mse = least_squares_SGD(y_train, tX_train, initial_w, max_iters, gamma, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.682"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict_labels(w, tX_train)\n",
    "np.mean(y_train.reshape(-1,1)==y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68116"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict_labels(w, tX_val)\n",
    "np.mean(y_val.reshape(-1,1)==y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros(tX.shape[1])#np.random.normal(loc=0, scale=1, size=(tX.shape[1],1))\n",
    "max_iters = 1000\n",
    "gamma = 1.5e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_lr = y_train>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(0/999): loss=0.6669003579525978\n",
      "Log Regression(1/999): loss=0.6633099924185696\n",
      "Log Regression(2/999): loss=0.6671924049787051\n",
      "Log Regression(3/999): loss=0.6625946626484256\n",
      "Log Regression(4/999): loss=0.6555295559377617\n",
      "Log Regression(5/999): loss=0.6537296167985533\n",
      "Log Regression(6/999): loss=0.6512040560434509\n",
      "Log Regression(7/999): loss=0.669039467151037\n",
      "Log Regression(8/999): loss=0.6545967006305298\n",
      "Log Regression(9/999): loss=0.6662910205997788\n",
      "Log Regression(10/999): loss=0.671063695532839\n",
      "Log Regression(11/999): loss=0.6534930625490328\n",
      "Log Regression(12/999): loss=0.6442113514190045\n",
      "Log Regression(13/999): loss=0.6502131136760555\n",
      "Log Regression(14/999): loss=0.6811939006131291\n",
      "Log Regression(15/999): loss=0.6503067418237227\n",
      "Log Regression(16/999): loss=0.6391423637090622\n",
      "Log Regression(17/999): loss=0.6362725304905471\n",
      "Log Regression(18/999): loss=0.653626634791935\n",
      "Log Regression(19/999): loss=0.6486101647910342\n",
      "Log Regression(20/999): loss=0.6315788207595077\n",
      "Log Regression(21/999): loss=0.6358913911358474\n",
      "Log Regression(22/999): loss=0.627898217798026\n",
      "Log Regression(23/999): loss=0.6299929895864896\n",
      "Log Regression(24/999): loss=0.6318283209448347\n",
      "Log Regression(25/999): loss=0.626699201203951\n",
      "Log Regression(26/999): loss=0.6248678619308665\n",
      "Log Regression(27/999): loss=0.6204256811618978\n",
      "Log Regression(28/999): loss=0.6199913424145859\n",
      "Log Regression(29/999): loss=0.628206762903143\n",
      "Log Regression(30/999): loss=0.6287067585937707\n",
      "Log Regression(31/999): loss=0.6197695560558639\n",
      "Log Regression(32/999): loss=0.6176889849642475\n",
      "Log Regression(33/999): loss=0.6204636789383455\n",
      "Log Regression(34/999): loss=0.6200304500487184\n",
      "Log Regression(35/999): loss=0.6156913488595539\n",
      "Log Regression(36/999): loss=0.6262996857594639\n",
      "Log Regression(37/999): loss=0.6172999694424577\n",
      "Log Regression(38/999): loss=0.6179716797058626\n",
      "Log Regression(39/999): loss=0.6146232916396371\n",
      "Log Regression(40/999): loss=0.6109307238345438\n",
      "Log Regression(41/999): loss=0.6124622404029463\n",
      "Log Regression(42/999): loss=0.6209696007065213\n",
      "Log Regression(43/999): loss=0.6454379442817207\n",
      "Log Regression(44/999): loss=0.6083362569629255\n",
      "Log Regression(45/999): loss=0.6150656817522204\n",
      "Log Regression(46/999): loss=0.6245954625628258\n",
      "Log Regression(47/999): loss=0.6189726297475281\n",
      "Log Regression(48/999): loss=0.6064535411204816\n",
      "Log Regression(49/999): loss=0.605872783266224\n",
      "Log Regression(50/999): loss=0.6051914470274973\n",
      "Log Regression(51/999): loss=0.6058234133774067\n",
      "Log Regression(52/999): loss=0.6036726856301857\n",
      "Log Regression(53/999): loss=0.6055236456899153\n",
      "Log Regression(54/999): loss=0.6410230673829747\n",
      "Log Regression(55/999): loss=0.6226031121342825\n",
      "Log Regression(56/999): loss=0.60710470155118\n",
      "Log Regression(57/999): loss=0.6023896447691784\n",
      "Log Regression(58/999): loss=0.6191517203492901\n",
      "Log Regression(59/999): loss=0.6024597995782486\n",
      "Log Regression(60/999): loss=0.6015656929365328\n",
      "Log Regression(61/999): loss=0.6358459807878767\n",
      "Log Regression(62/999): loss=0.6002524828166271\n",
      "Log Regression(63/999): loss=0.5996469223252326\n",
      "Log Regression(64/999): loss=0.5981712002824159\n",
      "Log Regression(65/999): loss=0.5981296886170279\n",
      "Log Regression(66/999): loss=0.5988482909434155\n",
      "Log Regression(67/999): loss=0.5983070443631001\n",
      "Log Regression(68/999): loss=0.5972398745546699\n",
      "Log Regression(69/999): loss=0.6083964680195447\n",
      "Log Regression(70/999): loss=0.6128338129390675\n",
      "Log Regression(71/999): loss=0.5993908973015711\n",
      "Log Regression(72/999): loss=0.5948363086891538\n",
      "Log Regression(73/999): loss=0.6037257209309411\n",
      "Log Regression(74/999): loss=0.5958737506875489\n",
      "Log Regression(75/999): loss=0.6161094000794756\n",
      "Log Regression(76/999): loss=0.605185601660873\n",
      "Log Regression(77/999): loss=0.5931730369299569\n",
      "Log Regression(78/999): loss=0.5923278688091442\n",
      "Log Regression(79/999): loss=0.6092629203331833\n",
      "Log Regression(80/999): loss=0.59178286480362\n",
      "Log Regression(81/999): loss=0.6027736439297938\n",
      "Log Regression(82/999): loss=0.5902680046062604\n",
      "Log Regression(83/999): loss=0.5898250110873018\n",
      "Log Regression(84/999): loss=0.589816711750753\n",
      "Log Regression(85/999): loss=0.5894700566101359\n",
      "Log Regression(86/999): loss=0.5911520858077417\n",
      "Log Regression(87/999): loss=0.5927916006792234\n",
      "Log Regression(88/999): loss=0.588666452214789\n",
      "Log Regression(89/999): loss=0.5877900597869391\n",
      "Log Regression(90/999): loss=0.5948082887942882\n",
      "Log Regression(91/999): loss=0.5870049900894789\n",
      "Log Regression(92/999): loss=0.5868589807894696\n",
      "Log Regression(93/999): loss=0.5910648637133813\n",
      "Log Regression(94/999): loss=0.5936844363169472\n",
      "Log Regression(95/999): loss=0.5883435697180176\n",
      "Log Regression(96/999): loss=0.5853775601835726\n",
      "Log Regression(97/999): loss=0.5914972814611404\n",
      "Log Regression(98/999): loss=0.5867104063101237\n",
      "Log Regression(99/999): loss=0.5859443068186119\n",
      "Log Regression(100/999): loss=0.589623564519277\n",
      "Log Regression(101/999): loss=0.5844985939917351\n",
      "Log Regression(102/999): loss=0.5850057976668535\n",
      "Log Regression(103/999): loss=0.6038701808146123\n",
      "Log Regression(104/999): loss=0.5915125622729966\n",
      "Log Regression(105/999): loss=0.5847937957111968\n",
      "Log Regression(106/999): loss=0.5832673966819271\n",
      "Log Regression(107/999): loss=0.5840019686961047\n",
      "Log Regression(108/999): loss=0.5836202417483988\n",
      "Log Regression(109/999): loss=0.5864383415989262\n",
      "Log Regression(110/999): loss=0.5824071359628914\n",
      "Log Regression(111/999): loss=0.5938077461910644\n",
      "Log Regression(112/999): loss=0.5842440250425834\n",
      "Log Regression(113/999): loss=0.5829153519301323\n",
      "Log Regression(114/999): loss=0.5891349293438943\n",
      "Log Regression(115/999): loss=0.581500771463861\n",
      "Log Regression(116/999): loss=0.5864147724953954\n",
      "Log Regression(117/999): loss=0.5833158883572418\n",
      "Log Regression(118/999): loss=0.6332463465069155\n",
      "Log Regression(119/999): loss=0.5943179511172755\n",
      "Log Regression(120/999): loss=0.5953715665062385\n",
      "Log Regression(121/999): loss=0.5815286834791326\n",
      "Log Regression(122/999): loss=0.5825578223890384\n",
      "Log Regression(123/999): loss=0.5818511738997867\n",
      "Log Regression(124/999): loss=0.5853559551827127\n",
      "Log Regression(125/999): loss=0.5829959413294089\n",
      "Log Regression(126/999): loss=0.579908262117042\n",
      "Log Regression(127/999): loss=0.5815156775396392\n",
      "Log Regression(128/999): loss=0.5998737077946366\n",
      "Log Regression(129/999): loss=0.5787155143449371\n",
      "Log Regression(130/999): loss=0.5832988895821202\n",
      "Log Regression(131/999): loss=0.5834944253140637\n",
      "Log Regression(132/999): loss=0.5786345537181494\n",
      "Log Regression(133/999): loss=0.588222672490228\n",
      "Log Regression(134/999): loss=0.5833282239845727\n",
      "Log Regression(135/999): loss=0.5795071604426605\n",
      "Log Regression(136/999): loss=0.5793830734579107\n",
      "Log Regression(137/999): loss=0.5958947779052929\n",
      "Log Regression(138/999): loss=0.5898132457247239\n",
      "Log Regression(139/999): loss=0.5825033188427193\n",
      "Log Regression(140/999): loss=0.5873634522906868\n",
      "Log Regression(141/999): loss=0.5764250472340597\n",
      "Log Regression(142/999): loss=0.5750115277391264\n",
      "Log Regression(143/999): loss=0.5764877492546773\n",
      "Log Regression(144/999): loss=0.5758485559302757\n",
      "Log Regression(145/999): loss=0.5747513525919663\n",
      "Log Regression(146/999): loss=0.5744745376593933\n",
      "Log Regression(147/999): loss=0.5744938723273458\n",
      "Log Regression(148/999): loss=0.5755953386997582\n",
      "Log Regression(149/999): loss=0.5798676548792246\n",
      "Log Regression(150/999): loss=0.5737711123283915\n",
      "Log Regression(151/999): loss=0.5734106665075107\n",
      "Log Regression(152/999): loss=0.5793358250891427\n",
      "Log Regression(153/999): loss=0.5747429850068198\n",
      "Log Regression(154/999): loss=0.579831294085958\n",
      "Log Regression(155/999): loss=0.5899133187260763\n",
      "Log Regression(156/999): loss=0.5722255080843502\n",
      "Log Regression(157/999): loss=0.5726455664867105\n",
      "Log Regression(158/999): loss=0.5724008858017245\n",
      "Log Regression(159/999): loss=0.5732218160484226\n",
      "Log Regression(160/999): loss=0.5721523162244708\n",
      "Log Regression(161/999): loss=0.5714211349333171\n",
      "Log Regression(162/999): loss=0.5710022389472373\n",
      "Log Regression(163/999): loss=0.5736345929153626\n",
      "Log Regression(164/999): loss=0.5819810791741852\n",
      "Log Regression(165/999): loss=0.5719472109528663\n",
      "Log Regression(166/999): loss=0.5719443373614872\n",
      "Log Regression(167/999): loss=0.5757896460603021\n",
      "Log Regression(168/999): loss=0.5724515707480473\n",
      "Log Regression(169/999): loss=0.5720431643273678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(170/999): loss=0.5756956018932959\n",
      "Log Regression(171/999): loss=0.5951193760522699\n",
      "Log Regression(172/999): loss=0.5727527707600798\n",
      "Log Regression(173/999): loss=0.5694921814001845\n",
      "Log Regression(174/999): loss=0.5755948804533964\n",
      "Log Regression(175/999): loss=0.5923561218540812\n",
      "Log Regression(176/999): loss=0.5715374797465191\n",
      "Log Regression(177/999): loss=0.5743034042865179\n",
      "Log Regression(178/999): loss=0.5694402342310692\n",
      "Log Regression(179/999): loss=0.5821561537038866\n",
      "Log Regression(180/999): loss=0.5858227375795596\n",
      "Log Regression(181/999): loss=0.5709957685153568\n",
      "Log Regression(182/999): loss=0.5766654130162381\n",
      "Log Regression(183/999): loss=0.5692215456407501\n",
      "Log Regression(184/999): loss=0.5694268649071155\n",
      "Log Regression(185/999): loss=0.5674342357603802\n",
      "Log Regression(186/999): loss=0.5862894127663627\n",
      "Log Regression(187/999): loss=0.5786403954398477\n",
      "Log Regression(188/999): loss=0.5686122643535161\n",
      "Log Regression(189/999): loss=0.5881487475307083\n",
      "Log Regression(190/999): loss=0.5949723022941962\n",
      "Log Regression(191/999): loss=0.6011197640970126\n",
      "Log Regression(192/999): loss=0.5773939604379564\n",
      "Log Regression(193/999): loss=0.5656330244523923\n",
      "Log Regression(194/999): loss=0.5707861365647924\n",
      "Log Regression(195/999): loss=0.569347203352858\n",
      "Log Regression(196/999): loss=0.5657381745894349\n",
      "Log Regression(197/999): loss=0.568434864348652\n",
      "Log Regression(198/999): loss=0.5696614620221602\n",
      "Log Regression(199/999): loss=0.5669330161537043\n",
      "Log Regression(200/999): loss=0.5704599263963465\n",
      "Log Regression(201/999): loss=0.5745021609387259\n",
      "Log Regression(202/999): loss=0.5834698184374506\n",
      "Log Regression(203/999): loss=0.5651889630267511\n",
      "Log Regression(204/999): loss=0.5652889690401889\n",
      "Log Regression(205/999): loss=0.5665344886689416\n",
      "Log Regression(206/999): loss=0.5658723165190263\n",
      "Log Regression(207/999): loss=0.5637634555662522\n",
      "Log Regression(208/999): loss=0.5638434676125496\n",
      "Log Regression(209/999): loss=0.5664575227508275\n",
      "Log Regression(210/999): loss=0.5728883700743194\n",
      "Log Regression(211/999): loss=0.5632736501459367\n",
      "Log Regression(212/999): loss=0.5627181189853465\n",
      "Log Regression(213/999): loss=0.5634426082546524\n",
      "Log Regression(214/999): loss=0.5657974142333413\n",
      "Log Regression(215/999): loss=0.5634429231577203\n",
      "Log Regression(216/999): loss=0.5793511296368822\n",
      "Log Regression(217/999): loss=0.5820330301502493\n",
      "Log Regression(218/999): loss=0.5662402512595077\n",
      "Log Regression(219/999): loss=0.5807713565656037\n",
      "Log Regression(220/999): loss=0.5686743681904577\n",
      "Log Regression(221/999): loss=0.5766117150468587\n",
      "Log Regression(222/999): loss=0.5652329103109831\n",
      "Log Regression(223/999): loss=0.5675883113986734\n",
      "Log Regression(224/999): loss=0.5614766402181322\n",
      "Log Regression(225/999): loss=0.5660527402468355\n",
      "Log Regression(226/999): loss=0.5613472763001988\n",
      "Log Regression(227/999): loss=0.5784344824693802\n",
      "Log Regression(228/999): loss=0.5613182420049855\n",
      "Log Regression(229/999): loss=0.5650005640819166\n",
      "Log Regression(230/999): loss=0.5730206584865157\n",
      "Log Regression(231/999): loss=0.570649400541101\n",
      "Log Regression(232/999): loss=0.5642098880863335\n",
      "Log Regression(233/999): loss=0.5619323356535173\n",
      "Log Regression(234/999): loss=0.5616728583020639\n",
      "Log Regression(235/999): loss=0.560984395146893\n",
      "Log Regression(236/999): loss=0.5598298538234904\n",
      "Log Regression(237/999): loss=0.5616131753331012\n",
      "Log Regression(238/999): loss=0.5605592596391089\n",
      "Log Regression(239/999): loss=0.5604946764112592\n",
      "Log Regression(240/999): loss=0.5714434205032142\n",
      "Log Regression(241/999): loss=0.5595853248181442\n",
      "Log Regression(242/999): loss=0.5597117387084881\n",
      "Log Regression(243/999): loss=0.5634805607283359\n",
      "Log Regression(244/999): loss=0.5592924212233084\n",
      "Log Regression(245/999): loss=0.5611820634066111\n",
      "Log Regression(246/999): loss=0.5587115069224144\n",
      "Log Regression(247/999): loss=0.5586940923272873\n",
      "Log Regression(248/999): loss=0.5627971550106269\n",
      "Log Regression(249/999): loss=0.5582924485526185\n",
      "Log Regression(250/999): loss=0.5581408985557239\n",
      "Log Regression(251/999): loss=0.5613540310921933\n",
      "Log Regression(252/999): loss=0.558821369940778\n",
      "Log Regression(253/999): loss=0.5584990364915559\n",
      "Log Regression(254/999): loss=0.5574889440571186\n",
      "Log Regression(255/999): loss=0.5591453821596376\n",
      "Log Regression(256/999): loss=0.5581848185810604\n",
      "Log Regression(257/999): loss=0.5715498493995459\n",
      "Log Regression(258/999): loss=0.5579888269116201\n",
      "Log Regression(259/999): loss=0.5668397814547645\n",
      "Log Regression(260/999): loss=0.561888393966926\n",
      "Log Regression(261/999): loss=0.5575374844689386\n",
      "Log Regression(262/999): loss=0.5614483641037656\n",
      "Log Regression(263/999): loss=0.5569808691013921\n",
      "Log Regression(264/999): loss=0.5571339967734618\n",
      "Log Regression(265/999): loss=0.5565743667861683\n",
      "Log Regression(266/999): loss=0.5573165516056376\n",
      "Log Regression(267/999): loss=0.5593622344696437\n",
      "Log Regression(268/999): loss=0.562275620812841\n",
      "Log Regression(269/999): loss=0.556723427835113\n",
      "Log Regression(270/999): loss=0.5608256635377314\n",
      "Log Regression(271/999): loss=0.5658725353798408\n",
      "Log Regression(272/999): loss=0.5559653152827226\n",
      "Log Regression(273/999): loss=0.556704765438135\n",
      "Log Regression(274/999): loss=0.5616427193615223\n",
      "Log Regression(275/999): loss=0.5582976403560055\n",
      "Log Regression(276/999): loss=0.5699631084196419\n",
      "Log Regression(277/999): loss=0.5551639532723646\n",
      "Log Regression(278/999): loss=0.5557111314813322\n",
      "Log Regression(279/999): loss=0.555776564232058\n",
      "Log Regression(280/999): loss=0.5551406504822256\n",
      "Log Regression(281/999): loss=0.5555560853895005\n",
      "Log Regression(282/999): loss=0.5568798696158902\n",
      "Log Regression(283/999): loss=0.5729933858352947\n",
      "Log Regression(284/999): loss=0.5735483081546896\n",
      "Log Regression(285/999): loss=0.5750121096039311\n",
      "Log Regression(286/999): loss=0.5638342524440465\n",
      "Log Regression(287/999): loss=0.5545000991305726\n",
      "Log Regression(288/999): loss=0.5542047700196523\n",
      "Log Regression(289/999): loss=0.5628698886498034\n",
      "Log Regression(290/999): loss=0.5539501630231914\n",
      "Log Regression(291/999): loss=0.5626546573114768\n",
      "Log Regression(292/999): loss=0.5561111969512538\n",
      "Log Regression(293/999): loss=0.5569914769718817\n",
      "Log Regression(294/999): loss=0.5538127531263789\n",
      "Log Regression(295/999): loss=0.5550294280355477\n",
      "Log Regression(296/999): loss=0.5836205456048521\n",
      "Log Regression(297/999): loss=0.5558913695076686\n",
      "Log Regression(298/999): loss=0.5579366466308754\n",
      "Log Regression(299/999): loss=0.5539380171035173\n",
      "Log Regression(300/999): loss=0.5618858893677028\n",
      "Log Regression(301/999): loss=0.5565138307294205\n",
      "Log Regression(302/999): loss=0.5639061468488671\n",
      "Log Regression(303/999): loss=0.5546108186467397\n",
      "Log Regression(304/999): loss=0.5553880762763173\n",
      "Log Regression(305/999): loss=0.5560598620823404\n",
      "Log Regression(306/999): loss=0.5718705123325544\n",
      "Log Regression(307/999): loss=0.571438957235369\n",
      "Log Regression(308/999): loss=0.5570472793651154\n",
      "Log Regression(309/999): loss=0.5559375756728011\n",
      "Log Regression(310/999): loss=0.564491105962908\n",
      "Log Regression(311/999): loss=0.5565781614728814\n",
      "Log Regression(312/999): loss=0.5597372694378308\n",
      "Log Regression(313/999): loss=0.5637524488830933\n",
      "Log Regression(314/999): loss=0.5580466461453332\n",
      "Log Regression(315/999): loss=0.5567232417988222\n",
      "Log Regression(316/999): loss=0.551944227759044\n",
      "Log Regression(317/999): loss=0.5517775811693145\n",
      "Log Regression(318/999): loss=0.5596630918466272\n",
      "Log Regression(319/999): loss=0.5559818532805507\n",
      "Log Regression(320/999): loss=0.604319125191887\n",
      "Log Regression(321/999): loss=0.5562885657350067\n",
      "Log Regression(322/999): loss=0.5528246424846345\n",
      "Log Regression(323/999): loss=0.5665233151421168\n",
      "Log Regression(324/999): loss=0.5587302891879755\n",
      "Log Regression(325/999): loss=0.5515414329130016\n",
      "Log Regression(326/999): loss=0.5511512160519905\n",
      "Log Regression(327/999): loss=0.551623437055306\n",
      "Log Regression(328/999): loss=0.550960092980999\n",
      "Log Regression(329/999): loss=0.5523547159781734\n",
      "Log Regression(330/999): loss=0.5507914348470224\n",
      "Log Regression(331/999): loss=0.5507113154154591\n",
      "Log Regression(332/999): loss=0.5556573429831762\n",
      "Log Regression(333/999): loss=0.5522259514157571\n",
      "Log Regression(334/999): loss=0.5558999201439008\n",
      "Log Regression(335/999): loss=0.5568101731892866\n",
      "Log Regression(336/999): loss=0.5866901081877793\n",
      "Log Regression(337/999): loss=0.60707525993992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(338/999): loss=0.5507694166378119\n",
      "Log Regression(339/999): loss=0.5529003783780055\n",
      "Log Regression(340/999): loss=0.5522306479733862\n",
      "Log Regression(341/999): loss=0.5535085384676557\n",
      "Log Regression(342/999): loss=0.5516356195404456\n",
      "Log Regression(343/999): loss=0.551980499101624\n",
      "Log Regression(344/999): loss=0.5656792345289345\n",
      "Log Regression(345/999): loss=0.5751878511371592\n",
      "Log Regression(346/999): loss=0.5506807716745037\n",
      "Log Regression(347/999): loss=0.5510272250199534\n",
      "Log Regression(348/999): loss=0.5513022909092896\n",
      "Log Regression(349/999): loss=0.5502512059005009\n",
      "Log Regression(350/999): loss=0.5591084226523675\n",
      "Log Regression(351/999): loss=0.5511578783568112\n",
      "Log Regression(352/999): loss=0.5553077436919296\n",
      "Log Regression(353/999): loss=0.5523258700441149\n",
      "Log Regression(354/999): loss=0.5571658584450149\n",
      "Log Regression(355/999): loss=0.554000776022056\n",
      "Log Regression(356/999): loss=0.5641749816349942\n",
      "Log Regression(357/999): loss=0.5633548246254152\n",
      "Log Regression(358/999): loss=0.5495646285273267\n",
      "Log Regression(359/999): loss=0.5512408943683884\n",
      "Log Regression(360/999): loss=0.5918699126163754\n",
      "Log Regression(361/999): loss=0.5517055657655965\n",
      "Log Regression(362/999): loss=0.5524690934595978\n",
      "Log Regression(363/999): loss=0.5521092912261468\n",
      "Log Regression(364/999): loss=0.5565347460195796\n",
      "Log Regression(365/999): loss=0.5609752530973457\n",
      "Log Regression(366/999): loss=0.5540479276259647\n",
      "Log Regression(367/999): loss=0.5505360868026806\n",
      "Log Regression(368/999): loss=0.5493638720327035\n",
      "Log Regression(369/999): loss=0.5495363460415548\n",
      "Log Regression(370/999): loss=0.5525361201898703\n",
      "Log Regression(371/999): loss=0.5501646971578986\n",
      "Log Regression(372/999): loss=0.5512162024564027\n",
      "Log Regression(373/999): loss=0.5510367743877852\n",
      "Log Regression(374/999): loss=0.5511722050149473\n",
      "Log Regression(375/999): loss=0.5498862103103945\n",
      "Log Regression(376/999): loss=0.558055140840679\n",
      "Log Regression(377/999): loss=0.5506857502520424\n",
      "Log Regression(378/999): loss=0.5560831660164557\n",
      "Log Regression(379/999): loss=0.5517877973903828\n",
      "Log Regression(380/999): loss=0.552867239773709\n",
      "Log Regression(381/999): loss=0.5490899141475433\n",
      "Log Regression(382/999): loss=0.5487976256736636\n",
      "Log Regression(383/999): loss=0.5615599501551791\n",
      "Log Regression(384/999): loss=0.5546668848315472\n",
      "Log Regression(385/999): loss=0.5621894598173596\n",
      "Log Regression(386/999): loss=0.5483642244522944\n",
      "Log Regression(387/999): loss=0.5525409376744095\n",
      "Log Regression(388/999): loss=0.5486505685651792\n",
      "Log Regression(389/999): loss=0.5505064241623767\n",
      "Log Regression(390/999): loss=0.5474986082079659\n",
      "Log Regression(391/999): loss=0.5473554819185035\n",
      "Log Regression(392/999): loss=0.5477578901152739\n",
      "Log Regression(393/999): loss=0.5482395154367884\n",
      "Log Regression(394/999): loss=0.5603064518427632\n",
      "Log Regression(395/999): loss=0.5508131368513354\n",
      "Log Regression(396/999): loss=0.5470604220467326\n",
      "Log Regression(397/999): loss=0.5470545315756832\n",
      "Log Regression(398/999): loss=0.5487695978151295\n",
      "Log Regression(399/999): loss=0.5493129082549563\n",
      "Log Regression(400/999): loss=0.5469006448963052\n",
      "Log Regression(401/999): loss=0.561142328146855\n",
      "Log Regression(402/999): loss=0.5469528631666994\n",
      "Log Regression(403/999): loss=0.5504715544292296\n",
      "Log Regression(404/999): loss=0.5514601606031192\n",
      "Log Regression(405/999): loss=0.5471731129718019\n",
      "Log Regression(406/999): loss=0.5567230212154255\n",
      "Log Regression(407/999): loss=0.549061295287405\n",
      "Log Regression(408/999): loss=0.5527892114468604\n",
      "Log Regression(409/999): loss=0.5474574692279957\n",
      "Log Regression(410/999): loss=0.5551245430396703\n",
      "Log Regression(411/999): loss=0.5478854922083304\n",
      "Log Regression(412/999): loss=0.5604342101252945\n",
      "Log Regression(413/999): loss=0.5478394917550381\n",
      "Log Regression(414/999): loss=0.5526758061210025\n",
      "Log Regression(415/999): loss=0.5508149967154264\n",
      "Log Regression(416/999): loss=0.5516160717789202\n",
      "Log Regression(417/999): loss=0.5477783258642629\n",
      "Log Regression(418/999): loss=0.5470048240283978\n",
      "Log Regression(419/999): loss=0.5473473330061436\n",
      "Log Regression(420/999): loss=0.548646844159823\n",
      "Log Regression(421/999): loss=0.5468158730675163\n",
      "Log Regression(422/999): loss=0.5480377647215704\n",
      "Log Regression(423/999): loss=0.559114613148306\n",
      "Log Regression(424/999): loss=0.5643429348034308\n",
      "Log Regression(425/999): loss=0.5556924831539645\n",
      "Log Regression(426/999): loss=0.5468463275478782\n",
      "Log Regression(427/999): loss=0.5459382524274123\n",
      "Log Regression(428/999): loss=0.5465143939093304\n",
      "Log Regression(429/999): loss=0.5465718590880412\n",
      "Log Regression(430/999): loss=0.5460650990890794\n",
      "Log Regression(431/999): loss=0.5496611377509182\n",
      "Log Regression(432/999): loss=0.5468736097438802\n",
      "Log Regression(433/999): loss=0.5487462870706087\n",
      "Log Regression(434/999): loss=0.5466813555058635\n",
      "Log Regression(435/999): loss=0.5456374395701272\n",
      "Log Regression(436/999): loss=0.5528629000792398\n",
      "Log Regression(437/999): loss=0.5459759473952649\n",
      "Log Regression(438/999): loss=0.5456049712762758\n",
      "Log Regression(439/999): loss=0.5455331675639278\n",
      "Log Regression(440/999): loss=0.5488438557269247\n",
      "Log Regression(441/999): loss=0.548465197127936\n",
      "Log Regression(442/999): loss=0.5479024721429441\n",
      "Log Regression(443/999): loss=0.547132425101632\n",
      "Log Regression(444/999): loss=0.547563537344703\n",
      "Log Regression(445/999): loss=0.5464667218600429\n",
      "Log Regression(446/999): loss=0.5455416470272652\n",
      "Log Regression(447/999): loss=0.5449524297035885\n",
      "Log Regression(448/999): loss=0.550429362988251\n",
      "Log Regression(449/999): loss=0.546442882261094\n",
      "Log Regression(450/999): loss=0.549747480278952\n",
      "Log Regression(451/999): loss=0.5483062373124663\n",
      "Log Regression(452/999): loss=0.565320663713159\n",
      "Log Regression(453/999): loss=0.5508663707891696\n",
      "Log Regression(454/999): loss=0.5458093323240952\n",
      "Log Regression(455/999): loss=0.5459383148923173\n",
      "Log Regression(456/999): loss=0.5544987797585317\n",
      "Log Regression(457/999): loss=0.5448647697223734\n",
      "Log Regression(458/999): loss=0.5457270105168159\n",
      "Log Regression(459/999): loss=0.5449850941842946\n",
      "Log Regression(460/999): loss=0.5448959549273205\n",
      "Log Regression(461/999): loss=0.5477401043051837\n",
      "Log Regression(462/999): loss=0.5489773773045498\n",
      "Log Regression(463/999): loss=0.5492410944169644\n",
      "Log Regression(464/999): loss=0.5475834963879929\n",
      "Log Regression(465/999): loss=0.5502050188737534\n",
      "Log Regression(466/999): loss=0.553287587850988\n",
      "Log Regression(467/999): loss=0.5445050074119063\n",
      "Log Regression(468/999): loss=0.5467979716154904\n",
      "Log Regression(469/999): loss=0.5489144533069548\n",
      "Log Regression(470/999): loss=0.5445000388169984\n",
      "Log Regression(471/999): loss=0.5444260567365775\n",
      "Log Regression(472/999): loss=0.5456212505116869\n",
      "Log Regression(473/999): loss=0.5454543270632188\n",
      "Log Regression(474/999): loss=0.5522332276536872\n",
      "Log Regression(475/999): loss=0.5502756863040412\n",
      "Log Regression(476/999): loss=0.5453747172943326\n",
      "Log Regression(477/999): loss=0.5456629390194542\n",
      "Log Regression(478/999): loss=0.5476717041417813\n",
      "Log Regression(479/999): loss=0.5454067489753411\n",
      "Log Regression(480/999): loss=0.5461702563468194\n",
      "Log Regression(481/999): loss=0.5458555808449276\n",
      "Log Regression(482/999): loss=0.5462595533609583\n",
      "Log Regression(483/999): loss=0.5493492932844868\n",
      "Log Regression(484/999): loss=0.5455303152621924\n",
      "Log Regression(485/999): loss=0.5470107524810476\n",
      "Log Regression(486/999): loss=0.5463144527083402\n",
      "Log Regression(487/999): loss=0.5454303164224543\n",
      "Log Regression(488/999): loss=0.545241326486941\n",
      "Log Regression(489/999): loss=0.5660455534752209\n",
      "Log Regression(490/999): loss=0.5534617628217344\n",
      "Log Regression(491/999): loss=0.5452999658741574\n",
      "Log Regression(492/999): loss=0.5489468661755887\n",
      "Log Regression(493/999): loss=0.5488307887547735\n",
      "Log Regression(494/999): loss=0.5458699741494946\n",
      "Log Regression(495/999): loss=0.5437192672528677\n",
      "Log Regression(496/999): loss=0.5451761869296201\n",
      "Log Regression(497/999): loss=0.5510988431801142\n",
      "Log Regression(498/999): loss=0.5459524104696738\n",
      "Log Regression(499/999): loss=0.5477348335382979\n",
      "Log Regression(500/999): loss=0.5512673174081842\n",
      "Log Regression(501/999): loss=0.5478610195322969\n",
      "Log Regression(502/999): loss=0.5477496408225762\n",
      "Log Regression(503/999): loss=0.5547088606211207\n",
      "Log Regression(504/999): loss=0.5748613996646391\n",
      "Log Regression(505/999): loss=0.5474336150501551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(506/999): loss=0.5604156682213456\n",
      "Log Regression(507/999): loss=0.5439440019308672\n",
      "Log Regression(508/999): loss=0.5456442473149347\n",
      "Log Regression(509/999): loss=0.5580595975377055\n",
      "Log Regression(510/999): loss=0.5476082132709089\n",
      "Log Regression(511/999): loss=0.5436789007672072\n",
      "Log Regression(512/999): loss=0.543491752256352\n",
      "Log Regression(513/999): loss=0.5646121768787361\n",
      "Log Regression(514/999): loss=0.5447078496614186\n",
      "Log Regression(515/999): loss=0.5436576973476529\n",
      "Log Regression(516/999): loss=0.5442288206391822\n",
      "Log Regression(517/999): loss=0.5517763871436627\n",
      "Log Regression(518/999): loss=0.5444760411240853\n",
      "Log Regression(519/999): loss=0.5433443334122119\n",
      "Log Regression(520/999): loss=0.543841158687368\n",
      "Log Regression(521/999): loss=0.5464316021983273\n",
      "Log Regression(522/999): loss=0.5463025816565533\n",
      "Log Regression(523/999): loss=0.5432956873008453\n",
      "Log Regression(524/999): loss=0.5496647339000572\n",
      "Log Regression(525/999): loss=0.543810480195864\n",
      "Log Regression(526/999): loss=0.5443378089072136\n",
      "Log Regression(527/999): loss=0.5435711607177658\n",
      "Log Regression(528/999): loss=0.5427293527917665\n",
      "Log Regression(529/999): loss=0.5436091566486109\n",
      "Log Regression(530/999): loss=0.5464260120799812\n",
      "Log Regression(531/999): loss=0.5439240753300242\n",
      "Log Regression(532/999): loss=0.548756767597371\n",
      "Log Regression(533/999): loss=0.5428031639926466\n",
      "Log Regression(534/999): loss=0.542679470212931\n",
      "Log Regression(535/999): loss=0.543713279712701\n",
      "Log Regression(536/999): loss=0.5432682033035047\n",
      "Log Regression(537/999): loss=0.5480983290173382\n",
      "Log Regression(538/999): loss=0.5540360735642713\n",
      "Log Regression(539/999): loss=0.5453169194842671\n",
      "Log Regression(540/999): loss=0.5442012374662889\n",
      "Log Regression(541/999): loss=0.5426043090364444\n",
      "Log Regression(542/999): loss=0.5470206882151789\n",
      "Log Regression(543/999): loss=0.543302514602849\n",
      "Log Regression(544/999): loss=0.5428101214683597\n",
      "Log Regression(545/999): loss=0.5440031723051266\n",
      "Log Regression(546/999): loss=0.5454170603571704\n",
      "Log Regression(547/999): loss=0.5495573321081826\n",
      "Log Regression(548/999): loss=0.5492516758277369\n",
      "Log Regression(549/999): loss=0.5444597236250557\n",
      "Log Regression(550/999): loss=0.5445326590807654\n",
      "Log Regression(551/999): loss=0.5436946147254759\n",
      "Log Regression(552/999): loss=0.5430864274796773\n",
      "Log Regression(553/999): loss=0.5463011293861068\n",
      "Log Regression(554/999): loss=0.5448346259404658\n",
      "Log Regression(555/999): loss=0.5460155249611827\n",
      "Log Regression(556/999): loss=0.5452780690760879\n",
      "Log Regression(557/999): loss=0.5448232416008087\n",
      "Log Regression(558/999): loss=0.5511280027479627\n",
      "Log Regression(559/999): loss=0.5426272277522619\n",
      "Log Regression(560/999): loss=0.5451634429559985\n",
      "Log Regression(561/999): loss=0.5420326811329552\n",
      "Log Regression(562/999): loss=0.5468217287898204\n",
      "Log Regression(563/999): loss=0.5468392340450126\n",
      "Log Regression(564/999): loss=0.5468316072566836\n",
      "Log Regression(565/999): loss=0.5581944517667067\n",
      "Log Regression(566/999): loss=0.5444886079710464\n",
      "Log Regression(567/999): loss=0.5530240258638939\n",
      "Log Regression(568/999): loss=0.5426505960056796\n",
      "Log Regression(569/999): loss=0.5438276028807494\n",
      "Log Regression(570/999): loss=0.5421243303192604\n",
      "Log Regression(571/999): loss=0.5418981479930722\n",
      "Log Regression(572/999): loss=0.5482999717536677\n",
      "Log Regression(573/999): loss=0.5558104659202161\n",
      "Log Regression(574/999): loss=0.5424219726520436\n",
      "Log Regression(575/999): loss=0.5437714437742045\n",
      "Log Regression(576/999): loss=0.5488424296401937\n",
      "Log Regression(577/999): loss=0.5428173648273933\n",
      "Log Regression(578/999): loss=0.5417520832229435\n",
      "Log Regression(579/999): loss=0.5437293157516963\n",
      "Log Regression(580/999): loss=0.5629064082041965\n",
      "Log Regression(581/999): loss=0.5689257024911338\n",
      "Log Regression(582/999): loss=0.5434557144254603\n",
      "Log Regression(583/999): loss=0.5449043868505838\n",
      "Log Regression(584/999): loss=0.5423122898124592\n",
      "Log Regression(585/999): loss=0.5422958704327638\n",
      "Log Regression(586/999): loss=0.5498897411128135\n",
      "Log Regression(587/999): loss=0.5598249421680821\n",
      "Log Regression(588/999): loss=0.542764344579284\n",
      "Log Regression(589/999): loss=0.5438949539663066\n",
      "Log Regression(590/999): loss=0.542724641514558\n",
      "Log Regression(591/999): loss=0.5496012268515157\n",
      "Log Regression(592/999): loss=0.5433797016122268\n",
      "Log Regression(593/999): loss=0.5431307769294638\n",
      "Log Regression(594/999): loss=0.5447314393601822\n",
      "Log Regression(595/999): loss=0.5423994840240391\n",
      "Log Regression(596/999): loss=0.5568692962106551\n",
      "Log Regression(597/999): loss=0.5496136725131217\n",
      "Log Regression(598/999): loss=0.5465690284311523\n",
      "Log Regression(599/999): loss=0.5417889996299785\n",
      "Log Regression(600/999): loss=0.5416542403069649\n",
      "Log Regression(601/999): loss=0.5444704947058712\n",
      "Log Regression(602/999): loss=0.5458369328848212\n",
      "Log Regression(603/999): loss=0.5414658142195702\n",
      "Log Regression(604/999): loss=0.547083518776887\n",
      "Log Regression(605/999): loss=0.5441865351103904\n",
      "Log Regression(606/999): loss=0.5429578716694372\n",
      "Log Regression(607/999): loss=0.5464934742047782\n",
      "Log Regression(608/999): loss=0.542939046305887\n",
      "Log Regression(609/999): loss=0.5448236682593676\n",
      "Log Regression(610/999): loss=0.5458372077803052\n",
      "Log Regression(611/999): loss=0.5444400068913868\n",
      "Log Regression(612/999): loss=0.549481008798064\n",
      "Log Regression(613/999): loss=0.5434254091663799\n",
      "Log Regression(614/999): loss=0.5452946533207988\n",
      "Log Regression(615/999): loss=0.5435042073794756\n",
      "Log Regression(616/999): loss=0.5439296097906127\n",
      "Log Regression(617/999): loss=0.54316762704496\n",
      "Log Regression(618/999): loss=0.5431079392066626\n",
      "Log Regression(619/999): loss=0.5467071064125737\n",
      "Log Regression(620/999): loss=0.5432903346917535\n",
      "Log Regression(621/999): loss=0.553232558246558\n",
      "Log Regression(622/999): loss=0.5421764901203355\n",
      "Log Regression(623/999): loss=0.5420659175628064\n",
      "Log Regression(624/999): loss=0.5416729336291365\n",
      "Log Regression(625/999): loss=0.5435121654753857\n",
      "Log Regression(626/999): loss=0.5415966611520168\n",
      "Log Regression(627/999): loss=0.5568234858361528\n",
      "Log Regression(628/999): loss=0.5411229411063322\n",
      "Log Regression(629/999): loss=0.5413893208264124\n",
      "Log Regression(630/999): loss=0.5432880979418218\n",
      "Log Regression(631/999): loss=0.543977485600371\n",
      "Log Regression(632/999): loss=0.541534864297667\n",
      "Log Regression(633/999): loss=0.5412255654514191\n",
      "Log Regression(634/999): loss=0.5416482532020946\n",
      "Log Regression(635/999): loss=0.5434790511158\n",
      "Log Regression(636/999): loss=0.5433990143770365\n",
      "Log Regression(637/999): loss=0.5440754613122888\n",
      "Log Regression(638/999): loss=0.5496769669045795\n",
      "Log Regression(639/999): loss=0.541762579944455\n",
      "Log Regression(640/999): loss=0.5477591880808588\n",
      "Log Regression(641/999): loss=0.5418096723960362\n",
      "Log Regression(642/999): loss=0.5414261031426276\n",
      "Log Regression(643/999): loss=0.5458565176991921\n",
      "Log Regression(644/999): loss=0.540644420610334\n",
      "Log Regression(645/999): loss=0.5459686990382724\n",
      "Log Regression(646/999): loss=0.5404876661256462\n",
      "Log Regression(647/999): loss=0.5424426911488237\n",
      "Log Regression(648/999): loss=0.5447160550530828\n",
      "Log Regression(649/999): loss=0.5419408628228563\n",
      "Log Regression(650/999): loss=0.5499610816574574\n",
      "Log Regression(651/999): loss=0.5432894908652443\n",
      "Log Regression(652/999): loss=0.5407989070500044\n",
      "Log Regression(653/999): loss=0.5433750963229809\n",
      "Log Regression(654/999): loss=0.5414393080764848\n",
      "Log Regression(655/999): loss=0.5490375322849069\n",
      "Log Regression(656/999): loss=0.5411558793411505\n",
      "Log Regression(657/999): loss=0.541119431766715\n",
      "Log Regression(658/999): loss=0.5417843575182479\n",
      "Log Regression(659/999): loss=0.541132099677415\n",
      "Log Regression(660/999): loss=0.5416693702727773\n",
      "Log Regression(661/999): loss=0.5426948447809179\n",
      "Log Regression(662/999): loss=0.5503037551033407\n",
      "Log Regression(663/999): loss=0.5524820116239274\n",
      "Log Regression(664/999): loss=0.5425555428179136\n",
      "Log Regression(665/999): loss=0.5414477815208472\n",
      "Log Regression(666/999): loss=0.5406799058627487\n",
      "Log Regression(667/999): loss=0.5419956630164923\n",
      "Log Regression(668/999): loss=0.5413912979451349\n",
      "Log Regression(669/999): loss=0.5405688674187491\n",
      "Log Regression(670/999): loss=0.5424787942164091\n",
      "Log Regression(671/999): loss=0.5494693145288939\n",
      "Log Regression(672/999): loss=0.5499078134550717\n",
      "Log Regression(673/999): loss=0.5439246774101275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(674/999): loss=0.5428736767007154\n",
      "Log Regression(675/999): loss=0.551038394278235\n",
      "Log Regression(676/999): loss=0.5516699626371613\n",
      "Log Regression(677/999): loss=0.5409041826971458\n",
      "Log Regression(678/999): loss=0.5445383818937107\n",
      "Log Regression(679/999): loss=0.5405644974128889\n",
      "Log Regression(680/999): loss=0.5521384626566209\n",
      "Log Regression(681/999): loss=0.54039874981917\n",
      "Log Regression(682/999): loss=0.5409340254443054\n",
      "Log Regression(683/999): loss=0.5407752946696782\n",
      "Log Regression(684/999): loss=0.5451025401595405\n",
      "Log Regression(685/999): loss=0.5430722208924603\n",
      "Log Regression(686/999): loss=0.5527040920235816\n",
      "Log Regression(687/999): loss=0.5503944723286819\n",
      "Log Regression(688/999): loss=0.5473269969324154\n",
      "Log Regression(689/999): loss=0.541712452018906\n",
      "Log Regression(690/999): loss=0.5696246244597589\n",
      "Log Regression(691/999): loss=0.5487913074491185\n",
      "Log Regression(692/999): loss=0.541791507935627\n",
      "Log Regression(693/999): loss=0.5471340422909883\n",
      "Log Regression(694/999): loss=0.5494441040306711\n",
      "Log Regression(695/999): loss=0.565345813757983\n",
      "Log Regression(696/999): loss=0.5437215231382547\n",
      "Log Regression(697/999): loss=0.5497115514998362\n",
      "Log Regression(698/999): loss=0.5421736848127036\n",
      "Log Regression(699/999): loss=0.5416010810976517\n",
      "Log Regression(700/999): loss=0.5414318619000512\n",
      "Log Regression(701/999): loss=0.5412675805967923\n",
      "Log Regression(702/999): loss=0.5492515933742644\n",
      "Log Regression(703/999): loss=0.5425112090294977\n",
      "Log Regression(704/999): loss=0.559371805679583\n",
      "Log Regression(705/999): loss=0.540644359008574\n",
      "Log Regression(706/999): loss=0.5406685003374899\n",
      "Log Regression(707/999): loss=0.5411121412043376\n",
      "Log Regression(708/999): loss=0.5422842203263076\n",
      "Log Regression(709/999): loss=0.5416192202860861\n",
      "Log Regression(710/999): loss=0.5407556858166662\n",
      "Log Regression(711/999): loss=0.5409218418716628\n",
      "Log Regression(712/999): loss=0.5440213151505137\n",
      "Log Regression(713/999): loss=0.5433913770609673\n",
      "Log Regression(714/999): loss=0.5412147825693189\n",
      "Log Regression(715/999): loss=0.541139792520153\n",
      "Log Regression(716/999): loss=0.5469951269286975\n",
      "Log Regression(717/999): loss=0.5412116097677868\n",
      "Log Regression(718/999): loss=0.5396937073129477\n",
      "Log Regression(719/999): loss=0.5663992950166364\n",
      "Log Regression(720/999): loss=0.5501844783886244\n",
      "Log Regression(721/999): loss=0.5412864764338615\n",
      "Log Regression(722/999): loss=0.5403903937804352\n",
      "Log Regression(723/999): loss=0.5602832471207163\n",
      "Log Regression(724/999): loss=0.5429030103844908\n",
      "Log Regression(725/999): loss=0.540861652375316\n",
      "Log Regression(726/999): loss=0.5402799926505764\n",
      "Log Regression(727/999): loss=0.5424160899966829\n",
      "Log Regression(728/999): loss=0.5435536838184885\n",
      "Log Regression(729/999): loss=0.5705370890806633\n",
      "Log Regression(730/999): loss=0.5400144837430242\n",
      "Log Regression(731/999): loss=0.5554869543901404\n",
      "Log Regression(732/999): loss=0.5485281991605624\n",
      "Log Regression(733/999): loss=0.5410420843531661\n",
      "Log Regression(734/999): loss=0.5400288241679032\n",
      "Log Regression(735/999): loss=0.5445011069304637\n",
      "Log Regression(736/999): loss=0.5511733799576058\n",
      "Log Regression(737/999): loss=0.5524935551167548\n",
      "Log Regression(738/999): loss=0.5553302062852769\n",
      "Log Regression(739/999): loss=0.5395786591377314\n",
      "Log Regression(740/999): loss=0.5413886356105562\n",
      "Log Regression(741/999): loss=0.5431340618196326\n",
      "Log Regression(742/999): loss=0.5406018584948635\n",
      "Log Regression(743/999): loss=0.5418245874925475\n",
      "Log Regression(744/999): loss=0.5511936391721621\n",
      "Log Regression(745/999): loss=0.5402945343058082\n",
      "Log Regression(746/999): loss=0.5395812863975886\n",
      "Log Regression(747/999): loss=0.5446714912778909\n",
      "Log Regression(748/999): loss=0.5439163527522332\n",
      "Log Regression(749/999): loss=0.5407601714008158\n",
      "Log Regression(750/999): loss=0.5523121487249181\n",
      "Log Regression(751/999): loss=0.5402377241540314\n",
      "Log Regression(752/999): loss=0.5417695895678771\n",
      "Log Regression(753/999): loss=0.5421936385193187\n",
      "Log Regression(754/999): loss=0.5409044299040734\n",
      "Log Regression(755/999): loss=0.5631964002287418\n",
      "Log Regression(756/999): loss=0.542335132372769\n",
      "Log Regression(757/999): loss=0.5416268904434752\n",
      "Log Regression(758/999): loss=0.5406055250166452\n",
      "Log Regression(759/999): loss=0.5408976107318993\n",
      "Log Regression(760/999): loss=0.5449085421452131\n",
      "Log Regression(761/999): loss=0.5462364365919318\n",
      "Log Regression(762/999): loss=0.5456684935094264\n",
      "Log Regression(763/999): loss=0.5426676020310374\n",
      "Log Regression(764/999): loss=0.5449595395870357\n",
      "Log Regression(765/999): loss=0.5404593972360615\n",
      "Log Regression(766/999): loss=0.5400123752763487\n",
      "Log Regression(767/999): loss=0.5428802244136922\n",
      "Log Regression(768/999): loss=0.5396343992193436\n",
      "Log Regression(769/999): loss=0.5429696354328086\n",
      "Log Regression(770/999): loss=0.5410855028525328\n",
      "Log Regression(771/999): loss=0.5502781138654623\n",
      "Log Regression(772/999): loss=0.5409942394858958\n",
      "Log Regression(773/999): loss=0.5464195098288439\n",
      "Log Regression(774/999): loss=0.5456555863051249\n",
      "Log Regression(775/999): loss=0.5594532710971032\n",
      "Log Regression(776/999): loss=0.5411847664203843\n",
      "Log Regression(777/999): loss=0.558072186234006\n",
      "Log Regression(778/999): loss=0.5435873223823392\n",
      "Log Regression(779/999): loss=0.5398456335432619\n",
      "Log Regression(780/999): loss=0.5396698106407636\n",
      "Log Regression(781/999): loss=0.5397521171717884\n",
      "Log Regression(782/999): loss=0.5397287988510839\n",
      "Log Regression(783/999): loss=0.5393478704757123\n",
      "Log Regression(784/999): loss=0.5616430716299012\n",
      "Log Regression(785/999): loss=0.5422151998974228\n",
      "Log Regression(786/999): loss=0.5399772859845816\n",
      "Log Regression(787/999): loss=0.539878849536583\n",
      "Log Regression(788/999): loss=0.5431782329847052\n",
      "Log Regression(789/999): loss=0.5402909309004593\n",
      "Log Regression(790/999): loss=0.5452393626007204\n",
      "Log Regression(791/999): loss=0.539412251608462\n",
      "Log Regression(792/999): loss=0.5397816542349823\n",
      "Log Regression(793/999): loss=0.5393157087781444\n",
      "Log Regression(794/999): loss=0.5421033930919007\n",
      "Log Regression(795/999): loss=0.5392201252832078\n",
      "Log Regression(796/999): loss=0.5397659958994694\n",
      "Log Regression(797/999): loss=0.539605341406937\n",
      "Log Regression(798/999): loss=0.5407993361134804\n",
      "Log Regression(799/999): loss=0.5413818312171201\n",
      "Log Regression(800/999): loss=0.5402358729813755\n",
      "Log Regression(801/999): loss=0.5391434591469194\n",
      "Log Regression(802/999): loss=0.5391262629979657\n",
      "Log Regression(803/999): loss=0.5396566317290483\n",
      "Log Regression(804/999): loss=0.5397605758233583\n",
      "Log Regression(805/999): loss=0.5391364438848271\n",
      "Log Regression(806/999): loss=0.5396823842401652\n",
      "Log Regression(807/999): loss=0.5391438402713856\n",
      "Log Regression(808/999): loss=0.539370554969016\n",
      "Log Regression(809/999): loss=0.5391701331292427\n",
      "Log Regression(810/999): loss=0.5434403210275806\n",
      "Log Regression(811/999): loss=0.5414513330965369\n",
      "Log Regression(812/999): loss=0.5391878163984481\n",
      "Log Regression(813/999): loss=0.5392064719056592\n",
      "Log Regression(814/999): loss=0.5396190962125582\n",
      "Log Regression(815/999): loss=0.5465554687460105\n",
      "Log Regression(816/999): loss=0.5404506234763158\n",
      "Log Regression(817/999): loss=0.5397875234327629\n",
      "Log Regression(818/999): loss=0.5392520949831666\n",
      "Log Regression(819/999): loss=0.5391298644732172\n",
      "Log Regression(820/999): loss=0.5392096949463061\n",
      "Log Regression(821/999): loss=0.5397111138958681\n",
      "Log Regression(822/999): loss=0.5405802444587176\n",
      "Log Regression(823/999): loss=0.5390851991738427\n",
      "Log Regression(824/999): loss=0.5418635642974432\n",
      "Log Regression(825/999): loss=0.543460068797871\n",
      "Log Regression(826/999): loss=0.5429374401314805\n",
      "Log Regression(827/999): loss=0.5391409780028412\n",
      "Log Regression(828/999): loss=0.5396499289591487\n",
      "Log Regression(829/999): loss=0.556258024265767\n",
      "Log Regression(830/999): loss=0.5527780543377703\n",
      "Log Regression(831/999): loss=0.5493581882322215\n",
      "Log Regression(832/999): loss=0.541220837987145\n",
      "Log Regression(833/999): loss=0.5486119125901431\n",
      "Log Regression(834/999): loss=0.5397591305384126\n",
      "Log Regression(835/999): loss=0.5403510357006491\n",
      "Log Regression(836/999): loss=0.5393405067032546\n",
      "Log Regression(837/999): loss=0.5419694434715159\n",
      "Log Regression(838/999): loss=0.5389119940512757\n",
      "Log Regression(839/999): loss=0.5449481520271182\n",
      "Log Regression(840/999): loss=0.5400192312666587\n",
      "Log Regression(841/999): loss=0.5388552746002403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(842/999): loss=0.5389072391609611\n",
      "Log Regression(843/999): loss=0.5405467987482644\n",
      "Log Regression(844/999): loss=0.5467330514441289\n",
      "Log Regression(845/999): loss=0.538877098388668\n",
      "Log Regression(846/999): loss=0.5401601630839546\n",
      "Log Regression(847/999): loss=0.5459100567189282\n",
      "Log Regression(848/999): loss=0.5596430998786941\n",
      "Log Regression(849/999): loss=0.5415876190572702\n",
      "Log Regression(850/999): loss=0.5423994656876077\n",
      "Log Regression(851/999): loss=0.5524695358459355\n",
      "Log Regression(852/999): loss=0.5524755180429668\n",
      "Log Regression(853/999): loss=0.5445148082996143\n",
      "Log Regression(854/999): loss=0.5413455899699575\n",
      "Log Regression(855/999): loss=0.5395703366310561\n",
      "Log Regression(856/999): loss=0.5398988266518367\n",
      "Log Regression(857/999): loss=0.5437558541888022\n",
      "Log Regression(858/999): loss=0.5389673090269185\n",
      "Log Regression(859/999): loss=0.5389993746099844\n",
      "Log Regression(860/999): loss=0.5397761052756934\n",
      "Log Regression(861/999): loss=0.549665323240502\n",
      "Log Regression(862/999): loss=0.5394461213980002\n",
      "Log Regression(863/999): loss=0.5438493330797163\n",
      "Log Regression(864/999): loss=0.540497349060337\n",
      "Log Regression(865/999): loss=0.5395719730052068\n",
      "Log Regression(866/999): loss=0.5394679124155883\n",
      "Log Regression(867/999): loss=0.5427819926835018\n",
      "Log Regression(868/999): loss=0.5543675482084961\n",
      "Log Regression(869/999): loss=0.539036824106121\n",
      "Log Regression(870/999): loss=0.539635784665401\n",
      "Log Regression(871/999): loss=0.5402508880214226\n",
      "Log Regression(872/999): loss=0.5397305092493065\n",
      "Log Regression(873/999): loss=0.5422121449054182\n",
      "Log Regression(874/999): loss=0.5415580241551805\n",
      "Log Regression(875/999): loss=0.5392426018473259\n",
      "Log Regression(876/999): loss=0.5390970013091201\n",
      "Log Regression(877/999): loss=0.5460426190207767\n",
      "Log Regression(878/999): loss=0.5536005060918696\n",
      "Log Regression(879/999): loss=0.5421617890754316\n",
      "Log Regression(880/999): loss=0.539506951700115\n",
      "Log Regression(881/999): loss=0.5410576144342168\n",
      "Log Regression(882/999): loss=0.5443233564451857\n",
      "Log Regression(883/999): loss=0.5404407289312625\n",
      "Log Regression(884/999): loss=0.5393487247346415\n",
      "Log Regression(885/999): loss=0.5393148228654\n",
      "Log Regression(886/999): loss=0.5395253062174582\n",
      "Log Regression(887/999): loss=0.5403181445816393\n",
      "Log Regression(888/999): loss=0.5417021924500893\n",
      "Log Regression(889/999): loss=0.5400427397025805\n",
      "Log Regression(890/999): loss=0.5385694202298417\n",
      "Log Regression(891/999): loss=0.538829011401907\n",
      "Log Regression(892/999): loss=0.5422257318285119\n",
      "Log Regression(893/999): loss=0.5393088743486495\n",
      "Log Regression(894/999): loss=0.5384945215197486\n",
      "Log Regression(895/999): loss=0.5387194683591036\n",
      "Log Regression(896/999): loss=0.5387843539595527\n",
      "Log Regression(897/999): loss=0.5389986053178141\n",
      "Log Regression(898/999): loss=0.5548262478376395\n",
      "Log Regression(899/999): loss=0.5461476607146393\n",
      "Log Regression(900/999): loss=0.5404544005118174\n",
      "Log Regression(901/999): loss=0.5417846985773176\n",
      "Log Regression(902/999): loss=0.5390826929493724\n",
      "Log Regression(903/999): loss=0.5402543399905421\n",
      "Log Regression(904/999): loss=0.538578372925337\n",
      "Log Regression(905/999): loss=0.5416845661592715\n",
      "Log Regression(906/999): loss=0.5412592422881153\n",
      "Log Regression(907/999): loss=0.5430564811173162\n",
      "Log Regression(908/999): loss=0.5432024988766537\n",
      "Log Regression(909/999): loss=0.5384964366350543\n",
      "Log Regression(910/999): loss=0.5395599058125313\n",
      "Log Regression(911/999): loss=0.5388195784530411\n",
      "Log Regression(912/999): loss=0.5500844496851653\n",
      "Log Regression(913/999): loss=0.547398614600833\n",
      "Log Regression(914/999): loss=0.5562589368241304\n",
      "Log Regression(915/999): loss=0.5418053778354928\n",
      "Log Regression(916/999): loss=0.5390946657722917\n",
      "Log Regression(917/999): loss=0.5397783685425788\n",
      "Log Regression(918/999): loss=0.5422791222892362\n",
      "Log Regression(919/999): loss=0.5390672888232055\n",
      "Log Regression(920/999): loss=0.5392947777475241\n",
      "Log Regression(921/999): loss=0.5394809158016852\n",
      "Log Regression(922/999): loss=0.542187888411709\n",
      "Log Regression(923/999): loss=0.5383423095158232\n",
      "Log Regression(924/999): loss=0.5383669762613861\n",
      "Log Regression(925/999): loss=0.5493585557877139\n",
      "Log Regression(926/999): loss=0.5406245202114598\n",
      "Log Regression(927/999): loss=0.5384895391443424\n",
      "Log Regression(928/999): loss=0.5395365697975557\n",
      "Log Regression(929/999): loss=0.5438907369932188\n",
      "Log Regression(930/999): loss=0.5406009383355045\n",
      "Log Regression(931/999): loss=0.5383969511026511\n",
      "Log Regression(932/999): loss=0.5400444131252121\n",
      "Log Regression(933/999): loss=0.5457273670598913\n",
      "Log Regression(934/999): loss=0.5419265988007522\n",
      "Log Regression(935/999): loss=0.5417588075550159\n",
      "Log Regression(936/999): loss=0.5406619684050659\n",
      "Log Regression(937/999): loss=0.5393810492325377\n",
      "Log Regression(938/999): loss=0.5407218932406168\n",
      "Log Regression(939/999): loss=0.5388723632230987\n",
      "Log Regression(940/999): loss=0.5387718954173218\n",
      "Log Regression(941/999): loss=0.5413047083626811\n",
      "Log Regression(942/999): loss=0.538392416411645\n",
      "Log Regression(943/999): loss=0.5422717362505597\n",
      "Log Regression(944/999): loss=0.5381963152200243\n",
      "Log Regression(945/999): loss=0.5450749127866603\n",
      "Log Regression(946/999): loss=0.5415563386289958\n",
      "Log Regression(947/999): loss=0.5424530333801726\n",
      "Log Regression(948/999): loss=0.5384709135609036\n",
      "Log Regression(949/999): loss=0.5395941420186297\n",
      "Log Regression(950/999): loss=0.5454159870158005\n",
      "Log Regression(951/999): loss=0.5392720587366522\n",
      "Log Regression(952/999): loss=0.5597722567239567\n",
      "Log Regression(953/999): loss=0.5395566983244431\n",
      "Log Regression(954/999): loss=0.5405238253724031\n",
      "Log Regression(955/999): loss=0.5394064761396015\n",
      "Log Regression(956/999): loss=0.5386181571007469\n",
      "Log Regression(957/999): loss=0.5386755013818099\n",
      "Log Regression(958/999): loss=0.5382718438376837\n",
      "Log Regression(959/999): loss=0.5457652926333143\n",
      "Log Regression(960/999): loss=0.5403731690776384\n",
      "Log Regression(961/999): loss=0.5438140559074631\n",
      "Log Regression(962/999): loss=0.540494002692039\n",
      "Log Regression(963/999): loss=0.5381732871345483\n",
      "Log Regression(964/999): loss=0.5387097485183282\n",
      "Log Regression(965/999): loss=0.5417133428983318\n",
      "Log Regression(966/999): loss=0.5396472932025442\n",
      "Log Regression(967/999): loss=0.5402693395974111\n",
      "Log Regression(968/999): loss=0.5395305753847168\n",
      "Log Regression(969/999): loss=0.538414460783243\n",
      "Log Regression(970/999): loss=0.5426519569253474\n",
      "Log Regression(971/999): loss=0.5492577717512356\n",
      "Log Regression(972/999): loss=0.5385786157110997\n",
      "Log Regression(973/999): loss=0.5381046765462394\n",
      "Log Regression(974/999): loss=0.5386419872303728\n",
      "Log Regression(975/999): loss=0.5518346863245397\n",
      "Log Regression(976/999): loss=0.5452461751753295\n",
      "Log Regression(977/999): loss=0.5515407529421377\n",
      "Log Regression(978/999): loss=0.5382917934923245\n",
      "Log Regression(979/999): loss=0.5402879679773872\n",
      "Log Regression(980/999): loss=0.5386663458179825\n",
      "Log Regression(981/999): loss=0.5500927890187671\n",
      "Log Regression(982/999): loss=0.5441497054146637\n",
      "Log Regression(983/999): loss=0.5585451087243809\n",
      "Log Regression(984/999): loss=0.5385389013768521\n",
      "Log Regression(985/999): loss=0.5394227810120255\n",
      "Log Regression(986/999): loss=0.5482285388523109\n",
      "Log Regression(987/999): loss=0.5381753254037547\n",
      "Log Regression(988/999): loss=0.5454615923621202\n",
      "Log Regression(989/999): loss=0.5383160624992107\n",
      "Log Regression(990/999): loss=0.5399386037900418\n",
      "Log Regression(991/999): loss=0.5484621033073158\n",
      "Log Regression(992/999): loss=0.5431122874770193\n",
      "Log Regression(993/999): loss=0.5389771625570898\n",
      "Log Regression(994/999): loss=0.5482746892815414\n",
      "Log Regression(995/999): loss=0.5462074649646275\n",
      "Log Regression(996/999): loss=0.5419709176801609\n",
      "Log Regression(997/999): loss=0.5429062382393366\n",
      "Log Regression(998/999): loss=0.5382624963682421\n",
      "Log Regression(999/999): loss=0.5383841747448588\n"
     ]
    }
   ],
   "source": [
    "w, mse = logistic_regression(y_train_lr, tX_train, initial_w, max_iters, gamma, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7269155555555555"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict_labels(w, tX_train)\n",
    "np.mean(y_train.reshape(-1,1)==y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.727"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict_labels(w, tX_val)\n",
    "np.mean(y_val.reshape(-1,1)==y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Regularized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_lr = y_train>0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lambda = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros(tX.shape[1])#np.random.normal(loc=0, scale=1, size=(tX.shape[1],1))\n",
    "max_iters = 5000\n",
    "gamma = 2e-7\n",
    "lambda_ = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(0/4999): loss=0.6784966228564583\n",
      "Log Regression(1/4999): loss=0.6728777545647245\n",
      "Log Regression(2/4999): loss=0.6708599776250701\n",
      "Log Regression(3/4999): loss=0.6660818263001932\n",
      "Log Regression(4/4999): loss=0.6649078174277324\n",
      "Log Regression(5/4999): loss=0.6704532069215893\n",
      "Log Regression(6/4999): loss=0.6678752254563738\n",
      "Log Regression(7/4999): loss=0.6694979935716667\n",
      "Log Regression(8/4999): loss=0.6665048256311488\n",
      "Log Regression(9/4999): loss=0.6864026823746251\n",
      "Log Regression(10/4999): loss=0.6587512639394648\n",
      "Log Regression(11/4999): loss=0.6565441552690753\n",
      "Log Regression(12/4999): loss=0.6578398256117568\n",
      "Log Regression(13/4999): loss=0.6547958507810951\n",
      "Log Regression(14/4999): loss=0.6677523108259908\n",
      "Log Regression(15/4999): loss=0.6528702864391277\n",
      "Log Regression(16/4999): loss=0.6535367575972096\n",
      "Log Regression(17/4999): loss=0.6505342941981145\n",
      "Log Regression(18/4999): loss=0.6531550543678706\n",
      "Log Regression(19/4999): loss=0.6540315843090969\n",
      "Log Regression(20/4999): loss=0.6489394236679269\n",
      "Log Regression(21/4999): loss=0.6490824096941529\n",
      "Log Regression(22/4999): loss=0.651276137736324\n",
      "Log Regression(23/4999): loss=0.6456239637007866\n",
      "Log Regression(24/4999): loss=0.6456052419981767\n",
      "Log Regression(25/4999): loss=0.6534449890546066\n",
      "Log Regression(26/4999): loss=0.6440263420817224\n",
      "Log Regression(27/4999): loss=0.6444055822828884\n",
      "Log Regression(28/4999): loss=0.643059479187097\n",
      "Log Regression(29/4999): loss=0.6401539780494095\n",
      "Log Regression(30/4999): loss=0.6394595626262891\n",
      "Log Regression(31/4999): loss=0.6387759234779574\n",
      "Log Regression(32/4999): loss=0.6382237619289396\n",
      "Log Regression(33/4999): loss=0.6400165631704912\n",
      "Log Regression(34/4999): loss=0.6374325194945734\n",
      "Log Regression(35/4999): loss=0.6362906271065307\n",
      "Log Regression(36/4999): loss=0.6401213899153638\n",
      "Log Regression(37/4999): loss=0.6346018004996713\n",
      "Log Regression(38/4999): loss=0.6390735729769104\n",
      "Log Regression(39/4999): loss=0.6364832176627543\n",
      "Log Regression(40/4999): loss=0.6336726353596902\n",
      "Log Regression(41/4999): loss=0.6362902328084031\n",
      "Log Regression(42/4999): loss=0.6323844994011975\n",
      "Log Regression(43/4999): loss=0.6324120970008054\n",
      "Log Regression(44/4999): loss=0.6307274191895748\n",
      "Log Regression(45/4999): loss=0.6312145051748963\n",
      "Log Regression(46/4999): loss=0.6319250611890116\n",
      "Log Regression(47/4999): loss=0.6348861458191906\n",
      "Log Regression(48/4999): loss=0.6295424649920222\n",
      "Log Regression(49/4999): loss=0.6275286159975628\n",
      "Log Regression(50/4999): loss=0.6264349459692561\n",
      "Log Regression(51/4999): loss=0.6262707963391282\n",
      "Log Regression(52/4999): loss=0.6262757422027193\n",
      "Log Regression(53/4999): loss=0.6254900497065856\n",
      "Log Regression(54/4999): loss=0.6264139945883361\n",
      "Log Regression(55/4999): loss=0.6242710097439809\n",
      "Log Regression(56/4999): loss=0.6243334398660019\n",
      "Log Regression(57/4999): loss=0.6226107503421434\n",
      "Log Regression(58/4999): loss=0.6311765680692027\n",
      "Log Regression(59/4999): loss=0.6347724158483065\n",
      "Log Regression(60/4999): loss=0.6265958475971173\n",
      "Log Regression(61/4999): loss=0.6216488900475295\n",
      "Log Regression(62/4999): loss=0.6219499356030381\n",
      "Log Regression(63/4999): loss=0.6206497441261108\n",
      "Log Regression(64/4999): loss=0.6233959286874278\n",
      "Log Regression(65/4999): loss=0.6221465789841077\n",
      "Log Regression(66/4999): loss=0.6227173232251445\n",
      "Log Regression(67/4999): loss=0.6250357765096655\n",
      "Log Regression(68/4999): loss=0.6312131133562502\n",
      "Log Regression(69/4999): loss=0.6240946749816071\n",
      "Log Regression(70/4999): loss=0.6360522332953708\n",
      "Log Regression(71/4999): loss=0.6205502593576974\n",
      "Log Regression(72/4999): loss=0.6166864385097781\n",
      "Log Regression(73/4999): loss=0.6161557362172916\n",
      "Log Regression(74/4999): loss=0.6193070704686381\n",
      "Log Regression(75/4999): loss=0.6276517245370297\n",
      "Log Regression(76/4999): loss=0.6154860309160419\n",
      "Log Regression(77/4999): loss=0.6147767381572159\n",
      "Log Regression(78/4999): loss=0.6173547257524434\n",
      "Log Regression(79/4999): loss=0.6144650549964051\n",
      "Log Regression(80/4999): loss=0.6178613471692959\n",
      "Log Regression(81/4999): loss=0.616596259790468\n",
      "Log Regression(82/4999): loss=0.6122987661503807\n",
      "Log Regression(83/4999): loss=0.6119182972434234\n",
      "Log Regression(84/4999): loss=0.6118867020648596\n",
      "Log Regression(85/4999): loss=0.6149285110495983\n",
      "Log Regression(86/4999): loss=0.6115469318532702\n",
      "Log Regression(87/4999): loss=0.6115736013448103\n",
      "Log Regression(88/4999): loss=0.6100089120471365\n",
      "Log Regression(89/4999): loss=0.6098034881675873\n",
      "Log Regression(90/4999): loss=0.6097009661684687\n",
      "Log Regression(91/4999): loss=0.6098227234166285\n",
      "Log Regression(92/4999): loss=0.6089773016049688\n",
      "Log Regression(93/4999): loss=0.6107735353688258\n",
      "Log Regression(94/4999): loss=0.6095290821347511\n",
      "Log Regression(95/4999): loss=0.6080857038645425\n",
      "Log Regression(96/4999): loss=0.6085566162853532\n",
      "Log Regression(97/4999): loss=0.6085373259358683\n",
      "Log Regression(98/4999): loss=0.6138275937311071\n",
      "Log Regression(99/4999): loss=0.6099296843209396\n",
      "Log Regression(100/4999): loss=0.6068478781338755\n",
      "Log Regression(101/4999): loss=0.6066416640886514\n",
      "Log Regression(102/4999): loss=0.6057478827223383\n",
      "Log Regression(103/4999): loss=0.6061584979863387\n",
      "Log Regression(104/4999): loss=0.6057629322005308\n",
      "Log Regression(105/4999): loss=0.6049650877317475\n",
      "Log Regression(106/4999): loss=0.6058760704461518\n",
      "Log Regression(107/4999): loss=0.6066261756622023\n",
      "Log Regression(108/4999): loss=0.6072807152365964\n",
      "Log Regression(109/4999): loss=0.604432921557384\n",
      "Log Regression(110/4999): loss=0.6046518154871571\n",
      "Log Regression(111/4999): loss=0.6043545668529563\n",
      "Log Regression(112/4999): loss=0.6037041276683949\n",
      "Log Regression(113/4999): loss=0.603315881446286\n",
      "Log Regression(114/4999): loss=0.6056369361878025\n",
      "Log Regression(115/4999): loss=0.6039993755010006\n",
      "Log Regression(116/4999): loss=0.6020570203991737\n",
      "Log Regression(117/4999): loss=0.6031422685712948\n",
      "Log Regression(118/4999): loss=0.6025927743049916\n",
      "Log Regression(119/4999): loss=0.6028050154998655\n",
      "Log Regression(120/4999): loss=0.6008940256735644\n",
      "Log Regression(121/4999): loss=0.6025371847282829\n",
      "Log Regression(122/4999): loss=0.6006280495219358\n",
      "Log Regression(123/4999): loss=0.6002826150103898\n",
      "Log Regression(124/4999): loss=0.6002854135420016\n",
      "Log Regression(125/4999): loss=0.6007476867005354\n",
      "Log Regression(126/4999): loss=0.5992892262412077\n",
      "Log Regression(127/4999): loss=0.6016086507418371\n",
      "Log Regression(128/4999): loss=0.5989291408973675\n",
      "Log Regression(129/4999): loss=0.598944490242082\n",
      "Log Regression(130/4999): loss=0.5990117943528138\n",
      "Log Regression(131/4999): loss=0.5982989667852493\n",
      "Log Regression(132/4999): loss=0.5979414220170034\n",
      "Log Regression(133/4999): loss=0.6040984827591026\n",
      "Log Regression(134/4999): loss=0.6061697707077075\n",
      "Log Regression(135/4999): loss=0.6149005131353243\n",
      "Log Regression(136/4999): loss=0.5994128802181141\n",
      "Log Regression(137/4999): loss=0.5966949286238545\n",
      "Log Regression(138/4999): loss=0.5962724526625268\n",
      "Log Regression(139/4999): loss=0.5966052918331808\n",
      "Log Regression(140/4999): loss=0.5978883304906414\n",
      "Log Regression(141/4999): loss=0.5998483516019655\n",
      "Log Regression(142/4999): loss=0.5962478022233627\n",
      "Log Regression(143/4999): loss=0.5987594046956516\n",
      "Log Regression(144/4999): loss=0.5982236805799261\n",
      "Log Regression(145/4999): loss=0.5946310054718217\n",
      "Log Regression(146/4999): loss=0.5951098612739775\n",
      "Log Regression(147/4999): loss=0.5997059548727702\n",
      "Log Regression(148/4999): loss=0.5963413818190576\n",
      "Log Regression(149/4999): loss=0.5944448880801745\n",
      "Log Regression(150/4999): loss=0.5954934993901508\n",
      "Log Regression(151/4999): loss=0.5965073567248601\n",
      "Log Regression(152/4999): loss=0.5939400138803005\n",
      "Log Regression(153/4999): loss=0.593362738548627\n",
      "Log Regression(154/4999): loss=0.5934494399770718\n",
      "Log Regression(155/4999): loss=0.5942131034370715\n",
      "Log Regression(156/4999): loss=0.5932717930676594\n",
      "Log Regression(157/4999): loss=0.5957538209064448\n",
      "Log Regression(158/4999): loss=0.5943611573729753\n",
      "Log Regression(159/4999): loss=0.5935994807041007\n",
      "Log Regression(160/4999): loss=0.5943750947765576\n",
      "Log Regression(161/4999): loss=0.595391339271113\n",
      "Log Regression(162/4999): loss=0.5954818062138925\n",
      "Log Regression(163/4999): loss=0.596530855728369\n",
      "Log Regression(164/4999): loss=0.5917313820362193\n",
      "Log Regression(165/4999): loss=0.5962121924439291\n",
      "Log Regression(166/4999): loss=0.5952995788328328\n",
      "Log Regression(167/4999): loss=0.5913118561339814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(168/4999): loss=0.5913439115681086\n",
      "Log Regression(169/4999): loss=0.5909536986259676\n",
      "Log Regression(170/4999): loss=0.5937213943878525\n",
      "Log Regression(171/4999): loss=0.5902936894654216\n",
      "Log Regression(172/4999): loss=0.5901597914087361\n",
      "Log Regression(173/4999): loss=0.5905063737293693\n",
      "Log Regression(174/4999): loss=0.589952027658257\n",
      "Log Regression(175/4999): loss=0.5903084911694756\n",
      "Log Regression(176/4999): loss=0.590204336695589\n",
      "Log Regression(177/4999): loss=0.5907896204674562\n",
      "Log Regression(178/4999): loss=0.5908814362408069\n",
      "Log Regression(179/4999): loss=0.5928667646624776\n",
      "Log Regression(180/4999): loss=0.594538289215381\n",
      "Log Regression(181/4999): loss=0.5892790135638756\n",
      "Log Regression(182/4999): loss=0.5906767139308614\n",
      "Log Regression(183/4999): loss=0.5885797999049345\n",
      "Log Regression(184/4999): loss=0.5884338620044091\n",
      "Log Regression(185/4999): loss=0.5882080438819548\n",
      "Log Regression(186/4999): loss=0.5889101640622264\n",
      "Log Regression(187/4999): loss=0.5925123731764742\n",
      "Log Regression(188/4999): loss=0.596735062120155\n",
      "Log Regression(189/4999): loss=0.5916252579289234\n",
      "Log Regression(190/4999): loss=0.5884574509890396\n",
      "Log Regression(191/4999): loss=0.588883372807831\n",
      "Log Regression(192/4999): loss=0.5878695217808829\n",
      "Log Regression(193/4999): loss=0.5898871614482765\n",
      "Log Regression(194/4999): loss=0.5890942363893346\n",
      "Log Regression(195/4999): loss=0.5881714984615367\n",
      "Log Regression(196/4999): loss=0.5867749290796318\n",
      "Log Regression(197/4999): loss=0.5917176075759273\n",
      "Log Regression(198/4999): loss=0.5884161046240474\n",
      "Log Regression(199/4999): loss=0.5910882801646103\n",
      "Log Regression(200/4999): loss=0.5882450793492768\n",
      "Log Regression(201/4999): loss=0.5896570490784676\n",
      "Log Regression(202/4999): loss=0.5869442560074474\n",
      "Log Regression(203/4999): loss=0.5866585993394411\n",
      "Log Regression(204/4999): loss=0.5856419192803315\n",
      "Log Regression(205/4999): loss=0.5883967225671608\n",
      "Log Regression(206/4999): loss=0.5891504366689347\n",
      "Log Regression(207/4999): loss=0.5858423829016648\n",
      "Log Regression(208/4999): loss=0.5861036915061942\n",
      "Log Regression(209/4999): loss=0.586691472392425\n",
      "Log Regression(210/4999): loss=0.5855866677366601\n",
      "Log Regression(211/4999): loss=0.5894565758828911\n",
      "Log Regression(212/4999): loss=0.5874875615631877\n",
      "Log Regression(213/4999): loss=0.5853137666806101\n",
      "Log Regression(214/4999): loss=0.5847498526167342\n",
      "Log Regression(215/4999): loss=0.5847122236562631\n",
      "Log Regression(216/4999): loss=0.5853365558201005\n",
      "Log Regression(217/4999): loss=0.5848688424903872\n",
      "Log Regression(218/4999): loss=0.5856682671454563\n",
      "Log Regression(219/4999): loss=0.584752399820629\n",
      "Log Regression(220/4999): loss=0.5835596952691816\n",
      "Log Regression(221/4999): loss=0.5902055799195375\n",
      "Log Regression(222/4999): loss=0.5829997361725174\n",
      "Log Regression(223/4999): loss=0.5834278847126233\n",
      "Log Regression(224/4999): loss=0.5834276714469502\n",
      "Log Regression(225/4999): loss=0.5924899772443987\n",
      "Log Regression(226/4999): loss=0.5933339771094238\n",
      "Log Regression(227/4999): loss=0.5858857482438953\n",
      "Log Regression(228/4999): loss=0.5837080625650741\n",
      "Log Regression(229/4999): loss=0.5826060143996643\n",
      "Log Regression(230/4999): loss=0.5827641961729421\n",
      "Log Regression(231/4999): loss=0.5819627371151787\n",
      "Log Regression(232/4999): loss=0.5817113534454964\n",
      "Log Regression(233/4999): loss=0.5840665078001246\n",
      "Log Regression(234/4999): loss=0.5857323597101229\n",
      "Log Regression(235/4999): loss=0.5823378196430987\n",
      "Log Regression(236/4999): loss=0.5825883502382176\n",
      "Log Regression(237/4999): loss=0.5809038105016764\n",
      "Log Regression(238/4999): loss=0.5810682623179481\n",
      "Log Regression(239/4999): loss=0.5806851136278204\n",
      "Log Regression(240/4999): loss=0.5836834408277461\n",
      "Log Regression(241/4999): loss=0.580745485790204\n",
      "Log Regression(242/4999): loss=0.5817573783419621\n",
      "Log Regression(243/4999): loss=0.5802704533322844\n",
      "Log Regression(244/4999): loss=0.580044033054634\n",
      "Log Regression(245/4999): loss=0.5827295145630691\n",
      "Log Regression(246/4999): loss=0.5807934783120307\n",
      "Log Regression(247/4999): loss=0.5795230524976198\n",
      "Log Regression(248/4999): loss=0.5855348599902448\n",
      "Log Regression(249/4999): loss=0.5802024885468049\n",
      "Log Regression(250/4999): loss=0.5803895056237491\n",
      "Log Regression(251/4999): loss=0.5795892953744379\n",
      "Log Regression(252/4999): loss=0.5805910537446283\n",
      "Log Regression(253/4999): loss=0.5797205072209126\n",
      "Log Regression(254/4999): loss=0.5817448202895124\n",
      "Log Regression(255/4999): loss=0.5818108678220798\n",
      "Log Regression(256/4999): loss=0.5822489748720057\n",
      "Log Regression(257/4999): loss=0.5797079512827307\n",
      "Log Regression(258/4999): loss=0.5796576175634521\n",
      "Log Regression(259/4999): loss=0.5788261560672242\n",
      "Log Regression(260/4999): loss=0.5794793050189251\n",
      "Log Regression(261/4999): loss=0.5792173432444935\n",
      "Log Regression(262/4999): loss=0.5792236362798104\n",
      "Log Regression(263/4999): loss=0.5785181817833424\n",
      "Log Regression(264/4999): loss=0.5797973032893966\n",
      "Log Regression(265/4999): loss=0.5788876096546903\n",
      "Log Regression(266/4999): loss=0.5824789618926823\n",
      "Log Regression(267/4999): loss=0.579227081747716\n",
      "Log Regression(268/4999): loss=0.5846907203468278\n",
      "Log Regression(269/4999): loss=0.5778340342506609\n",
      "Log Regression(270/4999): loss=0.5776746202950808\n",
      "Log Regression(271/4999): loss=0.5806779440687072\n",
      "Log Regression(272/4999): loss=0.5777703929370481\n",
      "Log Regression(273/4999): loss=0.5812172349182715\n",
      "Log Regression(274/4999): loss=0.577159962715852\n",
      "Log Regression(275/4999): loss=0.5774140690767605\n",
      "Log Regression(276/4999): loss=0.5783653442241409\n",
      "Log Regression(277/4999): loss=0.5779650313709577\n",
      "Log Regression(278/4999): loss=0.5799347478811853\n",
      "Log Regression(279/4999): loss=0.5866270579215068\n",
      "Log Regression(280/4999): loss=0.5782343433491637\n",
      "Log Regression(281/4999): loss=0.5764760022868595\n",
      "Log Regression(282/4999): loss=0.5763664995487718\n",
      "Log Regression(283/4999): loss=0.5780708856338118\n",
      "Log Regression(284/4999): loss=0.5772998239557937\n",
      "Log Regression(285/4999): loss=0.5774384472452274\n",
      "Log Regression(286/4999): loss=0.5774486319881806\n",
      "Log Regression(287/4999): loss=0.5767113862405564\n",
      "Log Regression(288/4999): loss=0.5759249818188901\n",
      "Log Regression(289/4999): loss=0.5756056491681275\n",
      "Log Regression(290/4999): loss=0.5757418679014361\n",
      "Log Regression(291/4999): loss=0.5756583010602988\n",
      "Log Regression(292/4999): loss=0.5751775420562116\n",
      "Log Regression(293/4999): loss=0.5750523525255233\n",
      "Log Regression(294/4999): loss=0.5751374608916175\n",
      "Log Regression(295/4999): loss=0.5748698716095341\n",
      "Log Regression(296/4999): loss=0.5756084072739289\n",
      "Log Regression(297/4999): loss=0.5746625659756411\n",
      "Log Regression(298/4999): loss=0.5745663264282611\n",
      "Log Regression(299/4999): loss=0.5746654875212845\n",
      "Log Regression(300/4999): loss=0.5744846579763755\n",
      "Log Regression(301/4999): loss=0.5772973563385672\n",
      "Log Regression(302/4999): loss=0.5740883846819146\n",
      "Log Regression(303/4999): loss=0.574568760647332\n",
      "Log Regression(304/4999): loss=0.5775609721882036\n",
      "Log Regression(305/4999): loss=0.5752926323911114\n",
      "Log Regression(306/4999): loss=0.5738914943138759\n",
      "Log Regression(307/4999): loss=0.5735277388733415\n",
      "Log Regression(308/4999): loss=0.5738134408579912\n",
      "Log Regression(309/4999): loss=0.5734609361890806\n",
      "Log Regression(310/4999): loss=0.5733110261089026\n",
      "Log Regression(311/4999): loss=0.5741949983483987\n",
      "Log Regression(312/4999): loss=0.5754756055289895\n",
      "Log Regression(313/4999): loss=0.5761273415649348\n",
      "Log Regression(314/4999): loss=0.5802554789904026\n",
      "Log Regression(315/4999): loss=0.5817834281081067\n",
      "Log Regression(316/4999): loss=0.5735567304213975\n",
      "Log Regression(317/4999): loss=0.5731369392948834\n",
      "Log Regression(318/4999): loss=0.5789258978680799\n",
      "Log Regression(319/4999): loss=0.573694824169869\n",
      "Log Regression(320/4999): loss=0.5731799955850146\n",
      "Log Regression(321/4999): loss=0.5726918133369493\n",
      "Log Regression(322/4999): loss=0.5756681906799982\n",
      "Log Regression(323/4999): loss=0.5735085235702317\n",
      "Log Regression(324/4999): loss=0.5743540776310189\n",
      "Log Regression(325/4999): loss=0.58063522284714\n",
      "Log Regression(326/4999): loss=0.572532252058996\n",
      "Log Regression(327/4999): loss=0.5734782741980147\n",
      "Log Regression(328/4999): loss=0.5741798189882372\n",
      "Log Regression(329/4999): loss=0.5719470068393206\n",
      "Log Regression(330/4999): loss=0.5749284116908919\n",
      "Log Regression(331/4999): loss=0.5719801242266909\n",
      "Log Regression(332/4999): loss=0.5725578596305984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(333/4999): loss=0.571252259314384\n",
      "Log Regression(334/4999): loss=0.5720077486159174\n",
      "Log Regression(335/4999): loss=0.5723709556180596\n",
      "Log Regression(336/4999): loss=0.5757301222377004\n",
      "Log Regression(337/4999): loss=0.5711117668922824\n",
      "Log Regression(338/4999): loss=0.5728015430116175\n",
      "Log Regression(339/4999): loss=0.570675002841907\n",
      "Log Regression(340/4999): loss=0.5707911015097233\n",
      "Log Regression(341/4999): loss=0.5705621632101079\n",
      "Log Regression(342/4999): loss=0.570464487800399\n",
      "Log Regression(343/4999): loss=0.5709135343718872\n",
      "Log Regression(344/4999): loss=0.5702127804404351\n",
      "Log Regression(345/4999): loss=0.5706047499878729\n",
      "Log Regression(346/4999): loss=0.5700222104557217\n",
      "Log Regression(347/4999): loss=0.5710026654459901\n",
      "Log Regression(348/4999): loss=0.5700534882019314\n",
      "Log Regression(349/4999): loss=0.5699120661677908\n",
      "Log Regression(350/4999): loss=0.571142427381001\n",
      "Log Regression(351/4999): loss=0.5705791227849579\n",
      "Log Regression(352/4999): loss=0.5707069530198827\n",
      "Log Regression(353/4999): loss=0.569822883225359\n",
      "Log Regression(354/4999): loss=0.569496066787324\n",
      "Log Regression(355/4999): loss=0.5765084923961423\n",
      "Log Regression(356/4999): loss=0.5765715493848989\n",
      "Log Regression(357/4999): loss=0.5690207754228324\n",
      "Log Regression(358/4999): loss=0.5692711315686017\n",
      "Log Regression(359/4999): loss=0.5688348232611413\n",
      "Log Regression(360/4999): loss=0.5691907487350204\n",
      "Log Regression(361/4999): loss=0.5736996669600447\n",
      "Log Regression(362/4999): loss=0.5707248638950106\n",
      "Log Regression(363/4999): loss=0.5686526920782597\n",
      "Log Regression(364/4999): loss=0.570292497658523\n",
      "Log Regression(365/4999): loss=0.5697682965393596\n",
      "Log Regression(366/4999): loss=0.5684153385737553\n",
      "Log Regression(367/4999): loss=0.5688469993320429\n",
      "Log Regression(368/4999): loss=0.5728311289812905\n",
      "Log Regression(369/4999): loss=0.5688479650913728\n",
      "Log Regression(370/4999): loss=0.5682064628539087\n",
      "Log Regression(371/4999): loss=0.5694971060520821\n",
      "Log Regression(372/4999): loss=0.569791093530355\n",
      "Log Regression(373/4999): loss=0.5704281381562906\n",
      "Log Regression(374/4999): loss=0.5679089271211739\n",
      "Log Regression(375/4999): loss=0.5711584346504008\n",
      "Log Regression(376/4999): loss=0.570114573160629\n",
      "Log Regression(377/4999): loss=0.5694378082206238\n",
      "Log Regression(378/4999): loss=0.5691217202071818\n",
      "Log Regression(379/4999): loss=0.5730471213935735\n",
      "Log Regression(380/4999): loss=0.5673543403887058\n",
      "Log Regression(381/4999): loss=0.5676182219032726\n",
      "Log Regression(382/4999): loss=0.5672448803365219\n",
      "Log Regression(383/4999): loss=0.5693933677940954\n",
      "Log Regression(384/4999): loss=0.5674118702897567\n",
      "Log Regression(385/4999): loss=0.5670633347578827\n",
      "Log Regression(386/4999): loss=0.5691861221589025\n",
      "Log Regression(387/4999): loss=0.5688666765128754\n",
      "Log Regression(388/4999): loss=0.5675967273840968\n",
      "Log Regression(389/4999): loss=0.5677376978221298\n",
      "Log Regression(390/4999): loss=0.5665133066510606\n",
      "Log Regression(391/4999): loss=0.5664244203252574\n",
      "Log Regression(392/4999): loss=0.5663495291939131\n",
      "Log Regression(393/4999): loss=0.5664074353647651\n",
      "Log Regression(394/4999): loss=0.5660451899629166\n",
      "Log Regression(395/4999): loss=0.5661052811756919\n",
      "Log Regression(396/4999): loss=0.5733928751484292\n",
      "Log Regression(397/4999): loss=0.5680837181514309\n",
      "Log Regression(398/4999): loss=0.5684099045258676\n",
      "Log Regression(399/4999): loss=0.569073203028512\n",
      "Log Regression(400/4999): loss=0.5713751565748856\n",
      "Log Regression(401/4999): loss=0.5662142944715319\n",
      "Log Regression(402/4999): loss=0.5663547525457153\n",
      "Log Regression(403/4999): loss=0.5687352088678269\n",
      "Log Regression(404/4999): loss=0.5678904957356355\n",
      "Log Regression(405/4999): loss=0.5657386423368289\n",
      "Log Regression(406/4999): loss=0.5654429277768234\n",
      "Log Regression(407/4999): loss=0.5686046822474857\n",
      "Log Regression(408/4999): loss=0.5659160783952703\n",
      "Log Regression(409/4999): loss=0.5653634062604013\n",
      "Log Regression(410/4999): loss=0.5650711022192361\n",
      "Log Regression(411/4999): loss=0.5659399007036287\n",
      "Log Regression(412/4999): loss=0.5652945043093138\n",
      "Log Regression(413/4999): loss=0.5650639352821304\n",
      "Log Regression(414/4999): loss=0.5647145343556974\n",
      "Log Regression(415/4999): loss=0.5652926978437938\n",
      "Log Regression(416/4999): loss=0.5661880954701171\n",
      "Log Regression(417/4999): loss=0.564994930567119\n",
      "Log Regression(418/4999): loss=0.5705688344277526\n",
      "Log Regression(419/4999): loss=0.5643820004217436\n",
      "Log Regression(420/4999): loss=0.5642619816337139\n",
      "Log Regression(421/4999): loss=0.5649692192778659\n",
      "Log Regression(422/4999): loss=0.5663188049864561\n",
      "Log Regression(423/4999): loss=0.5642235194678052\n",
      "Log Regression(424/4999): loss=0.564255407317255\n",
      "Log Regression(425/4999): loss=0.5645925810182849\n",
      "Log Regression(426/4999): loss=0.5644380969685976\n",
      "Log Regression(427/4999): loss=0.565917158306182\n",
      "Log Regression(428/4999): loss=0.5646496493494016\n",
      "Log Regression(429/4999): loss=0.5640308296459863\n",
      "Log Regression(430/4999): loss=0.5640625528403468\n",
      "Log Regression(431/4999): loss=0.5637881901809703\n",
      "Log Regression(432/4999): loss=0.5640685704657844\n",
      "Log Regression(433/4999): loss=0.5721135546755386\n",
      "Log Regression(434/4999): loss=0.5635711145957232\n",
      "Log Regression(435/4999): loss=0.5634460811284926\n",
      "Log Regression(436/4999): loss=0.5643003040768405\n",
      "Log Regression(437/4999): loss=0.5653596746423579\n",
      "Log Regression(438/4999): loss=0.5673323263334294\n",
      "Log Regression(439/4999): loss=0.5636589108856476\n",
      "Log Regression(440/4999): loss=0.5636430282750238\n",
      "Log Regression(441/4999): loss=0.564200297873191\n",
      "Log Regression(442/4999): loss=0.5631661389348958\n",
      "Log Regression(443/4999): loss=0.5631998332685937\n",
      "Log Regression(444/4999): loss=0.5733210939570685\n",
      "Log Regression(445/4999): loss=0.575949360717833\n",
      "Log Regression(446/4999): loss=0.5650595736226699\n",
      "Log Regression(447/4999): loss=0.5636742015925102\n",
      "Log Regression(448/4999): loss=0.5653997068331283\n",
      "Log Regression(449/4999): loss=0.5629619929448639\n",
      "Log Regression(450/4999): loss=0.5628660181947859\n",
      "Log Regression(451/4999): loss=0.5626467471163438\n",
      "Log Regression(452/4999): loss=0.5682035401748264\n",
      "Log Regression(453/4999): loss=0.5624775171612315\n",
      "Log Regression(454/4999): loss=0.565558986504745\n",
      "Log Regression(455/4999): loss=0.5628441624026669\n",
      "Log Regression(456/4999): loss=0.5624979271587894\n",
      "Log Regression(457/4999): loss=0.5637202339702516\n",
      "Log Regression(458/4999): loss=0.56302446769161\n",
      "Log Regression(459/4999): loss=0.56346125090904\n",
      "Log Regression(460/4999): loss=0.5630383331609369\n",
      "Log Regression(461/4999): loss=0.5639248548735919\n",
      "Log Regression(462/4999): loss=0.5624675810155316\n",
      "Log Regression(463/4999): loss=0.5622148607398813\n",
      "Log Regression(464/4999): loss=0.5622926729913919\n",
      "Log Regression(465/4999): loss=0.5637014397067466\n",
      "Log Regression(466/4999): loss=0.561965046157574\n",
      "Log Regression(467/4999): loss=0.5637505884537766\n",
      "Log Regression(468/4999): loss=0.5619500060734225\n",
      "Log Regression(469/4999): loss=0.561806688268154\n",
      "Log Regression(470/4999): loss=0.5617838002334903\n",
      "Log Regression(471/4999): loss=0.5617778094710368\n",
      "Log Regression(472/4999): loss=0.5633794282233986\n",
      "Log Regression(473/4999): loss=0.5618491689180534\n",
      "Log Regression(474/4999): loss=0.5666181082375746\n",
      "Log Regression(475/4999): loss=0.5638963649162881\n",
      "Log Regression(476/4999): loss=0.5636929763543777\n",
      "Log Regression(477/4999): loss=0.562610158953844\n",
      "Log Regression(478/4999): loss=0.5671006679977456\n",
      "Log Regression(479/4999): loss=0.5671869085483001\n",
      "Log Regression(480/4999): loss=0.5640869766470199\n",
      "Log Regression(481/4999): loss=0.5621588040180219\n",
      "Log Regression(482/4999): loss=0.5615555718413489\n",
      "Log Regression(483/4999): loss=0.5623213201319822\n",
      "Log Regression(484/4999): loss=0.5628372085489268\n",
      "Log Regression(485/4999): loss=0.5639148277763831\n",
      "Log Regression(486/4999): loss=0.5632312106695393\n",
      "Log Regression(487/4999): loss=0.5609298854180129\n",
      "Log Regression(488/4999): loss=0.5609166201592535\n",
      "Log Regression(489/4999): loss=0.5610794034104446\n",
      "Log Regression(490/4999): loss=0.562048680782427\n",
      "Log Regression(491/4999): loss=0.5612695088596288\n",
      "Log Regression(492/4999): loss=0.5612439615588435\n",
      "Log Regression(493/4999): loss=0.5610602403810313\n",
      "Log Regression(494/4999): loss=0.5633869094437878\n",
      "Log Regression(495/4999): loss=0.5619940063894566\n",
      "Log Regression(496/4999): loss=0.5612515337796523\n",
      "Log Regression(497/4999): loss=0.5631005494997412\n",
      "Log Regression(498/4999): loss=0.5630407834129958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(499/4999): loss=0.5637242267113239\n",
      "Log Regression(500/4999): loss=0.560898344407372\n",
      "Log Regression(501/4999): loss=0.5601668831195317\n",
      "Log Regression(502/4999): loss=0.5605058887371627\n",
      "Log Regression(503/4999): loss=0.5605171938980315\n",
      "Log Regression(504/4999): loss=0.5600223472760419\n",
      "Log Regression(505/4999): loss=0.5612766512714116\n",
      "Log Regression(506/4999): loss=0.5621813029103014\n",
      "Log Regression(507/4999): loss=0.5625395578953822\n",
      "Log Regression(508/4999): loss=0.5606370686192802\n",
      "Log Regression(509/4999): loss=0.5598064069667685\n",
      "Log Regression(510/4999): loss=0.5599460660278602\n",
      "Log Regression(511/4999): loss=0.5600354911005548\n",
      "Log Regression(512/4999): loss=0.5660438805362955\n",
      "Log Regression(513/4999): loss=0.5604976708842115\n",
      "Log Regression(514/4999): loss=0.5603675711175682\n",
      "Log Regression(515/4999): loss=0.5699975575439977\n",
      "Log Regression(516/4999): loss=0.5778670361674876\n",
      "Log Regression(517/4999): loss=0.5676115700491734\n",
      "Log Regression(518/4999): loss=0.5610090342173989\n",
      "Log Regression(519/4999): loss=0.5592750414872272\n",
      "Log Regression(520/4999): loss=0.5634736622610353\n",
      "Log Regression(521/4999): loss=0.5603870474346302\n",
      "Log Regression(522/4999): loss=0.5612943602807378\n",
      "Log Regression(523/4999): loss=0.559392428574064\n",
      "Log Regression(524/4999): loss=0.5592032211605718\n",
      "Log Regression(525/4999): loss=0.5616775965370752\n",
      "Log Regression(526/4999): loss=0.5651588041199358\n",
      "Log Regression(527/4999): loss=0.5624500060309885\n",
      "Log Regression(528/4999): loss=0.5601719194650489\n",
      "Log Regression(529/4999): loss=0.5633114586671932\n",
      "Log Regression(530/4999): loss=0.5599332236634189\n",
      "Log Regression(531/4999): loss=0.5589533007186271\n",
      "Log Regression(532/4999): loss=0.5595157463733303\n",
      "Log Regression(533/4999): loss=0.5617813379109158\n",
      "Log Regression(534/4999): loss=0.5602332345987969\n",
      "Log Regression(535/4999): loss=0.5617429708058108\n",
      "Log Regression(536/4999): loss=0.5609068415660715\n",
      "Log Regression(537/4999): loss=0.5592976466365137\n",
      "Log Regression(538/4999): loss=0.5635901648255093\n",
      "Log Regression(539/4999): loss=0.5587471881222943\n",
      "Log Regression(540/4999): loss=0.5590545799775046\n",
      "Log Regression(541/4999): loss=0.5592080233290512\n",
      "Log Regression(542/4999): loss=0.5584976277338752\n",
      "Log Regression(543/4999): loss=0.5600847745116205\n",
      "Log Regression(544/4999): loss=0.5590720406349514\n",
      "Log Regression(545/4999): loss=0.5584164564089616\n",
      "Log Regression(546/4999): loss=0.55885585411515\n",
      "Log Regression(547/4999): loss=0.5650624918229933\n",
      "Log Regression(548/4999): loss=0.5597887603636752\n",
      "Log Regression(549/4999): loss=0.5583145050312719\n",
      "Log Regression(550/4999): loss=0.5594180083927693\n",
      "Log Regression(551/4999): loss=0.5589627069612967\n",
      "Log Regression(552/4999): loss=0.5608774450404126\n",
      "Log Regression(553/4999): loss=0.5582108863811218\n",
      "Log Regression(554/4999): loss=0.5621562645896389\n",
      "Log Regression(555/4999): loss=0.5587848945755248\n",
      "Log Regression(556/4999): loss=0.5597785883516778\n",
      "Log Regression(557/4999): loss=0.5621362239230295\n",
      "Log Regression(558/4999): loss=0.5583608393735666\n",
      "Log Regression(559/4999): loss=0.5582605895370667\n",
      "Log Regression(560/4999): loss=0.5583306005214042\n",
      "Log Regression(561/4999): loss=0.5584330967053722\n",
      "Log Regression(562/4999): loss=0.5603693075795596\n",
      "Log Regression(563/4999): loss=0.5579351844753965\n",
      "Log Regression(564/4999): loss=0.557995461366013\n",
      "Log Regression(565/4999): loss=0.5578862412250345\n",
      "Log Regression(566/4999): loss=0.5583709471232868\n",
      "Log Regression(567/4999): loss=0.5576587915839856\n",
      "Log Regression(568/4999): loss=0.5579725325719114\n",
      "Log Regression(569/4999): loss=0.5635821672285053\n",
      "Log Regression(570/4999): loss=0.5582999811613658\n",
      "Log Regression(571/4999): loss=0.557578283380217\n",
      "Log Regression(572/4999): loss=0.5577651376127413\n",
      "Log Regression(573/4999): loss=0.5579482459691351\n",
      "Log Regression(574/4999): loss=0.5586254006099614\n",
      "Log Regression(575/4999): loss=0.5594308794792621\n",
      "Log Regression(576/4999): loss=0.5587471928746335\n",
      "Log Regression(577/4999): loss=0.5586393859830968\n",
      "Log Regression(578/4999): loss=0.5584322553817271\n",
      "Log Regression(579/4999): loss=0.5573106747741808\n",
      "Log Regression(580/4999): loss=0.5590768095812522\n",
      "Log Regression(581/4999): loss=0.5569590836629128\n",
      "Log Regression(582/4999): loss=0.5570456000820247\n",
      "Log Regression(583/4999): loss=0.5568654877259444\n",
      "Log Regression(584/4999): loss=0.5567976903348163\n",
      "Log Regression(585/4999): loss=0.5598548330831923\n",
      "Log Regression(586/4999): loss=0.5567743554314181\n",
      "Log Regression(587/4999): loss=0.5566639247404396\n",
      "Log Regression(588/4999): loss=0.5584894700864809\n",
      "Log Regression(589/4999): loss=0.5588432190755134\n",
      "Log Regression(590/4999): loss=0.5587606541321982\n",
      "Log Regression(591/4999): loss=0.556442522994356\n",
      "Log Regression(592/4999): loss=0.5567608540241769\n",
      "Log Regression(593/4999): loss=0.5564733266251132\n",
      "Log Regression(594/4999): loss=0.5590180281382945\n",
      "Log Regression(595/4999): loss=0.5569130978860777\n",
      "Log Regression(596/4999): loss=0.5577372299516793\n",
      "Log Regression(597/4999): loss=0.5565932775503132\n",
      "Log Regression(598/4999): loss=0.556745154803992\n",
      "Log Regression(599/4999): loss=0.5576601682672221\n",
      "Log Regression(600/4999): loss=0.5561175698389008\n",
      "Log Regression(601/4999): loss=0.5561375768897175\n",
      "Log Regression(602/4999): loss=0.5567912221189156\n",
      "Log Regression(603/4999): loss=0.5562251111813079\n",
      "Log Regression(604/4999): loss=0.5571975568233379\n",
      "Log Regression(605/4999): loss=0.5566777758437108\n",
      "Log Regression(606/4999): loss=0.5598592392698859\n",
      "Log Regression(607/4999): loss=0.5569751154179358\n",
      "Log Regression(608/4999): loss=0.5561318225114676\n",
      "Log Regression(609/4999): loss=0.5559788126448646\n",
      "Log Regression(610/4999): loss=0.555955928627096\n",
      "Log Regression(611/4999): loss=0.5563086766888918\n",
      "Log Regression(612/4999): loss=0.5553955032870774\n",
      "Log Regression(613/4999): loss=0.5554729053627286\n",
      "Log Regression(614/4999): loss=0.5567646394627992\n",
      "Log Regression(615/4999): loss=0.5586585833674561\n",
      "Log Regression(616/4999): loss=0.5596117780802362\n",
      "Log Regression(617/4999): loss=0.5556425075928326\n",
      "Log Regression(618/4999): loss=0.5606446571520793\n",
      "Log Regression(619/4999): loss=0.5586628857328967\n",
      "Log Regression(620/4999): loss=0.555547431837055\n",
      "Log Regression(621/4999): loss=0.5622133835991364\n",
      "Log Regression(622/4999): loss=0.5555360963059868\n",
      "Log Regression(623/4999): loss=0.5553178445277287\n",
      "Log Regression(624/4999): loss=0.5555494822594116\n",
      "Log Regression(625/4999): loss=0.5559752038611087\n",
      "Log Regression(626/4999): loss=0.5557263441252815\n",
      "Log Regression(627/4999): loss=0.5552540612901916\n",
      "Log Regression(628/4999): loss=0.555226360160359\n",
      "Log Regression(629/4999): loss=0.5551031584129857\n",
      "Log Regression(630/4999): loss=0.5575770130756167\n",
      "Log Regression(631/4999): loss=0.5557832407207431\n",
      "Log Regression(632/4999): loss=0.5554807331161078\n",
      "Log Regression(633/4999): loss=0.5656163273517001\n",
      "Log Regression(634/4999): loss=0.5586787330746154\n",
      "Log Regression(635/4999): loss=0.5579481182668231\n",
      "Log Regression(636/4999): loss=0.5561007842437408\n",
      "Log Regression(637/4999): loss=0.557659414984435\n",
      "Log Regression(638/4999): loss=0.554738155709128\n",
      "Log Regression(639/4999): loss=0.5547085379622222\n",
      "Log Regression(640/4999): loss=0.5550651998707963\n",
      "Log Regression(641/4999): loss=0.5558990145869405\n",
      "Log Regression(642/4999): loss=0.554591386405591\n",
      "Log Regression(643/4999): loss=0.5555940612541491\n",
      "Log Regression(644/4999): loss=0.5629395344941177\n",
      "Log Regression(645/4999): loss=0.555992957086692\n",
      "Log Regression(646/4999): loss=0.5554850693095623\n",
      "Log Regression(647/4999): loss=0.5548683539133665\n",
      "Log Regression(648/4999): loss=0.5548921012825898\n",
      "Log Regression(649/4999): loss=0.5554673924531971\n",
      "Log Regression(650/4999): loss=0.5543017158058519\n",
      "Log Regression(651/4999): loss=0.5546015144246409\n",
      "Log Regression(652/4999): loss=0.5557867143721973\n",
      "Log Regression(653/4999): loss=0.5543542498235454\n",
      "Log Regression(654/4999): loss=0.5545860118545735\n",
      "Log Regression(655/4999): loss=0.5541289830208133\n",
      "Log Regression(656/4999): loss=0.5540557078694612\n",
      "Log Regression(657/4999): loss=0.5541105537025486\n",
      "Log Regression(658/4999): loss=0.5542356883675417\n",
      "Log Regression(659/4999): loss=0.5566175363790541\n",
      "Log Regression(660/4999): loss=0.5562759704507033\n",
      "Log Regression(661/4999): loss=0.562010322103955\n",
      "Log Regression(662/4999): loss=0.5566035931199896\n",
      "Log Regression(663/4999): loss=0.5571359997118766\n",
      "Log Regression(664/4999): loss=0.5552907047511166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(665/4999): loss=0.5602341627163508\n",
      "Log Regression(666/4999): loss=0.5553961935063999\n",
      "Log Regression(667/4999): loss=0.5539017214747416\n",
      "Log Regression(668/4999): loss=0.5539187887560787\n",
      "Log Regression(669/4999): loss=0.5545400229843725\n",
      "Log Regression(670/4999): loss=0.5552219488423459\n",
      "Log Regression(671/4999): loss=0.5579205443548472\n",
      "Log Regression(672/4999): loss=0.5536761401415047\n",
      "Log Regression(673/4999): loss=0.5536931758008755\n",
      "Log Regression(674/4999): loss=0.553513779481798\n",
      "Log Regression(675/4999): loss=0.5538362921290907\n",
      "Log Regression(676/4999): loss=0.5538429252799129\n",
      "Log Regression(677/4999): loss=0.5537105280928998\n",
      "Log Regression(678/4999): loss=0.5552863597272875\n",
      "Log Regression(679/4999): loss=0.5533513090002976\n",
      "Log Regression(680/4999): loss=0.5557320840939914\n",
      "Log Regression(681/4999): loss=0.5543382306943101\n",
      "Log Regression(682/4999): loss=0.5537848103909838\n",
      "Log Regression(683/4999): loss=0.5538487412461147\n",
      "Log Regression(684/4999): loss=0.5601593131265589\n",
      "Log Regression(685/4999): loss=0.5660662319035992\n",
      "Log Regression(686/4999): loss=0.5541306860006825\n",
      "Log Regression(687/4999): loss=0.5546004213223995\n",
      "Log Regression(688/4999): loss=0.5541764270614503\n",
      "Log Regression(689/4999): loss=0.5559398043665726\n",
      "Log Regression(690/4999): loss=0.5566089859956064\n",
      "Log Regression(691/4999): loss=0.5555976914588874\n",
      "Log Regression(692/4999): loss=0.5529854410264583\n",
      "Log Regression(693/4999): loss=0.5549411105307925\n",
      "Log Regression(694/4999): loss=0.5604467826029726\n",
      "Log Regression(695/4999): loss=0.561164809238738\n",
      "Log Regression(696/4999): loss=0.5563118186741881\n",
      "Log Regression(697/4999): loss=0.5568508445505\n",
      "Log Regression(698/4999): loss=0.557355040864287\n",
      "Log Regression(699/4999): loss=0.5530938675367446\n",
      "Log Regression(700/4999): loss=0.5529558928348254\n",
      "Log Regression(701/4999): loss=0.553425219922737\n",
      "Log Regression(702/4999): loss=0.552908435330786\n",
      "Log Regression(703/4999): loss=0.5529240654609844\n",
      "Log Regression(704/4999): loss=0.55288717317107\n",
      "Log Regression(705/4999): loss=0.5548650475136672\n",
      "Log Regression(706/4999): loss=0.5537226525668546\n",
      "Log Regression(707/4999): loss=0.5528930830935048\n",
      "Log Regression(708/4999): loss=0.5615984148905272\n",
      "Log Regression(709/4999): loss=0.5543310011901186\n",
      "Log Regression(710/4999): loss=0.5576779699903631\n",
      "Log Regression(711/4999): loss=0.5535727821930262\n",
      "Log Regression(712/4999): loss=0.5569633560260474\n",
      "Log Regression(713/4999): loss=0.5542754977427893\n",
      "Log Regression(714/4999): loss=0.5536928801470576\n",
      "Log Regression(715/4999): loss=0.5531103627165085\n",
      "Log Regression(716/4999): loss=0.5553350355326221\n",
      "Log Regression(717/4999): loss=0.5634965592811\n",
      "Log Regression(718/4999): loss=0.5548260681909415\n",
      "Log Regression(719/4999): loss=0.5537986076647218\n",
      "Log Regression(720/4999): loss=0.5548512045443655\n",
      "Log Regression(721/4999): loss=0.5634008298384424\n",
      "Log Regression(722/4999): loss=0.5598576681437907\n",
      "Log Regression(723/4999): loss=0.556624970070365\n",
      "Log Regression(724/4999): loss=0.5523777480663358\n",
      "Log Regression(725/4999): loss=0.55236598111512\n",
      "Log Regression(726/4999): loss=0.5537108159596128\n",
      "Log Regression(727/4999): loss=0.5547225449257278\n",
      "Log Regression(728/4999): loss=0.5555507595784427\n",
      "Log Regression(729/4999): loss=0.5554458438400962\n",
      "Log Regression(730/4999): loss=0.5524445173811942\n",
      "Log Regression(731/4999): loss=0.5531286225053159\n",
      "Log Regression(732/4999): loss=0.5553132452824251\n",
      "Log Regression(733/4999): loss=0.5525258200795654\n",
      "Log Regression(734/4999): loss=0.5535993814815818\n",
      "Log Regression(735/4999): loss=0.5534721958609473\n",
      "Log Regression(736/4999): loss=0.5578584668691516\n",
      "Log Regression(737/4999): loss=0.5524662280778357\n",
      "Log Regression(738/4999): loss=0.5538750644993204\n",
      "Log Regression(739/4999): loss=0.5519237300709955\n",
      "Log Regression(740/4999): loss=0.5530877327937938\n",
      "Log Regression(741/4999): loss=0.552059944512131\n",
      "Log Regression(742/4999): loss=0.5525044932531905\n",
      "Log Regression(743/4999): loss=0.5526144341110708\n",
      "Log Regression(744/4999): loss=0.5519419597506972\n",
      "Log Regression(745/4999): loss=0.5523123318688642\n",
      "Log Regression(746/4999): loss=0.5523539747507736\n",
      "Log Regression(747/4999): loss=0.5566955793630837\n",
      "Log Regression(748/4999): loss=0.5562231113382028\n",
      "Log Regression(749/4999): loss=0.5546549369762662\n",
      "Log Regression(750/4999): loss=0.5535308656352285\n",
      "Log Regression(751/4999): loss=0.5536656943740297\n",
      "Log Regression(752/4999): loss=0.5516998840360473\n",
      "Log Regression(753/4999): loss=0.5516167235862559\n",
      "Log Regression(754/4999): loss=0.5536376954042996\n",
      "Log Regression(755/4999): loss=0.5530370897125679\n",
      "Log Regression(756/4999): loss=0.5517264816323357\n",
      "Log Regression(757/4999): loss=0.5515055992409005\n",
      "Log Regression(758/4999): loss=0.5525130196386754\n",
      "Log Regression(759/4999): loss=0.5530989176320581\n",
      "Log Regression(760/4999): loss=0.5513421465464169\n",
      "Log Regression(761/4999): loss=0.5514382521775435\n",
      "Log Regression(762/4999): loss=0.5517067017281138\n",
      "Log Regression(763/4999): loss=0.5516202314237442\n",
      "Log Regression(764/4999): loss=0.551667792545908\n",
      "Log Regression(765/4999): loss=0.5513500939535192\n",
      "Log Regression(766/4999): loss=0.5519340545730109\n",
      "Log Regression(767/4999): loss=0.5515390356288644\n",
      "Log Regression(768/4999): loss=0.5519254558321207\n",
      "Log Regression(769/4999): loss=0.5566819433492679\n",
      "Log Regression(770/4999): loss=0.5515551402694057\n",
      "Log Regression(771/4999): loss=0.5571325893350503\n",
      "Log Regression(772/4999): loss=0.5518407521380602\n",
      "Log Regression(773/4999): loss=0.552420190555589\n",
      "Log Regression(774/4999): loss=0.5514367946111636\n",
      "Log Regression(775/4999): loss=0.5511081305499839\n",
      "Log Regression(776/4999): loss=0.5524080726725268\n",
      "Log Regression(777/4999): loss=0.5513159910705155\n",
      "Log Regression(778/4999): loss=0.5538551208340368\n",
      "Log Regression(779/4999): loss=0.5525711654271677\n",
      "Log Regression(780/4999): loss=0.5508745810781859\n",
      "Log Regression(781/4999): loss=0.5515552254486145\n",
      "Log Regression(782/4999): loss=0.5536248800021698\n",
      "Log Regression(783/4999): loss=0.5605011076060232\n",
      "Log Regression(784/4999): loss=0.5555718554171953\n",
      "Log Regression(785/4999): loss=0.5510856598600106\n",
      "Log Regression(786/4999): loss=0.5506775481974792\n",
      "Log Regression(787/4999): loss=0.5546312512214241\n",
      "Log Regression(788/4999): loss=0.55144476081513\n",
      "Log Regression(789/4999): loss=0.5508608488166803\n",
      "Log Regression(790/4999): loss=0.550769476134495\n",
      "Log Regression(791/4999): loss=0.5505877868657099\n",
      "Log Regression(792/4999): loss=0.5515539092167123\n",
      "Log Regression(793/4999): loss=0.5519050615099808\n",
      "Log Regression(794/4999): loss=0.5506971868655799\n",
      "Log Regression(795/4999): loss=0.5505462398162488\n",
      "Log Regression(796/4999): loss=0.5506779640528792\n",
      "Log Regression(797/4999): loss=0.5513498768483256\n",
      "Log Regression(798/4999): loss=0.5504513091824339\n",
      "Log Regression(799/4999): loss=0.5530397477428993\n",
      "Log Regression(800/4999): loss=0.5535091281377558\n",
      "Log Regression(801/4999): loss=0.5506140811114238\n",
      "Log Regression(802/4999): loss=0.5505664651114461\n",
      "Log Regression(803/4999): loss=0.5506384927138698\n",
      "Log Regression(804/4999): loss=0.5505320526872015\n",
      "Log Regression(805/4999): loss=0.5508861335878088\n",
      "Log Regression(806/4999): loss=0.5521406703767954\n",
      "Log Regression(807/4999): loss=0.5509755443323302\n",
      "Log Regression(808/4999): loss=0.5587786560723046\n",
      "Log Regression(809/4999): loss=0.5565951940884989\n",
      "Log Regression(810/4999): loss=0.553491518684629\n",
      "Log Regression(811/4999): loss=0.5554824103020284\n",
      "Log Regression(812/4999): loss=0.5529818873812054\n",
      "Log Regression(813/4999): loss=0.5549156007023217\n",
      "Log Regression(814/4999): loss=0.5518678502677324\n",
      "Log Regression(815/4999): loss=0.5517983375312642\n",
      "Log Regression(816/4999): loss=0.5501587547573762\n",
      "Log Regression(817/4999): loss=0.5656090793643042\n",
      "Log Regression(818/4999): loss=0.5705956438146973\n",
      "Log Regression(819/4999): loss=0.5603341873904851\n",
      "Log Regression(820/4999): loss=0.5533596331188909\n",
      "Log Regression(821/4999): loss=0.5605113932573579\n",
      "Log Regression(822/4999): loss=0.5566007161900088\n",
      "Log Regression(823/4999): loss=0.5538102183970894\n",
      "Log Regression(824/4999): loss=0.5519534116589784\n",
      "Log Regression(825/4999): loss=0.5514476934694552\n",
      "Log Regression(826/4999): loss=0.5497311353598058\n",
      "Log Regression(827/4999): loss=0.550095741157803\n",
      "Log Regression(828/4999): loss=0.5496928840613092\n",
      "Log Regression(829/4999): loss=0.5519578175716637\n",
      "Log Regression(830/4999): loss=0.5499235523630149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(831/4999): loss=0.5496711792064161\n",
      "Log Regression(832/4999): loss=0.5500987188978191\n",
      "Log Regression(833/4999): loss=0.5503905063085334\n",
      "Log Regression(834/4999): loss=0.5495951728890114\n",
      "Log Regression(835/4999): loss=0.5500044646923857\n",
      "Log Regression(836/4999): loss=0.5543901209839704\n",
      "Log Regression(837/4999): loss=0.5529786340179351\n",
      "Log Regression(838/4999): loss=0.5502764597274238\n",
      "Log Regression(839/4999): loss=0.5532393967631901\n",
      "Log Regression(840/4999): loss=0.5494233372099698\n",
      "Log Regression(841/4999): loss=0.5496546051585562\n",
      "Log Regression(842/4999): loss=0.5494073189114573\n",
      "Log Regression(843/4999): loss=0.554936968408945\n",
      "Log Regression(844/4999): loss=0.5551534660851126\n",
      "Log Regression(845/4999): loss=0.5551763955205227\n",
      "Log Regression(846/4999): loss=0.5537651052081507\n",
      "Log Regression(847/4999): loss=0.549716078256842\n",
      "Log Regression(848/4999): loss=0.5500850048822601\n",
      "Log Regression(849/4999): loss=0.5519893020028582\n",
      "Log Regression(850/4999): loss=0.549253897087549\n",
      "Log Regression(851/4999): loss=0.5497207690883995\n",
      "Log Regression(852/4999): loss=0.5497402609660736\n",
      "Log Regression(853/4999): loss=0.5496465252758784\n",
      "Log Regression(854/4999): loss=0.5490908893042146\n",
      "Log Regression(855/4999): loss=0.5491219705858064\n",
      "Log Regression(856/4999): loss=0.5495326021726563\n",
      "Log Regression(857/4999): loss=0.5494336198125246\n",
      "Log Regression(858/4999): loss=0.5496621957551932\n",
      "Log Regression(859/4999): loss=0.5527287978230502\n",
      "Log Regression(860/4999): loss=0.551066948605096\n",
      "Log Regression(861/4999): loss=0.5491860465418522\n",
      "Log Regression(862/4999): loss=0.5499368164767549\n",
      "Log Regression(863/4999): loss=0.5496195127566166\n",
      "Log Regression(864/4999): loss=0.5491533432129746\n",
      "Log Regression(865/4999): loss=0.5504731883113312\n",
      "Log Regression(866/4999): loss=0.5522530509484497\n",
      "Log Regression(867/4999): loss=0.5489184810802196\n",
      "Log Regression(868/4999): loss=0.5546162016635067\n",
      "Log Regression(869/4999): loss=0.5506226152523308\n",
      "Log Regression(870/4999): loss=0.5490974953819192\n",
      "Log Regression(871/4999): loss=0.5499382990805078\n",
      "Log Regression(872/4999): loss=0.5488624851269944\n",
      "Log Regression(873/4999): loss=0.5498215907349838\n",
      "Log Regression(874/4999): loss=0.5489377015523875\n",
      "Log Regression(875/4999): loss=0.5500267162794503\n",
      "Log Regression(876/4999): loss=0.5488745185585504\n",
      "Log Regression(877/4999): loss=0.5496998227856639\n",
      "Log Regression(878/4999): loss=0.5495004013847872\n",
      "Log Regression(879/4999): loss=0.5498967649140772\n",
      "Log Regression(880/4999): loss=0.5493615093824827\n",
      "Log Regression(881/4999): loss=0.5507577659871719\n",
      "Log Regression(882/4999): loss=0.5523885679777242\n",
      "Log Regression(883/4999): loss=0.551073259378432\n",
      "Log Regression(884/4999): loss=0.5487327693980396\n",
      "Log Regression(885/4999): loss=0.550153703357828\n",
      "Log Regression(886/4999): loss=0.5495295013607328\n",
      "Log Regression(887/4999): loss=0.5527121109782375\n",
      "Log Regression(888/4999): loss=0.5489191912423225\n",
      "Log Regression(889/4999): loss=0.5485867469826651\n",
      "Log Regression(890/4999): loss=0.549010159029162\n",
      "Log Regression(891/4999): loss=0.5487139323732578\n",
      "Log Regression(892/4999): loss=0.549246430606464\n",
      "Log Regression(893/4999): loss=0.5495861002775276\n",
      "Log Regression(894/4999): loss=0.550544590115632\n",
      "Log Regression(895/4999): loss=0.548965273421867\n",
      "Log Regression(896/4999): loss=0.550262949497283\n",
      "Log Regression(897/4999): loss=0.5499282198987682\n",
      "Log Regression(898/4999): loss=0.5553946429077299\n",
      "Log Regression(899/4999): loss=0.5495726346389351\n",
      "Log Regression(900/4999): loss=0.548309740191833\n",
      "Log Regression(901/4999): loss=0.5487104860615385\n",
      "Log Regression(902/4999): loss=0.5502236151867361\n",
      "Log Regression(903/4999): loss=0.5494763048138422\n",
      "Log Regression(904/4999): loss=0.5484969087258085\n",
      "Log Regression(905/4999): loss=0.5481673150826591\n",
      "Log Regression(906/4999): loss=0.5482132247453578\n",
      "Log Regression(907/4999): loss=0.548641609116701\n",
      "Log Regression(908/4999): loss=0.5483093407407196\n",
      "Log Regression(909/4999): loss=0.5481642664636177\n",
      "Log Regression(910/4999): loss=0.548508562249341\n",
      "Log Regression(911/4999): loss=0.5480764804853925\n",
      "Log Regression(912/4999): loss=0.5496096824183898\n",
      "Log Regression(913/4999): loss=0.5486295444433117\n",
      "Log Regression(914/4999): loss=0.5480931967899029\n",
      "Log Regression(915/4999): loss=0.5483459381732521\n",
      "Log Regression(916/4999): loss=0.5481642494656457\n",
      "Log Regression(917/4999): loss=0.5482955362590578\n",
      "Log Regression(918/4999): loss=0.5498617609491793\n",
      "Log Regression(919/4999): loss=0.5480635047974016\n",
      "Log Regression(920/4999): loss=0.5483180570029949\n",
      "Log Regression(921/4999): loss=0.5482353890826931\n",
      "Log Regression(922/4999): loss=0.5479373421268463\n",
      "Log Regression(923/4999): loss=0.5504060822201493\n",
      "Log Regression(924/4999): loss=0.548022501992404\n",
      "Log Regression(925/4999): loss=0.5478384440502131\n",
      "Log Regression(926/4999): loss=0.5478566516722648\n",
      "Log Regression(927/4999): loss=0.5499995013176668\n",
      "Log Regression(928/4999): loss=0.5479356714908723\n",
      "Log Regression(929/4999): loss=0.549385335258368\n",
      "Log Regression(930/4999): loss=0.5488566728742019\n",
      "Log Regression(931/4999): loss=0.549292093646222\n",
      "Log Regression(932/4999): loss=0.5477061398558122\n",
      "Log Regression(933/4999): loss=0.5481940223246015\n",
      "Log Regression(934/4999): loss=0.5476946102589245\n",
      "Log Regression(935/4999): loss=0.5476551352515723\n",
      "Log Regression(936/4999): loss=0.5487093085086582\n",
      "Log Regression(937/4999): loss=0.5479768309409107\n",
      "Log Regression(938/4999): loss=0.5478890900881714\n",
      "Log Regression(939/4999): loss=0.547894199257478\n",
      "Log Regression(940/4999): loss=0.5475900442288976\n",
      "Log Regression(941/4999): loss=0.5485983623523991\n",
      "Log Regression(942/4999): loss=0.548862130334417\n",
      "Log Regression(943/4999): loss=0.5476824514794806\n",
      "Log Regression(944/4999): loss=0.5483122927112816\n",
      "Log Regression(945/4999): loss=0.5527743308094683\n",
      "Log Regression(946/4999): loss=0.5508167445533955\n",
      "Log Regression(947/4999): loss=0.5524253207982192\n",
      "Log Regression(948/4999): loss=0.5508772295770862\n",
      "Log Regression(949/4999): loss=0.5474634916542845\n",
      "Log Regression(950/4999): loss=0.54742889647778\n",
      "Log Regression(951/4999): loss=0.5474696395912507\n",
      "Log Regression(952/4999): loss=0.5495213413993965\n",
      "Log Regression(953/4999): loss=0.5478066834291129\n",
      "Log Regression(954/4999): loss=0.5482899441364334\n",
      "Log Regression(955/4999): loss=0.5490645005397181\n",
      "Log Regression(956/4999): loss=0.5474377827992966\n",
      "Log Regression(957/4999): loss=0.5498290019974267\n",
      "Log Regression(958/4999): loss=0.5478203968159044\n",
      "Log Regression(959/4999): loss=0.5480202977171474\n",
      "Log Regression(960/4999): loss=0.5474864795712365\n",
      "Log Regression(961/4999): loss=0.5483384202050775\n",
      "Log Regression(962/4999): loss=0.5485260003366982\n",
      "Log Regression(963/4999): loss=0.5491182853997518\n",
      "Log Regression(964/4999): loss=0.5482126298429106\n",
      "Log Regression(965/4999): loss=0.5472584113526855\n",
      "Log Regression(966/4999): loss=0.5481488123362517\n",
      "Log Regression(967/4999): loss=0.5482633056939978\n",
      "Log Regression(968/4999): loss=0.5478656555854752\n",
      "Log Regression(969/4999): loss=0.5477423214354321\n",
      "Log Regression(970/4999): loss=0.5477520036445993\n",
      "Log Regression(971/4999): loss=0.547188545466084\n",
      "Log Regression(972/4999): loss=0.5531970412363979\n",
      "Log Regression(973/4999): loss=0.5519685853136751\n",
      "Log Regression(974/4999): loss=0.5480095350915458\n",
      "Log Regression(975/4999): loss=0.548020876736304\n",
      "Log Regression(976/4999): loss=0.5479920166975286\n",
      "Log Regression(977/4999): loss=0.553844765927774\n",
      "Log Regression(978/4999): loss=0.547891504099473\n",
      "Log Regression(979/4999): loss=0.5478717340815616\n",
      "Log Regression(980/4999): loss=0.5507629608678367\n",
      "Log Regression(981/4999): loss=0.5476117877486028\n",
      "Log Regression(982/4999): loss=0.5477512353404032\n",
      "Log Regression(983/4999): loss=0.5501140702672954\n",
      "Log Regression(984/4999): loss=0.5509411956069303\n",
      "Log Regression(985/4999): loss=0.5492176098680077\n",
      "Log Regression(986/4999): loss=0.5529258502014484\n",
      "Log Regression(987/4999): loss=0.5510097979818392\n",
      "Log Regression(988/4999): loss=0.5496228859152902\n",
      "Log Regression(989/4999): loss=0.5491577735329761\n",
      "Log Regression(990/4999): loss=0.5513623847326488\n",
      "Log Regression(991/4999): loss=0.550959734925336\n",
      "Log Regression(992/4999): loss=0.5511066239447254\n",
      "Log Regression(993/4999): loss=0.5491249695476197\n",
      "Log Regression(994/4999): loss=0.5484614902139283\n",
      "Log Regression(995/4999): loss=0.5483658270772307\n",
      "Log Regression(996/4999): loss=0.5495687195811779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(997/4999): loss=0.5489670847356679\n",
      "Log Regression(998/4999): loss=0.5510808263857452\n",
      "Log Regression(999/4999): loss=0.5505558116310918\n",
      "Log Regression(1000/4999): loss=0.5484326146855707\n",
      "Log Regression(1001/4999): loss=0.5482150904413872\n",
      "Log Regression(1002/4999): loss=0.549240430456087\n",
      "Log Regression(1003/4999): loss=0.5494108437399274\n",
      "Log Regression(1004/4999): loss=0.5496944741082016\n",
      "Log Regression(1005/4999): loss=0.5475073638619259\n",
      "Log Regression(1006/4999): loss=0.5471550819875318\n",
      "Log Regression(1007/4999): loss=0.5471121939556366\n",
      "Log Regression(1008/4999): loss=0.5479134482149142\n",
      "Log Regression(1009/4999): loss=0.5500765910108603\n",
      "Log Regression(1010/4999): loss=0.5479839245357511\n",
      "Log Regression(1011/4999): loss=0.5481864530430532\n",
      "Log Regression(1012/4999): loss=0.5512518024485734\n",
      "Log Regression(1013/4999): loss=0.5474574057850329\n",
      "Log Regression(1014/4999): loss=0.5478076014617571\n",
      "Log Regression(1015/4999): loss=0.5501992332386777\n",
      "Log Regression(1016/4999): loss=0.5468729807731606\n",
      "Log Regression(1017/4999): loss=0.546809243923294\n",
      "Log Regression(1018/4999): loss=0.5465933223960787\n",
      "Log Regression(1019/4999): loss=0.5466720418815633\n",
      "Log Regression(1020/4999): loss=0.5486581254830866\n",
      "Log Regression(1021/4999): loss=0.5484237681159998\n",
      "Log Regression(1022/4999): loss=0.5465756418302532\n",
      "Log Regression(1023/4999): loss=0.5475539340890252\n",
      "Log Regression(1024/4999): loss=0.5483278381437929\n",
      "Log Regression(1025/4999): loss=0.5466374096878076\n",
      "Log Regression(1026/4999): loss=0.5467921496326734\n",
      "Log Regression(1027/4999): loss=0.5477002570374658\n",
      "Log Regression(1028/4999): loss=0.5465902368591067\n",
      "Log Regression(1029/4999): loss=0.5497757450581774\n",
      "Log Regression(1030/4999): loss=0.5469557462467816\n",
      "Log Regression(1031/4999): loss=0.5467904567162197\n",
      "Log Regression(1032/4999): loss=0.5470539343435814\n",
      "Log Regression(1033/4999): loss=0.5468045391585501\n",
      "Log Regression(1034/4999): loss=0.5478485331365567\n",
      "Log Regression(1035/4999): loss=0.5469525402002722\n",
      "Log Regression(1036/4999): loss=0.5472993967076236\n",
      "Log Regression(1037/4999): loss=0.5475494260093343\n",
      "Log Regression(1038/4999): loss=0.5464936559997756\n",
      "Log Regression(1039/4999): loss=0.5472320151832201\n",
      "Log Regression(1040/4999): loss=0.5488650554869094\n",
      "Log Regression(1041/4999): loss=0.5464497869118841\n",
      "Log Regression(1042/4999): loss=0.5486691626087344\n",
      "Log Regression(1043/4999): loss=0.5463636770541721\n",
      "Log Regression(1044/4999): loss=0.5488288057134237\n",
      "Log Regression(1045/4999): loss=0.5474840206512869\n",
      "Log Regression(1046/4999): loss=0.5466950208959919\n",
      "Log Regression(1047/4999): loss=0.549115972613582\n",
      "Log Regression(1048/4999): loss=0.549822835557389\n",
      "Log Regression(1049/4999): loss=0.5463266281126102\n",
      "Log Regression(1050/4999): loss=0.5463067955659406\n",
      "Log Regression(1051/4999): loss=0.5510065409118878\n",
      "Log Regression(1052/4999): loss=0.5469115773697005\n",
      "Log Regression(1053/4999): loss=0.5461728418734436\n",
      "Log Regression(1054/4999): loss=0.5472170397266867\n",
      "Log Regression(1055/4999): loss=0.5472467216387761\n",
      "Log Regression(1056/4999): loss=0.5465425731556902\n",
      "Log Regression(1057/4999): loss=0.5462691544589257\n",
      "Log Regression(1058/4999): loss=0.5462532007118938\n",
      "Log Regression(1059/4999): loss=0.5460543514300202\n",
      "Log Regression(1060/4999): loss=0.5468491606886201\n",
      "Log Regression(1061/4999): loss=0.5473597285195363\n",
      "Log Regression(1062/4999): loss=0.5460392588429449\n",
      "Log Regression(1063/4999): loss=0.5474749620628149\n",
      "Log Regression(1064/4999): loss=0.5472853188960554\n",
      "Log Regression(1065/4999): loss=0.5515986567966332\n",
      "Log Regression(1066/4999): loss=0.5471721030962592\n",
      "Log Regression(1067/4999): loss=0.5461940485046509\n",
      "Log Regression(1068/4999): loss=0.5479377014543991\n",
      "Log Regression(1069/4999): loss=0.5470482175096574\n",
      "Log Regression(1070/4999): loss=0.5459552844743505\n",
      "Log Regression(1071/4999): loss=0.5485894464335919\n",
      "Log Regression(1072/4999): loss=0.5467161856665517\n",
      "Log Regression(1073/4999): loss=0.5484832452419955\n",
      "Log Regression(1074/4999): loss=0.5511657112741161\n",
      "Log Regression(1075/4999): loss=0.5497938648392404\n",
      "Log Regression(1076/4999): loss=0.5520362955059548\n",
      "Log Regression(1077/4999): loss=0.5459221395044036\n",
      "Log Regression(1078/4999): loss=0.5468365705276521\n",
      "Log Regression(1079/4999): loss=0.5468059255089731\n",
      "Log Regression(1080/4999): loss=0.5465867121166447\n",
      "Log Regression(1081/4999): loss=0.5461774963268305\n",
      "Log Regression(1082/4999): loss=0.5483912289853469\n",
      "Log Regression(1083/4999): loss=0.5459407140166253\n",
      "Log Regression(1084/4999): loss=0.5458356571368067\n",
      "Log Regression(1085/4999): loss=0.546260982277697\n",
      "Log Regression(1086/4999): loss=0.5478651828832183\n",
      "Log Regression(1087/4999): loss=0.5482243951860543\n",
      "Log Regression(1088/4999): loss=0.5457857503947091\n",
      "Log Regression(1089/4999): loss=0.5459558066332652\n",
      "Log Regression(1090/4999): loss=0.5476598813122008\n",
      "Log Regression(1091/4999): loss=0.5458220331507742\n",
      "Log Regression(1092/4999): loss=0.548379595427473\n",
      "Log Regression(1093/4999): loss=0.5475676665315364\n",
      "Log Regression(1094/4999): loss=0.545831442619481\n",
      "Log Regression(1095/4999): loss=0.5466167331893876\n",
      "Log Regression(1096/4999): loss=0.5479122187985287\n",
      "Log Regression(1097/4999): loss=0.5457658865047263\n",
      "Log Regression(1098/4999): loss=0.5541802401840142\n",
      "Log Regression(1099/4999): loss=0.5497280201207112\n",
      "Log Regression(1100/4999): loss=0.5493506085789565\n",
      "Log Regression(1101/4999): loss=0.5458847235610568\n",
      "Log Regression(1102/4999): loss=0.5461208013939307\n",
      "Log Regression(1103/4999): loss=0.5460280543846624\n",
      "Log Regression(1104/4999): loss=0.5467326259751792\n",
      "Log Regression(1105/4999): loss=0.5458217476299119\n",
      "Log Regression(1106/4999): loss=0.5457842801335161\n",
      "Log Regression(1107/4999): loss=0.5502660396367227\n",
      "Log Regression(1108/4999): loss=0.5464692527622111\n",
      "Log Regression(1109/4999): loss=0.5457339765101517\n",
      "Log Regression(1110/4999): loss=0.5455683333468259\n",
      "Log Regression(1111/4999): loss=0.5458100516373833\n",
      "Log Regression(1112/4999): loss=0.5481288361838833\n",
      "Log Regression(1113/4999): loss=0.5455319712607483\n",
      "Log Regression(1114/4999): loss=0.5455447397398913\n",
      "Log Regression(1115/4999): loss=0.5455633112955888\n",
      "Log Regression(1116/4999): loss=0.5461781543422994\n",
      "Log Regression(1117/4999): loss=0.5455016763433332\n",
      "Log Regression(1118/4999): loss=0.5456261096180304\n",
      "Log Regression(1119/4999): loss=0.5461756537203586\n",
      "Log Regression(1120/4999): loss=0.5463381278647317\n",
      "Log Regression(1121/4999): loss=0.546261123114602\n",
      "Log Regression(1122/4999): loss=0.5467353748403325\n",
      "Log Regression(1123/4999): loss=0.5463343980328776\n",
      "Log Regression(1124/4999): loss=0.5461457770389091\n",
      "Log Regression(1125/4999): loss=0.5459068352906272\n",
      "Log Regression(1126/4999): loss=0.5475586954231842\n",
      "Log Regression(1127/4999): loss=0.5462613192675676\n",
      "Log Regression(1128/4999): loss=0.5458594022459763\n",
      "Log Regression(1129/4999): loss=0.5468869307683991\n",
      "Log Regression(1130/4999): loss=0.5465819053993178\n",
      "Log Regression(1131/4999): loss=0.5454919254682025\n",
      "Log Regression(1132/4999): loss=0.5455316258808591\n",
      "Log Regression(1133/4999): loss=0.5471175362313001\n",
      "Log Regression(1134/4999): loss=0.5455703148063225\n",
      "Log Regression(1135/4999): loss=0.5473654682029236\n",
      "Log Regression(1136/4999): loss=0.5526578580557626\n",
      "Log Regression(1137/4999): loss=0.5454032075969373\n",
      "Log Regression(1138/4999): loss=0.5455475693084632\n",
      "Log Regression(1139/4999): loss=0.5455732343178911\n",
      "Log Regression(1140/4999): loss=0.5459833943765977\n",
      "Log Regression(1141/4999): loss=0.5454177318766765\n",
      "Log Regression(1142/4999): loss=0.5490156313339394\n",
      "Log Regression(1143/4999): loss=0.5488065844679795\n",
      "Log Regression(1144/4999): loss=0.5524961324796916\n",
      "Log Regression(1145/4999): loss=0.545954450384546\n",
      "Log Regression(1146/4999): loss=0.5457288239352001\n",
      "Log Regression(1147/4999): loss=0.545914953969768\n",
      "Log Regression(1148/4999): loss=0.5454361824836541\n",
      "Log Regression(1149/4999): loss=0.545991448408783\n",
      "Log Regression(1150/4999): loss=0.5452814371423208\n",
      "Log Regression(1151/4999): loss=0.5458509264411159\n",
      "Log Regression(1152/4999): loss=0.5456394594868237\n",
      "Log Regression(1153/4999): loss=0.5455002378857657\n",
      "Log Regression(1154/4999): loss=0.5468841898704486\n",
      "Log Regression(1155/4999): loss=0.5523867322056683\n",
      "Log Regression(1156/4999): loss=0.548295772852267\n",
      "Log Regression(1157/4999): loss=0.5455046340115024\n",
      "Log Regression(1158/4999): loss=0.5456528327525475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(1159/4999): loss=0.545787650385867\n",
      "Log Regression(1160/4999): loss=0.5461666079939301\n",
      "Log Regression(1161/4999): loss=0.5495745402303325\n",
      "Log Regression(1162/4999): loss=0.5496531570190849\n",
      "Log Regression(1163/4999): loss=0.5474240035103046\n",
      "Log Regression(1164/4999): loss=0.5456108288624152\n",
      "Log Regression(1165/4999): loss=0.5454800948441723\n",
      "Log Regression(1166/4999): loss=0.5463937321283888\n",
      "Log Regression(1167/4999): loss=0.5478239288272\n",
      "Log Regression(1168/4999): loss=0.5454884878675017\n",
      "Log Regression(1169/4999): loss=0.5456633459930224\n",
      "Log Regression(1170/4999): loss=0.5467255588863774\n",
      "Log Regression(1171/4999): loss=0.5458905505137147\n",
      "Log Regression(1172/4999): loss=0.5473629633062504\n",
      "Log Regression(1173/4999): loss=0.5458682482375803\n",
      "Log Regression(1174/4999): loss=0.5457913415528953\n",
      "Log Regression(1175/4999): loss=0.5456930191103688\n",
      "Log Regression(1176/4999): loss=0.5454586179826976\n",
      "Log Regression(1177/4999): loss=0.5455267754143096\n",
      "Log Regression(1178/4999): loss=0.5477457120693554\n",
      "Log Regression(1179/4999): loss=0.546111759964574\n",
      "Log Regression(1180/4999): loss=0.5458794039470138\n",
      "Log Regression(1181/4999): loss=0.5453734427619688\n",
      "Log Regression(1182/4999): loss=0.5458431468079863\n",
      "Log Regression(1183/4999): loss=0.545372486512444\n",
      "Log Regression(1184/4999): loss=0.5469391973256527\n",
      "Log Regression(1185/4999): loss=0.5471515844781059\n",
      "Log Regression(1186/4999): loss=0.5453599590567325\n",
      "Log Regression(1187/4999): loss=0.5477250327110484\n",
      "Log Regression(1188/4999): loss=0.5455929506538609\n",
      "Log Regression(1189/4999): loss=0.5453503789556527\n",
      "Log Regression(1190/4999): loss=0.5452553357560254\n",
      "Log Regression(1191/4999): loss=0.545582349541277\n",
      "Log Regression(1192/4999): loss=0.545498716679621\n",
      "Log Regression(1193/4999): loss=0.5456126299102884\n",
      "Log Regression(1194/4999): loss=0.5452343026243653\n",
      "Log Regression(1195/4999): loss=0.5453694531640662\n",
      "Log Regression(1196/4999): loss=0.5452292061375762\n",
      "Log Regression(1197/4999): loss=0.545922061833509\n",
      "Log Regression(1198/4999): loss=0.5459825944978102\n",
      "Log Regression(1199/4999): loss=0.5454131108253488\n",
      "Log Regression(1200/4999): loss=0.5464954405203313\n",
      "Log Regression(1201/4999): loss=0.5454634860684154\n",
      "Log Regression(1202/4999): loss=0.5460150636151647\n",
      "Log Regression(1203/4999): loss=0.5490055756550073\n",
      "Log Regression(1204/4999): loss=0.5452391354418793\n",
      "Log Regression(1205/4999): loss=0.5453084829326398\n",
      "Log Regression(1206/4999): loss=0.5451801105173287\n",
      "Log Regression(1207/4999): loss=0.5458900778771887\n",
      "Log Regression(1208/4999): loss=0.5451545232892568\n",
      "Log Regression(1209/4999): loss=0.5570482078021186\n",
      "Log Regression(1210/4999): loss=0.5515945110851959\n",
      "Log Regression(1211/4999): loss=0.5507315703006228\n",
      "Log Regression(1212/4999): loss=0.5471628342712583\n",
      "Log Regression(1213/4999): loss=0.5469473706704979\n",
      "Log Regression(1214/4999): loss=0.5450542875505965\n",
      "Log Regression(1215/4999): loss=0.5519731812494761\n",
      "Log Regression(1216/4999): loss=0.5567853298310164\n",
      "Log Regression(1217/4999): loss=0.5465251348722817\n",
      "Log Regression(1218/4999): loss=0.5457706597522337\n",
      "Log Regression(1219/4999): loss=0.5459012209152215\n",
      "Log Regression(1220/4999): loss=0.5459733724723341\n",
      "Log Regression(1221/4999): loss=0.5459878306968035\n",
      "Log Regression(1222/4999): loss=0.5465593583003667\n",
      "Log Regression(1223/4999): loss=0.5457266866591125\n",
      "Log Regression(1224/4999): loss=0.5507849317636684\n",
      "Log Regression(1225/4999): loss=0.5503753188606125\n",
      "Log Regression(1226/4999): loss=0.5449232478736357\n",
      "Log Regression(1227/4999): loss=0.5447310884958388\n",
      "Log Regression(1228/4999): loss=0.5449887084702828\n",
      "Log Regression(1229/4999): loss=0.5447594753796409\n",
      "Log Regression(1230/4999): loss=0.5450325641500889\n",
      "Log Regression(1231/4999): loss=0.5494405257110216\n",
      "Log Regression(1232/4999): loss=0.5446739948326722\n",
      "Log Regression(1233/4999): loss=0.5447700899118355\n",
      "Log Regression(1234/4999): loss=0.5455582033642412\n",
      "Log Regression(1235/4999): loss=0.5467979516734087\n",
      "Log Regression(1236/4999): loss=0.54686069310692\n",
      "Log Regression(1237/4999): loss=0.544936053887707\n",
      "Log Regression(1238/4999): loss=0.5459370094059909\n",
      "Log Regression(1239/4999): loss=0.5516258167823616\n",
      "Log Regression(1240/4999): loss=0.5491538745948544\n",
      "Log Regression(1241/4999): loss=0.553333355935361\n",
      "Log Regression(1242/4999): loss=0.5508557718245677\n",
      "Log Regression(1243/4999): loss=0.5516064918566728\n",
      "Log Regression(1244/4999): loss=0.5475821950077824\n",
      "Log Regression(1245/4999): loss=0.5463948412000996\n",
      "Log Regression(1246/4999): loss=0.5449978421218439\n",
      "Log Regression(1247/4999): loss=0.5450364184902952\n",
      "Log Regression(1248/4999): loss=0.5473399377924574\n",
      "Log Regression(1249/4999): loss=0.5458562227637607\n",
      "Log Regression(1250/4999): loss=0.546582859878292\n",
      "Log Regression(1251/4999): loss=0.5446996794209975\n",
      "Log Regression(1252/4999): loss=0.5444561230601553\n",
      "Log Regression(1253/4999): loss=0.5447145593720772\n",
      "Log Regression(1254/4999): loss=0.5444626836599107\n",
      "Log Regression(1255/4999): loss=0.5444905056144558\n",
      "Log Regression(1256/4999): loss=0.5445720718504995\n",
      "Log Regression(1257/4999): loss=0.5446075524712539\n",
      "Log Regression(1258/4999): loss=0.5448741202778227\n",
      "Log Regression(1259/4999): loss=0.5489254898213473\n",
      "Log Regression(1260/4999): loss=0.5450164454353893\n",
      "Log Regression(1261/4999): loss=0.5502042767991114\n",
      "Log Regression(1262/4999): loss=0.5522377689138528\n",
      "Log Regression(1263/4999): loss=0.5519373720324992\n",
      "Log Regression(1264/4999): loss=0.547236770624857\n",
      "Log Regression(1265/4999): loss=0.5471389536839337\n",
      "Log Regression(1266/4999): loss=0.5445443209310844\n",
      "Log Regression(1267/4999): loss=0.5455229528659526\n",
      "Log Regression(1268/4999): loss=0.5453765069847403\n",
      "Log Regression(1269/4999): loss=0.5490351146601027\n",
      "Log Regression(1270/4999): loss=0.5446677152542633\n",
      "Log Regression(1271/4999): loss=0.5445688312259632\n",
      "Log Regression(1272/4999): loss=0.5446909159261792\n",
      "Log Regression(1273/4999): loss=0.5448631812796217\n",
      "Log Regression(1274/4999): loss=0.5449166619749449\n",
      "Log Regression(1275/4999): loss=0.5451497168588384\n",
      "Log Regression(1276/4999): loss=0.5454484630725972\n",
      "Log Regression(1277/4999): loss=0.5476959841826629\n",
      "Log Regression(1278/4999): loss=0.5448159734418214\n",
      "Log Regression(1279/4999): loss=0.5442596173171754\n",
      "Log Regression(1280/4999): loss=0.5482665952233521\n",
      "Log Regression(1281/4999): loss=0.5464995371683831\n",
      "Log Regression(1282/4999): loss=0.5474676892827619\n",
      "Log Regression(1283/4999): loss=0.5463195535116087\n",
      "Log Regression(1284/4999): loss=0.5447032313304123\n",
      "Log Regression(1285/4999): loss=0.5459930630862978\n",
      "Log Regression(1286/4999): loss=0.544538975416047\n",
      "Log Regression(1287/4999): loss=0.5445325955558792\n",
      "Log Regression(1288/4999): loss=0.5461489691682684\n",
      "Log Regression(1289/4999): loss=0.5465620699246999\n",
      "Log Regression(1290/4999): loss=0.5528310765688801\n",
      "Log Regression(1291/4999): loss=0.5461434574575001\n",
      "Log Regression(1292/4999): loss=0.5455469027261544\n",
      "Log Regression(1293/4999): loss=0.5471848823294014\n",
      "Log Regression(1294/4999): loss=0.548671819338064\n",
      "Log Regression(1295/4999): loss=0.5543232680437592\n",
      "Log Regression(1296/4999): loss=0.5456194937736976\n",
      "Log Regression(1297/4999): loss=0.5491434916471597\n",
      "Log Regression(1298/4999): loss=0.5448782398117294\n",
      "Log Regression(1299/4999): loss=0.548657577644095\n",
      "Log Regression(1300/4999): loss=0.5464067022820797\n",
      "Log Regression(1301/4999): loss=0.5443915973698983\n",
      "Log Regression(1302/4999): loss=0.5442228507569096\n",
      "Log Regression(1303/4999): loss=0.5441968187273999\n",
      "Log Regression(1304/4999): loss=0.5449349423587562\n",
      "Log Regression(1305/4999): loss=0.544255000933756\n",
      "Log Regression(1306/4999): loss=0.5441426110010604\n",
      "Log Regression(1307/4999): loss=0.5486109578089804\n",
      "Log Regression(1308/4999): loss=0.5466130963229856\n",
      "Log Regression(1309/4999): loss=0.5440642394360834\n",
      "Log Regression(1310/4999): loss=0.5473155616656217\n",
      "Log Regression(1311/4999): loss=0.5490185642108871\n",
      "Log Regression(1312/4999): loss=0.5542417637703796\n",
      "Log Regression(1313/4999): loss=0.5481880444869309\n",
      "Log Regression(1314/4999): loss=0.5452556935326712\n",
      "Log Regression(1315/4999): loss=0.5450821819979865\n",
      "Log Regression(1316/4999): loss=0.544603556736806\n",
      "Log Regression(1317/4999): loss=0.5441498807734051\n",
      "Log Regression(1318/4999): loss=0.5444995562740662\n",
      "Log Regression(1319/4999): loss=0.5492583941387983\n",
      "Log Regression(1320/4999): loss=0.5476742630006339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(1321/4999): loss=0.5475567314862018\n",
      "Log Regression(1322/4999): loss=0.5481986250978664\n",
      "Log Regression(1323/4999): loss=0.5498915884692421\n",
      "Log Regression(1324/4999): loss=0.5442330333934642\n",
      "Log Regression(1325/4999): loss=0.54630872834265\n",
      "Log Regression(1326/4999): loss=0.5446039259025917\n",
      "Log Regression(1327/4999): loss=0.5441891148518238\n",
      "Log Regression(1328/4999): loss=0.5440226589845009\n",
      "Log Regression(1329/4999): loss=0.5439750689403238\n",
      "Log Regression(1330/4999): loss=0.5446981773964197\n",
      "Log Regression(1331/4999): loss=0.5518572810832788\n",
      "Log Regression(1332/4999): loss=0.5524841121299108\n",
      "Log Regression(1333/4999): loss=0.5455958001413855\n",
      "Log Regression(1334/4999): loss=0.5446219639792904\n",
      "Log Regression(1335/4999): loss=0.5492590067289007\n",
      "Log Regression(1336/4999): loss=0.5468526653877224\n",
      "Log Regression(1337/4999): loss=0.5453474387382841\n",
      "Log Regression(1338/4999): loss=0.5456442294396284\n",
      "Log Regression(1339/4999): loss=0.5500542910549533\n",
      "Log Regression(1340/4999): loss=0.5460584602973209\n",
      "Log Regression(1341/4999): loss=0.5450493531309311\n",
      "Log Regression(1342/4999): loss=0.544616697733646\n",
      "Log Regression(1343/4999): loss=0.5443341400810953\n",
      "Log Regression(1344/4999): loss=0.5459806625534069\n",
      "Log Regression(1345/4999): loss=0.5489177739081087\n",
      "Log Regression(1346/4999): loss=0.5495721574805916\n",
      "Log Regression(1347/4999): loss=0.5471109228624859\n",
      "Log Regression(1348/4999): loss=0.553459801506098\n",
      "Log Regression(1349/4999): loss=0.5455389567194554\n",
      "Log Regression(1350/4999): loss=0.5471780317448453\n",
      "Log Regression(1351/4999): loss=0.5448115742643745\n",
      "Log Regression(1352/4999): loss=0.5443762096712186\n",
      "Log Regression(1353/4999): loss=0.5441006714037495\n",
      "Log Regression(1354/4999): loss=0.5442321776422349\n",
      "Log Regression(1355/4999): loss=0.5445893337910318\n",
      "Log Regression(1356/4999): loss=0.54545472898529\n",
      "Log Regression(1357/4999): loss=0.5500953947834071\n",
      "Log Regression(1358/4999): loss=0.5461029326016666\n",
      "Log Regression(1359/4999): loss=0.5521690269606273\n",
      "Log Regression(1360/4999): loss=0.5468690445362299\n",
      "Log Regression(1361/4999): loss=0.5448435961292912\n",
      "Log Regression(1362/4999): loss=0.544929038056858\n",
      "Log Regression(1363/4999): loss=0.5455411685519443\n",
      "Log Regression(1364/4999): loss=0.5469688915866852\n",
      "Log Regression(1365/4999): loss=0.5467063468128733\n",
      "Log Regression(1366/4999): loss=0.5443161913943757\n",
      "Log Regression(1367/4999): loss=0.5450309715633653\n",
      "Log Regression(1368/4999): loss=0.5441614873707901\n",
      "Log Regression(1369/4999): loss=0.5438913557029267\n",
      "Log Regression(1370/4999): loss=0.5439360885760943\n",
      "Log Regression(1371/4999): loss=0.5458558453572421\n",
      "Log Regression(1372/4999): loss=0.543811524366626\n",
      "Log Regression(1373/4999): loss=0.5441186913647913\n",
      "Log Regression(1374/4999): loss=0.5449069556498282\n",
      "Log Regression(1375/4999): loss=0.5460077648119633\n",
      "Log Regression(1376/4999): loss=0.5437845145098771\n",
      "Log Regression(1377/4999): loss=0.544539598207582\n",
      "Log Regression(1378/4999): loss=0.5438038406403832\n",
      "Log Regression(1379/4999): loss=0.5439905226972014\n",
      "Log Regression(1380/4999): loss=0.5439409616903238\n",
      "Log Regression(1381/4999): loss=0.5437665310031734\n",
      "Log Regression(1382/4999): loss=0.543873416334493\n",
      "Log Regression(1383/4999): loss=0.5443064293357803\n",
      "Log Regression(1384/4999): loss=0.5448997304732314\n",
      "Log Regression(1385/4999): loss=0.5438620956416347\n",
      "Log Regression(1386/4999): loss=0.5442396016337381\n",
      "Log Regression(1387/4999): loss=0.544350031286982\n",
      "Log Regression(1388/4999): loss=0.5440697905565908\n",
      "Log Regression(1389/4999): loss=0.5474993263040021\n",
      "Log Regression(1390/4999): loss=0.5448071206113194\n",
      "Log Regression(1391/4999): loss=0.5452759143596069\n",
      "Log Regression(1392/4999): loss=0.5437835487541754\n",
      "Log Regression(1393/4999): loss=0.5438355671135535\n",
      "Log Regression(1394/4999): loss=0.5447594530310672\n",
      "Log Regression(1395/4999): loss=0.5465530275169909\n",
      "Log Regression(1396/4999): loss=0.5444387031131861\n",
      "Log Regression(1397/4999): loss=0.5437293388634532\n",
      "Log Regression(1398/4999): loss=0.5438095865743523\n",
      "Log Regression(1399/4999): loss=0.5478751842074594\n",
      "Log Regression(1400/4999): loss=0.54501043656954\n",
      "Log Regression(1401/4999): loss=0.5437498461100456\n",
      "Log Regression(1402/4999): loss=0.5440965412767874\n",
      "Log Regression(1403/4999): loss=0.543689370178025\n",
      "Log Regression(1404/4999): loss=0.5442015597887353\n",
      "Log Regression(1405/4999): loss=0.5441272605352934\n",
      "Log Regression(1406/4999): loss=0.5439460045424763\n",
      "Log Regression(1407/4999): loss=0.5455628541469415\n",
      "Log Regression(1408/4999): loss=0.5443418727258025\n",
      "Log Regression(1409/4999): loss=0.546183970013418\n",
      "Log Regression(1410/4999): loss=0.550452069336708\n",
      "Log Regression(1411/4999): loss=0.5445572399580446\n",
      "Log Regression(1412/4999): loss=0.5435127640867397\n",
      "Log Regression(1413/4999): loss=0.5460885883876182\n",
      "Log Regression(1414/4999): loss=0.5450758633780661\n",
      "Log Regression(1415/4999): loss=0.5464832642865918\n",
      "Log Regression(1416/4999): loss=0.5454047324597129\n",
      "Log Regression(1417/4999): loss=0.5498093437480773\n",
      "Log Regression(1418/4999): loss=0.5454784699432741\n",
      "Log Regression(1419/4999): loss=0.5441129347561792\n",
      "Log Regression(1420/4999): loss=0.544748573480449\n",
      "Log Regression(1421/4999): loss=0.5434951757451019\n",
      "Log Regression(1422/4999): loss=0.5471318784351524\n",
      "Log Regression(1423/4999): loss=0.5519272964348912\n",
      "Log Regression(1424/4999): loss=0.5495493608152276\n",
      "Log Regression(1425/4999): loss=0.5470064235215623\n",
      "Log Regression(1426/4999): loss=0.5463906325837946\n",
      "Log Regression(1427/4999): loss=0.5436151161173847\n",
      "Log Regression(1428/4999): loss=0.5469422677369056\n",
      "Log Regression(1429/4999): loss=0.5475115033741154\n",
      "Log Regression(1430/4999): loss=0.5442596520687079\n",
      "Log Regression(1431/4999): loss=0.5445712430538758\n",
      "Log Regression(1432/4999): loss=0.5437009591405277\n",
      "Log Regression(1433/4999): loss=0.5436738928144053\n",
      "Log Regression(1434/4999): loss=0.544318761234135\n",
      "Log Regression(1435/4999): loss=0.5435641782142491\n",
      "Log Regression(1436/4999): loss=0.5438337481582503\n",
      "Log Regression(1437/4999): loss=0.5443063870976312\n",
      "Log Regression(1438/4999): loss=0.5454498153563047\n",
      "Log Regression(1439/4999): loss=0.5504929246133237\n",
      "Log Regression(1440/4999): loss=0.5525234937584066\n",
      "Log Regression(1441/4999): loss=0.5441554475253785\n",
      "Log Regression(1442/4999): loss=0.5457902979800161\n",
      "Log Regression(1443/4999): loss=0.5496572797441585\n",
      "Log Regression(1444/4999): loss=0.5475031369742681\n",
      "Log Regression(1445/4999): loss=0.54396387784535\n",
      "Log Regression(1446/4999): loss=0.5436274779447694\n",
      "Log Regression(1447/4999): loss=0.5436027009754718\n",
      "Log Regression(1448/4999): loss=0.5434905138980528\n",
      "Log Regression(1449/4999): loss=0.5438955408491372\n",
      "Log Regression(1450/4999): loss=0.5457648807816652\n",
      "Log Regression(1451/4999): loss=0.5446321303743004\n",
      "Log Regression(1452/4999): loss=0.5435405250175305\n",
      "Log Regression(1453/4999): loss=0.5452345353926168\n",
      "Log Regression(1454/4999): loss=0.5439062469070624\n",
      "Log Regression(1455/4999): loss=0.5434341816391155\n",
      "Log Regression(1456/4999): loss=0.5442675349232904\n",
      "Log Regression(1457/4999): loss=0.5440456355424778\n",
      "Log Regression(1458/4999): loss=0.5473105336046807\n",
      "Log Regression(1459/4999): loss=0.5440529821701806\n",
      "Log Regression(1460/4999): loss=0.545015206292672\n",
      "Log Regression(1461/4999): loss=0.5446489757454999\n",
      "Log Regression(1462/4999): loss=0.5451284174844472\n",
      "Log Regression(1463/4999): loss=0.5442037394928506\n",
      "Log Regression(1464/4999): loss=0.544203251044416\n",
      "Log Regression(1465/4999): loss=0.5448393878981054\n",
      "Log Regression(1466/4999): loss=0.547113747061331\n",
      "Log Regression(1467/4999): loss=0.5437476366731646\n",
      "Log Regression(1468/4999): loss=0.5455524221809778\n",
      "Log Regression(1469/4999): loss=0.5453339059716003\n",
      "Log Regression(1470/4999): loss=0.547123185638562\n",
      "Log Regression(1471/4999): loss=0.5506245885403249\n",
      "Log Regression(1472/4999): loss=0.5465800158095476\n",
      "Log Regression(1473/4999): loss=0.5438050957754765\n",
      "Log Regression(1474/4999): loss=0.5450429724481854\n",
      "Log Regression(1475/4999): loss=0.5441029350535156\n",
      "Log Regression(1476/4999): loss=0.5436697873283142\n",
      "Log Regression(1477/4999): loss=0.5438438846915019\n",
      "Log Regression(1478/4999): loss=0.5460270952485727\n",
      "Log Regression(1479/4999): loss=0.5437240733322941\n",
      "Log Regression(1480/4999): loss=0.543191419266321\n",
      "Log Regression(1481/4999): loss=0.5433493619109486\n",
      "Log Regression(1482/4999): loss=0.5433585049062793\n",
      "Log Regression(1483/4999): loss=0.5437338074218242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(1484/4999): loss=0.5451372848011964\n",
      "Log Regression(1485/4999): loss=0.5433136654122434\n",
      "Log Regression(1486/4999): loss=0.5439165262977989\n",
      "Log Regression(1487/4999): loss=0.5435638867719212\n",
      "Log Regression(1488/4999): loss=0.5448818100406757\n",
      "Log Regression(1489/4999): loss=0.5463490137957496\n",
      "Log Regression(1490/4999): loss=0.5607543576151693\n",
      "Log Regression(1491/4999): loss=0.5495716472399713\n",
      "Log Regression(1492/4999): loss=0.5467793730018339\n",
      "Log Regression(1493/4999): loss=0.5453748837213621\n",
      "Log Regression(1494/4999): loss=0.5472610232522819\n",
      "Log Regression(1495/4999): loss=0.5448568517279594\n",
      "Log Regression(1496/4999): loss=0.543418649307963\n",
      "Log Regression(1497/4999): loss=0.5441330413111084\n",
      "Log Regression(1498/4999): loss=0.5435478019871882\n",
      "Log Regression(1499/4999): loss=0.545108445431921\n",
      "Log Regression(1500/4999): loss=0.5432990391697522\n",
      "Log Regression(1501/4999): loss=0.5463524489427874\n",
      "Log Regression(1502/4999): loss=0.5432229421699072\n",
      "Log Regression(1503/4999): loss=0.5431377680519117\n",
      "Log Regression(1504/4999): loss=0.5434410135559498\n",
      "Log Regression(1505/4999): loss=0.5430580631937002\n",
      "Log Regression(1506/4999): loss=0.5431350315774747\n",
      "Log Regression(1507/4999): loss=0.5431333039433947\n",
      "Log Regression(1508/4999): loss=0.5435614734887204\n",
      "Log Regression(1509/4999): loss=0.5439003817626554\n",
      "Log Regression(1510/4999): loss=0.5435245016036077\n",
      "Log Regression(1511/4999): loss=0.5431559808158325\n",
      "Log Regression(1512/4999): loss=0.5432992861543654\n",
      "Log Regression(1513/4999): loss=0.5430718590039391\n",
      "Log Regression(1514/4999): loss=0.5467993929244067\n",
      "Log Regression(1515/4999): loss=0.5466922253337329\n",
      "Log Regression(1516/4999): loss=0.5452767141898984\n",
      "Log Regression(1517/4999): loss=0.5443918957034297\n",
      "Log Regression(1518/4999): loss=0.5450727299396033\n",
      "Log Regression(1519/4999): loss=0.5488677861246009\n",
      "Log Regression(1520/4999): loss=0.5437937296561168\n",
      "Log Regression(1521/4999): loss=0.5436867049285339\n",
      "Log Regression(1522/4999): loss=0.5436242312555426\n",
      "Log Regression(1523/4999): loss=0.5435646163018696\n",
      "Log Regression(1524/4999): loss=0.5435858449164703\n",
      "Log Regression(1525/4999): loss=0.5444395053014756\n",
      "Log Regression(1526/4999): loss=0.5432447860164192\n",
      "Log Regression(1527/4999): loss=0.5444914806321406\n",
      "Log Regression(1528/4999): loss=0.5433450057351211\n",
      "Log Regression(1529/4999): loss=0.5430599119269637\n",
      "Log Regression(1530/4999): loss=0.5435094092418502\n",
      "Log Regression(1531/4999): loss=0.5435267053391959\n",
      "Log Regression(1532/4999): loss=0.5437542132495716\n",
      "Log Regression(1533/4999): loss=0.5491331716259269\n",
      "Log Regression(1534/4999): loss=0.5446157645269466\n",
      "Log Regression(1535/4999): loss=0.5479817753772079\n",
      "Log Regression(1536/4999): loss=0.5446785550568183\n",
      "Log Regression(1537/4999): loss=0.5456894268235585\n",
      "Log Regression(1538/4999): loss=0.5466729510536193\n",
      "Log Regression(1539/4999): loss=0.5447144023550582\n",
      "Log Regression(1540/4999): loss=0.5440724676634406\n",
      "Log Regression(1541/4999): loss=0.5474555163003662\n",
      "Log Regression(1542/4999): loss=0.5433223275383051\n",
      "Log Regression(1543/4999): loss=0.5440774803836301\n",
      "Log Regression(1544/4999): loss=0.5440752391397324\n",
      "Log Regression(1545/4999): loss=0.5432105290914858\n",
      "Log Regression(1546/4999): loss=0.5445743851762318\n",
      "Log Regression(1547/4999): loss=0.5470467196874618\n",
      "Log Regression(1548/4999): loss=0.5473299387195739\n",
      "Log Regression(1549/4999): loss=0.5440294354526607\n",
      "Log Regression(1550/4999): loss=0.5492840391147061\n",
      "Log Regression(1551/4999): loss=0.5469135469626852\n",
      "Log Regression(1552/4999): loss=0.5475887170857899\n",
      "Log Regression(1553/4999): loss=0.5441369615704558\n",
      "Log Regression(1554/4999): loss=0.5467218248079462\n",
      "Log Regression(1555/4999): loss=0.5494043930764282\n",
      "Log Regression(1556/4999): loss=0.5428778422742329\n",
      "Log Regression(1557/4999): loss=0.5428622512041913\n",
      "Log Regression(1558/4999): loss=0.5461476322155653\n",
      "Log Regression(1559/4999): loss=0.5448296077762365\n",
      "Log Regression(1560/4999): loss=0.5448202980433983\n",
      "Log Regression(1561/4999): loss=0.5442040518823582\n",
      "Log Regression(1562/4999): loss=0.5428298283875947\n",
      "Log Regression(1563/4999): loss=0.5433679355658693\n",
      "Log Regression(1564/4999): loss=0.543921886148146\n",
      "Log Regression(1565/4999): loss=0.5444488098987983\n",
      "Log Regression(1566/4999): loss=0.5428854866556011\n",
      "Log Regression(1567/4999): loss=0.5429558561776401\n",
      "Log Regression(1568/4999): loss=0.5430036975451248\n",
      "Log Regression(1569/4999): loss=0.5440303909230102\n",
      "Log Regression(1570/4999): loss=0.5465791800341211\n",
      "Log Regression(1571/4999): loss=0.543477829236243\n",
      "Log Regression(1572/4999): loss=0.5446991935847413\n",
      "Log Regression(1573/4999): loss=0.5431651404622435\n",
      "Log Regression(1574/4999): loss=0.5448641413152787\n",
      "Log Regression(1575/4999): loss=0.5441452527778508\n",
      "Log Regression(1576/4999): loss=0.5442605288895562\n",
      "Log Regression(1577/4999): loss=0.5428487590418067\n",
      "Log Regression(1578/4999): loss=0.5461697095311627\n",
      "Log Regression(1579/4999): loss=0.5448485908533613\n",
      "Log Regression(1580/4999): loss=0.5436615616161531\n",
      "Log Regression(1581/4999): loss=0.5436557642477889\n",
      "Log Regression(1582/4999): loss=0.5470069955686013\n",
      "Log Regression(1583/4999): loss=0.5446806143365849\n",
      "Log Regression(1584/4999): loss=0.543290114972995\n",
      "Log Regression(1585/4999): loss=0.5429301272013096\n",
      "Log Regression(1586/4999): loss=0.5437033153772758\n",
      "Log Regression(1587/4999): loss=0.5427775068311025\n",
      "Log Regression(1588/4999): loss=0.5428018923703766\n",
      "Log Regression(1589/4999): loss=0.5427745829149678\n",
      "Log Regression(1590/4999): loss=0.5430391644523217\n",
      "Log Regression(1591/4999): loss=0.5436801588337575\n",
      "Log Regression(1592/4999): loss=0.544502653311154\n",
      "Log Regression(1593/4999): loss=0.5427833539272221\n",
      "Log Regression(1594/4999): loss=0.5442131726875294\n",
      "Log Regression(1595/4999): loss=0.5441449924889087\n",
      "Log Regression(1596/4999): loss=0.5471422826445668\n",
      "Log Regression(1597/4999): loss=0.5444243641215005\n",
      "Log Regression(1598/4999): loss=0.5447233520804009\n",
      "Log Regression(1599/4999): loss=0.5469927733510643\n",
      "Log Regression(1600/4999): loss=0.5485153692811923\n",
      "Log Regression(1601/4999): loss=0.5438946599005366\n",
      "Log Regression(1602/4999): loss=0.5445347749366591\n",
      "Log Regression(1603/4999): loss=0.5427245252909\n",
      "Log Regression(1604/4999): loss=0.543930073090117\n",
      "Log Regression(1605/4999): loss=0.5430454094823055\n",
      "Log Regression(1606/4999): loss=0.5443297106453093\n",
      "Log Regression(1607/4999): loss=0.542706143645257\n",
      "Log Regression(1608/4999): loss=0.5427917299157088\n",
      "Log Regression(1609/4999): loss=0.5431595816878377\n",
      "Log Regression(1610/4999): loss=0.5436029121547253\n",
      "Log Regression(1611/4999): loss=0.5426931818771397\n",
      "Log Regression(1612/4999): loss=0.5428318188796238\n",
      "Log Regression(1613/4999): loss=0.5427474895238344\n",
      "Log Regression(1614/4999): loss=0.5489234848198347\n",
      "Log Regression(1615/4999): loss=0.5439814604344887\n",
      "Log Regression(1616/4999): loss=0.5456334759527516\n",
      "Log Regression(1617/4999): loss=0.5467442382460059\n",
      "Log Regression(1618/4999): loss=0.5476263958531753\n",
      "Log Regression(1619/4999): loss=0.5480195781354397\n",
      "Log Regression(1620/4999): loss=0.5483875580362557\n",
      "Log Regression(1621/4999): loss=0.5429610795412758\n",
      "Log Regression(1622/4999): loss=0.543066890884623\n",
      "Log Regression(1623/4999): loss=0.5428544027625778\n",
      "Log Regression(1624/4999): loss=0.5443092203817618\n",
      "Log Regression(1625/4999): loss=0.5425894763894332\n",
      "Log Regression(1626/4999): loss=0.5430565089375299\n",
      "Log Regression(1627/4999): loss=0.5467226621181742\n",
      "Log Regression(1628/4999): loss=0.5462385823028725\n",
      "Log Regression(1629/4999): loss=0.5432693780198254\n",
      "Log Regression(1630/4999): loss=0.5460239845084536\n",
      "Log Regression(1631/4999): loss=0.5429642440042285\n",
      "Log Regression(1632/4999): loss=0.5428381021467932\n",
      "Log Regression(1633/4999): loss=0.5458941945526268\n",
      "Log Regression(1634/4999): loss=0.5437377817912067\n",
      "Log Regression(1635/4999): loss=0.5427056008999811\n",
      "Log Regression(1636/4999): loss=0.5435180024616153\n",
      "Log Regression(1637/4999): loss=0.5428397625347567\n",
      "Log Regression(1638/4999): loss=0.545153004996947\n",
      "Log Regression(1639/4999): loss=0.5474522185621243\n",
      "Log Regression(1640/4999): loss=0.5497323326012918\n",
      "Log Regression(1641/4999): loss=0.544502454639429\n",
      "Log Regression(1642/4999): loss=0.546989535624834\n",
      "Log Regression(1643/4999): loss=0.5438803223811784\n",
      "Log Regression(1644/4999): loss=0.5443134875996393\n",
      "Log Regression(1645/4999): loss=0.5429726506484435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(1646/4999): loss=0.5457718174712617\n",
      "Log Regression(1647/4999): loss=0.5444084072667349\n",
      "Log Regression(1648/4999): loss=0.54298308288193\n",
      "Log Regression(1649/4999): loss=0.5425578682665539\n",
      "Log Regression(1650/4999): loss=0.5446994545899146\n",
      "Log Regression(1651/4999): loss=0.5448128271136129\n",
      "Log Regression(1652/4999): loss=0.5429451918512943\n",
      "Log Regression(1653/4999): loss=0.5425487060528079\n",
      "Log Regression(1654/4999): loss=0.5434146795599509\n",
      "Log Regression(1655/4999): loss=0.5425353277189573\n",
      "Log Regression(1656/4999): loss=0.5426548552349083\n",
      "Log Regression(1657/4999): loss=0.5433850051224098\n",
      "Log Regression(1658/4999): loss=0.5478307774772786\n",
      "Log Regression(1659/4999): loss=0.5438029266077613\n",
      "Log Regression(1660/4999): loss=0.5511444716000761\n",
      "Log Regression(1661/4999): loss=0.5458775899360534\n",
      "Log Regression(1662/4999): loss=0.5466946983589956\n",
      "Log Regression(1663/4999): loss=0.5428853992397483\n",
      "Log Regression(1664/4999): loss=0.542527309523876\n",
      "Log Regression(1665/4999): loss=0.542559371033161\n",
      "Log Regression(1666/4999): loss=0.5426798809966757\n",
      "Log Regression(1667/4999): loss=0.5429373040037667\n",
      "Log Regression(1668/4999): loss=0.5425146574820696\n",
      "Log Regression(1669/4999): loss=0.542530918649394\n",
      "Log Regression(1670/4999): loss=0.5456357359325948\n",
      "Log Regression(1671/4999): loss=0.5462203047506763\n",
      "Log Regression(1672/4999): loss=0.5444571067437249\n",
      "Log Regression(1673/4999): loss=0.542560612146741\n",
      "Log Regression(1674/4999): loss=0.542875567675866\n",
      "Log Regression(1675/4999): loss=0.5427721615227916\n",
      "Log Regression(1676/4999): loss=0.543367259304016\n",
      "Log Regression(1677/4999): loss=0.5427210130497744\n",
      "Log Regression(1678/4999): loss=0.5425343082325487\n",
      "Log Regression(1679/4999): loss=0.5430158911682114\n",
      "Log Regression(1680/4999): loss=0.5430827880443582\n",
      "Log Regression(1681/4999): loss=0.5431181074436126\n",
      "Log Regression(1682/4999): loss=0.5424339344930362\n",
      "Log Regression(1683/4999): loss=0.5424686731026723\n",
      "Log Regression(1684/4999): loss=0.5426018947500946\n",
      "Log Regression(1685/4999): loss=0.5468368947392048\n",
      "Log Regression(1686/4999): loss=0.546516237377975\n",
      "Log Regression(1687/4999): loss=0.5424262015491053\n",
      "Log Regression(1688/4999): loss=0.5424386106223759\n",
      "Log Regression(1689/4999): loss=0.5425961819865953\n",
      "Log Regression(1690/4999): loss=0.5426780949849513\n",
      "Log Regression(1691/4999): loss=0.543786518948735\n",
      "Log Regression(1692/4999): loss=0.5429611808652\n",
      "Log Regression(1693/4999): loss=0.5444206130554556\n",
      "Log Regression(1694/4999): loss=0.5487062321384435\n",
      "Log Regression(1695/4999): loss=0.5537103531797145\n",
      "Log Regression(1696/4999): loss=0.5479013070496469\n",
      "Log Regression(1697/4999): loss=0.5425559443864926\n",
      "Log Regression(1698/4999): loss=0.5455398963948219\n",
      "Log Regression(1699/4999): loss=0.5423420600332208\n",
      "Log Regression(1700/4999): loss=0.5423781719558962\n",
      "Log Regression(1701/4999): loss=0.5433680902729519\n",
      "Log Regression(1702/4999): loss=0.5430411163970649\n",
      "Log Regression(1703/4999): loss=0.5462049115417359\n",
      "Log Regression(1704/4999): loss=0.5439887545571916\n",
      "Log Regression(1705/4999): loss=0.5424110846707118\n",
      "Log Regression(1706/4999): loss=0.5424214942243222\n",
      "Log Regression(1707/4999): loss=0.5425902082634597\n",
      "Log Regression(1708/4999): loss=0.542821861140052\n",
      "Log Regression(1709/4999): loss=0.5427142565670673\n",
      "Log Regression(1710/4999): loss=0.5424841071634501\n",
      "Log Regression(1711/4999): loss=0.5434794317251307\n",
      "Log Regression(1712/4999): loss=0.5423784586969751\n",
      "Log Regression(1713/4999): loss=0.5424155712070646\n",
      "Log Regression(1714/4999): loss=0.5428379843951159\n",
      "Log Regression(1715/4999): loss=0.5443144224010837\n",
      "Log Regression(1716/4999): loss=0.5430633081292462\n",
      "Log Regression(1717/4999): loss=0.5432858131521972\n",
      "Log Regression(1718/4999): loss=0.5434869630915311\n",
      "Log Regression(1719/4999): loss=0.5466940027320885\n",
      "Log Regression(1720/4999): loss=0.5449697207247066\n",
      "Log Regression(1721/4999): loss=0.5453640511264514\n",
      "Log Regression(1722/4999): loss=0.5457314299039377\n",
      "Log Regression(1723/4999): loss=0.5429122976983045\n",
      "Log Regression(1724/4999): loss=0.5427861474617309\n",
      "Log Regression(1725/4999): loss=0.5423166277444387\n",
      "Log Regression(1726/4999): loss=0.5427167370072763\n",
      "Log Regression(1727/4999): loss=0.5423011885042731\n",
      "Log Regression(1728/4999): loss=0.5425244285112651\n",
      "Log Regression(1729/4999): loss=0.5427774862415256\n",
      "Log Regression(1730/4999): loss=0.5422860780392514\n",
      "Log Regression(1731/4999): loss=0.5424443393814302\n",
      "Log Regression(1732/4999): loss=0.5429127957494897\n",
      "Log Regression(1733/4999): loss=0.5486886432909078\n",
      "Log Regression(1734/4999): loss=0.5448646523656098\n",
      "Log Regression(1735/4999): loss=0.5440471215257282\n",
      "Log Regression(1736/4999): loss=0.5424619005696343\n",
      "Log Regression(1737/4999): loss=0.54246410663515\n",
      "Log Regression(1738/4999): loss=0.5440500453521505\n",
      "Log Regression(1739/4999): loss=0.547090257367714\n",
      "Log Regression(1740/4999): loss=0.5468138138182196\n",
      "Log Regression(1741/4999): loss=0.5485408875568483\n",
      "Log Regression(1742/4999): loss=0.5498372160630713\n",
      "Log Regression(1743/4999): loss=0.5557448450636592\n",
      "Log Regression(1744/4999): loss=0.5495728593046629\n",
      "Log Regression(1745/4999): loss=0.5445538531882183\n",
      "Log Regression(1746/4999): loss=0.5442792272238873\n",
      "Log Regression(1747/4999): loss=0.542456480236632\n",
      "Log Regression(1748/4999): loss=0.542271333960258\n",
      "Log Regression(1749/4999): loss=0.5423474545159515\n",
      "Log Regression(1750/4999): loss=0.5424073205844312\n",
      "Log Regression(1751/4999): loss=0.54345747052518\n",
      "Log Regression(1752/4999): loss=0.5439389573830704\n",
      "Log Regression(1753/4999): loss=0.5467701333377499\n",
      "Log Regression(1754/4999): loss=0.5428574193387243\n",
      "Log Regression(1755/4999): loss=0.5429565328871144\n",
      "Log Regression(1756/4999): loss=0.5425963109461435\n",
      "Log Regression(1757/4999): loss=0.543113364615115\n",
      "Log Regression(1758/4999): loss=0.5432905370504423\n",
      "Log Regression(1759/4999): loss=0.5465375220566357\n",
      "Log Regression(1760/4999): loss=0.5425325197185302\n",
      "Log Regression(1761/4999): loss=0.5430950886284717\n",
      "Log Regression(1762/4999): loss=0.5423380497387914\n",
      "Log Regression(1763/4999): loss=0.5431064261765431\n",
      "Log Regression(1764/4999): loss=0.5457697642221306\n",
      "Log Regression(1765/4999): loss=0.5429828689854062\n",
      "Log Regression(1766/4999): loss=0.5428366835147059\n",
      "Log Regression(1767/4999): loss=0.542437004812238\n",
      "Log Regression(1768/4999): loss=0.542739672995077\n",
      "Log Regression(1769/4999): loss=0.5427121220211523\n",
      "Log Regression(1770/4999): loss=0.5455908071969278\n",
      "Log Regression(1771/4999): loss=0.5426403699954631\n",
      "Log Regression(1772/4999): loss=0.5463340067960751\n",
      "Log Regression(1773/4999): loss=0.5470000655309406\n",
      "Log Regression(1774/4999): loss=0.5489155450588569\n",
      "Log Regression(1775/4999): loss=0.5449215806017395\n",
      "Log Regression(1776/4999): loss=0.5460480532097435\n",
      "Log Regression(1777/4999): loss=0.5502465615846908\n",
      "Log Regression(1778/4999): loss=0.5448578453053542\n",
      "Log Regression(1779/4999): loss=0.5430681884411254\n",
      "Log Regression(1780/4999): loss=0.542461480506852\n",
      "Log Regression(1781/4999): loss=0.5437278051656764\n",
      "Log Regression(1782/4999): loss=0.5442533478741336\n",
      "Log Regression(1783/4999): loss=0.5426098320553715\n",
      "Log Regression(1784/4999): loss=0.5424436779620977\n",
      "Log Regression(1785/4999): loss=0.5422532077150607\n",
      "Log Regression(1786/4999): loss=0.5475845755460489\n",
      "Log Regression(1787/4999): loss=0.543870390550169\n",
      "Log Regression(1788/4999): loss=0.5461797867284703\n",
      "Log Regression(1789/4999): loss=0.5432915777067472\n",
      "Log Regression(1790/4999): loss=0.5426092399249403\n",
      "Log Regression(1791/4999): loss=0.5429438428641593\n",
      "Log Regression(1792/4999): loss=0.5430169756656781\n",
      "Log Regression(1793/4999): loss=0.5452438355700046\n",
      "Log Regression(1794/4999): loss=0.5445467967636646\n",
      "Log Regression(1795/4999): loss=0.5441859562562995\n",
      "Log Regression(1796/4999): loss=0.54572391571463\n",
      "Log Regression(1797/4999): loss=0.5452633896830489\n",
      "Log Regression(1798/4999): loss=0.5458681905495333\n",
      "Log Regression(1799/4999): loss=0.5424145729641074\n",
      "Log Regression(1800/4999): loss=0.5424431368324546\n",
      "Log Regression(1801/4999): loss=0.5458005358311885\n",
      "Log Regression(1802/4999): loss=0.5515279932179994\n",
      "Log Regression(1803/4999): loss=0.5432326210082451\n",
      "Log Regression(1804/4999): loss=0.5427464757108635\n",
      "Log Regression(1805/4999): loss=0.5427823523050863\n",
      "Log Regression(1806/4999): loss=0.5439010164337752\n",
      "Log Regression(1807/4999): loss=0.5435086736883782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(1808/4999): loss=0.5451721012455576\n",
      "Log Regression(1809/4999): loss=0.5513311755486996\n",
      "Log Regression(1810/4999): loss=0.5483335817734512\n",
      "Log Regression(1811/4999): loss=0.5488804673392723\n",
      "Log Regression(1812/4999): loss=0.5432017378657991\n",
      "Log Regression(1813/4999): loss=0.5424440474033688\n",
      "Log Regression(1814/4999): loss=0.5423170652916111\n",
      "Log Regression(1815/4999): loss=0.5423453341753116\n",
      "Log Regression(1816/4999): loss=0.5422741316894325\n",
      "Log Regression(1817/4999): loss=0.5424793986881559\n",
      "Log Regression(1818/4999): loss=0.5422411833195894\n",
      "Log Regression(1819/4999): loss=0.542531752897721\n",
      "Log Regression(1820/4999): loss=0.5429988494924691\n",
      "Log Regression(1821/4999): loss=0.5422061584104246\n",
      "Log Regression(1822/4999): loss=0.5454157570642875\n",
      "Log Regression(1823/4999): loss=0.547787301777442\n",
      "Log Regression(1824/4999): loss=0.5428907073848972\n",
      "Log Regression(1825/4999): loss=0.5439886040136238\n",
      "Log Regression(1826/4999): loss=0.5424732486438574\n",
      "Log Regression(1827/4999): loss=0.5421342641581646\n",
      "Log Regression(1828/4999): loss=0.5421807957919029\n",
      "Log Regression(1829/4999): loss=0.5421383541685877\n",
      "Log Regression(1830/4999): loss=0.5435769471475909\n",
      "Log Regression(1831/4999): loss=0.5452423511376034\n",
      "Log Regression(1832/4999): loss=0.5442475283614651\n",
      "Log Regression(1833/4999): loss=0.5447000028374981\n",
      "Log Regression(1834/4999): loss=0.546577440500752\n",
      "Log Regression(1835/4999): loss=0.5450343663391221\n",
      "Log Regression(1836/4999): loss=0.5429548485367532\n",
      "Log Regression(1837/4999): loss=0.5428104153238821\n",
      "Log Regression(1838/4999): loss=0.5426175200541522\n",
      "Log Regression(1839/4999): loss=0.5429396419093229\n",
      "Log Regression(1840/4999): loss=0.5421898739570677\n",
      "Log Regression(1841/4999): loss=0.5422540911583239\n",
      "Log Regression(1842/4999): loss=0.5422792752084329\n",
      "Log Regression(1843/4999): loss=0.5431777650131961\n",
      "Log Regression(1844/4999): loss=0.5435016061807043\n",
      "Log Regression(1845/4999): loss=0.543313607567422\n",
      "Log Regression(1846/4999): loss=0.5430635758482232\n",
      "Log Regression(1847/4999): loss=0.5420726997400532\n",
      "Log Regression(1848/4999): loss=0.542061726943517\n",
      "Log Regression(1849/4999): loss=0.5420166844412628\n",
      "Log Regression(1850/4999): loss=0.5422474386475497\n",
      "Log Regression(1851/4999): loss=0.5434450367273089\n",
      "Log Regression(1852/4999): loss=0.5432400180567735\n",
      "Log Regression(1853/4999): loss=0.5429571882551241\n",
      "Log Regression(1854/4999): loss=0.5429521879774939\n",
      "Log Regression(1855/4999): loss=0.542068539975402\n",
      "Log Regression(1856/4999): loss=0.5449008462045497\n",
      "Log Regression(1857/4999): loss=0.5423786715629042\n",
      "Log Regression(1858/4999): loss=0.542403742104718\n",
      "Log Regression(1859/4999): loss=0.5422419209388764\n",
      "Log Regression(1860/4999): loss=0.5445528734465601\n",
      "Log Regression(1861/4999): loss=0.5434442536744811\n",
      "Log Regression(1862/4999): loss=0.5424680294565447\n",
      "Log Regression(1863/4999): loss=0.5422998268838285\n",
      "Log Regression(1864/4999): loss=0.5459330475808776\n",
      "Log Regression(1865/4999): loss=0.5425863351686696\n",
      "Log Regression(1866/4999): loss=0.5421972816368432\n",
      "Log Regression(1867/4999): loss=0.5435589549602197\n",
      "Log Regression(1868/4999): loss=0.5428611383301482\n",
      "Log Regression(1869/4999): loss=0.5420164110043423\n",
      "Log Regression(1870/4999): loss=0.5420407146503152\n",
      "Log Regression(1871/4999): loss=0.542499899099621\n",
      "Log Regression(1872/4999): loss=0.5421112097540688\n",
      "Log Regression(1873/4999): loss=0.5424959099733565\n",
      "Log Regression(1874/4999): loss=0.5422588512410149\n",
      "Log Regression(1875/4999): loss=0.5441766677976096\n",
      "Log Regression(1876/4999): loss=0.5432176587706393\n",
      "Log Regression(1877/4999): loss=0.5421120873807497\n",
      "Log Regression(1878/4999): loss=0.5419466241191521\n",
      "Log Regression(1879/4999): loss=0.5425668728685774\n",
      "Log Regression(1880/4999): loss=0.5429841807053736\n",
      "Log Regression(1881/4999): loss=0.5419874292806274\n",
      "Log Regression(1882/4999): loss=0.5419732239252216\n",
      "Log Regression(1883/4999): loss=0.5421012701502035\n",
      "Log Regression(1884/4999): loss=0.5419452197188871\n",
      "Log Regression(1885/4999): loss=0.5427579526173665\n",
      "Log Regression(1886/4999): loss=0.5428623658926182\n",
      "Log Regression(1887/4999): loss=0.5520182084380817\n",
      "Log Regression(1888/4999): loss=0.5431283234369901\n",
      "Log Regression(1889/4999): loss=0.542058758234069\n",
      "Log Regression(1890/4999): loss=0.5421120394487312\n",
      "Log Regression(1891/4999): loss=0.5434241670380068\n",
      "Log Regression(1892/4999): loss=0.5443140584046934\n",
      "Log Regression(1893/4999): loss=0.5422640923067248\n",
      "Log Regression(1894/4999): loss=0.5432713563440043\n",
      "Log Regression(1895/4999): loss=0.5421013867893942\n",
      "Log Regression(1896/4999): loss=0.5420106308533362\n",
      "Log Regression(1897/4999): loss=0.5428344841149048\n",
      "Log Regression(1898/4999): loss=0.5435027144296867\n",
      "Log Regression(1899/4999): loss=0.5421124593157287\n",
      "Log Regression(1900/4999): loss=0.5425364320008041\n",
      "Log Regression(1901/4999): loss=0.5428916585375377\n",
      "Log Regression(1902/4999): loss=0.5420410589612117\n",
      "Log Regression(1903/4999): loss=0.5440398538458773\n",
      "Log Regression(1904/4999): loss=0.544397981211878\n",
      "Log Regression(1905/4999): loss=0.543673036700321\n",
      "Log Regression(1906/4999): loss=0.5436443745173424\n",
      "Log Regression(1907/4999): loss=0.5432113823608332\n",
      "Log Regression(1908/4999): loss=0.541978753250343\n",
      "Log Regression(1909/4999): loss=0.5423276077566164\n",
      "Log Regression(1910/4999): loss=0.5435425311748934\n",
      "Log Regression(1911/4999): loss=0.5436644674845068\n",
      "Log Regression(1912/4999): loss=0.5421164171525104\n",
      "Log Regression(1913/4999): loss=0.5419393798579906\n",
      "Log Regression(1914/4999): loss=0.5420736860214114\n",
      "Log Regression(1915/4999): loss=0.5435194878831137\n",
      "Log Regression(1916/4999): loss=0.5423962997651742\n",
      "Log Regression(1917/4999): loss=0.5427640498795426\n",
      "Log Regression(1918/4999): loss=0.5424338260434355\n",
      "Log Regression(1919/4999): loss=0.5423184586635437\n",
      "Log Regression(1920/4999): loss=0.5446431175383818\n",
      "Log Regression(1921/4999): loss=0.5424830580682117\n",
      "Log Regression(1922/4999): loss=0.5431582071862318\n",
      "Log Regression(1923/4999): loss=0.5420936017087054\n",
      "Log Regression(1924/4999): loss=0.5460546224449683\n",
      "Log Regression(1925/4999): loss=0.5423993416462118\n",
      "Log Regression(1926/4999): loss=0.5422675586209249\n",
      "Log Regression(1927/4999): loss=0.5428197875131752\n",
      "Log Regression(1928/4999): loss=0.5425508800286359\n",
      "Log Regression(1929/4999): loss=0.5427605320084458\n",
      "Log Regression(1930/4999): loss=0.5437039503413287\n",
      "Log Regression(1931/4999): loss=0.5435214382070722\n",
      "Log Regression(1932/4999): loss=0.5422611685158546\n",
      "Log Regression(1933/4999): loss=0.5436435778931563\n",
      "Log Regression(1934/4999): loss=0.542413538305752\n",
      "Log Regression(1935/4999): loss=0.5436607285047124\n",
      "Log Regression(1936/4999): loss=0.544378377081452\n",
      "Log Regression(1937/4999): loss=0.5432990875021798\n",
      "Log Regression(1938/4999): loss=0.5434172401956745\n",
      "Log Regression(1939/4999): loss=0.5454661813621159\n",
      "Log Regression(1940/4999): loss=0.5453001316102385\n",
      "Log Regression(1941/4999): loss=0.5429439184846803\n",
      "Log Regression(1942/4999): loss=0.5425771305715444\n",
      "Log Regression(1943/4999): loss=0.5445198121074443\n",
      "Log Regression(1944/4999): loss=0.5470220171108489\n",
      "Log Regression(1945/4999): loss=0.5433966720178387\n",
      "Log Regression(1946/4999): loss=0.5437244554948578\n",
      "Log Regression(1947/4999): loss=0.5444067111370339\n",
      "Log Regression(1948/4999): loss=0.5441786368460158\n",
      "Log Regression(1949/4999): loss=0.5432314166965405\n",
      "Log Regression(1950/4999): loss=0.5420959649400211\n",
      "Log Regression(1951/4999): loss=0.5473186826350491\n",
      "Log Regression(1952/4999): loss=0.5444799211408314\n",
      "Log Regression(1953/4999): loss=0.5454578669965102\n",
      "Log Regression(1954/4999): loss=0.5466582499681002\n",
      "Log Regression(1955/4999): loss=0.5444460869661467\n",
      "Log Regression(1956/4999): loss=0.5420120336998235\n",
      "Log Regression(1957/4999): loss=0.5429207079845559\n",
      "Log Regression(1958/4999): loss=0.5430857481058683\n",
      "Log Regression(1959/4999): loss=0.5419271255255449\n",
      "Log Regression(1960/4999): loss=0.5428186025953771\n",
      "Log Regression(1961/4999): loss=0.5423370759929418\n",
      "Log Regression(1962/4999): loss=0.5430277608102054\n",
      "Log Regression(1963/4999): loss=0.545013798050254\n",
      "Log Regression(1964/4999): loss=0.5434409380020385\n",
      "Log Regression(1965/4999): loss=0.5418011152983143\n",
      "Log Regression(1966/4999): loss=0.5427288590112664\n",
      "Log Regression(1967/4999): loss=0.5419080676502983\n",
      "Log Regression(1968/4999): loss=0.5418646360732067\n",
      "Log Regression(1969/4999): loss=0.5421138822765275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(1970/4999): loss=0.5447306095407446\n",
      "Log Regression(1971/4999): loss=0.5440708141660722\n",
      "Log Regression(1972/4999): loss=0.5420749834860887\n",
      "Log Regression(1973/4999): loss=0.5426112917841309\n",
      "Log Regression(1974/4999): loss=0.5417862082434349\n",
      "Log Regression(1975/4999): loss=0.5423076461078433\n",
      "Log Regression(1976/4999): loss=0.5417900435335147\n",
      "Log Regression(1977/4999): loss=0.5419468817550696\n",
      "Log Regression(1978/4999): loss=0.5418313323449636\n",
      "Log Regression(1979/4999): loss=0.5419723855951579\n",
      "Log Regression(1980/4999): loss=0.5418173465356454\n",
      "Log Regression(1981/4999): loss=0.5439155669306065\n",
      "Log Regression(1982/4999): loss=0.5506233915704812\n",
      "Log Regression(1983/4999): loss=0.547603979615032\n",
      "Log Regression(1984/4999): loss=0.5453777855227909\n",
      "Log Regression(1985/4999): loss=0.5438855627431743\n",
      "Log Regression(1986/4999): loss=0.5459629068586312\n",
      "Log Regression(1987/4999): loss=0.5485147787922064\n",
      "Log Regression(1988/4999): loss=0.5452109516109386\n",
      "Log Regression(1989/4999): loss=0.5421441210433893\n",
      "Log Regression(1990/4999): loss=0.5418109288788697\n",
      "Log Regression(1991/4999): loss=0.5441481189919332\n",
      "Log Regression(1992/4999): loss=0.5422378127811666\n",
      "Log Regression(1993/4999): loss=0.5418556557905427\n",
      "Log Regression(1994/4999): loss=0.5428731520784352\n",
      "Log Regression(1995/4999): loss=0.5427950305301813\n",
      "Log Regression(1996/4999): loss=0.5420136065799269\n",
      "Log Regression(1997/4999): loss=0.5449724908497157\n",
      "Log Regression(1998/4999): loss=0.5418818323862464\n",
      "Log Regression(1999/4999): loss=0.5419877246541203\n",
      "Log Regression(2000/4999): loss=0.5421993663842853\n",
      "Log Regression(2001/4999): loss=0.542512746010252\n",
      "Log Regression(2002/4999): loss=0.543063264877512\n",
      "Log Regression(2003/4999): loss=0.5429016783360199\n",
      "Log Regression(2004/4999): loss=0.5433047010127993\n",
      "Log Regression(2005/4999): loss=0.5423046409886292\n",
      "Log Regression(2006/4999): loss=0.5422040604068592\n",
      "Log Regression(2007/4999): loss=0.5419198213412786\n",
      "Log Regression(2008/4999): loss=0.5420329938641367\n",
      "Log Regression(2009/4999): loss=0.5450041616428525\n",
      "Log Regression(2010/4999): loss=0.5461314854841716\n",
      "Log Regression(2011/4999): loss=0.5437167850212308\n",
      "Log Regression(2012/4999): loss=0.5425720629628196\n",
      "Log Regression(2013/4999): loss=0.541904606053446\n",
      "Log Regression(2014/4999): loss=0.542238689291858\n",
      "Log Regression(2015/4999): loss=0.5418729115236003\n",
      "Log Regression(2016/4999): loss=0.5417867275101736\n",
      "Log Regression(2017/4999): loss=0.542054233282298\n",
      "Log Regression(2018/4999): loss=0.541868869428138\n",
      "Log Regression(2019/4999): loss=0.5418517757819631\n",
      "Log Regression(2020/4999): loss=0.5417153418621945\n",
      "Log Regression(2021/4999): loss=0.5438043858350537\n",
      "Log Regression(2022/4999): loss=0.5417924879112573\n",
      "Log Regression(2023/4999): loss=0.5420159634002172\n",
      "Log Regression(2024/4999): loss=0.5442638518080505\n",
      "Log Regression(2025/4999): loss=0.5428739357539656\n",
      "Log Regression(2026/4999): loss=0.5419556106783497\n",
      "Log Regression(2027/4999): loss=0.5419301752756442\n",
      "Log Regression(2028/4999): loss=0.5456950680013761\n",
      "Log Regression(2029/4999): loss=0.5472327947279662\n",
      "Log Regression(2030/4999): loss=0.5419221910085311\n",
      "Log Regression(2031/4999): loss=0.5438188150131239\n",
      "Log Regression(2032/4999): loss=0.545851731418265\n",
      "Log Regression(2033/4999): loss=0.5455457924008795\n",
      "Log Regression(2034/4999): loss=0.5445543128978311\n",
      "Log Regression(2035/4999): loss=0.5456270644390333\n",
      "Log Regression(2036/4999): loss=0.5421120162656773\n",
      "Log Regression(2037/4999): loss=0.5419645258793873\n",
      "Log Regression(2038/4999): loss=0.54205138334268\n",
      "Log Regression(2039/4999): loss=0.5419430758855223\n",
      "Log Regression(2040/4999): loss=0.5421162544188513\n",
      "Log Regression(2041/4999): loss=0.5427217011237108\n",
      "Log Regression(2042/4999): loss=0.5475485478861203\n",
      "Log Regression(2043/4999): loss=0.5517250999456901\n",
      "Log Regression(2044/4999): loss=0.5473302082515756\n",
      "Log Regression(2045/4999): loss=0.5476272578797847\n",
      "Log Regression(2046/4999): loss=0.5450557470333751\n",
      "Log Regression(2047/4999): loss=0.5447946207561315\n",
      "Log Regression(2048/4999): loss=0.5447574787088392\n",
      "Log Regression(2049/4999): loss=0.5432800062535739\n",
      "Log Regression(2050/4999): loss=0.5420931305837893\n",
      "Log Regression(2051/4999): loss=0.5418887421568679\n",
      "Log Regression(2052/4999): loss=0.5447231270178154\n",
      "Log Regression(2053/4999): loss=0.5418622112384233\n",
      "Log Regression(2054/4999): loss=0.5418443354083755\n",
      "Log Regression(2055/4999): loss=0.5421209716489165\n",
      "Log Regression(2056/4999): loss=0.5419665129234225\n",
      "Log Regression(2057/4999): loss=0.5435722248595996\n",
      "Log Regression(2058/4999): loss=0.5423420304940472\n",
      "Log Regression(2059/4999): loss=0.543561282224995\n",
      "Log Regression(2060/4999): loss=0.543694068607622\n",
      "Log Regression(2061/4999): loss=0.5442783468463664\n",
      "Log Regression(2062/4999): loss=0.5439723267972372\n",
      "Log Regression(2063/4999): loss=0.546178336902662\n",
      "Log Regression(2064/4999): loss=0.5419621032261913\n",
      "Log Regression(2065/4999): loss=0.5419652473178995\n",
      "Log Regression(2066/4999): loss=0.5424855005313196\n",
      "Log Regression(2067/4999): loss=0.5418903779064729\n",
      "Log Regression(2068/4999): loss=0.5440838536863721\n",
      "Log Regression(2069/4999): loss=0.5477890570846712\n",
      "Log Regression(2070/4999): loss=0.5471267952070387\n",
      "Log Regression(2071/4999): loss=0.5428133832741889\n",
      "Log Regression(2072/4999): loss=0.5424180220684935\n",
      "Log Regression(2073/4999): loss=0.5416604348838965\n",
      "Log Regression(2074/4999): loss=0.5416579277989899\n",
      "Log Regression(2075/4999): loss=0.5420375456765417\n",
      "Log Regression(2076/4999): loss=0.5440155412788507\n",
      "Log Regression(2077/4999): loss=0.5461345385720009\n",
      "Log Regression(2078/4999): loss=0.5456059922778093\n",
      "Log Regression(2079/4999): loss=0.5444551639181718\n",
      "Log Regression(2080/4999): loss=0.5444466282239359\n",
      "Log Regression(2081/4999): loss=0.5460524237659106\n",
      "Log Regression(2082/4999): loss=0.5424389254949484\n",
      "Log Regression(2083/4999): loss=0.5423634582358259\n",
      "Log Regression(2084/4999): loss=0.5418539064798423\n",
      "Log Regression(2085/4999): loss=0.5419926419389024\n",
      "Log Regression(2086/4999): loss=0.5416960521653822\n",
      "Log Regression(2087/4999): loss=0.5421142318898531\n",
      "Log Regression(2088/4999): loss=0.5416797944839447\n",
      "Log Regression(2089/4999): loss=0.5438433019108485\n",
      "Log Regression(2090/4999): loss=0.544220995934645\n",
      "Log Regression(2091/4999): loss=0.5429284996913284\n",
      "Log Regression(2092/4999): loss=0.5430737825950579\n",
      "Log Regression(2093/4999): loss=0.5450645238125358\n",
      "Log Regression(2094/4999): loss=0.5430992096175864\n",
      "Log Regression(2095/4999): loss=0.5418264010863444\n",
      "Log Regression(2096/4999): loss=0.5435783977283001\n",
      "Log Regression(2097/4999): loss=0.5436616971252505\n",
      "Log Regression(2098/4999): loss=0.5429819480417201\n",
      "Log Regression(2099/4999): loss=0.5426038350754484\n",
      "Log Regression(2100/4999): loss=0.5420451023720041\n",
      "Log Regression(2101/4999): loss=0.5428387042349898\n",
      "Log Regression(2102/4999): loss=0.5417411482151293\n",
      "Log Regression(2103/4999): loss=0.5427142081784866\n",
      "Log Regression(2104/4999): loss=0.542041983915712\n",
      "Log Regression(2105/4999): loss=0.5417322646650112\n",
      "Log Regression(2106/4999): loss=0.5426266788257881\n",
      "Log Regression(2107/4999): loss=0.5425857852829198\n",
      "Log Regression(2108/4999): loss=0.5419015521951518\n",
      "Log Regression(2109/4999): loss=0.5433728993551171\n",
      "Log Regression(2110/4999): loss=0.54797446877123\n",
      "Log Regression(2111/4999): loss=0.5463506852778325\n",
      "Log Regression(2112/4999): loss=0.5419914867026703\n",
      "Log Regression(2113/4999): loss=0.5424616877128352\n",
      "Log Regression(2114/4999): loss=0.5482193345659744\n",
      "Log Regression(2115/4999): loss=0.5417919383446373\n",
      "Log Regression(2116/4999): loss=0.5422754898128134\n",
      "Log Regression(2117/4999): loss=0.5420690226914788\n",
      "Log Regression(2118/4999): loss=0.5417627766510672\n",
      "Log Regression(2119/4999): loss=0.5422258667298525\n",
      "Log Regression(2120/4999): loss=0.5465408559197868\n",
      "Log Regression(2121/4999): loss=0.5435672599806733\n",
      "Log Regression(2122/4999): loss=0.5430567734028958\n",
      "Log Regression(2123/4999): loss=0.5422093504096165\n",
      "Log Regression(2124/4999): loss=0.5416632116178756\n",
      "Log Regression(2125/4999): loss=0.5416841878173778\n",
      "Log Regression(2126/4999): loss=0.5421940970832355\n",
      "Log Regression(2127/4999): loss=0.541639368993723\n",
      "Log Regression(2128/4999): loss=0.5415991203689136\n",
      "Log Regression(2129/4999): loss=0.542017429739135\n",
      "Log Regression(2130/4999): loss=0.542357700586407\n",
      "Log Regression(2131/4999): loss=0.5418943615055282\n",
      "Log Regression(2132/4999): loss=0.5416294106940123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(2133/4999): loss=0.5434718006887275\n",
      "Log Regression(2134/4999): loss=0.5425792103874361\n",
      "Log Regression(2135/4999): loss=0.5443474275261514\n",
      "Log Regression(2136/4999): loss=0.5416244111137289\n",
      "Log Regression(2137/4999): loss=0.5417471891353125\n",
      "Log Regression(2138/4999): loss=0.5425783268048706\n",
      "Log Regression(2139/4999): loss=0.5505754079428269\n",
      "Log Regression(2140/4999): loss=0.5429156396355758\n",
      "Log Regression(2141/4999): loss=0.5497513093213198\n",
      "Log Regression(2142/4999): loss=0.5434979142787488\n",
      "Log Regression(2143/4999): loss=0.5418450841612417\n",
      "Log Regression(2144/4999): loss=0.5431424256044162\n",
      "Log Regression(2145/4999): loss=0.5472300957353533\n",
      "Log Regression(2146/4999): loss=0.5443529916567366\n",
      "Log Regression(2147/4999): loss=0.5423197416926007\n",
      "Log Regression(2148/4999): loss=0.5416172633988374\n",
      "Log Regression(2149/4999): loss=0.5419382406206646\n",
      "Log Regression(2150/4999): loss=0.5420812073531389\n",
      "Log Regression(2151/4999): loss=0.5495232031549514\n",
      "Log Regression(2152/4999): loss=0.5463770542446075\n",
      "Log Regression(2153/4999): loss=0.5489411053426735\n",
      "Log Regression(2154/4999): loss=0.5455547717051847\n",
      "Log Regression(2155/4999): loss=0.5439148985454505\n",
      "Log Regression(2156/4999): loss=0.5425968663722498\n",
      "Log Regression(2157/4999): loss=0.5416353175651607\n",
      "Log Regression(2158/4999): loss=0.5415655321984412\n",
      "Log Regression(2159/4999): loss=0.5419362796777419\n",
      "Log Regression(2160/4999): loss=0.5432396670957813\n",
      "Log Regression(2161/4999): loss=0.5416402824458684\n",
      "Log Regression(2162/4999): loss=0.5417569691066335\n",
      "Log Regression(2163/4999): loss=0.5433346921176397\n",
      "Log Regression(2164/4999): loss=0.5416885979179373\n",
      "Log Regression(2165/4999): loss=0.5417234522173583\n",
      "Log Regression(2166/4999): loss=0.5419031791603173\n",
      "Log Regression(2167/4999): loss=0.5456284998164628\n",
      "Log Regression(2168/4999): loss=0.5443744635655312\n",
      "Log Regression(2169/4999): loss=0.5446971461274059\n",
      "Log Regression(2170/4999): loss=0.5425107529860037\n",
      "Log Regression(2171/4999): loss=0.5426991807516081\n",
      "Log Regression(2172/4999): loss=0.5419251148978868\n",
      "Log Regression(2173/4999): loss=0.5418989234320499\n",
      "Log Regression(2174/4999): loss=0.5419181173489795\n",
      "Log Regression(2175/4999): loss=0.5430235605276607\n",
      "Log Regression(2176/4999): loss=0.542980824166055\n",
      "Log Regression(2177/4999): loss=0.5487242553733135\n",
      "Log Regression(2178/4999): loss=0.5430893347201695\n",
      "Log Regression(2179/4999): loss=0.5438939737549273\n",
      "Log Regression(2180/4999): loss=0.5422911408915122\n",
      "Log Regression(2181/4999): loss=0.5417855388501632\n",
      "Log Regression(2182/4999): loss=0.5426932329701937\n",
      "Log Regression(2183/4999): loss=0.5439203876021772\n",
      "Log Regression(2184/4999): loss=0.5417260523119227\n",
      "Log Regression(2185/4999): loss=0.5420440002582211\n",
      "Log Regression(2186/4999): loss=0.5442647785699297\n",
      "Log Regression(2187/4999): loss=0.5440686998054571\n",
      "Log Regression(2188/4999): loss=0.5464956472637873\n",
      "Log Regression(2189/4999): loss=0.5457291773750569\n",
      "Log Regression(2190/4999): loss=0.5511736234399048\n",
      "Log Regression(2191/4999): loss=0.5432242559231051\n",
      "Log Regression(2192/4999): loss=0.5422749127611782\n",
      "Log Regression(2193/4999): loss=0.5419740752441727\n",
      "Log Regression(2194/4999): loss=0.5417942323093231\n",
      "Log Regression(2195/4999): loss=0.5419627911513595\n",
      "Log Regression(2196/4999): loss=0.5464284131217926\n",
      "Log Regression(2197/4999): loss=0.5644879021242781\n",
      "Log Regression(2198/4999): loss=0.5631046489466741\n",
      "Log Regression(2199/4999): loss=0.5612172743632943\n",
      "Log Regression(2200/4999): loss=0.5444179572090836\n",
      "Log Regression(2201/4999): loss=0.5478613782449077\n",
      "Log Regression(2202/4999): loss=0.542481696054615\n",
      "Log Regression(2203/4999): loss=0.5418823329630098\n",
      "Log Regression(2204/4999): loss=0.5436040877384691\n",
      "Log Regression(2205/4999): loss=0.5420891227682183\n",
      "Log Regression(2206/4999): loss=0.542144885301436\n",
      "Log Regression(2207/4999): loss=0.5417833947147532\n",
      "Log Regression(2208/4999): loss=0.5437616011642094\n",
      "Log Regression(2209/4999): loss=0.5416542384690364\n",
      "Log Regression(2210/4999): loss=0.544214039568741\n",
      "Log Regression(2211/4999): loss=0.5418700145254908\n",
      "Log Regression(2212/4999): loss=0.5418682317511259\n",
      "Log Regression(2213/4999): loss=0.5423112049568946\n",
      "Log Regression(2214/4999): loss=0.5419691271437708\n",
      "Log Regression(2215/4999): loss=0.5417040418493987\n",
      "Log Regression(2216/4999): loss=0.5418108878137439\n",
      "Log Regression(2217/4999): loss=0.5456049948955939\n",
      "Log Regression(2218/4999): loss=0.5418129186147\n",
      "Log Regression(2219/4999): loss=0.5417615568667167\n",
      "Log Regression(2220/4999): loss=0.5420170294217306\n",
      "Log Regression(2221/4999): loss=0.5434404169564266\n",
      "Log Regression(2222/4999): loss=0.5416686471570082\n",
      "Log Regression(2223/4999): loss=0.5472251257956967\n",
      "Log Regression(2224/4999): loss=0.5472734882961352\n",
      "Log Regression(2225/4999): loss=0.5427287013589165\n",
      "Log Regression(2226/4999): loss=0.5443813215072815\n",
      "Log Regression(2227/4999): loss=0.543289015383378\n",
      "Log Regression(2228/4999): loss=0.5416554056447982\n",
      "Log Regression(2229/4999): loss=0.541987927712487\n",
      "Log Regression(2230/4999): loss=0.542449715253462\n",
      "Log Regression(2231/4999): loss=0.544787972448514\n",
      "Log Regression(2232/4999): loss=0.5466920319571268\n",
      "Log Regression(2233/4999): loss=0.5470480090544203\n",
      "Log Regression(2234/4999): loss=0.5423267926752258\n",
      "Log Regression(2235/4999): loss=0.5455119345115244\n",
      "Log Regression(2236/4999): loss=0.5415037129905368\n",
      "Log Regression(2237/4999): loss=0.5414566928314314\n",
      "Log Regression(2238/4999): loss=0.5421905683132909\n",
      "Log Regression(2239/4999): loss=0.5416499823388782\n",
      "Log Regression(2240/4999): loss=0.5419210245340129\n",
      "Log Regression(2241/4999): loss=0.5414624166147144\n",
      "Log Regression(2242/4999): loss=0.541651621149156\n",
      "Log Regression(2243/4999): loss=0.5418437575664664\n",
      "Log Regression(2244/4999): loss=0.5414448723866465\n",
      "Log Regression(2245/4999): loss=0.5418636073634806\n",
      "Log Regression(2246/4999): loss=0.5419924787966492\n",
      "Log Regression(2247/4999): loss=0.5423297989143226\n",
      "Log Regression(2248/4999): loss=0.5437082100999951\n",
      "Log Regression(2249/4999): loss=0.5417555763255013\n",
      "Log Regression(2250/4999): loss=0.541423168840921\n",
      "Log Regression(2251/4999): loss=0.5415490628186571\n",
      "Log Regression(2252/4999): loss=0.5441146176277183\n",
      "Log Regression(2253/4999): loss=0.5426250062748397\n",
      "Log Regression(2254/4999): loss=0.541426011087296\n",
      "Log Regression(2255/4999): loss=0.541587992070382\n",
      "Log Regression(2256/4999): loss=0.5414160113802073\n",
      "Log Regression(2257/4999): loss=0.5454073385594813\n",
      "Log Regression(2258/4999): loss=0.5419562647873711\n",
      "Log Regression(2259/4999): loss=0.5435607482211193\n",
      "Log Regression(2260/4999): loss=0.5415277971863601\n",
      "Log Regression(2261/4999): loss=0.5469619619862658\n",
      "Log Regression(2262/4999): loss=0.5511171600475522\n",
      "Log Regression(2263/4999): loss=0.5460647465750844\n",
      "Log Regression(2264/4999): loss=0.5416556970902495\n",
      "Log Regression(2265/4999): loss=0.5418087431753392\n",
      "Log Regression(2266/4999): loss=0.5415687766397361\n",
      "Log Regression(2267/4999): loss=0.5416539273953067\n",
      "Log Regression(2268/4999): loss=0.5422953782461534\n",
      "Log Regression(2269/4999): loss=0.544114130679579\n",
      "Log Regression(2270/4999): loss=0.5415734716765722\n",
      "Log Regression(2271/4999): loss=0.5420283715211561\n",
      "Log Regression(2272/4999): loss=0.5414501692330883\n",
      "Log Regression(2273/4999): loss=0.541469500708035\n",
      "Log Regression(2274/4999): loss=0.5414235766154448\n",
      "Log Regression(2275/4999): loss=0.5413641663114857\n",
      "Log Regression(2276/4999): loss=0.5426049982765605\n",
      "Log Regression(2277/4999): loss=0.5415658249473415\n",
      "Log Regression(2278/4999): loss=0.5414360546813866\n",
      "Log Regression(2279/4999): loss=0.5419627813740301\n",
      "Log Regression(2280/4999): loss=0.5414624221538051\n",
      "Log Regression(2281/4999): loss=0.5415221882814482\n",
      "Log Regression(2282/4999): loss=0.5414990217435354\n",
      "Log Regression(2283/4999): loss=0.5419256215393388\n",
      "Log Regression(2284/4999): loss=0.5414262912436378\n",
      "Log Regression(2285/4999): loss=0.5416808094750076\n",
      "Log Regression(2286/4999): loss=0.5414484721596405\n",
      "Log Regression(2287/4999): loss=0.5415770466284571\n",
      "Log Regression(2288/4999): loss=0.5419497134153016\n",
      "Log Regression(2289/4999): loss=0.5429682752945723\n",
      "Log Regression(2290/4999): loss=0.5431010985753818\n",
      "Log Regression(2291/4999): loss=0.5416412714044571\n",
      "Log Regression(2292/4999): loss=0.5421830037732321\n",
      "Log Regression(2293/4999): loss=0.5414741409206942\n",
      "Log Regression(2294/4999): loss=0.5414200348158583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(2295/4999): loss=0.5414535688194247\n",
      "Log Regression(2296/4999): loss=0.5413402941500168\n",
      "Log Regression(2297/4999): loss=0.5413793813110434\n",
      "Log Regression(2298/4999): loss=0.5417549531670421\n",
      "Log Regression(2299/4999): loss=0.5413371291003092\n",
      "Log Regression(2300/4999): loss=0.5414106768176369\n",
      "Log Regression(2301/4999): loss=0.5418375989908968\n",
      "Log Regression(2302/4999): loss=0.5436281015363047\n",
      "Log Regression(2303/4999): loss=0.5414529983487\n",
      "Log Regression(2304/4999): loss=0.5415209139519356\n",
      "Log Regression(2305/4999): loss=0.5425690613436144\n",
      "Log Regression(2306/4999): loss=0.541707379258598\n",
      "Log Regression(2307/4999): loss=0.5416813557566503\n",
      "Log Regression(2308/4999): loss=0.5413515478043778\n",
      "Log Regression(2309/4999): loss=0.5416452239756457\n",
      "Log Regression(2310/4999): loss=0.5414866629070343\n",
      "Log Regression(2311/4999): loss=0.54213121413172\n",
      "Log Regression(2312/4999): loss=0.5414635865598814\n",
      "Log Regression(2313/4999): loss=0.5413993312254501\n",
      "Log Regression(2314/4999): loss=0.5421142360259068\n",
      "Log Regression(2315/4999): loss=0.5414798783590931\n",
      "Log Regression(2316/4999): loss=0.54338593888012\n",
      "Log Regression(2317/4999): loss=0.5507059418177742\n",
      "Log Regression(2318/4999): loss=0.5515587662556621\n",
      "Log Regression(2319/4999): loss=0.5502894795954898\n",
      "Log Regression(2320/4999): loss=0.5443281737589231\n",
      "Log Regression(2321/4999): loss=0.5475890349641406\n",
      "Log Regression(2322/4999): loss=0.547505625539203\n",
      "Log Regression(2323/4999): loss=0.5428106413198015\n",
      "Log Regression(2324/4999): loss=0.5413589467569625\n",
      "Log Regression(2325/4999): loss=0.5422850934759236\n",
      "Log Regression(2326/4999): loss=0.545740162363293\n",
      "Log Regression(2327/4999): loss=0.5437128390690309\n",
      "Log Regression(2328/4999): loss=0.5417955143648572\n",
      "Log Regression(2329/4999): loss=0.5415932171432635\n",
      "Log Regression(2330/4999): loss=0.541372654135948\n",
      "Log Regression(2331/4999): loss=0.5442403831447045\n",
      "Log Regression(2332/4999): loss=0.5413106804389666\n",
      "Log Regression(2333/4999): loss=0.5424265664912987\n",
      "Log Regression(2334/4999): loss=0.5469068323114354\n",
      "Log Regression(2335/4999): loss=0.5501079910636116\n",
      "Log Regression(2336/4999): loss=0.5460578356339301\n",
      "Log Regression(2337/4999): loss=0.5423803202381615\n",
      "Log Regression(2338/4999): loss=0.5423581609014918\n",
      "Log Regression(2339/4999): loss=0.5419436218898523\n",
      "Log Regression(2340/4999): loss=0.5417760156074383\n",
      "Log Regression(2341/4999): loss=0.5430519749294788\n",
      "Log Regression(2342/4999): loss=0.5432353167247298\n",
      "Log Regression(2343/4999): loss=0.5436060253249193\n",
      "Log Regression(2344/4999): loss=0.5430857709092204\n",
      "Log Regression(2345/4999): loss=0.5477246885234702\n",
      "Log Regression(2346/4999): loss=0.5458609214542086\n",
      "Log Regression(2347/4999): loss=0.5427895805056361\n",
      "Log Regression(2348/4999): loss=0.5447555748618926\n",
      "Log Regression(2349/4999): loss=0.5430351115936874\n",
      "Log Regression(2350/4999): loss=0.5487603962391915\n",
      "Log Regression(2351/4999): loss=0.5450789892033866\n",
      "Log Regression(2352/4999): loss=0.548906912235281\n",
      "Log Regression(2353/4999): loss=0.552213123927277\n",
      "Log Regression(2354/4999): loss=0.5481593311403105\n",
      "Log Regression(2355/4999): loss=0.541556243287067\n",
      "Log Regression(2356/4999): loss=0.5415063788639064\n",
      "Log Regression(2357/4999): loss=0.5426166427298424\n",
      "Log Regression(2358/4999): loss=0.5416387917240849\n",
      "Log Regression(2359/4999): loss=0.5417265359652991\n",
      "Log Regression(2360/4999): loss=0.5446202757202204\n",
      "Log Regression(2361/4999): loss=0.5442488343721158\n",
      "Log Regression(2362/4999): loss=0.5416371673084213\n",
      "Log Regression(2363/4999): loss=0.5413114641478605\n",
      "Log Regression(2364/4999): loss=0.5412632952692208\n",
      "Log Regression(2365/4999): loss=0.5413405310351695\n",
      "Log Regression(2366/4999): loss=0.5414450315506891\n",
      "Log Regression(2367/4999): loss=0.5428190954728385\n",
      "Log Regression(2368/4999): loss=0.5461435944814953\n",
      "Log Regression(2369/4999): loss=0.5472882327364706\n",
      "Log Regression(2370/4999): loss=0.5430287605796094\n",
      "Log Regression(2371/4999): loss=0.5418685630493891\n",
      "Log Regression(2372/4999): loss=0.5414081692260792\n",
      "Log Regression(2373/4999): loss=0.5412697207216078\n",
      "Log Regression(2374/4999): loss=0.5420254551438832\n",
      "Log Regression(2375/4999): loss=0.5419701723553414\n",
      "Log Regression(2376/4999): loss=0.5412284536736924\n",
      "Log Regression(2377/4999): loss=0.5416961215033398\n",
      "Log Regression(2378/4999): loss=0.5412323582589769\n",
      "Log Regression(2379/4999): loss=0.5412381312739909\n",
      "Log Regression(2380/4999): loss=0.5424303366821361\n",
      "Log Regression(2381/4999): loss=0.5423126916118748\n",
      "Log Regression(2382/4999): loss=0.5413059129067577\n",
      "Log Regression(2383/4999): loss=0.541612726230642\n",
      "Log Regression(2384/4999): loss=0.5462692248465533\n",
      "Log Regression(2385/4999): loss=0.5508627375301939\n",
      "Log Regression(2386/4999): loss=0.5558088791467328\n",
      "Log Regression(2387/4999): loss=0.5573447532526541\n",
      "Log Regression(2388/4999): loss=0.5540725152072112\n",
      "Log Regression(2389/4999): loss=0.5461931041044755\n",
      "Log Regression(2390/4999): loss=0.5415576964693577\n",
      "Log Regression(2391/4999): loss=0.541526007899517\n",
      "Log Regression(2392/4999): loss=0.5416946906259041\n",
      "Log Regression(2393/4999): loss=0.5412659554306342\n",
      "Log Regression(2394/4999): loss=0.5415166270865243\n",
      "Log Regression(2395/4999): loss=0.5416693003515138\n",
      "Log Regression(2396/4999): loss=0.5422633379135622\n",
      "Log Regression(2397/4999): loss=0.5412425231219368\n",
      "Log Regression(2398/4999): loss=0.5418420389243188\n",
      "Log Regression(2399/4999): loss=0.5413649700037889\n",
      "Log Regression(2400/4999): loss=0.5451573513337182\n",
      "Log Regression(2401/4999): loss=0.5414146672059386\n",
      "Log Regression(2402/4999): loss=0.5428128190965955\n",
      "Log Regression(2403/4999): loss=0.5449414200461046\n",
      "Log Regression(2404/4999): loss=0.5437451972102209\n",
      "Log Regression(2405/4999): loss=0.541416410841174\n",
      "Log Regression(2406/4999): loss=0.541346632929532\n",
      "Log Regression(2407/4999): loss=0.543841023030692\n",
      "Log Regression(2408/4999): loss=0.5423216349115616\n",
      "Log Regression(2409/4999): loss=0.5417255023025375\n",
      "Log Regression(2410/4999): loss=0.5444268774676043\n",
      "Log Regression(2411/4999): loss=0.5450117677575447\n",
      "Log Regression(2412/4999): loss=0.5418852326375297\n",
      "Log Regression(2413/4999): loss=0.5417552129294914\n",
      "Log Regression(2414/4999): loss=0.5417390331582049\n",
      "Log Regression(2415/4999): loss=0.5428907050406524\n",
      "Log Regression(2416/4999): loss=0.5419480656991469\n",
      "Log Regression(2417/4999): loss=0.5415273631839612\n",
      "Log Regression(2418/4999): loss=0.5414099070663988\n",
      "Log Regression(2419/4999): loss=0.54120436886134\n",
      "Log Regression(2420/4999): loss=0.5421048455160267\n",
      "Log Regression(2421/4999): loss=0.5447735012967452\n",
      "Log Regression(2422/4999): loss=0.5454749401946039\n",
      "Log Regression(2423/4999): loss=0.5443422248768155\n",
      "Log Regression(2424/4999): loss=0.5438874812712456\n",
      "Log Regression(2425/4999): loss=0.5427683178668322\n",
      "Log Regression(2426/4999): loss=0.5412042757841969\n",
      "Log Regression(2427/4999): loss=0.5411890199890728\n",
      "Log Regression(2428/4999): loss=0.5412267504297974\n",
      "Log Regression(2429/4999): loss=0.5417144249252112\n",
      "Log Regression(2430/4999): loss=0.5412071970596465\n",
      "Log Regression(2431/4999): loss=0.5413723847217874\n",
      "Log Regression(2432/4999): loss=0.5427200210006287\n",
      "Log Regression(2433/4999): loss=0.5415315922938057\n",
      "Log Regression(2434/4999): loss=0.5432063630928615\n",
      "Log Regression(2435/4999): loss=0.5440796814198415\n",
      "Log Regression(2436/4999): loss=0.5472219973999928\n",
      "Log Regression(2437/4999): loss=0.5469360519514747\n",
      "Log Regression(2438/4999): loss=0.5414549979061929\n",
      "Log Regression(2439/4999): loss=0.5421630182130031\n",
      "Log Regression(2440/4999): loss=0.5454828937239046\n",
      "Log Regression(2441/4999): loss=0.541248571000297\n",
      "Log Regression(2442/4999): loss=0.5440132546740083\n",
      "Log Regression(2443/4999): loss=0.541350398428716\n",
      "Log Regression(2444/4999): loss=0.5421963040831188\n",
      "Log Regression(2445/4999): loss=0.5413783101719156\n",
      "Log Regression(2446/4999): loss=0.5475014556604209\n",
      "Log Regression(2447/4999): loss=0.5418057240331641\n",
      "Log Regression(2448/4999): loss=0.5419855382485124\n",
      "Log Regression(2449/4999): loss=0.5435120327071921\n",
      "Log Regression(2450/4999): loss=0.5422444727960565\n",
      "Log Regression(2451/4999): loss=0.5413756914784014\n",
      "Log Regression(2452/4999): loss=0.541610163082899\n",
      "Log Regression(2453/4999): loss=0.5475484377958861\n",
      "Log Regression(2454/4999): loss=0.5413602658662702\n",
      "Log Regression(2455/4999): loss=0.5414206396125951\n",
      "Log Regression(2456/4999): loss=0.542640012892341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(2457/4999): loss=0.541604233700527\n",
      "Log Regression(2458/4999): loss=0.5414758918654431\n",
      "Log Regression(2459/4999): loss=0.5435743700487787\n",
      "Log Regression(2460/4999): loss=0.5477102432823401\n",
      "Log Regression(2461/4999): loss=0.5451396896794214\n",
      "Log Regression(2462/4999): loss=0.5436083813731124\n",
      "Log Regression(2463/4999): loss=0.542174420721865\n",
      "Log Regression(2464/4999): loss=0.5412662692810469\n",
      "Log Regression(2465/4999): loss=0.541224801308627\n",
      "Log Regression(2466/4999): loss=0.5412741535692331\n",
      "Log Regression(2467/4999): loss=0.5414689097058929\n",
      "Log Regression(2468/4999): loss=0.5411518540745215\n",
      "Log Regression(2469/4999): loss=0.5422093963627963\n",
      "Log Regression(2470/4999): loss=0.5416064983377309\n",
      "Log Regression(2471/4999): loss=0.545385251301042\n",
      "Log Regression(2472/4999): loss=0.5420742127568524\n",
      "Log Regression(2473/4999): loss=0.5414808824573022\n",
      "Log Regression(2474/4999): loss=0.54178847629655\n",
      "Log Regression(2475/4999): loss=0.5419507042274084\n",
      "Log Regression(2476/4999): loss=0.5415063544056434\n",
      "Log Regression(2477/4999): loss=0.5421427397388688\n",
      "Log Regression(2478/4999): loss=0.5420954708557213\n",
      "Log Regression(2479/4999): loss=0.5412576529209643\n",
      "Log Regression(2480/4999): loss=0.541951218280273\n",
      "Log Regression(2481/4999): loss=0.5411192382229995\n",
      "Log Regression(2482/4999): loss=0.541636301146498\n",
      "Log Regression(2483/4999): loss=0.5415143115123151\n",
      "Log Regression(2484/4999): loss=0.5416670958038965\n",
      "Log Regression(2485/4999): loss=0.5425546366007882\n",
      "Log Regression(2486/4999): loss=0.5411735529127754\n",
      "Log Regression(2487/4999): loss=0.5412611210723464\n",
      "Log Regression(2488/4999): loss=0.5477225094734294\n",
      "Log Regression(2489/4999): loss=0.544961386336906\n",
      "Log Regression(2490/4999): loss=0.5434191624879217\n",
      "Log Regression(2491/4999): loss=0.5434788600756745\n",
      "Log Regression(2492/4999): loss=0.5412530859403566\n",
      "Log Regression(2493/4999): loss=0.5413431618038792\n",
      "Log Regression(2494/4999): loss=0.543247085761177\n",
      "Log Regression(2495/4999): loss=0.5420448303911927\n",
      "Log Regression(2496/4999): loss=0.541218674402056\n",
      "Log Regression(2497/4999): loss=0.5411310590297183\n",
      "Log Regression(2498/4999): loss=0.5416655627680071\n",
      "Log Regression(2499/4999): loss=0.5415500012779338\n",
      "Log Regression(2500/4999): loss=0.5416405409762723\n",
      "Log Regression(2501/4999): loss=0.5414693098596307\n",
      "Log Regression(2502/4999): loss=0.5414964226827951\n",
      "Log Regression(2503/4999): loss=0.541583367938388\n",
      "Log Regression(2504/4999): loss=0.5450099631476091\n",
      "Log Regression(2505/4999): loss=0.5431230970601192\n",
      "Log Regression(2506/4999): loss=0.5448186446908387\n",
      "Log Regression(2507/4999): loss=0.5428259035493218\n",
      "Log Regression(2508/4999): loss=0.5421126587985317\n",
      "Log Regression(2509/4999): loss=0.5437412766908992\n",
      "Log Regression(2510/4999): loss=0.5413607153261477\n",
      "Log Regression(2511/4999): loss=0.5411974681028007\n",
      "Log Regression(2512/4999): loss=0.5414645303546525\n",
      "Log Regression(2513/4999): loss=0.5417117721497795\n",
      "Log Regression(2514/4999): loss=0.5425445312596625\n",
      "Log Regression(2515/4999): loss=0.5410913372241524\n",
      "Log Regression(2516/4999): loss=0.5414921009888004\n",
      "Log Regression(2517/4999): loss=0.541318796990714\n",
      "Log Regression(2518/4999): loss=0.5415619427021973\n",
      "Log Regression(2519/4999): loss=0.5418427706410246\n",
      "Log Regression(2520/4999): loss=0.5438155654439923\n",
      "Log Regression(2521/4999): loss=0.5413112504659752\n",
      "Log Regression(2522/4999): loss=0.5411956504896699\n",
      "Log Regression(2523/4999): loss=0.5412845551508589\n",
      "Log Regression(2524/4999): loss=0.5442777624183934\n",
      "Log Regression(2525/4999): loss=0.5421052952464572\n",
      "Log Regression(2526/4999): loss=0.543713184746479\n",
      "Log Regression(2527/4999): loss=0.5419659438476262\n",
      "Log Regression(2528/4999): loss=0.5432701345338012\n",
      "Log Regression(2529/4999): loss=0.5435867152012847\n",
      "Log Regression(2530/4999): loss=0.5534724950058688\n",
      "Log Regression(2531/4999): loss=0.5466696839639252\n",
      "Log Regression(2532/4999): loss=0.5446624852687315\n",
      "Log Regression(2533/4999): loss=0.5432688584041788\n",
      "Log Regression(2534/4999): loss=0.5429376078544934\n",
      "Log Regression(2535/4999): loss=0.5414644945220741\n",
      "Log Regression(2536/4999): loss=0.541313813153748\n",
      "Log Regression(2537/4999): loss=0.5411483455606133\n",
      "Log Regression(2538/4999): loss=0.5428714197735753\n",
      "Log Regression(2539/4999): loss=0.5431034379243818\n",
      "Log Regression(2540/4999): loss=0.5413587111402046\n",
      "Log Regression(2541/4999): loss=0.542564628467001\n",
      "Log Regression(2542/4999): loss=0.5414304568461877\n",
      "Log Regression(2543/4999): loss=0.5453939360987844\n",
      "Log Regression(2544/4999): loss=0.547662684585584\n",
      "Log Regression(2545/4999): loss=0.5460887241005455\n",
      "Log Regression(2546/4999): loss=0.5416933239669057\n",
      "Log Regression(2547/4999): loss=0.5417701718279387\n",
      "Log Regression(2548/4999): loss=0.5411942495449957\n",
      "Log Regression(2549/4999): loss=0.5412249070082911\n",
      "Log Regression(2550/4999): loss=0.5440699070127343\n",
      "Log Regression(2551/4999): loss=0.5430490628922017\n",
      "Log Regression(2552/4999): loss=0.541553173415165\n",
      "Log Regression(2553/4999): loss=0.5412297524738138\n",
      "Log Regression(2554/4999): loss=0.5410830655792752\n",
      "Log Regression(2555/4999): loss=0.5415063542237711\n",
      "Log Regression(2556/4999): loss=0.5442532581265029\n",
      "Log Regression(2557/4999): loss=0.5437937595137496\n",
      "Log Regression(2558/4999): loss=0.5418369547599776\n",
      "Log Regression(2559/4999): loss=0.5419464058768897\n",
      "Log Regression(2560/4999): loss=0.5433585400899287\n",
      "Log Regression(2561/4999): loss=0.5417127654606131\n",
      "Log Regression(2562/4999): loss=0.5419023894636847\n",
      "Log Regression(2563/4999): loss=0.5410176211898243\n",
      "Log Regression(2564/4999): loss=0.541156826348422\n",
      "Log Regression(2565/4999): loss=0.5413218435812805\n",
      "Log Regression(2566/4999): loss=0.5415238939129603\n",
      "Log Regression(2567/4999): loss=0.5418679378032392\n",
      "Log Regression(2568/4999): loss=0.5437248605560099\n",
      "Log Regression(2569/4999): loss=0.5430133516799523\n",
      "Log Regression(2570/4999): loss=0.5416400318726641\n",
      "Log Regression(2571/4999): loss=0.5420493646888535\n",
      "Log Regression(2572/4999): loss=0.5421237347073682\n",
      "Log Regression(2573/4999): loss=0.5414154374928889\n",
      "Log Regression(2574/4999): loss=0.5426427146567836\n",
      "Log Regression(2575/4999): loss=0.5419936160886069\n",
      "Log Regression(2576/4999): loss=0.5440773472358756\n",
      "Log Regression(2577/4999): loss=0.5418021068776393\n",
      "Log Regression(2578/4999): loss=0.5425848273439813\n",
      "Log Regression(2579/4999): loss=0.5467677534312896\n",
      "Log Regression(2580/4999): loss=0.5450161776751603\n",
      "Log Regression(2581/4999): loss=0.5421043003953643\n",
      "Log Regression(2582/4999): loss=0.5411276272117965\n",
      "Log Regression(2583/4999): loss=0.5411558931854621\n",
      "Log Regression(2584/4999): loss=0.5418564309465957\n",
      "Log Regression(2585/4999): loss=0.5429354906695748\n",
      "Log Regression(2586/4999): loss=0.5458759343320414\n",
      "Log Regression(2587/4999): loss=0.5440208699644808\n",
      "Log Regression(2588/4999): loss=0.5431133451768957\n",
      "Log Regression(2589/4999): loss=0.5411051341351999\n",
      "Log Regression(2590/4999): loss=0.5421644147375376\n",
      "Log Regression(2591/4999): loss=0.542427560412978\n",
      "Log Regression(2592/4999): loss=0.5423824864254111\n",
      "Log Regression(2593/4999): loss=0.5415785599803079\n",
      "Log Regression(2594/4999): loss=0.5423195760171815\n",
      "Log Regression(2595/4999): loss=0.543524809808694\n",
      "Log Regression(2596/4999): loss=0.5425613503655969\n",
      "Log Regression(2597/4999): loss=0.5450280824843102\n",
      "Log Regression(2598/4999): loss=0.5415177524897428\n",
      "Log Regression(2599/4999): loss=0.5410036312481021\n",
      "Log Regression(2600/4999): loss=0.5413419869169986\n",
      "Log Regression(2601/4999): loss=0.5410006793723056\n",
      "Log Regression(2602/4999): loss=0.5411426066396569\n",
      "Log Regression(2603/4999): loss=0.5417299741099756\n",
      "Log Regression(2604/4999): loss=0.5416554579217425\n",
      "Log Regression(2605/4999): loss=0.5411119575764666\n",
      "Log Regression(2606/4999): loss=0.5422326430352425\n",
      "Log Regression(2607/4999): loss=0.5422970951957885\n",
      "Log Regression(2608/4999): loss=0.5420241825020368\n",
      "Log Regression(2609/4999): loss=0.542392872967937\n",
      "Log Regression(2610/4999): loss=0.5466670228160796\n",
      "Log Regression(2611/4999): loss=0.5440112967151244\n",
      "Log Regression(2612/4999): loss=0.5450095446294227\n",
      "Log Regression(2613/4999): loss=0.5413083006632045\n",
      "Log Regression(2614/4999): loss=0.5412799194470618\n",
      "Log Regression(2615/4999): loss=0.5412379351515479\n",
      "Log Regression(2616/4999): loss=0.5412201946744624\n",
      "Log Regression(2617/4999): loss=0.5413394348884183\n",
      "Log Regression(2618/4999): loss=0.5430781883587402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(2619/4999): loss=0.540989666565783\n",
      "Log Regression(2620/4999): loss=0.5422335815876052\n",
      "Log Regression(2621/4999): loss=0.5410180663084186\n",
      "Log Regression(2622/4999): loss=0.5409627975218174\n",
      "Log Regression(2623/4999): loss=0.5412385048122352\n",
      "Log Regression(2624/4999): loss=0.5434920923449017\n",
      "Log Regression(2625/4999): loss=0.5429147934294652\n",
      "Log Regression(2626/4999): loss=0.5427263540408982\n",
      "Log Regression(2627/4999): loss=0.5433853899403601\n",
      "Log Regression(2628/4999): loss=0.5420678022270636\n",
      "Log Regression(2629/4999): loss=0.5414796072795907\n",
      "Log Regression(2630/4999): loss=0.5409583278013731\n",
      "Log Regression(2631/4999): loss=0.5427492085667998\n",
      "Log Regression(2632/4999): loss=0.5432301746406802\n",
      "Log Regression(2633/4999): loss=0.5440205570555385\n",
      "Log Regression(2634/4999): loss=0.5467466411502113\n",
      "Log Regression(2635/4999): loss=0.5458727282102123\n",
      "Log Regression(2636/4999): loss=0.54692783436325\n",
      "Log Regression(2637/4999): loss=0.5409550067398021\n",
      "Log Regression(2638/4999): loss=0.540944013556994\n",
      "Log Regression(2639/4999): loss=0.5422131251896345\n",
      "Log Regression(2640/4999): loss=0.542135536924727\n",
      "Log Regression(2641/4999): loss=0.5490220427458856\n",
      "Log Regression(2642/4999): loss=0.54309104233511\n",
      "Log Regression(2643/4999): loss=0.541321433560945\n",
      "Log Regression(2644/4999): loss=0.5423132672309386\n",
      "Log Regression(2645/4999): loss=0.5416817273920871\n",
      "Log Regression(2646/4999): loss=0.541923807343326\n",
      "Log Regression(2647/4999): loss=0.5419678069588368\n",
      "Log Regression(2648/4999): loss=0.5422992557800866\n",
      "Log Regression(2649/4999): loss=0.5415276454705448\n",
      "Log Regression(2650/4999): loss=0.5427193014291926\n",
      "Log Regression(2651/4999): loss=0.5415193416409538\n",
      "Log Regression(2652/4999): loss=0.5419891885152935\n",
      "Log Regression(2653/4999): loss=0.5414415892483441\n",
      "Log Regression(2654/4999): loss=0.541615434441785\n",
      "Log Regression(2655/4999): loss=0.5415918504437427\n",
      "Log Regression(2656/4999): loss=0.5435766436461597\n",
      "Log Regression(2657/4999): loss=0.541948778795826\n",
      "Log Regression(2658/4999): loss=0.5414550746612545\n",
      "Log Regression(2659/4999): loss=0.5418293237401582\n",
      "Log Regression(2660/4999): loss=0.5422675313268815\n",
      "Log Regression(2661/4999): loss=0.5427331200177117\n",
      "Log Regression(2662/4999): loss=0.5418546273945053\n",
      "Log Regression(2663/4999): loss=0.5465041620514828\n",
      "Log Regression(2664/4999): loss=0.5472894035021871\n",
      "Log Regression(2665/4999): loss=0.5416661662524452\n",
      "Log Regression(2666/4999): loss=0.5416164828966511\n",
      "Log Regression(2667/4999): loss=0.5455019213447861\n",
      "Log Regression(2668/4999): loss=0.5423670148580403\n",
      "Log Regression(2669/4999): loss=0.5410239963912563\n",
      "Log Regression(2670/4999): loss=0.541642239199163\n",
      "Log Regression(2671/4999): loss=0.5437860193709255\n",
      "Log Regression(2672/4999): loss=0.5427897227611082\n",
      "Log Regression(2673/4999): loss=0.5446904264363284\n",
      "Log Regression(2674/4999): loss=0.5416572740787733\n",
      "Log Regression(2675/4999): loss=0.5424377879916459\n",
      "Log Regression(2676/4999): loss=0.5413509533957818\n",
      "Log Regression(2677/4999): loss=0.5420250262826786\n",
      "Log Regression(2678/4999): loss=0.5409583783199744\n",
      "Log Regression(2679/4999): loss=0.541214957573256\n",
      "Log Regression(2680/4999): loss=0.541720553035465\n",
      "Log Regression(2681/4999): loss=0.5427455311209628\n",
      "Log Regression(2682/4999): loss=0.5410856308692693\n",
      "Log Regression(2683/4999): loss=0.5409271930761381\n",
      "Log Regression(2684/4999): loss=0.5409412340155763\n",
      "Log Regression(2685/4999): loss=0.540894203479529\n",
      "Log Regression(2686/4999): loss=0.5456680854616437\n",
      "Log Regression(2687/4999): loss=0.5417111097446901\n",
      "Log Regression(2688/4999): loss=0.5417770465265102\n",
      "Log Regression(2689/4999): loss=0.5410799176853007\n",
      "Log Regression(2690/4999): loss=0.5411066962784854\n",
      "Log Regression(2691/4999): loss=0.5420620261518072\n",
      "Log Regression(2692/4999): loss=0.5417670343365154\n",
      "Log Regression(2693/4999): loss=0.5409689083965897\n",
      "Log Regression(2694/4999): loss=0.5427270522395309\n",
      "Log Regression(2695/4999): loss=0.5410684522519726\n",
      "Log Regression(2696/4999): loss=0.5417510625396533\n",
      "Log Regression(2697/4999): loss=0.5419436861282706\n",
      "Log Regression(2698/4999): loss=0.5410657635782202\n",
      "Log Regression(2699/4999): loss=0.5417574428934759\n",
      "Log Regression(2700/4999): loss=0.5416756651334087\n",
      "Log Regression(2701/4999): loss=0.5505247643139568\n",
      "Log Regression(2702/4999): loss=0.5472083142353771\n",
      "Log Regression(2703/4999): loss=0.5414223830533501\n",
      "Log Regression(2704/4999): loss=0.5411550290080099\n",
      "Log Regression(2705/4999): loss=0.5448821451766724\n",
      "Log Regression(2706/4999): loss=0.5469112429742599\n",
      "Log Regression(2707/4999): loss=0.5419850082388307\n",
      "Log Regression(2708/4999): loss=0.5422479077286981\n",
      "Log Regression(2709/4999): loss=0.5425486298527413\n",
      "Log Regression(2710/4999): loss=0.5421126514874973\n",
      "Log Regression(2711/4999): loss=0.5413592751858888\n",
      "Log Regression(2712/4999): loss=0.5412443237366371\n",
      "Log Regression(2713/4999): loss=0.5427331872260875\n",
      "Log Regression(2714/4999): loss=0.5426524036297213\n",
      "Log Regression(2715/4999): loss=0.5429978354309407\n",
      "Log Regression(2716/4999): loss=0.5408579272917524\n",
      "Log Regression(2717/4999): loss=0.5416714609840823\n",
      "Log Regression(2718/4999): loss=0.5424052437625881\n",
      "Log Regression(2719/4999): loss=0.5433820000568805\n",
      "Log Regression(2720/4999): loss=0.5428884031238379\n",
      "Log Regression(2721/4999): loss=0.540947538818592\n",
      "Log Regression(2722/4999): loss=0.5411113218328163\n",
      "Log Regression(2723/4999): loss=0.5416994140910911\n",
      "Log Regression(2724/4999): loss=0.5408613362620003\n",
      "Log Regression(2725/4999): loss=0.5408376482948349\n",
      "Log Regression(2726/4999): loss=0.5418534266161752\n",
      "Log Regression(2727/4999): loss=0.5413135247891836\n",
      "Log Regression(2728/4999): loss=0.5419171721121758\n",
      "Log Regression(2729/4999): loss=0.5411445367861302\n",
      "Log Regression(2730/4999): loss=0.5409542833458746\n",
      "Log Regression(2731/4999): loss=0.5409419534720665\n",
      "Log Regression(2732/4999): loss=0.5412902008673081\n",
      "Log Regression(2733/4999): loss=0.5420451700512854\n",
      "Log Regression(2734/4999): loss=0.5416806867583946\n",
      "Log Regression(2735/4999): loss=0.5414671886571805\n",
      "Log Regression(2736/4999): loss=0.5451524243003725\n",
      "Log Regression(2737/4999): loss=0.5441575754241026\n",
      "Log Regression(2738/4999): loss=0.5417554849887375\n",
      "Log Regression(2739/4999): loss=0.5411668209265182\n",
      "Log Regression(2740/4999): loss=0.5422408115322581\n",
      "Log Regression(2741/4999): loss=0.5442184783787056\n",
      "Log Regression(2742/4999): loss=0.5446073294191632\n",
      "Log Regression(2743/4999): loss=0.5411955461567188\n",
      "Log Regression(2744/4999): loss=0.5438586030121252\n",
      "Log Regression(2745/4999): loss=0.5486749203253701\n",
      "Log Regression(2746/4999): loss=0.5504336161128147\n",
      "Log Regression(2747/4999): loss=0.5442541339349652\n",
      "Log Regression(2748/4999): loss=0.5438916077262728\n",
      "Log Regression(2749/4999): loss=0.5415221400352374\n",
      "Log Regression(2750/4999): loss=0.542986381418295\n",
      "Log Regression(2751/4999): loss=0.5408572300312637\n",
      "Log Regression(2752/4999): loss=0.5409008411922442\n",
      "Log Regression(2753/4999): loss=0.5420734162118593\n",
      "Log Regression(2754/4999): loss=0.5409715761987616\n",
      "Log Regression(2755/4999): loss=0.5409909624940913\n",
      "Log Regression(2756/4999): loss=0.5409652616038448\n",
      "Log Regression(2757/4999): loss=0.5427885599903121\n",
      "Log Regression(2758/4999): loss=0.5408347726988668\n",
      "Log Regression(2759/4999): loss=0.5418501282486812\n",
      "Log Regression(2760/4999): loss=0.541641222224855\n",
      "Log Regression(2761/4999): loss=0.5453573130694862\n",
      "Log Regression(2762/4999): loss=0.5486308966115573\n",
      "Log Regression(2763/4999): loss=0.5543153340705811\n",
      "Log Regression(2764/4999): loss=0.5464624900495413\n",
      "Log Regression(2765/4999): loss=0.5411748490422394\n",
      "Log Regression(2766/4999): loss=0.541081206094284\n",
      "Log Regression(2767/4999): loss=0.5417443195006929\n",
      "Log Regression(2768/4999): loss=0.5416603235538774\n",
      "Log Regression(2769/4999): loss=0.5408621407342686\n",
      "Log Regression(2770/4999): loss=0.5420138630082537\n",
      "Log Regression(2771/4999): loss=0.5428871339761172\n",
      "Log Regression(2772/4999): loss=0.5410692762689226\n",
      "Log Regression(2773/4999): loss=0.5410082900667454\n",
      "Log Regression(2774/4999): loss=0.5410826513259777\n",
      "Log Regression(2775/4999): loss=0.5413421718990302\n",
      "Log Regression(2776/4999): loss=0.5491670146656834\n",
      "Log Regression(2777/4999): loss=0.5433124310227101\n",
      "Log Regression(2778/4999): loss=0.5448199251556752\n",
      "Log Regression(2779/4999): loss=0.5460304841322876\n",
      "Log Regression(2780/4999): loss=0.5409359607798392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(2781/4999): loss=0.5410354357622843\n",
      "Log Regression(2782/4999): loss=0.5417611685877636\n",
      "Log Regression(2783/4999): loss=0.5421035967840091\n",
      "Log Regression(2784/4999): loss=0.5410741318482946\n",
      "Log Regression(2785/4999): loss=0.5417086786614896\n",
      "Log Regression(2786/4999): loss=0.5409072791248728\n",
      "Log Regression(2787/4999): loss=0.5415140644424142\n",
      "Log Regression(2788/4999): loss=0.5423843122454137\n",
      "Log Regression(2789/4999): loss=0.5420854529240714\n",
      "Log Regression(2790/4999): loss=0.5421066255594812\n",
      "Log Regression(2791/4999): loss=0.5410766925539341\n",
      "Log Regression(2792/4999): loss=0.5476425450149572\n",
      "Log Regression(2793/4999): loss=0.5426453726201322\n",
      "Log Regression(2794/4999): loss=0.5480372266310725\n",
      "Log Regression(2795/4999): loss=0.5416599915340509\n",
      "Log Regression(2796/4999): loss=0.5426506964079605\n",
      "Log Regression(2797/4999): loss=0.5436031803669535\n",
      "Log Regression(2798/4999): loss=0.5473258317966038\n",
      "Log Regression(2799/4999): loss=0.5416814381931363\n",
      "Log Regression(2800/4999): loss=0.5464769801087097\n",
      "Log Regression(2801/4999): loss=0.5427956472129184\n",
      "Log Regression(2802/4999): loss=0.5408983028187441\n",
      "Log Regression(2803/4999): loss=0.5409471781406219\n",
      "Log Regression(2804/4999): loss=0.5438490504251758\n",
      "Log Regression(2805/4999): loss=0.5410038221340766\n",
      "Log Regression(2806/4999): loss=0.5415944019907506\n",
      "Log Regression(2807/4999): loss=0.5424043369594197\n",
      "Log Regression(2808/4999): loss=0.5408903751966205\n",
      "Log Regression(2809/4999): loss=0.5411313216146807\n",
      "Log Regression(2810/4999): loss=0.5411535767175407\n",
      "Log Regression(2811/4999): loss=0.5409615086932221\n",
      "Log Regression(2812/4999): loss=0.5413064473233578\n",
      "Log Regression(2813/4999): loss=0.5420567704976077\n",
      "Log Regression(2814/4999): loss=0.5409806019357992\n",
      "Log Regression(2815/4999): loss=0.5417237960365713\n",
      "Log Regression(2816/4999): loss=0.5411178238886233\n",
      "Log Regression(2817/4999): loss=0.5439021519866828\n",
      "Log Regression(2818/4999): loss=0.5434496338438369\n",
      "Log Regression(2819/4999): loss=0.5414198925243401\n",
      "Log Regression(2820/4999): loss=0.542870313320314\n",
      "Log Regression(2821/4999): loss=0.541247106253939\n",
      "Log Regression(2822/4999): loss=0.5409788822496644\n",
      "Log Regression(2823/4999): loss=0.5414055338056537\n",
      "Log Regression(2824/4999): loss=0.5412565779527682\n",
      "Log Regression(2825/4999): loss=0.5424279709248232\n",
      "Log Regression(2826/4999): loss=0.5425449527512128\n",
      "Log Regression(2827/4999): loss=0.5460512009000977\n",
      "Log Regression(2828/4999): loss=0.5437966449694212\n",
      "Log Regression(2829/4999): loss=0.5425021193528355\n",
      "Log Regression(2830/4999): loss=0.5418372102067767\n",
      "Log Regression(2831/4999): loss=0.5419800982021877\n",
      "Log Regression(2832/4999): loss=0.5426924885943596\n",
      "Log Regression(2833/4999): loss=0.5439312359267208\n",
      "Log Regression(2834/4999): loss=0.5465711660759979\n",
      "Log Regression(2835/4999): loss=0.5414501891337071\n",
      "Log Regression(2836/4999): loss=0.5450012345659672\n",
      "Log Regression(2837/4999): loss=0.5411259942852469\n",
      "Log Regression(2838/4999): loss=0.5425058963458035\n",
      "Log Regression(2839/4999): loss=0.5427815111236503\n",
      "Log Regression(2840/4999): loss=0.5419134504998399\n",
      "Log Regression(2841/4999): loss=0.5420721826017961\n",
      "Log Regression(2842/4999): loss=0.5421202072392224\n",
      "Log Regression(2843/4999): loss=0.5408875814676235\n",
      "Log Regression(2844/4999): loss=0.5414391125508042\n",
      "Log Regression(2845/4999): loss=0.5419063844159229\n",
      "Log Regression(2846/4999): loss=0.5480376095819186\n",
      "Log Regression(2847/4999): loss=0.5443552607929225\n",
      "Log Regression(2848/4999): loss=0.5433805974319513\n",
      "Log Regression(2849/4999): loss=0.5408231526822022\n",
      "Log Regression(2850/4999): loss=0.5417506468394735\n",
      "Log Regression(2851/4999): loss=0.5418930242126759\n",
      "Log Regression(2852/4999): loss=0.5423532918485401\n",
      "Log Regression(2853/4999): loss=0.5418434252791355\n",
      "Log Regression(2854/4999): loss=0.5408362947504921\n",
      "Log Regression(2855/4999): loss=0.5465901913520238\n",
      "Log Regression(2856/4999): loss=0.5420661657451596\n",
      "Log Regression(2857/4999): loss=0.5407323920382456\n",
      "Log Regression(2858/4999): loss=0.5429613099636742\n",
      "Log Regression(2859/4999): loss=0.5421040694325439\n",
      "Log Regression(2860/4999): loss=0.5439483366826641\n",
      "Log Regression(2861/4999): loss=0.5407724730373689\n",
      "Log Regression(2862/4999): loss=0.5407186155960597\n",
      "Log Regression(2863/4999): loss=0.5414321021239025\n",
      "Log Regression(2864/4999): loss=0.5414839730808347\n",
      "Log Regression(2865/4999): loss=0.5411610276582867\n",
      "Log Regression(2866/4999): loss=0.5410710639858123\n",
      "Log Regression(2867/4999): loss=0.5407496061841166\n",
      "Log Regression(2868/4999): loss=0.5407554650463574\n",
      "Log Regression(2869/4999): loss=0.5410505124396666\n",
      "Log Regression(2870/4999): loss=0.5407505401553391\n",
      "Log Regression(2871/4999): loss=0.5444538971510181\n",
      "Log Regression(2872/4999): loss=0.544035497722149\n",
      "Log Regression(2873/4999): loss=0.5438804082148662\n",
      "Log Regression(2874/4999): loss=0.555296914737099\n",
      "Log Regression(2875/4999): loss=0.5528572333531517\n",
      "Log Regression(2876/4999): loss=0.5503429763565632\n",
      "Log Regression(2877/4999): loss=0.5477225017487605\n",
      "Log Regression(2878/4999): loss=0.5522508979728333\n",
      "Log Regression(2879/4999): loss=0.5487054431160734\n",
      "Log Regression(2880/4999): loss=0.5527401818226957\n",
      "Log Regression(2881/4999): loss=0.5561873199736302\n",
      "Log Regression(2882/4999): loss=0.5494299975323583\n",
      "Log Regression(2883/4999): loss=0.5462346093128547\n",
      "Log Regression(2884/4999): loss=0.5470504045132452\n",
      "Log Regression(2885/4999): loss=0.5441730775327869\n",
      "Log Regression(2886/4999): loss=0.5410194786768917\n",
      "Log Regression(2887/4999): loss=0.5411986073508569\n",
      "Log Regression(2888/4999): loss=0.5411875768542895\n",
      "Log Regression(2889/4999): loss=0.5431604236505438\n",
      "Log Regression(2890/4999): loss=0.5414655144120128\n",
      "Log Regression(2891/4999): loss=0.5424125119550753\n",
      "Log Regression(2892/4999): loss=0.5410360976751524\n",
      "Log Regression(2893/4999): loss=0.541149813707164\n",
      "Log Regression(2894/4999): loss=0.5410219968507549\n",
      "Log Regression(2895/4999): loss=0.5423372771557581\n",
      "Log Regression(2896/4999): loss=0.5409549451946019\n",
      "Log Regression(2897/4999): loss=0.5412150087296159\n",
      "Log Regression(2898/4999): loss=0.5408713975187711\n",
      "Log Regression(2899/4999): loss=0.5414914745167441\n",
      "Log Regression(2900/4999): loss=0.5409629183291264\n",
      "Log Regression(2901/4999): loss=0.5411601888505059\n",
      "Log Regression(2902/4999): loss=0.5417315292863586\n",
      "Log Regression(2903/4999): loss=0.5413747719172554\n",
      "Log Regression(2904/4999): loss=0.5409576125996878\n",
      "Log Regression(2905/4999): loss=0.5423855660118183\n",
      "Log Regression(2906/4999): loss=0.5450328016798949\n",
      "Log Regression(2907/4999): loss=0.5420102714803413\n",
      "Log Regression(2908/4999): loss=0.5438840208339067\n",
      "Log Regression(2909/4999): loss=0.541017813804875\n",
      "Log Regression(2910/4999): loss=0.5410420800163137\n",
      "Log Regression(2911/4999): loss=0.541763938769988\n",
      "Log Regression(2912/4999): loss=0.5410669523663496\n",
      "Log Regression(2913/4999): loss=0.541172179012618\n",
      "Log Regression(2914/4999): loss=0.5430011907599841\n",
      "Log Regression(2915/4999): loss=0.542923955304389\n",
      "Log Regression(2916/4999): loss=0.5425295812903536\n",
      "Log Regression(2917/4999): loss=0.5465405999830741\n",
      "Log Regression(2918/4999): loss=0.5462511790426066\n",
      "Log Regression(2919/4999): loss=0.5410813411648024\n",
      "Log Regression(2920/4999): loss=0.5422715711396413\n",
      "Log Regression(2921/4999): loss=0.5407579118270012\n",
      "Log Regression(2922/4999): loss=0.5407263679916886\n",
      "Log Regression(2923/4999): loss=0.5417427842882747\n",
      "Log Regression(2924/4999): loss=0.544629588372944\n",
      "Log Regression(2925/4999): loss=0.5426981696323238\n",
      "Log Regression(2926/4999): loss=0.5407759416076052\n",
      "Log Regression(2927/4999): loss=0.5415748660938161\n",
      "Log Regression(2928/4999): loss=0.540700063482056\n",
      "Log Regression(2929/4999): loss=0.5429598517939803\n",
      "Log Regression(2930/4999): loss=0.548777838822305\n",
      "Log Regression(2931/4999): loss=0.5435818329918684\n",
      "Log Regression(2932/4999): loss=0.541005055990858\n",
      "Log Regression(2933/4999): loss=0.540998023972436\n",
      "Log Regression(2934/4999): loss=0.5407814744084379\n",
      "Log Regression(2935/4999): loss=0.5413839924364542\n",
      "Log Regression(2936/4999): loss=0.5407195611658209\n",
      "Log Regression(2937/4999): loss=0.5407819850291965\n",
      "Log Regression(2938/4999): loss=0.5420489584628214\n",
      "Log Regression(2939/4999): loss=0.5415581067223775\n",
      "Log Regression(2940/4999): loss=0.5426032578624428\n",
      "Log Regression(2941/4999): loss=0.5428353927475468\n",
      "Log Regression(2942/4999): loss=0.5439619846971835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(2943/4999): loss=0.541886191712387\n",
      "Log Regression(2944/4999): loss=0.5415318908847851\n",
      "Log Regression(2945/4999): loss=0.5431041044649894\n",
      "Log Regression(2946/4999): loss=0.5433659391350115\n",
      "Log Regression(2947/4999): loss=0.5469714107289719\n",
      "Log Regression(2948/4999): loss=0.5431334442781163\n",
      "Log Regression(2949/4999): loss=0.5416060389734687\n",
      "Log Regression(2950/4999): loss=0.5461146448039937\n",
      "Log Regression(2951/4999): loss=0.5441426401854158\n",
      "Log Regression(2952/4999): loss=0.5470847882650426\n",
      "Log Regression(2953/4999): loss=0.5453382909239147\n",
      "Log Regression(2954/4999): loss=0.5428687580950444\n",
      "Log Regression(2955/4999): loss=0.5473457923531133\n",
      "Log Regression(2956/4999): loss=0.5420094112915178\n",
      "Log Regression(2957/4999): loss=0.5412282657017718\n",
      "Log Regression(2958/4999): loss=0.5414109579361516\n",
      "Log Regression(2959/4999): loss=0.5416622919184623\n",
      "Log Regression(2960/4999): loss=0.5414052576867299\n",
      "Log Regression(2961/4999): loss=0.5412270119515549\n",
      "Log Regression(2962/4999): loss=0.5415046749150629\n",
      "Log Regression(2963/4999): loss=0.5407406590499926\n",
      "Log Regression(2964/4999): loss=0.5407728231686166\n",
      "Log Regression(2965/4999): loss=0.5408580796403369\n",
      "Log Regression(2966/4999): loss=0.5409443360442255\n",
      "Log Regression(2967/4999): loss=0.5435538153364843\n",
      "Log Regression(2968/4999): loss=0.5449281363830781\n",
      "Log Regression(2969/4999): loss=0.540864871176148\n",
      "Log Regression(2970/4999): loss=0.5420177095052259\n",
      "Log Regression(2971/4999): loss=0.54089529976831\n",
      "Log Regression(2972/4999): loss=0.5409413028302943\n",
      "Log Regression(2973/4999): loss=0.5408149626837709\n",
      "Log Regression(2974/4999): loss=0.541255898810944\n",
      "Log Regression(2975/4999): loss=0.541221707140307\n",
      "Log Regression(2976/4999): loss=0.5408275370314494\n",
      "Log Regression(2977/4999): loss=0.540723249904624\n",
      "Log Regression(2978/4999): loss=0.5413100969246218\n",
      "Log Regression(2979/4999): loss=0.5406919142787303\n",
      "Log Regression(2980/4999): loss=0.540685783717492\n",
      "Log Regression(2981/4999): loss=0.5407501845171788\n",
      "Log Regression(2982/4999): loss=0.5412901278243979\n",
      "Log Regression(2983/4999): loss=0.5410734968869885\n",
      "Log Regression(2984/4999): loss=0.5414681822207433\n",
      "Log Regression(2985/4999): loss=0.5407724630194152\n",
      "Log Regression(2986/4999): loss=0.5447963726879396\n",
      "Log Regression(2987/4999): loss=0.5455940966241857\n",
      "Log Regression(2988/4999): loss=0.549546877109172\n",
      "Log Regression(2989/4999): loss=0.5503763648719604\n",
      "Log Regression(2990/4999): loss=0.5494234246146377\n",
      "Log Regression(2991/4999): loss=0.5423852363499895\n",
      "Log Regression(2992/4999): loss=0.5423284047235706\n",
      "Log Regression(2993/4999): loss=0.5407323351091367\n",
      "Log Regression(2994/4999): loss=0.544316529440309\n",
      "Log Regression(2995/4999): loss=0.5407273943599941\n",
      "Log Regression(2996/4999): loss=0.5414738798920102\n",
      "Log Regression(2997/4999): loss=0.541464876404263\n",
      "Log Regression(2998/4999): loss=0.5432170891848855\n",
      "Log Regression(2999/4999): loss=0.5449263876952312\n",
      "Log Regression(3000/4999): loss=0.5416604995464007\n",
      "Log Regression(3001/4999): loss=0.5411270359929623\n",
      "Log Regression(3002/4999): loss=0.5425730384650552\n",
      "Log Regression(3003/4999): loss=0.5421638461211371\n",
      "Log Regression(3004/4999): loss=0.5417366093701405\n",
      "Log Regression(3005/4999): loss=0.5449819889305423\n",
      "Log Regression(3006/4999): loss=0.5414349387442812\n",
      "Log Regression(3007/4999): loss=0.5417156290488302\n",
      "Log Regression(3008/4999): loss=0.541459723921181\n",
      "Log Regression(3009/4999): loss=0.5424771509956138\n",
      "Log Regression(3010/4999): loss=0.5421689186850721\n",
      "Log Regression(3011/4999): loss=0.544505002902025\n",
      "Log Regression(3012/4999): loss=0.541900874148699\n",
      "Log Regression(3013/4999): loss=0.5418338284913851\n",
      "Log Regression(3014/4999): loss=0.5435442336293054\n",
      "Log Regression(3015/4999): loss=0.547078482091041\n",
      "Log Regression(3016/4999): loss=0.546270104818123\n",
      "Log Regression(3017/4999): loss=0.5428109850931901\n",
      "Log Regression(3018/4999): loss=0.5416796761522545\n",
      "Log Regression(3019/4999): loss=0.5417733343707759\n",
      "Log Regression(3020/4999): loss=0.5430114545393737\n",
      "Log Regression(3021/4999): loss=0.5448488490725942\n",
      "Log Regression(3022/4999): loss=0.5410121797512343\n",
      "Log Regression(3023/4999): loss=0.5409554916307977\n",
      "Log Regression(3024/4999): loss=0.5408596758838039\n",
      "Log Regression(3025/4999): loss=0.5419819043721883\n",
      "Log Regression(3026/4999): loss=0.5413639291539063\n",
      "Log Regression(3027/4999): loss=0.5442421575179276\n",
      "Log Regression(3028/4999): loss=0.5416302401645996\n",
      "Log Regression(3029/4999): loss=0.540647061281343\n",
      "Log Regression(3030/4999): loss=0.5409954050064134\n",
      "Log Regression(3031/4999): loss=0.5429673137824675\n",
      "Log Regression(3032/4999): loss=0.5453845433133722\n",
      "Log Regression(3033/4999): loss=0.5461717651904145\n",
      "Log Regression(3034/4999): loss=0.5478789595210792\n",
      "Log Regression(3035/4999): loss=0.540657885581331\n",
      "Log Regression(3036/4999): loss=0.5410465345293389\n",
      "Log Regression(3037/4999): loss=0.5407184877100547\n",
      "Log Regression(3038/4999): loss=0.5417851484310329\n",
      "Log Regression(3039/4999): loss=0.5408041532754857\n",
      "Log Regression(3040/4999): loss=0.5415132931140335\n",
      "Log Regression(3041/4999): loss=0.5417321264668583\n",
      "Log Regression(3042/4999): loss=0.5408077263100198\n",
      "Log Regression(3043/4999): loss=0.5409637586083335\n",
      "Log Regression(3044/4999): loss=0.540824386453583\n",
      "Log Regression(3045/4999): loss=0.5461990887444516\n",
      "Log Regression(3046/4999): loss=0.5476220983729942\n",
      "Log Regression(3047/4999): loss=0.5423568374952193\n",
      "Log Regression(3048/4999): loss=0.543667517573644\n",
      "Log Regression(3049/4999): loss=0.5418726452644784\n",
      "Log Regression(3050/4999): loss=0.5442705841771238\n",
      "Log Regression(3051/4999): loss=0.5497774070262782\n",
      "Log Regression(3052/4999): loss=0.5448083326163643\n",
      "Log Regression(3053/4999): loss=0.5462340084746763\n",
      "Log Regression(3054/4999): loss=0.543760729608601\n",
      "Log Regression(3055/4999): loss=0.5415173917157348\n",
      "Log Regression(3056/4999): loss=0.5407271654361911\n",
      "Log Regression(3057/4999): loss=0.5406780793741478\n",
      "Log Regression(3058/4999): loss=0.5413195612369849\n",
      "Log Regression(3059/4999): loss=0.5413106368535301\n",
      "Log Regression(3060/4999): loss=0.5419382267451462\n",
      "Log Regression(3061/4999): loss=0.5448097289904237\n",
      "Log Regression(3062/4999): loss=0.5422907353346835\n",
      "Log Regression(3063/4999): loss=0.5419341836901916\n",
      "Log Regression(3064/4999): loss=0.5450958033062016\n",
      "Log Regression(3065/4999): loss=0.5426005307893877\n",
      "Log Regression(3066/4999): loss=0.5411106152915487\n",
      "Log Regression(3067/4999): loss=0.5411115784723166\n",
      "Log Regression(3068/4999): loss=0.5408026868495656\n",
      "Log Regression(3069/4999): loss=0.5429478422752698\n",
      "Log Regression(3070/4999): loss=0.5427841608455698\n",
      "Log Regression(3071/4999): loss=0.5414545337532056\n",
      "Log Regression(3072/4999): loss=0.5408690334002191\n",
      "Log Regression(3073/4999): loss=0.5406321903219213\n",
      "Log Regression(3074/4999): loss=0.540791449611321\n",
      "Log Regression(3075/4999): loss=0.5408938852244549\n",
      "Log Regression(3076/4999): loss=0.5407871099710787\n",
      "Log Regression(3077/4999): loss=0.5409654741573209\n",
      "Log Regression(3078/4999): loss=0.5418750196911817\n",
      "Log Regression(3079/4999): loss=0.5419515530608378\n",
      "Log Regression(3080/4999): loss=0.5406271773772031\n",
      "Log Regression(3081/4999): loss=0.5416155469195686\n",
      "Log Regression(3082/4999): loss=0.5413843937797862\n",
      "Log Regression(3083/4999): loss=0.5427988119190067\n",
      "Log Regression(3084/4999): loss=0.5425710667853607\n",
      "Log Regression(3085/4999): loss=0.5434047493096392\n",
      "Log Regression(3086/4999): loss=0.5438223279486974\n",
      "Log Regression(3087/4999): loss=0.5429923113280769\n",
      "Log Regression(3088/4999): loss=0.5422734840526816\n",
      "Log Regression(3089/4999): loss=0.5410971658070625\n",
      "Log Regression(3090/4999): loss=0.5491977322044552\n",
      "Log Regression(3091/4999): loss=0.5421216139891296\n",
      "Log Regression(3092/4999): loss=0.5444654679765346\n",
      "Log Regression(3093/4999): loss=0.5411954153276446\n",
      "Log Regression(3094/4999): loss=0.5407714880776453\n",
      "Log Regression(3095/4999): loss=0.5419969624842081\n",
      "Log Regression(3096/4999): loss=0.5405680977881676\n",
      "Log Regression(3097/4999): loss=0.5406947980530368\n",
      "Log Regression(3098/4999): loss=0.5415384912219509\n",
      "Log Regression(3099/4999): loss=0.5413769354520469\n",
      "Log Regression(3100/4999): loss=0.5408189187805874\n",
      "Log Regression(3101/4999): loss=0.55079366708627\n",
      "Log Regression(3102/4999): loss=0.55014763427086\n",
      "Log Regression(3103/4999): loss=0.5426443513667905\n",
      "Log Regression(3104/4999): loss=0.5405945933425642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(3105/4999): loss=0.5424055367455085\n",
      "Log Regression(3106/4999): loss=0.5416145039455004\n",
      "Log Regression(3107/4999): loss=0.5406041417678296\n",
      "Log Regression(3108/4999): loss=0.5443557698343895\n",
      "Log Regression(3109/4999): loss=0.5422078586726077\n",
      "Log Regression(3110/4999): loss=0.5451514818488947\n",
      "Log Regression(3111/4999): loss=0.5420929041976432\n",
      "Log Regression(3112/4999): loss=0.5437934289490344\n",
      "Log Regression(3113/4999): loss=0.5417375396666924\n",
      "Log Regression(3114/4999): loss=0.5417636002521812\n",
      "Log Regression(3115/4999): loss=0.542653891609096\n",
      "Log Regression(3116/4999): loss=0.5443322383201395\n",
      "Log Regression(3117/4999): loss=0.5432128592603941\n",
      "Log Regression(3118/4999): loss=0.5448979872134242\n",
      "Log Regression(3119/4999): loss=0.5460585842291205\n",
      "Log Regression(3120/4999): loss=0.5504546286018792\n",
      "Log Regression(3121/4999): loss=0.5458852213093288\n",
      "Log Regression(3122/4999): loss=0.5444740681488272\n",
      "Log Regression(3123/4999): loss=0.5409474707978015\n",
      "Log Regression(3124/4999): loss=0.5414771544226159\n",
      "Log Regression(3125/4999): loss=0.5456241477536505\n",
      "Log Regression(3126/4999): loss=0.5442680759294647\n",
      "Log Regression(3127/4999): loss=0.5462808506358533\n",
      "Log Regression(3128/4999): loss=0.5446023862905998\n",
      "Log Regression(3129/4999): loss=0.5454322197942203\n",
      "Log Regression(3130/4999): loss=0.543638773001115\n",
      "Log Regression(3131/4999): loss=0.5409507918954207\n",
      "Log Regression(3132/4999): loss=0.5430516416342629\n",
      "Log Regression(3133/4999): loss=0.5408326831585426\n",
      "Log Regression(3134/4999): loss=0.5425433954298784\n",
      "Log Regression(3135/4999): loss=0.5418412463198696\n",
      "Log Regression(3136/4999): loss=0.5414010583618083\n",
      "Log Regression(3137/4999): loss=0.540665476000752\n",
      "Log Regression(3138/4999): loss=0.5412487118318325\n",
      "Log Regression(3139/4999): loss=0.5420729729969406\n",
      "Log Regression(3140/4999): loss=0.5416965767298796\n",
      "Log Regression(3141/4999): loss=0.5405758242365213\n",
      "Log Regression(3142/4999): loss=0.5442333526009222\n",
      "Log Regression(3143/4999): loss=0.5467254627743967\n",
      "Log Regression(3144/4999): loss=0.5432552670574887\n",
      "Log Regression(3145/4999): loss=0.540918175977867\n",
      "Log Regression(3146/4999): loss=0.5415485005440784\n",
      "Log Regression(3147/4999): loss=0.5457908065534267\n",
      "Log Regression(3148/4999): loss=0.5433364585822856\n",
      "Log Regression(3149/4999): loss=0.5409038444735476\n",
      "Log Regression(3150/4999): loss=0.5408053966117581\n",
      "Log Regression(3151/4999): loss=0.5425019753054974\n",
      "Log Regression(3152/4999): loss=0.5438098073948917\n",
      "Log Regression(3153/4999): loss=0.5412312845760671\n",
      "Log Regression(3154/4999): loss=0.540768322778948\n",
      "Log Regression(3155/4999): loss=0.5407678905530272\n",
      "Log Regression(3156/4999): loss=0.5418047360538861\n",
      "Log Regression(3157/4999): loss=0.5496069423489118\n",
      "Log Regression(3158/4999): loss=0.555311925581587\n",
      "Log Regression(3159/4999): loss=0.5512949537399402\n",
      "Log Regression(3160/4999): loss=0.5444972125465966\n",
      "Log Regression(3161/4999): loss=0.544179922247201\n",
      "Log Regression(3162/4999): loss=0.5436218163201987\n",
      "Log Regression(3163/4999): loss=0.5405622645529874\n",
      "Log Regression(3164/4999): loss=0.5409513564592188\n",
      "Log Regression(3165/4999): loss=0.5406816916231985\n",
      "Log Regression(3166/4999): loss=0.5406492304577842\n",
      "Log Regression(3167/4999): loss=0.5461878534299072\n",
      "Log Regression(3168/4999): loss=0.540522058852138\n",
      "Log Regression(3169/4999): loss=0.5405954092865801\n",
      "Log Regression(3170/4999): loss=0.5468297766176293\n",
      "Log Regression(3171/4999): loss=0.5499627974167652\n",
      "Log Regression(3172/4999): loss=0.5407731795629903\n",
      "Log Regression(3173/4999): loss=0.5458549533746776\n",
      "Log Regression(3174/4999): loss=0.5475630471717247\n",
      "Log Regression(3175/4999): loss=0.5418604705589773\n",
      "Log Regression(3176/4999): loss=0.5426456744392391\n",
      "Log Regression(3177/4999): loss=0.547466216624827\n",
      "Log Regression(3178/4999): loss=0.5429692504031761\n",
      "Log Regression(3179/4999): loss=0.5405232450696974\n",
      "Log Regression(3180/4999): loss=0.541430941799637\n",
      "Log Regression(3181/4999): loss=0.540512176853561\n",
      "Log Regression(3182/4999): loss=0.5405107430013276\n",
      "Log Regression(3183/4999): loss=0.5417072759617316\n",
      "Log Regression(3184/4999): loss=0.5408011031706151\n",
      "Log Regression(3185/4999): loss=0.5405228099982866\n",
      "Log Regression(3186/4999): loss=0.543729552114208\n",
      "Log Regression(3187/4999): loss=0.5413690063327794\n",
      "Log Regression(3188/4999): loss=0.540617225390979\n",
      "Log Regression(3189/4999): loss=0.5405210084265921\n",
      "Log Regression(3190/4999): loss=0.5413300812621935\n",
      "Log Regression(3191/4999): loss=0.5431139058923511\n",
      "Log Regression(3192/4999): loss=0.5429872942104957\n",
      "Log Regression(3193/4999): loss=0.5405241828652033\n",
      "Log Regression(3194/4999): loss=0.5409417736683865\n",
      "Log Regression(3195/4999): loss=0.5404951877754142\n",
      "Log Regression(3196/4999): loss=0.5427920444055618\n",
      "Log Regression(3197/4999): loss=0.5425550786178495\n",
      "Log Regression(3198/4999): loss=0.5427098917476533\n",
      "Log Regression(3199/4999): loss=0.5412063148601992\n",
      "Log Regression(3200/4999): loss=0.5406282306513113\n",
      "Log Regression(3201/4999): loss=0.5410774310223151\n",
      "Log Regression(3202/4999): loss=0.5429296046740849\n",
      "Log Regression(3203/4999): loss=0.5417817479924958\n",
      "Log Regression(3204/4999): loss=0.5416227279867009\n",
      "Log Regression(3205/4999): loss=0.5411746599177121\n",
      "Log Regression(3206/4999): loss=0.5408980744603241\n",
      "Log Regression(3207/4999): loss=0.5424319707144717\n",
      "Log Regression(3208/4999): loss=0.5425788552456966\n",
      "Log Regression(3209/4999): loss=0.5407506669161375\n",
      "Log Regression(3210/4999): loss=0.5416394683948478\n",
      "Log Regression(3211/4999): loss=0.5432773899350906\n",
      "Log Regression(3212/4999): loss=0.5422194345679113\n",
      "Log Regression(3213/4999): loss=0.5410318975200115\n",
      "Log Regression(3214/4999): loss=0.5406282559253427\n",
      "Log Regression(3215/4999): loss=0.5406289646077863\n",
      "Log Regression(3216/4999): loss=0.5442214471378111\n",
      "Log Regression(3217/4999): loss=0.5409124971978004\n",
      "Log Regression(3218/4999): loss=0.540779734962962\n",
      "Log Regression(3219/4999): loss=0.5404779177625988\n",
      "Log Regression(3220/4999): loss=0.5405714197294561\n",
      "Log Regression(3221/4999): loss=0.5444248874274699\n",
      "Log Regression(3222/4999): loss=0.5418906935761039\n",
      "Log Regression(3223/4999): loss=0.5404611216944746\n",
      "Log Regression(3224/4999): loss=0.5405477416812702\n",
      "Log Regression(3225/4999): loss=0.5406232834673834\n",
      "Log Regression(3226/4999): loss=0.5406919931943392\n",
      "Log Regression(3227/4999): loss=0.5405411821382236\n",
      "Log Regression(3228/4999): loss=0.54343005557589\n",
      "Log Regression(3229/4999): loss=0.5469631740644334\n",
      "Log Regression(3230/4999): loss=0.5410928795159157\n",
      "Log Regression(3231/4999): loss=0.5472610570662295\n",
      "Log Regression(3232/4999): loss=0.5454540849657499\n",
      "Log Regression(3233/4999): loss=0.5448748320400061\n",
      "Log Regression(3234/4999): loss=0.5410198550974344\n",
      "Log Regression(3235/4999): loss=0.544006397122749\n",
      "Log Regression(3236/4999): loss=0.5425182252155499\n",
      "Log Regression(3237/4999): loss=0.5412871133914201\n",
      "Log Regression(3238/4999): loss=0.5406282222859006\n",
      "Log Regression(3239/4999): loss=0.5406337326009797\n",
      "Log Regression(3240/4999): loss=0.5407345554701322\n",
      "Log Regression(3241/4999): loss=0.5413154344368742\n",
      "Log Regression(3242/4999): loss=0.5410397323853666\n",
      "Log Regression(3243/4999): loss=0.5406592130645774\n",
      "Log Regression(3244/4999): loss=0.5422363907577746\n",
      "Log Regression(3245/4999): loss=0.5429516825760208\n",
      "Log Regression(3246/4999): loss=0.5405415631761286\n",
      "Log Regression(3247/4999): loss=0.5412737603300964\n",
      "Log Regression(3248/4999): loss=0.5429714169682032\n",
      "Log Regression(3249/4999): loss=0.5423499536592452\n",
      "Log Regression(3250/4999): loss=0.5431685009270448\n",
      "Log Regression(3251/4999): loss=0.5411313905161699\n",
      "Log Regression(3252/4999): loss=0.542220683153124\n",
      "Log Regression(3253/4999): loss=0.5412144883392629\n",
      "Log Regression(3254/4999): loss=0.5409605864966969\n",
      "Log Regression(3255/4999): loss=0.5431772097197956\n",
      "Log Regression(3256/4999): loss=0.5449617204691799\n",
      "Log Regression(3257/4999): loss=0.5440905343573104\n",
      "Log Regression(3258/4999): loss=0.5421066000096423\n",
      "Log Regression(3259/4999): loss=0.5408413845346244\n",
      "Log Regression(3260/4999): loss=0.5415724026667185\n",
      "Log Regression(3261/4999): loss=0.5408892278283638\n",
      "Log Regression(3262/4999): loss=0.5409054722051878\n",
      "Log Regression(3263/4999): loss=0.5423175699703691\n",
      "Log Regression(3264/4999): loss=0.5454141321140077\n",
      "Log Regression(3265/4999): loss=0.543556175027373\n",
      "Log Regression(3266/4999): loss=0.5412529063456967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(3267/4999): loss=0.541404599522358\n",
      "Log Regression(3268/4999): loss=0.5413373507959988\n",
      "Log Regression(3269/4999): loss=0.5406678882455588\n",
      "Log Regression(3270/4999): loss=0.5422666170654095\n",
      "Log Regression(3271/4999): loss=0.5407185573043405\n",
      "Log Regression(3272/4999): loss=0.541888151810015\n",
      "Log Regression(3273/4999): loss=0.5449795846609946\n",
      "Log Regression(3274/4999): loss=0.5417712608626252\n",
      "Log Regression(3275/4999): loss=0.5481343693870495\n",
      "Log Regression(3276/4999): loss=0.555394000283974\n",
      "Log Regression(3277/4999): loss=0.5512230137686146\n",
      "Log Regression(3278/4999): loss=0.5410702422382322\n",
      "Log Regression(3279/4999): loss=0.5409589391510383\n",
      "Log Regression(3280/4999): loss=0.5426247195813537\n",
      "Log Regression(3281/4999): loss=0.5428692960563483\n",
      "Log Regression(3282/4999): loss=0.5504095492108734\n",
      "Log Regression(3283/4999): loss=0.541041973675185\n",
      "Log Regression(3284/4999): loss=0.5411809994304259\n",
      "Log Regression(3285/4999): loss=0.5406423920773125\n",
      "Log Regression(3286/4999): loss=0.5459225103323471\n",
      "Log Regression(3287/4999): loss=0.5417327881584383\n",
      "Log Regression(3288/4999): loss=0.5404325733335813\n",
      "Log Regression(3289/4999): loss=0.543396888749492\n",
      "Log Regression(3290/4999): loss=0.5433858811128061\n",
      "Log Regression(3291/4999): loss=0.5411533626086734\n",
      "Log Regression(3292/4999): loss=0.5406981664127465\n",
      "Log Regression(3293/4999): loss=0.5406148140638809\n",
      "Log Regression(3294/4999): loss=0.5414578137893155\n",
      "Log Regression(3295/4999): loss=0.5464664101686726\n",
      "Log Regression(3296/4999): loss=0.5422938377041839\n",
      "Log Regression(3297/4999): loss=0.5406430289493375\n",
      "Log Regression(3298/4999): loss=0.544508234635829\n",
      "Log Regression(3299/4999): loss=0.5409068985712611\n",
      "Log Regression(3300/4999): loss=0.5435161791607601\n",
      "Log Regression(3301/4999): loss=0.5524389989265427\n",
      "Log Regression(3302/4999): loss=0.5412203534253103\n",
      "Log Regression(3303/4999): loss=0.5410323244638386\n",
      "Log Regression(3304/4999): loss=0.5445915277384226\n",
      "Log Regression(3305/4999): loss=0.5433802383969007\n",
      "Log Regression(3306/4999): loss=0.5415830836901836\n",
      "Log Regression(3307/4999): loss=0.5428292369737437\n",
      "Log Regression(3308/4999): loss=0.5404467179894715\n",
      "Log Regression(3309/4999): loss=0.5409188272846486\n",
      "Log Regression(3310/4999): loss=0.5431722545540015\n",
      "Log Regression(3311/4999): loss=0.5405671477485342\n",
      "Log Regression(3312/4999): loss=0.5404435558861722\n",
      "Log Regression(3313/4999): loss=0.5415412310157312\n",
      "Log Regression(3314/4999): loss=0.5417286981967087\n",
      "Log Regression(3315/4999): loss=0.5411550595252368\n",
      "Log Regression(3316/4999): loss=0.540458975978807\n",
      "Log Regression(3317/4999): loss=0.5403977811028776\n",
      "Log Regression(3318/4999): loss=0.5429685891526499\n",
      "Log Regression(3319/4999): loss=0.5448658143023818\n",
      "Log Regression(3320/4999): loss=0.546802581303774\n",
      "Log Regression(3321/4999): loss=0.5447716554294054\n",
      "Log Regression(3322/4999): loss=0.5487490935363873\n",
      "Log Regression(3323/4999): loss=0.554523642770477\n",
      "Log Regression(3324/4999): loss=0.5474144979603061\n",
      "Log Regression(3325/4999): loss=0.5496615437923651\n",
      "Log Regression(3326/4999): loss=0.5452718463538851\n",
      "Log Regression(3327/4999): loss=0.5406715412324621\n",
      "Log Regression(3328/4999): loss=0.5413316713509323\n",
      "Log Regression(3329/4999): loss=0.5410971257480675\n",
      "Log Regression(3330/4999): loss=0.5410345742629951\n",
      "Log Regression(3331/4999): loss=0.541717441080634\n",
      "Log Regression(3332/4999): loss=0.5436645787658203\n",
      "Log Regression(3333/4999): loss=0.5440037168637932\n",
      "Log Regression(3334/4999): loss=0.5484613807530565\n",
      "Log Regression(3335/4999): loss=0.5497860687786447\n",
      "Log Regression(3336/4999): loss=0.5422950952058871\n",
      "Log Regression(3337/4999): loss=0.5405306862359912\n",
      "Log Regression(3338/4999): loss=0.5408111764189423\n",
      "Log Regression(3339/4999): loss=0.5403573999142038\n",
      "Log Regression(3340/4999): loss=0.5403714770381264\n",
      "Log Regression(3341/4999): loss=0.5404062623742047\n",
      "Log Regression(3342/4999): loss=0.5404340527432471\n",
      "Log Regression(3343/4999): loss=0.5403630375007297\n",
      "Log Regression(3344/4999): loss=0.5404370541577489\n",
      "Log Regression(3345/4999): loss=0.5403899487407413\n",
      "Log Regression(3346/4999): loss=0.5404884606054859\n",
      "Log Regression(3347/4999): loss=0.5411195894522629\n",
      "Log Regression(3348/4999): loss=0.5467583738945146\n",
      "Log Regression(3349/4999): loss=0.5420761028504517\n",
      "Log Regression(3350/4999): loss=0.5421039670983231\n",
      "Log Regression(3351/4999): loss=0.542445880613891\n",
      "Log Regression(3352/4999): loss=0.5419494109121556\n",
      "Log Regression(3353/4999): loss=0.5417629999784602\n",
      "Log Regression(3354/4999): loss=0.544148284304823\n",
      "Log Regression(3355/4999): loss=0.5422105554342243\n",
      "Log Regression(3356/4999): loss=0.5406017965333686\n",
      "Log Regression(3357/4999): loss=0.5404843016676617\n",
      "Log Regression(3358/4999): loss=0.5423051068803881\n",
      "Log Regression(3359/4999): loss=0.5423135021866345\n",
      "Log Regression(3360/4999): loss=0.5404731862672261\n",
      "Log Regression(3361/4999): loss=0.5413976726272004\n",
      "Log Regression(3362/4999): loss=0.542962201666233\n",
      "Log Regression(3363/4999): loss=0.5420349780101947\n",
      "Log Regression(3364/4999): loss=0.5422519127850287\n",
      "Log Regression(3365/4999): loss=0.5416647155488619\n",
      "Log Regression(3366/4999): loss=0.5480861534948235\n",
      "Log Regression(3367/4999): loss=0.5460177749176142\n",
      "Log Regression(3368/4999): loss=0.5473990994357035\n",
      "Log Regression(3369/4999): loss=0.5479785837901178\n",
      "Log Regression(3370/4999): loss=0.5416839440036586\n",
      "Log Regression(3371/4999): loss=0.543635955063684\n",
      "Log Regression(3372/4999): loss=0.5406825244954347\n",
      "Log Regression(3373/4999): loss=0.5421905302697294\n",
      "Log Regression(3374/4999): loss=0.5443371043021925\n",
      "Log Regression(3375/4999): loss=0.5447228597132041\n",
      "Log Regression(3376/4999): loss=0.5425358225152566\n",
      "Log Regression(3377/4999): loss=0.5410848794144417\n",
      "Log Regression(3378/4999): loss=0.5419924496070527\n",
      "Log Regression(3379/4999): loss=0.5416763000037561\n",
      "Log Regression(3380/4999): loss=0.5411285992700832\n",
      "Log Regression(3381/4999): loss=0.5417948091796115\n",
      "Log Regression(3382/4999): loss=0.5417020130017773\n",
      "Log Regression(3383/4999): loss=0.5417707603533551\n",
      "Log Regression(3384/4999): loss=0.5410050822069072\n",
      "Log Regression(3385/4999): loss=0.5412012678153173\n",
      "Log Regression(3386/4999): loss=0.5415982536303653\n",
      "Log Regression(3387/4999): loss=0.5413336213513337\n",
      "Log Regression(3388/4999): loss=0.5412496260626042\n",
      "Log Regression(3389/4999): loss=0.5410102991643484\n",
      "Log Regression(3390/4999): loss=0.5408601236894166\n",
      "Log Regression(3391/4999): loss=0.5435020171003709\n",
      "Log Regression(3392/4999): loss=0.5410303215173664\n",
      "Log Regression(3393/4999): loss=0.5415145780367304\n",
      "Log Regression(3394/4999): loss=0.5413951516481083\n",
      "Log Regression(3395/4999): loss=0.5411207105543576\n",
      "Log Regression(3396/4999): loss=0.5415140419115416\n",
      "Log Regression(3397/4999): loss=0.5408340132605641\n",
      "Log Regression(3398/4999): loss=0.5407409196309187\n",
      "Log Regression(3399/4999): loss=0.5404673596774078\n",
      "Log Regression(3400/4999): loss=0.5404182477333624\n",
      "Log Regression(3401/4999): loss=0.5404853373403228\n",
      "Log Regression(3402/4999): loss=0.5406047679633255\n",
      "Log Regression(3403/4999): loss=0.5420461380879072\n",
      "Log Regression(3404/4999): loss=0.5406869016383057\n",
      "Log Regression(3405/4999): loss=0.5406878608637617\n",
      "Log Regression(3406/4999): loss=0.540645667182237\n",
      "Log Regression(3407/4999): loss=0.5415662220072675\n",
      "Log Regression(3408/4999): loss=0.5417349277889619\n",
      "Log Regression(3409/4999): loss=0.541114772280934\n",
      "Log Regression(3410/4999): loss=0.540952282509007\n",
      "Log Regression(3411/4999): loss=0.5406950130974353\n",
      "Log Regression(3412/4999): loss=0.5406006080073095\n",
      "Log Regression(3413/4999): loss=0.5411758175934966\n",
      "Log Regression(3414/4999): loss=0.5407313202574809\n",
      "Log Regression(3415/4999): loss=0.5450962573787647\n",
      "Log Regression(3416/4999): loss=0.5406841969576226\n",
      "Log Regression(3417/4999): loss=0.5404222998393335\n",
      "Log Regression(3418/4999): loss=0.542723139215451\n",
      "Log Regression(3419/4999): loss=0.5408236975038542\n",
      "Log Regression(3420/4999): loss=0.54028635003427\n",
      "Log Regression(3421/4999): loss=0.5423985953182265\n",
      "Log Regression(3422/4999): loss=0.5419972637065262\n",
      "Log Regression(3423/4999): loss=0.5406928244112708\n",
      "Log Regression(3424/4999): loss=0.5441310246973119\n",
      "Log Regression(3425/4999): loss=0.5403405329789867\n",
      "Log Regression(3426/4999): loss=0.540287598744077\n",
      "Log Regression(3427/4999): loss=0.5404243512891925\n",
      "Log Regression(3428/4999): loss=0.5408626516992022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(3429/4999): loss=0.5417656189176131\n",
      "Log Regression(3430/4999): loss=0.5411259934437762\n",
      "Log Regression(3431/4999): loss=0.5404751822498576\n",
      "Log Regression(3432/4999): loss=0.5406163254043193\n",
      "Log Regression(3433/4999): loss=0.5417860282355076\n",
      "Log Regression(3434/4999): loss=0.5465506927454773\n",
      "Log Regression(3435/4999): loss=0.542245729958542\n",
      "Log Regression(3436/4999): loss=0.5418787335941746\n",
      "Log Regression(3437/4999): loss=0.5432486472960247\n",
      "Log Regression(3438/4999): loss=0.5404918881452714\n",
      "Log Regression(3439/4999): loss=0.5405174921391179\n",
      "Log Regression(3440/4999): loss=0.5405334079390696\n",
      "Log Regression(3441/4999): loss=0.5422904204614665\n",
      "Log Regression(3442/4999): loss=0.5422045057161786\n",
      "Log Regression(3443/4999): loss=0.541839931696302\n",
      "Log Regression(3444/4999): loss=0.5418115846102481\n",
      "Log Regression(3445/4999): loss=0.5459730150976578\n",
      "Log Regression(3446/4999): loss=0.5407761589473179\n",
      "Log Regression(3447/4999): loss=0.5414771214422657\n",
      "Log Regression(3448/4999): loss=0.5417622531330086\n",
      "Log Regression(3449/4999): loss=0.5426649775570258\n",
      "Log Regression(3450/4999): loss=0.5435665115687189\n",
      "Log Regression(3451/4999): loss=0.5411982002067398\n",
      "Log Regression(3452/4999): loss=0.5416556716510649\n",
      "Log Regression(3453/4999): loss=0.5425815197906425\n",
      "Log Regression(3454/4999): loss=0.5454363753226399\n",
      "Log Regression(3455/4999): loss=0.5486067176910403\n",
      "Log Regression(3456/4999): loss=0.5421409718139603\n",
      "Log Regression(3457/4999): loss=0.5405104157154819\n",
      "Log Regression(3458/4999): loss=0.5406850087695537\n",
      "Log Regression(3459/4999): loss=0.5437134741853219\n",
      "Log Regression(3460/4999): loss=0.5410510952889237\n",
      "Log Regression(3461/4999): loss=0.5404751817201201\n",
      "Log Regression(3462/4999): loss=0.5428079386585958\n",
      "Log Regression(3463/4999): loss=0.5454911918486284\n",
      "Log Regression(3464/4999): loss=0.5471557991921485\n",
      "Log Regression(3465/4999): loss=0.5415638707004277\n",
      "Log Regression(3466/4999): loss=0.5410043143633928\n",
      "Log Regression(3467/4999): loss=0.5447973461233357\n",
      "Log Regression(3468/4999): loss=0.5451765609213297\n",
      "Log Regression(3469/4999): loss=0.5423146053030801\n",
      "Log Regression(3470/4999): loss=0.5436900377154258\n",
      "Log Regression(3471/4999): loss=0.5427848709288362\n",
      "Log Regression(3472/4999): loss=0.5418508175917625\n",
      "Log Regression(3473/4999): loss=0.5414592926672123\n",
      "Log Regression(3474/4999): loss=0.543043986706537\n",
      "Log Regression(3475/4999): loss=0.5427135286849185\n",
      "Log Regression(3476/4999): loss=0.5409855402645231\n",
      "Log Regression(3477/4999): loss=0.5411818018545729\n",
      "Log Regression(3478/4999): loss=0.5427869920424561\n",
      "Log Regression(3479/4999): loss=0.5442834927391259\n",
      "Log Regression(3480/4999): loss=0.5455227814290164\n",
      "Log Regression(3481/4999): loss=0.5438369713488206\n",
      "Log Regression(3482/4999): loss=0.546338669906328\n",
      "Log Regression(3483/4999): loss=0.5440405437834396\n",
      "Log Regression(3484/4999): loss=0.5409779980361679\n",
      "Log Regression(3485/4999): loss=0.5411969062676892\n",
      "Log Regression(3486/4999): loss=0.5436413365329062\n",
      "Log Regression(3487/4999): loss=0.5428838867284417\n",
      "Log Regression(3488/4999): loss=0.5428950026621435\n",
      "Log Regression(3489/4999): loss=0.5407416630766722\n",
      "Log Regression(3490/4999): loss=0.5406969978542882\n",
      "Log Regression(3491/4999): loss=0.5429515656563864\n",
      "Log Regression(3492/4999): loss=0.5440335714783822\n",
      "Log Regression(3493/4999): loss=0.5431127022987721\n",
      "Log Regression(3494/4999): loss=0.5421189962312795\n",
      "Log Regression(3495/4999): loss=0.5426797957026711\n",
      "Log Regression(3496/4999): loss=0.5430119662903824\n",
      "Log Regression(3497/4999): loss=0.5406409429992106\n",
      "Log Regression(3498/4999): loss=0.5415089662998838\n",
      "Log Regression(3499/4999): loss=0.5407726989406988\n",
      "Log Regression(3500/4999): loss=0.540326151079496\n",
      "Log Regression(3501/4999): loss=0.5434673147936897\n",
      "Log Regression(3502/4999): loss=0.5409794004338434\n",
      "Log Regression(3503/4999): loss=0.5466729607313107\n",
      "Log Regression(3504/4999): loss=0.5472029109866635\n",
      "Log Regression(3505/4999): loss=0.5427861829387249\n",
      "Log Regression(3506/4999): loss=0.5405702137363396\n",
      "Log Regression(3507/4999): loss=0.5407058856865129\n",
      "Log Regression(3508/4999): loss=0.54104704149502\n",
      "Log Regression(3509/4999): loss=0.5409832373257352\n",
      "Log Regression(3510/4999): loss=0.5406110888214282\n",
      "Log Regression(3511/4999): loss=0.5428442089108528\n",
      "Log Regression(3512/4999): loss=0.5405952226245974\n",
      "Log Regression(3513/4999): loss=0.5403302472965854\n",
      "Log Regression(3514/4999): loss=0.5409433686821981\n",
      "Log Regression(3515/4999): loss=0.5403500518480094\n",
      "Log Regression(3516/4999): loss=0.5410660741935385\n",
      "Log Regression(3517/4999): loss=0.543508404884899\n",
      "Log Regression(3518/4999): loss=0.551322192818162\n",
      "Log Regression(3519/4999): loss=0.5563197836733197\n",
      "Log Regression(3520/4999): loss=0.5436900568904024\n",
      "Log Regression(3521/4999): loss=0.5407510463075532\n",
      "Log Regression(3522/4999): loss=0.5409880337201229\n",
      "Log Regression(3523/4999): loss=0.5413723561088223\n",
      "Log Regression(3524/4999): loss=0.5411703696713056\n",
      "Log Regression(3525/4999): loss=0.5464968047970492\n",
      "Log Regression(3526/4999): loss=0.5418272499012634\n",
      "Log Regression(3527/4999): loss=0.540667998516224\n",
      "Log Regression(3528/4999): loss=0.54053687670759\n",
      "Log Regression(3529/4999): loss=0.5430048880846119\n",
      "Log Regression(3530/4999): loss=0.5450400118612977\n",
      "Log Regression(3531/4999): loss=0.541191078459381\n",
      "Log Regression(3532/4999): loss=0.5409230047347131\n",
      "Log Regression(3533/4999): loss=0.5404906340867617\n",
      "Log Regression(3534/4999): loss=0.5409711370046517\n",
      "Log Regression(3535/4999): loss=0.5411688720522704\n",
      "Log Regression(3536/4999): loss=0.5403342376556244\n",
      "Log Regression(3537/4999): loss=0.5421339956165375\n",
      "Log Regression(3538/4999): loss=0.5430804497000963\n",
      "Log Regression(3539/4999): loss=0.5430373413522204\n",
      "Log Regression(3540/4999): loss=0.5424629509707031\n",
      "Log Regression(3541/4999): loss=0.5420490778122398\n",
      "Log Regression(3542/4999): loss=0.5407477839203226\n",
      "Log Regression(3543/4999): loss=0.5412811523591078\n",
      "Log Regression(3544/4999): loss=0.542019848793023\n",
      "Log Regression(3545/4999): loss=0.5410543963002397\n",
      "Log Regression(3546/4999): loss=0.5483457935490483\n",
      "Log Regression(3547/4999): loss=0.5419487676210129\n",
      "Log Regression(3548/4999): loss=0.5417291976852092\n",
      "Log Regression(3549/4999): loss=0.540578209435073\n",
      "Log Regression(3550/4999): loss=0.5420990294970874\n",
      "Log Regression(3551/4999): loss=0.5430148519526854\n",
      "Log Regression(3552/4999): loss=0.5416719323936307\n",
      "Log Regression(3553/4999): loss=0.5428205343028147\n",
      "Log Regression(3554/4999): loss=0.5483667858743297\n",
      "Log Regression(3555/4999): loss=0.5450727604080685\n",
      "Log Regression(3556/4999): loss=0.5435753028957457\n",
      "Log Regression(3557/4999): loss=0.5419946539147859\n",
      "Log Regression(3558/4999): loss=0.5411204865972246\n",
      "Log Regression(3559/4999): loss=0.5405239595414622\n",
      "Log Regression(3560/4999): loss=0.5413147083873363\n",
      "Log Regression(3561/4999): loss=0.5416019108236462\n",
      "Log Regression(3562/4999): loss=0.5408384359544407\n",
      "Log Regression(3563/4999): loss=0.5403183812161985\n",
      "Log Regression(3564/4999): loss=0.5411677590906067\n",
      "Log Regression(3565/4999): loss=0.5403567511603835\n",
      "Log Regression(3566/4999): loss=0.5403664775023612\n",
      "Log Regression(3567/4999): loss=0.5405467302721574\n",
      "Log Regression(3568/4999): loss=0.5408894493341538\n",
      "Log Regression(3569/4999): loss=0.542864257795878\n",
      "Log Regression(3570/4999): loss=0.5456054479017958\n",
      "Log Regression(3571/4999): loss=0.5499685800646359\n",
      "Log Regression(3572/4999): loss=0.5465149184794718\n",
      "Log Regression(3573/4999): loss=0.5428491121501178\n",
      "Log Regression(3574/4999): loss=0.5405772841983468\n",
      "Log Regression(3575/4999): loss=0.5413458781529138\n",
      "Log Regression(3576/4999): loss=0.5416010750219566\n",
      "Log Regression(3577/4999): loss=0.5443377970830112\n",
      "Log Regression(3578/4999): loss=0.5457235559329496\n",
      "Log Regression(3579/4999): loss=0.5409950104937578\n",
      "Log Regression(3580/4999): loss=0.5433530549407508\n",
      "Log Regression(3581/4999): loss=0.5415400571766892\n",
      "Log Regression(3582/4999): loss=0.5412730959161877\n",
      "Log Regression(3583/4999): loss=0.5411011162477806\n",
      "Log Regression(3584/4999): loss=0.541433518464753\n",
      "Log Regression(3585/4999): loss=0.541732582775202\n",
      "Log Regression(3586/4999): loss=0.5452946359551207\n",
      "Log Regression(3587/4999): loss=0.5423559019963454\n",
      "Log Regression(3588/4999): loss=0.5490231599577521\n",
      "Log Regression(3589/4999): loss=0.5447585222599434\n",
      "Log Regression(3590/4999): loss=0.5409182142231277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(3591/4999): loss=0.5422597336641652\n",
      "Log Regression(3592/4999): loss=0.5407245256309421\n",
      "Log Regression(3593/4999): loss=0.5433254382286402\n",
      "Log Regression(3594/4999): loss=0.541470157249919\n",
      "Log Regression(3595/4999): loss=0.540844117553087\n",
      "Log Regression(3596/4999): loss=0.5430197217192483\n",
      "Log Regression(3597/4999): loss=0.5420228159276735\n",
      "Log Regression(3598/4999): loss=0.5403885045440635\n",
      "Log Regression(3599/4999): loss=0.5420565860392477\n",
      "Log Regression(3600/4999): loss=0.5419850962982979\n",
      "Log Regression(3601/4999): loss=0.5406936157047024\n",
      "Log Regression(3602/4999): loss=0.5409637546274362\n",
      "Log Regression(3603/4999): loss=0.5504972571988689\n",
      "Log Regression(3604/4999): loss=0.5514226766305623\n",
      "Log Regression(3605/4999): loss=0.5442794184358717\n",
      "Log Regression(3606/4999): loss=0.5418054199227614\n",
      "Log Regression(3607/4999): loss=0.5402857698113596\n",
      "Log Regression(3608/4999): loss=0.5409042463215341\n",
      "Log Regression(3609/4999): loss=0.5522258032911214\n",
      "Log Regression(3610/4999): loss=0.554661619512317\n",
      "Log Regression(3611/4999): loss=0.5439435558940435\n",
      "Log Regression(3612/4999): loss=0.5412345896767873\n",
      "Log Regression(3613/4999): loss=0.5419752339080756\n",
      "Log Regression(3614/4999): loss=0.5406891598756572\n",
      "Log Regression(3615/4999): loss=0.5406011213784133\n",
      "Log Regression(3616/4999): loss=0.542007988586121\n",
      "Log Regression(3617/4999): loss=0.540904871299065\n",
      "Log Regression(3618/4999): loss=0.5410435241049415\n",
      "Log Regression(3619/4999): loss=0.541166914314409\n",
      "Log Regression(3620/4999): loss=0.546331107685973\n",
      "Log Regression(3621/4999): loss=0.5439248794111182\n",
      "Log Regression(3622/4999): loss=0.5436649489563435\n",
      "Log Regression(3623/4999): loss=0.5452478620691017\n",
      "Log Regression(3624/4999): loss=0.54045124827238\n",
      "Log Regression(3625/4999): loss=0.5403946920979085\n",
      "Log Regression(3626/4999): loss=0.5404999500295868\n",
      "Log Regression(3627/4999): loss=0.5415110916092843\n",
      "Log Regression(3628/4999): loss=0.5417563551143701\n",
      "Log Regression(3629/4999): loss=0.5409902755682144\n",
      "Log Regression(3630/4999): loss=0.5402514003997486\n",
      "Log Regression(3631/4999): loss=0.5418705990261891\n",
      "Log Regression(3632/4999): loss=0.5420070660561311\n",
      "Log Regression(3633/4999): loss=0.5431727699135582\n",
      "Log Regression(3634/4999): loss=0.5410473603249517\n",
      "Log Regression(3635/4999): loss=0.5427378123768568\n",
      "Log Regression(3636/4999): loss=0.5413829519513504\n",
      "Log Regression(3637/4999): loss=0.5407945151259566\n",
      "Log Regression(3638/4999): loss=0.5419807235186601\n",
      "Log Regression(3639/4999): loss=0.5404627693269078\n",
      "Log Regression(3640/4999): loss=0.5411168920106338\n",
      "Log Regression(3641/4999): loss=0.5427355157913937\n",
      "Log Regression(3642/4999): loss=0.5416426361142943\n",
      "Log Regression(3643/4999): loss=0.5466241806760626\n",
      "Log Regression(3644/4999): loss=0.5433941642407278\n",
      "Log Regression(3645/4999): loss=0.5411196437224258\n",
      "Log Regression(3646/4999): loss=0.5409045226745622\n",
      "Log Regression(3647/4999): loss=0.5404678806988881\n",
      "Log Regression(3648/4999): loss=0.5408499288419598\n",
      "Log Regression(3649/4999): loss=0.5410523174817597\n",
      "Log Regression(3650/4999): loss=0.5406713691415191\n",
      "Log Regression(3651/4999): loss=0.5472751074727878\n",
      "Log Regression(3652/4999): loss=0.5424714581121344\n",
      "Log Regression(3653/4999): loss=0.5411796064038453\n",
      "Log Regression(3654/4999): loss=0.541199161944562\n",
      "Log Regression(3655/4999): loss=0.5406410812741371\n",
      "Log Regression(3656/4999): loss=0.5404959641667821\n",
      "Log Regression(3657/4999): loss=0.5422211646820623\n",
      "Log Regression(3658/4999): loss=0.5419652250997149\n",
      "Log Regression(3659/4999): loss=0.540633361613845\n",
      "Log Regression(3660/4999): loss=0.5446356779821949\n",
      "Log Regression(3661/4999): loss=0.540846967954663\n",
      "Log Regression(3662/4999): loss=0.5414078910088316\n",
      "Log Regression(3663/4999): loss=0.543570735840622\n",
      "Log Regression(3664/4999): loss=0.5410355823581187\n",
      "Log Regression(3665/4999): loss=0.5404537359790811\n",
      "Log Regression(3666/4999): loss=0.5417512094317203\n",
      "Log Regression(3667/4999): loss=0.5404264536277724\n",
      "Log Regression(3668/4999): loss=0.5407596061585089\n",
      "Log Regression(3669/4999): loss=0.5420521768585328\n",
      "Log Regression(3670/4999): loss=0.5421214082307806\n",
      "Log Regression(3671/4999): loss=0.5402945247738232\n",
      "Log Regression(3672/4999): loss=0.5414235650674213\n",
      "Log Regression(3673/4999): loss=0.5409445472750285\n",
      "Log Regression(3674/4999): loss=0.5424080788454221\n",
      "Log Regression(3675/4999): loss=0.5429165532252744\n",
      "Log Regression(3676/4999): loss=0.540552069447172\n",
      "Log Regression(3677/4999): loss=0.5413979933819659\n",
      "Log Regression(3678/4999): loss=0.5411658401857738\n",
      "Log Regression(3679/4999): loss=0.5406590898885719\n",
      "Log Regression(3680/4999): loss=0.5421222242614979\n",
      "Log Regression(3681/4999): loss=0.5490547515842027\n",
      "Log Regression(3682/4999): loss=0.5413052620055911\n",
      "Log Regression(3683/4999): loss=0.5408427825217288\n",
      "Log Regression(3684/4999): loss=0.5402745746492302\n",
      "Log Regression(3685/4999): loss=0.5403967814089253\n",
      "Log Regression(3686/4999): loss=0.5403155540628212\n",
      "Log Regression(3687/4999): loss=0.5406712327694208\n",
      "Log Regression(3688/4999): loss=0.5404506590868131\n",
      "Log Regression(3689/4999): loss=0.5434937586914563\n",
      "Log Regression(3690/4999): loss=0.5427173100855133\n",
      "Log Regression(3691/4999): loss=0.5404800682277905\n",
      "Log Regression(3692/4999): loss=0.540773157244473\n",
      "Log Regression(3693/4999): loss=0.5419432584096553\n",
      "Log Regression(3694/4999): loss=0.5405713062515073\n",
      "Log Regression(3695/4999): loss=0.5405631485714656\n",
      "Log Regression(3696/4999): loss=0.5413125684960701\n",
      "Log Regression(3697/4999): loss=0.5406574464518119\n",
      "Log Regression(3698/4999): loss=0.5408794781570907\n",
      "Log Regression(3699/4999): loss=0.5406662913355793\n",
      "Log Regression(3700/4999): loss=0.5429302504074359\n",
      "Log Regression(3701/4999): loss=0.5406775971101412\n",
      "Log Regression(3702/4999): loss=0.5402819402570264\n",
      "Log Regression(3703/4999): loss=0.5402885596947228\n",
      "Log Regression(3704/4999): loss=0.5407324933625912\n",
      "Log Regression(3705/4999): loss=0.542092650794673\n",
      "Log Regression(3706/4999): loss=0.5489051215041917\n",
      "Log Regression(3707/4999): loss=0.5441899656539237\n",
      "Log Regression(3708/4999): loss=0.5500748164887012\n",
      "Log Regression(3709/4999): loss=0.543836794050153\n",
      "Log Regression(3710/4999): loss=0.5404491633280825\n",
      "Log Regression(3711/4999): loss=0.5402645496758424\n",
      "Log Regression(3712/4999): loss=0.5405976810928298\n",
      "Log Regression(3713/4999): loss=0.541715251622197\n",
      "Log Regression(3714/4999): loss=0.5474957064912197\n",
      "Log Regression(3715/4999): loss=0.552096349701499\n",
      "Log Regression(3716/4999): loss=0.5433686242676854\n",
      "Log Regression(3717/4999): loss=0.540226154262025\n",
      "Log Regression(3718/4999): loss=0.5409697607309103\n",
      "Log Regression(3719/4999): loss=0.540594058187939\n",
      "Log Regression(3720/4999): loss=0.5405049704473585\n",
      "Log Regression(3721/4999): loss=0.5403036657700199\n",
      "Log Regression(3722/4999): loss=0.5418868684238294\n",
      "Log Regression(3723/4999): loss=0.5414465714754082\n",
      "Log Regression(3724/4999): loss=0.5404184395781458\n",
      "Log Regression(3725/4999): loss=0.5410503575858789\n",
      "Log Regression(3726/4999): loss=0.5403121142333043\n",
      "Log Regression(3727/4999): loss=0.5410646353069303\n",
      "Log Regression(3728/4999): loss=0.5411906619847757\n",
      "Log Regression(3729/4999): loss=0.5402668421058665\n",
      "Log Regression(3730/4999): loss=0.5405889389563057\n",
      "Log Regression(3731/4999): loss=0.5403362280477526\n",
      "Log Regression(3732/4999): loss=0.5403572048598879\n",
      "Log Regression(3733/4999): loss=0.5402510551334423\n",
      "Log Regression(3734/4999): loss=0.5404643663905021\n",
      "Log Regression(3735/4999): loss=0.5403117005950806\n",
      "Log Regression(3736/4999): loss=0.5403633503980025\n",
      "Log Regression(3737/4999): loss=0.5414403585376617\n",
      "Log Regression(3738/4999): loss=0.5415400345044155\n",
      "Log Regression(3739/4999): loss=0.5424053583670801\n",
      "Log Regression(3740/4999): loss=0.5403410548512259\n",
      "Log Regression(3741/4999): loss=0.5406700318294599\n",
      "Log Regression(3742/4999): loss=0.5404264044712475\n",
      "Log Regression(3743/4999): loss=0.541906250998777\n",
      "Log Regression(3744/4999): loss=0.5421292507463548\n",
      "Log Regression(3745/4999): loss=0.5409706367084558\n",
      "Log Regression(3746/4999): loss=0.5403779396647659\n",
      "Log Regression(3747/4999): loss=0.5403358425640494\n",
      "Log Regression(3748/4999): loss=0.542069802408024\n",
      "Log Regression(3749/4999): loss=0.546076978377141\n",
      "Log Regression(3750/4999): loss=0.5420336257795563\n",
      "Log Regression(3751/4999): loss=0.545176208111087\n",
      "Log Regression(3752/4999): loss=0.546699256657306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(3753/4999): loss=0.5408147033793915\n",
      "Log Regression(3754/4999): loss=0.5403873001271778\n",
      "Log Regression(3755/4999): loss=0.5403673388483335\n",
      "Log Regression(3756/4999): loss=0.5405031318286986\n",
      "Log Regression(3757/4999): loss=0.5414460007493471\n",
      "Log Regression(3758/4999): loss=0.541424119311992\n",
      "Log Regression(3759/4999): loss=0.5431876037306046\n",
      "Log Regression(3760/4999): loss=0.5420842325005896\n",
      "Log Regression(3761/4999): loss=0.5405866293415367\n",
      "Log Regression(3762/4999): loss=0.5413259618894486\n",
      "Log Regression(3763/4999): loss=0.5488725519486036\n",
      "Log Regression(3764/4999): loss=0.5459520196949205\n",
      "Log Regression(3765/4999): loss=0.5496023471955069\n",
      "Log Regression(3766/4999): loss=0.5432380022536774\n",
      "Log Regression(3767/4999): loss=0.5414576686083515\n",
      "Log Regression(3768/4999): loss=0.5423323279083615\n",
      "Log Regression(3769/4999): loss=0.5403578023982488\n",
      "Log Regression(3770/4999): loss=0.5417621334222997\n",
      "Log Regression(3771/4999): loss=0.5407856325984257\n",
      "Log Regression(3772/4999): loss=0.5405123712400762\n",
      "Log Regression(3773/4999): loss=0.5403815228465063\n",
      "Log Regression(3774/4999): loss=0.5403268634323012\n",
      "Log Regression(3775/4999): loss=0.5402971269047058\n",
      "Log Regression(3776/4999): loss=0.5406780916302993\n",
      "Log Regression(3777/4999): loss=0.5412174843385449\n",
      "Log Regression(3778/4999): loss=0.5408687135240537\n",
      "Log Regression(3779/4999): loss=0.5431419651388586\n",
      "Log Regression(3780/4999): loss=0.5438651751674874\n",
      "Log Regression(3781/4999): loss=0.54201498478585\n",
      "Log Regression(3782/4999): loss=0.5402256046362836\n",
      "Log Regression(3783/4999): loss=0.543200723662811\n",
      "Log Regression(3784/4999): loss=0.5440737901285728\n",
      "Log Regression(3785/4999): loss=0.5405752633185823\n",
      "Log Regression(3786/4999): loss=0.5402874708373939\n",
      "Log Regression(3787/4999): loss=0.5405987672157504\n",
      "Log Regression(3788/4999): loss=0.5410764293834276\n",
      "Log Regression(3789/4999): loss=0.5416722383880153\n",
      "Log Regression(3790/4999): loss=0.5420632248019662\n",
      "Log Regression(3791/4999): loss=0.5402286482317586\n",
      "Log Regression(3792/4999): loss=0.540457945730458\n",
      "Log Regression(3793/4999): loss=0.5406270736056137\n",
      "Log Regression(3794/4999): loss=0.5404330315069538\n",
      "Log Regression(3795/4999): loss=0.540469827730122\n",
      "Log Regression(3796/4999): loss=0.5404567038330776\n",
      "Log Regression(3797/4999): loss=0.5402500150176746\n",
      "Log Regression(3798/4999): loss=0.5405358674083379\n",
      "Log Regression(3799/4999): loss=0.5427585701898563\n",
      "Log Regression(3800/4999): loss=0.5428771459203843\n",
      "Log Regression(3801/4999): loss=0.5402160422578414\n",
      "Log Regression(3802/4999): loss=0.5403407080991424\n",
      "Log Regression(3803/4999): loss=0.5403977168969304\n",
      "Log Regression(3804/4999): loss=0.5402401295939763\n",
      "Log Regression(3805/4999): loss=0.5403897582655414\n",
      "Log Regression(3806/4999): loss=0.5414909674626858\n",
      "Log Regression(3807/4999): loss=0.5419402135836835\n",
      "Log Regression(3808/4999): loss=0.5412103532854636\n",
      "Log Regression(3809/4999): loss=0.5441218301196784\n",
      "Log Regression(3810/4999): loss=0.5404690268791644\n",
      "Log Regression(3811/4999): loss=0.5421215076808438\n",
      "Log Regression(3812/4999): loss=0.5404323575692577\n",
      "Log Regression(3813/4999): loss=0.5415198045543012\n",
      "Log Regression(3814/4999): loss=0.5413899349747111\n",
      "Log Regression(3815/4999): loss=0.5403558759125852\n",
      "Log Regression(3816/4999): loss=0.5411703336623495\n",
      "Log Regression(3817/4999): loss=0.5404049311088432\n",
      "Log Regression(3818/4999): loss=0.5402769187221763\n",
      "Log Regression(3819/4999): loss=0.5410096237072513\n",
      "Log Regression(3820/4999): loss=0.5402964762546247\n",
      "Log Regression(3821/4999): loss=0.5465335973831225\n",
      "Log Regression(3822/4999): loss=0.5427157358299883\n",
      "Log Regression(3823/4999): loss=0.5405995728087583\n",
      "Log Regression(3824/4999): loss=0.5402349473411823\n",
      "Log Regression(3825/4999): loss=0.5410740481205212\n",
      "Log Regression(3826/4999): loss=0.5403921715918791\n",
      "Log Regression(3827/4999): loss=0.540314566153676\n",
      "Log Regression(3828/4999): loss=0.5437269080072179\n",
      "Log Regression(3829/4999): loss=0.5406520347001937\n",
      "Log Regression(3830/4999): loss=0.5420531121950181\n",
      "Log Regression(3831/4999): loss=0.5406259960436351\n",
      "Log Regression(3832/4999): loss=0.5412329424743807\n",
      "Log Regression(3833/4999): loss=0.540551611167839\n",
      "Log Regression(3834/4999): loss=0.5484492601682017\n",
      "Log Regression(3835/4999): loss=0.5407933868008001\n",
      "Log Regression(3836/4999): loss=0.5403143680910879\n",
      "Log Regression(3837/4999): loss=0.5407581597009299\n",
      "Log Regression(3838/4999): loss=0.5424450152592029\n",
      "Log Regression(3839/4999): loss=0.5402122878969469\n",
      "Log Regression(3840/4999): loss=0.5403088993165321\n",
      "Log Regression(3841/4999): loss=0.5406735270369691\n",
      "Log Regression(3842/4999): loss=0.5452665667951702\n",
      "Log Regression(3843/4999): loss=0.5458977596884269\n",
      "Log Regression(3844/4999): loss=0.5409576095413107\n",
      "Log Regression(3845/4999): loss=0.5431259340151362\n",
      "Log Regression(3846/4999): loss=0.5408367730340177\n",
      "Log Regression(3847/4999): loss=0.540938340179732\n",
      "Log Regression(3848/4999): loss=0.5406117735979068\n",
      "Log Regression(3849/4999): loss=0.5410893564372524\n",
      "Log Regression(3850/4999): loss=0.541013011296243\n",
      "Log Regression(3851/4999): loss=0.5419515119805406\n",
      "Log Regression(3852/4999): loss=0.5419059984202236\n",
      "Log Regression(3853/4999): loss=0.5426890917124153\n",
      "Log Regression(3854/4999): loss=0.5422883569581631\n",
      "Log Regression(3855/4999): loss=0.5405794602352082\n",
      "Log Regression(3856/4999): loss=0.5407409400368379\n",
      "Log Regression(3857/4999): loss=0.5403594496974438\n",
      "Log Regression(3858/4999): loss=0.5404426008564999\n",
      "Log Regression(3859/4999): loss=0.5416156742832956\n",
      "Log Regression(3860/4999): loss=0.543920794800067\n",
      "Log Regression(3861/4999): loss=0.5411907682135658\n",
      "Log Regression(3862/4999): loss=0.5402111731902287\n",
      "Log Regression(3863/4999): loss=0.540210974644892\n",
      "Log Regression(3864/4999): loss=0.5417792895141406\n",
      "Log Regression(3865/4999): loss=0.5417779968326252\n",
      "Log Regression(3866/4999): loss=0.541766238825993\n",
      "Log Regression(3867/4999): loss=0.5402251935123813\n",
      "Log Regression(3868/4999): loss=0.5411750499789452\n",
      "Log Regression(3869/4999): loss=0.5406279284925324\n",
      "Log Regression(3870/4999): loss=0.5419130812644904\n",
      "Log Regression(3871/4999): loss=0.5405147005157871\n",
      "Log Regression(3872/4999): loss=0.5404775992413108\n",
      "Log Regression(3873/4999): loss=0.5406490109312869\n",
      "Log Regression(3874/4999): loss=0.5432288216844561\n",
      "Log Regression(3875/4999): loss=0.5418993569328461\n",
      "Log Regression(3876/4999): loss=0.5403415762441162\n",
      "Log Regression(3877/4999): loss=0.5405098599526708\n",
      "Log Regression(3878/4999): loss=0.541454748208717\n",
      "Log Regression(3879/4999): loss=0.5413843227330335\n",
      "Log Regression(3880/4999): loss=0.5483114118865249\n",
      "Log Regression(3881/4999): loss=0.5518212937023097\n",
      "Log Regression(3882/4999): loss=0.5545139335301491\n",
      "Log Regression(3883/4999): loss=0.5411378711553303\n",
      "Log Regression(3884/4999): loss=0.5405712649500599\n",
      "Log Regression(3885/4999): loss=0.5404135490854298\n",
      "Log Regression(3886/4999): loss=0.540654731198401\n",
      "Log Regression(3887/4999): loss=0.5405504438129047\n",
      "Log Regression(3888/4999): loss=0.5444348497354993\n",
      "Log Regression(3889/4999): loss=0.5442189148709579\n",
      "Log Regression(3890/4999): loss=0.5465269235827439\n",
      "Log Regression(3891/4999): loss=0.5445515498591351\n",
      "Log Regression(3892/4999): loss=0.5407636831572749\n",
      "Log Regression(3893/4999): loss=0.5409918062838812\n",
      "Log Regression(3894/4999): loss=0.5432875019922985\n",
      "Log Regression(3895/4999): loss=0.5424931671839127\n",
      "Log Regression(3896/4999): loss=0.5412068030273927\n",
      "Log Regression(3897/4999): loss=0.5413937537377904\n",
      "Log Regression(3898/4999): loss=0.540461181687036\n",
      "Log Regression(3899/4999): loss=0.5407564230122028\n",
      "Log Regression(3900/4999): loss=0.5411834742355544\n",
      "Log Regression(3901/4999): loss=0.5417393074357373\n",
      "Log Regression(3902/4999): loss=0.540507757387027\n",
      "Log Regression(3903/4999): loss=0.5404386730557222\n",
      "Log Regression(3904/4999): loss=0.5507726088470402\n",
      "Log Regression(3905/4999): loss=0.5435839902137156\n",
      "Log Regression(3906/4999): loss=0.5431678553882173\n",
      "Log Regression(3907/4999): loss=0.5404938902553207\n",
      "Log Regression(3908/4999): loss=0.5418125671125721\n",
      "Log Regression(3909/4999): loss=0.5420485651182301\n",
      "Log Regression(3910/4999): loss=0.5406541874139996\n",
      "Log Regression(3911/4999): loss=0.5405111643616453\n",
      "Log Regression(3912/4999): loss=0.540507099969302\n",
      "Log Regression(3913/4999): loss=0.5432282336961952\n",
      "Log Regression(3914/4999): loss=0.5423375685937035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(3915/4999): loss=0.5403676637859233\n",
      "Log Regression(3916/4999): loss=0.5414845103467015\n",
      "Log Regression(3917/4999): loss=0.5413026841730532\n",
      "Log Regression(3918/4999): loss=0.5402321696286508\n",
      "Log Regression(3919/4999): loss=0.5405971714519896\n",
      "Log Regression(3920/4999): loss=0.5402588902293377\n",
      "Log Regression(3921/4999): loss=0.5403110117586455\n",
      "Log Regression(3922/4999): loss=0.542361800743062\n",
      "Log Regression(3923/4999): loss=0.5431652028452253\n",
      "Log Regression(3924/4999): loss=0.5412260342724589\n",
      "Log Regression(3925/4999): loss=0.5406525627377795\n",
      "Log Regression(3926/4999): loss=0.5403976759254189\n",
      "Log Regression(3927/4999): loss=0.5402824446775532\n",
      "Log Regression(3928/4999): loss=0.5406741185464622\n",
      "Log Regression(3929/4999): loss=0.5404156131162352\n",
      "Log Regression(3930/4999): loss=0.5407166034596728\n",
      "Log Regression(3931/4999): loss=0.5406912182302103\n",
      "Log Regression(3932/4999): loss=0.540240537377083\n",
      "Log Regression(3933/4999): loss=0.5436029884970461\n",
      "Log Regression(3934/4999): loss=0.5450810229799824\n",
      "Log Regression(3935/4999): loss=0.5423134205838199\n",
      "Log Regression(3936/4999): loss=0.5404560880147987\n",
      "Log Regression(3937/4999): loss=0.5412339865872912\n",
      "Log Regression(3938/4999): loss=0.5433037446201966\n",
      "Log Regression(3939/4999): loss=0.5405282637101048\n",
      "Log Regression(3940/4999): loss=0.5403279701453512\n",
      "Log Regression(3941/4999): loss=0.5410303472115912\n",
      "Log Regression(3942/4999): loss=0.5401923068986562\n",
      "Log Regression(3943/4999): loss=0.5403373335852392\n",
      "Log Regression(3944/4999): loss=0.5414322092801547\n",
      "Log Regression(3945/4999): loss=0.541349626603545\n",
      "Log Regression(3946/4999): loss=0.5403105450478755\n",
      "Log Regression(3947/4999): loss=0.5403371134770727\n",
      "Log Regression(3948/4999): loss=0.540311081714701\n",
      "Log Regression(3949/4999): loss=0.5414848048092291\n",
      "Log Regression(3950/4999): loss=0.5402537788562579\n",
      "Log Regression(3951/4999): loss=0.5414647184525098\n",
      "Log Regression(3952/4999): loss=0.5405833411318034\n",
      "Log Regression(3953/4999): loss=0.540892093894594\n",
      "Log Regression(3954/4999): loss=0.5414336747740874\n",
      "Log Regression(3955/4999): loss=0.5403754306262453\n",
      "Log Regression(3956/4999): loss=0.5402068393680947\n",
      "Log Regression(3957/4999): loss=0.5401880821347198\n",
      "Log Regression(3958/4999): loss=0.5410982108819206\n",
      "Log Regression(3959/4999): loss=0.5425643520125181\n",
      "Log Regression(3960/4999): loss=0.5426208907229239\n",
      "Log Regression(3961/4999): loss=0.540277624735328\n",
      "Log Regression(3962/4999): loss=0.5409398114764224\n",
      "Log Regression(3963/4999): loss=0.5431182685849779\n",
      "Log Regression(3964/4999): loss=0.5402632149759852\n",
      "Log Regression(3965/4999): loss=0.5406910863383271\n",
      "Log Regression(3966/4999): loss=0.5416908757254438\n",
      "Log Regression(3967/4999): loss=0.5402716121318744\n",
      "Log Regression(3968/4999): loss=0.5404691039226228\n",
      "Log Regression(3969/4999): loss=0.5402749616789585\n",
      "Log Regression(3970/4999): loss=0.5402949337588074\n",
      "Log Regression(3971/4999): loss=0.5402774143024338\n",
      "Log Regression(3972/4999): loss=0.5448335385943016\n",
      "Log Regression(3973/4999): loss=0.5450944758816726\n",
      "Log Regression(3974/4999): loss=0.5421978200654354\n",
      "Log Regression(3975/4999): loss=0.5447241417764478\n",
      "Log Regression(3976/4999): loss=0.5471767084219957\n",
      "Log Regression(3977/4999): loss=0.5438021613345225\n",
      "Log Regression(3978/4999): loss=0.5413569519366578\n",
      "Log Regression(3979/4999): loss=0.5414775378138289\n",
      "Log Regression(3980/4999): loss=0.5432784072809849\n",
      "Log Regression(3981/4999): loss=0.542659971596506\n",
      "Log Regression(3982/4999): loss=0.540940257335544\n",
      "Log Regression(3983/4999): loss=0.5410202979278091\n",
      "Log Regression(3984/4999): loss=0.542473801699297\n",
      "Log Regression(3985/4999): loss=0.5453342890405212\n",
      "Log Regression(3986/4999): loss=0.5426270028030283\n",
      "Log Regression(3987/4999): loss=0.5401734608992819\n",
      "Log Regression(3988/4999): loss=0.5408889852788079\n",
      "Log Regression(3989/4999): loss=0.5422222952561295\n",
      "Log Regression(3990/4999): loss=0.5403968581861418\n",
      "Log Regression(3991/4999): loss=0.5423843778299435\n",
      "Log Regression(3992/4999): loss=0.5414141398803178\n",
      "Log Regression(3993/4999): loss=0.5439575432814703\n",
      "Log Regression(3994/4999): loss=0.5431240837988918\n",
      "Log Regression(3995/4999): loss=0.5455164227600844\n",
      "Log Regression(3996/4999): loss=0.5419011495404888\n",
      "Log Regression(3997/4999): loss=0.541929785802673\n",
      "Log Regression(3998/4999): loss=0.5438708311950387\n",
      "Log Regression(3999/4999): loss=0.5410099646677474\n",
      "Log Regression(4000/4999): loss=0.541531625359462\n",
      "Log Regression(4001/4999): loss=0.5403458536224224\n",
      "Log Regression(4002/4999): loss=0.5406649278421247\n",
      "Log Regression(4003/4999): loss=0.5403842901725255\n",
      "Log Regression(4004/4999): loss=0.5418710065819404\n",
      "Log Regression(4005/4999): loss=0.5401893964015859\n",
      "Log Regression(4006/4999): loss=0.5410453709031182\n",
      "Log Regression(4007/4999): loss=0.5417157726641343\n",
      "Log Regression(4008/4999): loss=0.5413975675520596\n",
      "Log Regression(4009/4999): loss=0.5402729145300782\n",
      "Log Regression(4010/4999): loss=0.5414384917940478\n",
      "Log Regression(4011/4999): loss=0.5412254731893893\n",
      "Log Regression(4012/4999): loss=0.5406977981097651\n",
      "Log Regression(4013/4999): loss=0.5401628270423541\n",
      "Log Regression(4014/4999): loss=0.5402390964357549\n",
      "Log Regression(4015/4999): loss=0.5418432372735849\n",
      "Log Regression(4016/4999): loss=0.5403143073512315\n",
      "Log Regression(4017/4999): loss=0.5407748597323563\n",
      "Log Regression(4018/4999): loss=0.5409202068679276\n",
      "Log Regression(4019/4999): loss=0.5403359383635753\n",
      "Log Regression(4020/4999): loss=0.540254928063493\n",
      "Log Regression(4021/4999): loss=0.5402233355785372\n",
      "Log Regression(4022/4999): loss=0.5403647410515748\n",
      "Log Regression(4023/4999): loss=0.5412871367619605\n",
      "Log Regression(4024/4999): loss=0.5430956830822941\n",
      "Log Regression(4025/4999): loss=0.5410751339155025\n",
      "Log Regression(4026/4999): loss=0.5409217910418064\n",
      "Log Regression(4027/4999): loss=0.5411073061682025\n",
      "Log Regression(4028/4999): loss=0.5421817322244689\n",
      "Log Regression(4029/4999): loss=0.5421576064107619\n",
      "Log Regression(4030/4999): loss=0.5401551619611074\n",
      "Log Regression(4031/4999): loss=0.5404007679448726\n",
      "Log Regression(4032/4999): loss=0.5403617181879802\n",
      "Log Regression(4033/4999): loss=0.5407509301327879\n",
      "Log Regression(4034/4999): loss=0.5402554867290713\n",
      "Log Regression(4035/4999): loss=0.5409782104547662\n",
      "Log Regression(4036/4999): loss=0.5407279840941662\n",
      "Log Regression(4037/4999): loss=0.540938244684189\n",
      "Log Regression(4038/4999): loss=0.5418661218247842\n",
      "Log Regression(4039/4999): loss=0.5418916030090707\n",
      "Log Regression(4040/4999): loss=0.5401371391065802\n",
      "Log Regression(4041/4999): loss=0.5421176676791775\n",
      "Log Regression(4042/4999): loss=0.540204289597519\n",
      "Log Regression(4043/4999): loss=0.5401617925733826\n",
      "Log Regression(4044/4999): loss=0.5406210116326792\n",
      "Log Regression(4045/4999): loss=0.5402117296113887\n",
      "Log Regression(4046/4999): loss=0.5402300526921328\n",
      "Log Regression(4047/4999): loss=0.5405299603034414\n",
      "Log Regression(4048/4999): loss=0.5420288490103908\n",
      "Log Regression(4049/4999): loss=0.5404084123079682\n",
      "Log Regression(4050/4999): loss=0.5444620997657479\n",
      "Log Regression(4051/4999): loss=0.5406980363139134\n",
      "Log Regression(4052/4999): loss=0.5406287190814217\n",
      "Log Regression(4053/4999): loss=0.5409614841244845\n",
      "Log Regression(4054/4999): loss=0.5408143326515865\n",
      "Log Regression(4055/4999): loss=0.5429685089132867\n",
      "Log Regression(4056/4999): loss=0.5410023089429475\n",
      "Log Regression(4057/4999): loss=0.5411462482773465\n",
      "Log Regression(4058/4999): loss=0.5417800155565681\n",
      "Log Regression(4059/4999): loss=0.5423075250452053\n",
      "Log Regression(4060/4999): loss=0.5420988116320569\n",
      "Log Regression(4061/4999): loss=0.5412500591582983\n",
      "Log Regression(4062/4999): loss=0.5410534040786965\n",
      "Log Regression(4063/4999): loss=0.5408432080460632\n",
      "Log Regression(4064/4999): loss=0.5407782079718876\n",
      "Log Regression(4065/4999): loss=0.5407135084019946\n",
      "Log Regression(4066/4999): loss=0.5411819662903553\n",
      "Log Regression(4067/4999): loss=0.5416503147613982\n",
      "Log Regression(4068/4999): loss=0.5410293802551424\n",
      "Log Regression(4069/4999): loss=0.5414514130437127\n",
      "Log Regression(4070/4999): loss=0.5406621379601778\n",
      "Log Regression(4071/4999): loss=0.5405749513664351\n",
      "Log Regression(4072/4999): loss=0.5403980355000405\n",
      "Log Regression(4073/4999): loss=0.5436264927647824\n",
      "Log Regression(4074/4999): loss=0.5405767785008863\n",
      "Log Regression(4075/4999): loss=0.5412184615395589\n",
      "Log Regression(4076/4999): loss=0.5425256112659929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(4077/4999): loss=0.5436047397649634\n",
      "Log Regression(4078/4999): loss=0.5440063052743882\n",
      "Log Regression(4079/4999): loss=0.5417719183955774\n",
      "Log Regression(4080/4999): loss=0.5422943208237584\n",
      "Log Regression(4081/4999): loss=0.54038954200668\n",
      "Log Regression(4082/4999): loss=0.5402263980934636\n",
      "Log Regression(4083/4999): loss=0.5404925097746157\n",
      "Log Regression(4084/4999): loss=0.540381135857016\n",
      "Log Regression(4085/4999): loss=0.5401466540256984\n",
      "Log Regression(4086/4999): loss=0.5408755421637306\n",
      "Log Regression(4087/4999): loss=0.543691845133258\n",
      "Log Regression(4088/4999): loss=0.5431286895231132\n",
      "Log Regression(4089/4999): loss=0.5403083751068783\n",
      "Log Regression(4090/4999): loss=0.5412275571718198\n",
      "Log Regression(4091/4999): loss=0.5405839627812927\n",
      "Log Regression(4092/4999): loss=0.5438672819172329\n",
      "Log Regression(4093/4999): loss=0.5401466014968441\n",
      "Log Regression(4094/4999): loss=0.5418458521488698\n",
      "Log Regression(4095/4999): loss=0.5404051057938799\n",
      "Log Regression(4096/4999): loss=0.5416888013397052\n",
      "Log Regression(4097/4999): loss=0.5401409567528042\n",
      "Log Regression(4098/4999): loss=0.5402067008714396\n",
      "Log Regression(4099/4999): loss=0.5441906463542908\n",
      "Log Regression(4100/4999): loss=0.5422848901684634\n",
      "Log Regression(4101/4999): loss=0.5419715697113796\n",
      "Log Regression(4102/4999): loss=0.5401811404549292\n",
      "Log Regression(4103/4999): loss=0.5410622148732676\n",
      "Log Regression(4104/4999): loss=0.5404862415115221\n",
      "Log Regression(4105/4999): loss=0.5413888406932387\n",
      "Log Regression(4106/4999): loss=0.5410663897882664\n",
      "Log Regression(4107/4999): loss=0.540236893459424\n",
      "Log Regression(4108/4999): loss=0.5411766766304102\n",
      "Log Regression(4109/4999): loss=0.5403169601333548\n",
      "Log Regression(4110/4999): loss=0.5405444327691032\n",
      "Log Regression(4111/4999): loss=0.5424327613897222\n",
      "Log Regression(4112/4999): loss=0.5406532670279263\n",
      "Log Regression(4113/4999): loss=0.5478579105010007\n",
      "Log Regression(4114/4999): loss=0.54299960155342\n",
      "Log Regression(4115/4999): loss=0.5402672217529582\n",
      "Log Regression(4116/4999): loss=0.5419084842587779\n",
      "Log Regression(4117/4999): loss=0.5433938749928042\n",
      "Log Regression(4118/4999): loss=0.5439293167863651\n",
      "Log Regression(4119/4999): loss=0.5407587799263336\n",
      "Log Regression(4120/4999): loss=0.5444277683149004\n",
      "Log Regression(4121/4999): loss=0.540190632576638\n",
      "Log Regression(4122/4999): loss=0.5403007399957714\n",
      "Log Regression(4123/4999): loss=0.5405005853599685\n",
      "Log Regression(4124/4999): loss=0.5404467989890369\n",
      "Log Regression(4125/4999): loss=0.5404910154027884\n",
      "Log Regression(4126/4999): loss=0.5427020412752834\n",
      "Log Regression(4127/4999): loss=0.5402102357457806\n",
      "Log Regression(4128/4999): loss=0.5401735395235565\n",
      "Log Regression(4129/4999): loss=0.5451896822389433\n",
      "Log Regression(4130/4999): loss=0.5477201511334798\n",
      "Log Regression(4131/4999): loss=0.5452033792304839\n",
      "Log Regression(4132/4999): loss=0.5489824667660556\n",
      "Log Regression(4133/4999): loss=0.5488743326760096\n",
      "Log Regression(4134/4999): loss=0.5452161653412855\n",
      "Log Regression(4135/4999): loss=0.5404833731917635\n",
      "Log Regression(4136/4999): loss=0.5401007053588762\n",
      "Log Regression(4137/4999): loss=0.5410448815360824\n",
      "Log Regression(4138/4999): loss=0.5444464347795418\n",
      "Log Regression(4139/4999): loss=0.5480629210500315\n",
      "Log Regression(4140/4999): loss=0.5436318712387742\n",
      "Log Regression(4141/4999): loss=0.5406497132027424\n",
      "Log Regression(4142/4999): loss=0.5407317585929828\n",
      "Log Regression(4143/4999): loss=0.5405158296056609\n",
      "Log Regression(4144/4999): loss=0.5426762319198063\n",
      "Log Regression(4145/4999): loss=0.5409485450269415\n",
      "Log Regression(4146/4999): loss=0.5412122043652166\n",
      "Log Regression(4147/4999): loss=0.5405674514999786\n",
      "Log Regression(4148/4999): loss=0.5408298849032871\n",
      "Log Regression(4149/4999): loss=0.5412379897971228\n",
      "Log Regression(4150/4999): loss=0.5406854136945245\n",
      "Log Regression(4151/4999): loss=0.5411780161495511\n",
      "Log Regression(4152/4999): loss=0.540780242562666\n",
      "Log Regression(4153/4999): loss=0.5405310330020403\n",
      "Log Regression(4154/4999): loss=0.5407401347373256\n",
      "Log Regression(4155/4999): loss=0.5420360198979732\n",
      "Log Regression(4156/4999): loss=0.5404549518621604\n",
      "Log Regression(4157/4999): loss=0.5404070422484082\n",
      "Log Regression(4158/4999): loss=0.5411423925361337\n",
      "Log Regression(4159/4999): loss=0.5406626005756755\n",
      "Log Regression(4160/4999): loss=0.5408046343716983\n",
      "Log Regression(4161/4999): loss=0.5405648885269696\n",
      "Log Regression(4162/4999): loss=0.5404303021152197\n",
      "Log Regression(4163/4999): loss=0.5412379959069917\n",
      "Log Regression(4164/4999): loss=0.5455067517602251\n",
      "Log Regression(4165/4999): loss=0.5459202594850957\n",
      "Log Regression(4166/4999): loss=0.5407971195862683\n",
      "Log Regression(4167/4999): loss=0.5403494070055422\n",
      "Log Regression(4168/4999): loss=0.5464975727349108\n",
      "Log Regression(4169/4999): loss=0.5453093618163352\n",
      "Log Regression(4170/4999): loss=0.5430115438301564\n",
      "Log Regression(4171/4999): loss=0.5432287645977354\n",
      "Log Regression(4172/4999): loss=0.5415694918206095\n",
      "Log Regression(4173/4999): loss=0.5402201708170211\n",
      "Log Regression(4174/4999): loss=0.5407316611451315\n",
      "Log Regression(4175/4999): loss=0.540433868094658\n",
      "Log Regression(4176/4999): loss=0.5401256125753612\n",
      "Log Regression(4177/4999): loss=0.5404907854601307\n",
      "Log Regression(4178/4999): loss=0.5405013797695623\n",
      "Log Regression(4179/4999): loss=0.5406485452721878\n",
      "Log Regression(4180/4999): loss=0.5400525087741255\n",
      "Log Regression(4181/4999): loss=0.5400443044377086\n",
      "Log Regression(4182/4999): loss=0.5402228916102508\n",
      "Log Regression(4183/4999): loss=0.5437893191493444\n",
      "Log Regression(4184/4999): loss=0.5428037962473514\n",
      "Log Regression(4185/4999): loss=0.5402460675298806\n",
      "Log Regression(4186/4999): loss=0.542258833886109\n",
      "Log Regression(4187/4999): loss=0.5403667716833666\n",
      "Log Regression(4188/4999): loss=0.5432033281748968\n",
      "Log Regression(4189/4999): loss=0.5414306423014176\n",
      "Log Regression(4190/4999): loss=0.5440618607673974\n",
      "Log Regression(4191/4999): loss=0.5407926714035078\n",
      "Log Regression(4192/4999): loss=0.5408010858252664\n",
      "Log Regression(4193/4999): loss=0.5403019515016058\n",
      "Log Regression(4194/4999): loss=0.5407180987528114\n",
      "Log Regression(4195/4999): loss=0.5408271844435663\n",
      "Log Regression(4196/4999): loss=0.5408730365399718\n",
      "Log Regression(4197/4999): loss=0.5451400270297703\n",
      "Log Regression(4198/4999): loss=0.5429936387035573\n",
      "Log Regression(4199/4999): loss=0.5411072947793142\n",
      "Log Regression(4200/4999): loss=0.5412275072894849\n",
      "Log Regression(4201/4999): loss=0.5416312960880386\n",
      "Log Regression(4202/4999): loss=0.54135804826364\n",
      "Log Regression(4203/4999): loss=0.5416660131313105\n",
      "Log Regression(4204/4999): loss=0.5413962331194095\n",
      "Log Regression(4205/4999): loss=0.5411179103070688\n",
      "Log Regression(4206/4999): loss=0.5415796592945737\n",
      "Log Regression(4207/4999): loss=0.5414542468694312\n",
      "Log Regression(4208/4999): loss=0.5418369092207822\n",
      "Log Regression(4209/4999): loss=0.540845387240878\n",
      "Log Regression(4210/4999): loss=0.5411073000419695\n",
      "Log Regression(4211/4999): loss=0.5422759680520365\n",
      "Log Regression(4212/4999): loss=0.542399384131259\n",
      "Log Regression(4213/4999): loss=0.540682214063587\n",
      "Log Regression(4214/4999): loss=0.5411949446671611\n",
      "Log Regression(4215/4999): loss=0.5421171334210719\n",
      "Log Regression(4216/4999): loss=0.5404317090041927\n",
      "Log Regression(4217/4999): loss=0.5404275912629346\n",
      "Log Regression(4218/4999): loss=0.5408014060685608\n",
      "Log Regression(4219/4999): loss=0.5418192621228624\n",
      "Log Regression(4220/4999): loss=0.5411186365769444\n",
      "Log Regression(4221/4999): loss=0.5407116055690984\n",
      "Log Regression(4222/4999): loss=0.541766391861212\n",
      "Log Regression(4223/4999): loss=0.5424286484507365\n",
      "Log Regression(4224/4999): loss=0.5405426691178742\n",
      "Log Regression(4225/4999): loss=0.5425300584853155\n",
      "Log Regression(4226/4999): loss=0.5416457042073768\n",
      "Log Regression(4227/4999): loss=0.5445028304803129\n",
      "Log Regression(4228/4999): loss=0.5471842924375612\n",
      "Log Regression(4229/4999): loss=0.5425807702638336\n",
      "Log Regression(4230/4999): loss=0.5446215428505565\n",
      "Log Regression(4231/4999): loss=0.5420045455335383\n",
      "Log Regression(4232/4999): loss=0.5422859959958448\n",
      "Log Regression(4233/4999): loss=0.5423807198745776\n",
      "Log Regression(4234/4999): loss=0.5414527763851408\n",
      "Log Regression(4235/4999): loss=0.5444773117310748\n",
      "Log Regression(4236/4999): loss=0.5481021389232217\n",
      "Log Regression(4237/4999): loss=0.5451384388065315\n",
      "Log Regression(4238/4999): loss=0.5437887284167837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(4239/4999): loss=0.5432633105496097\n",
      "Log Regression(4240/4999): loss=0.5411663317290543\n",
      "Log Regression(4241/4999): loss=0.5402656710162611\n",
      "Log Regression(4242/4999): loss=0.5403694469669886\n",
      "Log Regression(4243/4999): loss=0.5401963860286627\n",
      "Log Regression(4244/4999): loss=0.5403161765040443\n",
      "Log Regression(4245/4999): loss=0.540564806361201\n",
      "Log Regression(4246/4999): loss=0.5472805585940311\n",
      "Log Regression(4247/4999): loss=0.5409449880624291\n",
      "Log Regression(4248/4999): loss=0.5404476172705778\n",
      "Log Regression(4249/4999): loss=0.5424110628620782\n",
      "Log Regression(4250/4999): loss=0.5432343789380197\n",
      "Log Regression(4251/4999): loss=0.541915735282912\n",
      "Log Regression(4252/4999): loss=0.5466250845128692\n",
      "Log Regression(4253/4999): loss=0.546240706787495\n",
      "Log Regression(4254/4999): loss=0.5437338861117359\n",
      "Log Regression(4255/4999): loss=0.5405178915093877\n",
      "Log Regression(4256/4999): loss=0.5402395585316417\n",
      "Log Regression(4257/4999): loss=0.5408505774592526\n",
      "Log Regression(4258/4999): loss=0.5407181515718361\n",
      "Log Regression(4259/4999): loss=0.5402364574053132\n",
      "Log Regression(4260/4999): loss=0.5428595350204483\n",
      "Log Regression(4261/4999): loss=0.5412162781837443\n",
      "Log Regression(4262/4999): loss=0.540092456458971\n",
      "Log Regression(4263/4999): loss=0.5403645303560508\n",
      "Log Regression(4264/4999): loss=0.5408779416856029\n",
      "Log Regression(4265/4999): loss=0.5406735641776945\n",
      "Log Regression(4266/4999): loss=0.5415098134233183\n",
      "Log Regression(4267/4999): loss=0.5407564114856449\n",
      "Log Regression(4268/4999): loss=0.5401594362671832\n",
      "Log Regression(4269/4999): loss=0.5404888106093411\n",
      "Log Regression(4270/4999): loss=0.5411445287706224\n",
      "Log Regression(4271/4999): loss=0.5414778464283235\n",
      "Log Regression(4272/4999): loss=0.5406870668471522\n",
      "Log Regression(4273/4999): loss=0.5408869896150467\n",
      "Log Regression(4274/4999): loss=0.5440739866421194\n",
      "Log Regression(4275/4999): loss=0.5409723820440653\n",
      "Log Regression(4276/4999): loss=0.5411442269091501\n",
      "Log Regression(4277/4999): loss=0.5407338744926712\n",
      "Log Regression(4278/4999): loss=0.5417273629265524\n",
      "Log Regression(4279/4999): loss=0.5421335579253641\n",
      "Log Regression(4280/4999): loss=0.543043024364245\n",
      "Log Regression(4281/4999): loss=0.540738312297688\n",
      "Log Regression(4282/4999): loss=0.5406365970418018\n",
      "Log Regression(4283/4999): loss=0.5417205868618421\n",
      "Log Regression(4284/4999): loss=0.5432208386039828\n",
      "Log Regression(4285/4999): loss=0.5419441663031533\n",
      "Log Regression(4286/4999): loss=0.5438743587193215\n",
      "Log Regression(4287/4999): loss=0.5516220786034819\n",
      "Log Regression(4288/4999): loss=0.5438096427078961\n",
      "Log Regression(4289/4999): loss=0.543715351952375\n",
      "Log Regression(4290/4999): loss=0.5439963070342456\n",
      "Log Regression(4291/4999): loss=0.5407302804577796\n",
      "Log Regression(4292/4999): loss=0.5474142721307799\n",
      "Log Regression(4293/4999): loss=0.5419548907543904\n",
      "Log Regression(4294/4999): loss=0.5408292609845662\n",
      "Log Regression(4295/4999): loss=0.5408160095460616\n",
      "Log Regression(4296/4999): loss=0.5405941981904624\n",
      "Log Regression(4297/4999): loss=0.5409381860656627\n",
      "Log Regression(4298/4999): loss=0.5408872431107777\n",
      "Log Regression(4299/4999): loss=0.5412149793078508\n",
      "Log Regression(4300/4999): loss=0.5413135292229273\n",
      "Log Regression(4301/4999): loss=0.5421468546511151\n",
      "Log Regression(4302/4999): loss=0.5417568271002315\n",
      "Log Regression(4303/4999): loss=0.5408749702264618\n",
      "Log Regression(4304/4999): loss=0.5408822978137166\n",
      "Log Regression(4305/4999): loss=0.5451765077265258\n",
      "Log Regression(4306/4999): loss=0.5443746252374997\n",
      "Log Regression(4307/4999): loss=0.5448488449180772\n",
      "Log Regression(4308/4999): loss=0.5467482347521438\n",
      "Log Regression(4309/4999): loss=0.541530166723777\n",
      "Log Regression(4310/4999): loss=0.5415424477623908\n",
      "Log Regression(4311/4999): loss=0.5417912405929894\n",
      "Log Regression(4312/4999): loss=0.5422862817504783\n",
      "Log Regression(4313/4999): loss=0.5483280022384325\n",
      "Log Regression(4314/4999): loss=0.5422364850104797\n",
      "Log Regression(4315/4999): loss=0.5412587107926448\n",
      "Log Regression(4316/4999): loss=0.5410701048004277\n",
      "Log Regression(4317/4999): loss=0.5414277711150257\n",
      "Log Regression(4318/4999): loss=0.5425036516177365\n",
      "Log Regression(4319/4999): loss=0.5423371078001377\n",
      "Log Regression(4320/4999): loss=0.540846135132784\n",
      "Log Regression(4321/4999): loss=0.5406520103884493\n",
      "Log Regression(4322/4999): loss=0.5406294949096176\n",
      "Log Regression(4323/4999): loss=0.5408117430182076\n",
      "Log Regression(4324/4999): loss=0.5415716938112827\n",
      "Log Regression(4325/4999): loss=0.5412297156973711\n",
      "Log Regression(4326/4999): loss=0.5407699893380727\n",
      "Log Regression(4327/4999): loss=0.5410463495794673\n",
      "Log Regression(4328/4999): loss=0.5405084376656717\n",
      "Log Regression(4329/4999): loss=0.5404626687038439\n",
      "Log Regression(4330/4999): loss=0.5401309118896074\n",
      "Log Regression(4331/4999): loss=0.540080552685974\n",
      "Log Regression(4332/4999): loss=0.5405070483955987\n",
      "Log Regression(4333/4999): loss=0.540575125629489\n",
      "Log Regression(4334/4999): loss=0.5403810467873819\n",
      "Log Regression(4335/4999): loss=0.5400915291820848\n",
      "Log Regression(4336/4999): loss=0.5412802548045793\n",
      "Log Regression(4337/4999): loss=0.5417427815342315\n",
      "Log Regression(4338/4999): loss=0.5400731082971837\n",
      "Log Regression(4339/4999): loss=0.541579026393278\n",
      "Log Regression(4340/4999): loss=0.5401074805587285\n",
      "Log Regression(4341/4999): loss=0.5400354118958487\n",
      "Log Regression(4342/4999): loss=0.5460559316203953\n",
      "Log Regression(4343/4999): loss=0.5444074451303074\n",
      "Log Regression(4344/4999): loss=0.5405633955863611\n",
      "Log Regression(4345/4999): loss=0.5402168337685906\n",
      "Log Regression(4346/4999): loss=0.540907995358655\n",
      "Log Regression(4347/4999): loss=0.5445444578142224\n",
      "Log Regression(4348/4999): loss=0.5433973813680473\n",
      "Log Regression(4349/4999): loss=0.5517596785720824\n",
      "Log Regression(4350/4999): loss=0.5454723851017097\n",
      "Log Regression(4351/4999): loss=0.5401965981596053\n",
      "Log Regression(4352/4999): loss=0.5410836368863333\n",
      "Log Regression(4353/4999): loss=0.5450472844655331\n",
      "Log Regression(4354/4999): loss=0.5439119119675291\n",
      "Log Regression(4355/4999): loss=0.543160018101626\n",
      "Log Regression(4356/4999): loss=0.5420221341066159\n",
      "Log Regression(4357/4999): loss=0.5403161534471118\n",
      "Log Regression(4358/4999): loss=0.5404601516003292\n",
      "Log Regression(4359/4999): loss=0.5403700577825792\n",
      "Log Regression(4360/4999): loss=0.5422865619225158\n",
      "Log Regression(4361/4999): loss=0.5442002513844763\n",
      "Log Regression(4362/4999): loss=0.5461303313854698\n",
      "Log Regression(4363/4999): loss=0.540149320695173\n",
      "Log Regression(4364/4999): loss=0.5402657764556272\n",
      "Log Regression(4365/4999): loss=0.5401508540452895\n",
      "Log Regression(4366/4999): loss=0.5406194510095363\n",
      "Log Regression(4367/4999): loss=0.5409478174135176\n",
      "Log Regression(4368/4999): loss=0.5405402793345072\n",
      "Log Regression(4369/4999): loss=0.5400222885513773\n",
      "Log Regression(4370/4999): loss=0.5418378698831696\n",
      "Log Regression(4371/4999): loss=0.540128450987439\n",
      "Log Regression(4372/4999): loss=0.5406502977727632\n",
      "Log Regression(4373/4999): loss=0.5404301290314045\n",
      "Log Regression(4374/4999): loss=0.5403605954191344\n",
      "Log Regression(4375/4999): loss=0.541163915741966\n",
      "Log Regression(4376/4999): loss=0.5416859177412673\n",
      "Log Regression(4377/4999): loss=0.5412338364898812\n",
      "Log Regression(4378/4999): loss=0.5419737628102629\n",
      "Log Regression(4379/4999): loss=0.5413783777191853\n",
      "Log Regression(4380/4999): loss=0.5422432869165918\n",
      "Log Regression(4381/4999): loss=0.5421495796594503\n",
      "Log Regression(4382/4999): loss=0.541782679299067\n",
      "Log Regression(4383/4999): loss=0.543542094663863\n",
      "Log Regression(4384/4999): loss=0.5425781268247141\n",
      "Log Regression(4385/4999): loss=0.5444952070214407\n",
      "Log Regression(4386/4999): loss=0.5415547309590567\n",
      "Log Regression(4387/4999): loss=0.5421100323445494\n",
      "Log Regression(4388/4999): loss=0.5404154710991236\n",
      "Log Regression(4389/4999): loss=0.5404937047470275\n",
      "Log Regression(4390/4999): loss=0.5414867254146639\n",
      "Log Regression(4391/4999): loss=0.5420345505122248\n",
      "Log Regression(4392/4999): loss=0.5445221813620247\n",
      "Log Regression(4393/4999): loss=0.5419888847082016\n",
      "Log Regression(4394/4999): loss=0.5454949244005018\n",
      "Log Regression(4395/4999): loss=0.5461194779311515\n",
      "Log Regression(4396/4999): loss=0.5418667473113584\n",
      "Log Regression(4397/4999): loss=0.5400885246179487\n",
      "Log Regression(4398/4999): loss=0.542089616544686\n",
      "Log Regression(4399/4999): loss=0.5403327000268053\n",
      "Log Regression(4400/4999): loss=0.5424469261495299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(4401/4999): loss=0.5435149366527687\n",
      "Log Regression(4402/4999): loss=0.5411132035897978\n",
      "Log Regression(4403/4999): loss=0.5400187352534896\n",
      "Log Regression(4404/4999): loss=0.5402848061882752\n",
      "Log Regression(4405/4999): loss=0.5408038509782724\n",
      "Log Regression(4406/4999): loss=0.5404226879251399\n",
      "Log Regression(4407/4999): loss=0.5470928898165939\n",
      "Log Regression(4408/4999): loss=0.552989055908269\n",
      "Log Regression(4409/4999): loss=0.5411113899209191\n",
      "Log Regression(4410/4999): loss=0.5440556739145249\n",
      "Log Regression(4411/4999): loss=0.5466483054176965\n",
      "Log Regression(4412/4999): loss=0.5446476615443084\n",
      "Log Regression(4413/4999): loss=0.5530484530185766\n",
      "Log Regression(4414/4999): loss=0.5511393964978037\n",
      "Log Regression(4415/4999): loss=0.5422491924422568\n",
      "Log Regression(4416/4999): loss=0.5400574622932551\n",
      "Log Regression(4417/4999): loss=0.5406830544589298\n",
      "Log Regression(4418/4999): loss=0.5426852232374301\n",
      "Log Regression(4419/4999): loss=0.5417247444340305\n",
      "Log Regression(4420/4999): loss=0.5403496524719927\n",
      "Log Regression(4421/4999): loss=0.5411709310717929\n",
      "Log Regression(4422/4999): loss=0.5405800787953206\n",
      "Log Regression(4423/4999): loss=0.5406239145425712\n",
      "Log Regression(4424/4999): loss=0.5419393168384649\n",
      "Log Regression(4425/4999): loss=0.5436001837684773\n",
      "Log Regression(4426/4999): loss=0.5420004684855751\n",
      "Log Regression(4427/4999): loss=0.541713060694194\n",
      "Log Regression(4428/4999): loss=0.543534821829603\n",
      "Log Regression(4429/4999): loss=0.5411419482517277\n",
      "Log Regression(4430/4999): loss=0.5402919432548474\n",
      "Log Regression(4431/4999): loss=0.5401948086349055\n",
      "Log Regression(4432/4999): loss=0.540356275069066\n",
      "Log Regression(4433/4999): loss=0.5402782305305963\n",
      "Log Regression(4434/4999): loss=0.5402164609543179\n",
      "Log Regression(4435/4999): loss=0.5400592294646779\n",
      "Log Regression(4436/4999): loss=0.5408006815599553\n",
      "Log Regression(4437/4999): loss=0.5402696027099192\n",
      "Log Regression(4438/4999): loss=0.5401723633507527\n",
      "Log Regression(4439/4999): loss=0.5411813565914815\n",
      "Log Regression(4440/4999): loss=0.5401156259554708\n",
      "Log Regression(4441/4999): loss=0.5406551584677932\n",
      "Log Regression(4442/4999): loss=0.5413011204841981\n",
      "Log Regression(4443/4999): loss=0.5455459635218805\n",
      "Log Regression(4444/4999): loss=0.5455859793554855\n",
      "Log Regression(4445/4999): loss=0.5464410206680472\n",
      "Log Regression(4446/4999): loss=0.5444943508585421\n",
      "Log Regression(4447/4999): loss=0.5425063370118138\n",
      "Log Regression(4448/4999): loss=0.5428635407971815\n",
      "Log Regression(4449/4999): loss=0.5410384978379261\n",
      "Log Regression(4450/4999): loss=0.5421236532898338\n",
      "Log Regression(4451/4999): loss=0.5414026793927664\n",
      "Log Regression(4452/4999): loss=0.5415659788445317\n",
      "Log Regression(4453/4999): loss=0.5452477953263077\n",
      "Log Regression(4454/4999): loss=0.5486768899441093\n",
      "Log Regression(4455/4999): loss=0.5419746317559194\n",
      "Log Regression(4456/4999): loss=0.5408960949737237\n",
      "Log Regression(4457/4999): loss=0.5406570324962618\n",
      "Log Regression(4458/4999): loss=0.5423054396091221\n",
      "Log Regression(4459/4999): loss=0.5439564877378573\n",
      "Log Regression(4460/4999): loss=0.5440905660412823\n",
      "Log Regression(4461/4999): loss=0.5438878821763016\n",
      "Log Regression(4462/4999): loss=0.5404945972582693\n",
      "Log Regression(4463/4999): loss=0.5408383841220229\n",
      "Log Regression(4464/4999): loss=0.5406791457600375\n",
      "Log Regression(4465/4999): loss=0.5407623546264503\n",
      "Log Regression(4466/4999): loss=0.5414011195935935\n",
      "Log Regression(4467/4999): loss=0.5412691361963357\n",
      "Log Regression(4468/4999): loss=0.5419207464555518\n",
      "Log Regression(4469/4999): loss=0.5445743804026477\n",
      "Log Regression(4470/4999): loss=0.5421944020173468\n",
      "Log Regression(4471/4999): loss=0.5411976084369561\n",
      "Log Regression(4472/4999): loss=0.5439235542795445\n",
      "Log Regression(4473/4999): loss=0.5404305441470035\n",
      "Log Regression(4474/4999): loss=0.5406173447728921\n",
      "Log Regression(4475/4999): loss=0.5407242028870101\n",
      "Log Regression(4476/4999): loss=0.5401101595990819\n",
      "Log Regression(4477/4999): loss=0.5408496448399361\n",
      "Log Regression(4478/4999): loss=0.5401966985598797\n",
      "Log Regression(4479/4999): loss=0.5406446538221531\n",
      "Log Regression(4480/4999): loss=0.5423362965042826\n",
      "Log Regression(4481/4999): loss=0.5401411379222372\n",
      "Log Regression(4482/4999): loss=0.5401977982233863\n",
      "Log Regression(4483/4999): loss=0.5403119095071675\n",
      "Log Regression(4484/4999): loss=0.5412806730546343\n",
      "Log Regression(4485/4999): loss=0.54006169307271\n",
      "Log Regression(4486/4999): loss=0.5401493152832365\n",
      "Log Regression(4487/4999): loss=0.5401848540794179\n",
      "Log Regression(4488/4999): loss=0.5400411584545854\n",
      "Log Regression(4489/4999): loss=0.5400292773569856\n",
      "Log Regression(4490/4999): loss=0.5408011639307595\n",
      "Log Regression(4491/4999): loss=0.540153330716858\n",
      "Log Regression(4492/4999): loss=0.5400334551541185\n",
      "Log Regression(4493/4999): loss=0.5405817249966145\n",
      "Log Regression(4494/4999): loss=0.5407747709639871\n",
      "Log Regression(4495/4999): loss=0.5403665223441332\n",
      "Log Regression(4496/4999): loss=0.5400699468254122\n",
      "Log Regression(4497/4999): loss=0.5403788373259895\n",
      "Log Regression(4498/4999): loss=0.5401951679292611\n",
      "Log Regression(4499/4999): loss=0.5405084007298148\n",
      "Log Regression(4500/4999): loss=0.5412323494231196\n",
      "Log Regression(4501/4999): loss=0.5400499658919423\n",
      "Log Regression(4502/4999): loss=0.5401344133155762\n",
      "Log Regression(4503/4999): loss=0.5407390514098248\n",
      "Log Regression(4504/4999): loss=0.5419494773273771\n",
      "Log Regression(4505/4999): loss=0.5432231362787443\n",
      "Log Regression(4506/4999): loss=0.5413801432150291\n",
      "Log Regression(4507/4999): loss=0.5423511023961191\n",
      "Log Regression(4508/4999): loss=0.5454302068203442\n",
      "Log Regression(4509/4999): loss=0.5495156715112154\n",
      "Log Regression(4510/4999): loss=0.5506277522849429\n",
      "Log Regression(4511/4999): loss=0.5484210312126775\n",
      "Log Regression(4512/4999): loss=0.5406358227815053\n",
      "Log Regression(4513/4999): loss=0.5406203983392195\n",
      "Log Regression(4514/4999): loss=0.5403678697297932\n",
      "Log Regression(4515/4999): loss=0.5406085994714966\n",
      "Log Regression(4516/4999): loss=0.5403962586363271\n",
      "Log Regression(4517/4999): loss=0.5409320924487669\n",
      "Log Regression(4518/4999): loss=0.5412186719348304\n",
      "Log Regression(4519/4999): loss=0.5420206643743295\n",
      "Log Regression(4520/4999): loss=0.5400361211127389\n",
      "Log Regression(4521/4999): loss=0.5403569539285532\n",
      "Log Regression(4522/4999): loss=0.5401782562312483\n",
      "Log Regression(4523/4999): loss=0.5406232316871709\n",
      "Log Regression(4524/4999): loss=0.5400835946765732\n",
      "Log Regression(4525/4999): loss=0.541515836230833\n",
      "Log Regression(4526/4999): loss=0.5435845402572099\n",
      "Log Regression(4527/4999): loss=0.5405590218062999\n",
      "Log Regression(4528/4999): loss=0.5401191451214515\n",
      "Log Regression(4529/4999): loss=0.5413509755355447\n",
      "Log Regression(4530/4999): loss=0.5412307069678604\n",
      "Log Regression(4531/4999): loss=0.544236029644735\n",
      "Log Regression(4532/4999): loss=0.5472701596927726\n",
      "Log Regression(4533/4999): loss=0.5448784266389682\n",
      "Log Regression(4534/4999): loss=0.5449834742820834\n",
      "Log Regression(4535/4999): loss=0.5402734861026908\n",
      "Log Regression(4536/4999): loss=0.5418765541479106\n",
      "Log Regression(4537/4999): loss=0.5417542218065192\n",
      "Log Regression(4538/4999): loss=0.541071679428647\n",
      "Log Regression(4539/4999): loss=0.5426908668518288\n",
      "Log Regression(4540/4999): loss=0.5404313994514134\n",
      "Log Regression(4541/4999): loss=0.5407693829894255\n",
      "Log Regression(4542/4999): loss=0.5434798428062849\n",
      "Log Regression(4543/4999): loss=0.5404618341105619\n",
      "Log Regression(4544/4999): loss=0.5401409710277384\n",
      "Log Regression(4545/4999): loss=0.5406418290174355\n",
      "Log Regression(4546/4999): loss=0.5420730224408634\n",
      "Log Regression(4547/4999): loss=0.5430298822044375\n",
      "Log Regression(4548/4999): loss=0.5413557865288777\n",
      "Log Regression(4549/4999): loss=0.5415637264717735\n",
      "Log Regression(4550/4999): loss=0.5401898106661959\n",
      "Log Regression(4551/4999): loss=0.5400405953986699\n",
      "Log Regression(4552/4999): loss=0.5413289502130781\n",
      "Log Regression(4553/4999): loss=0.5428982189842178\n",
      "Log Regression(4554/4999): loss=0.5411795970792913\n",
      "Log Regression(4555/4999): loss=0.5408204050137472\n",
      "Log Regression(4556/4999): loss=0.5404002919029295\n",
      "Log Regression(4557/4999): loss=0.5401330754910764\n",
      "Log Regression(4558/4999): loss=0.5402792698036482\n",
      "Log Regression(4559/4999): loss=0.5409623182046092\n",
      "Log Regression(4560/4999): loss=0.5433490345477805\n",
      "Log Regression(4561/4999): loss=0.5422863580961534\n",
      "Log Regression(4562/4999): loss=0.5403181453379469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(4563/4999): loss=0.5401155364593095\n",
      "Log Regression(4564/4999): loss=0.5404158436511216\n",
      "Log Regression(4565/4999): loss=0.5401370992786341\n",
      "Log Regression(4566/4999): loss=0.5418051084245501\n",
      "Log Regression(4567/4999): loss=0.5434381889464661\n",
      "Log Regression(4568/4999): loss=0.5415953044796102\n",
      "Log Regression(4569/4999): loss=0.5419540331895129\n",
      "Log Regression(4570/4999): loss=0.5400009747208119\n",
      "Log Regression(4571/4999): loss=0.5403880484313079\n",
      "Log Regression(4572/4999): loss=0.5400517322997024\n",
      "Log Regression(4573/4999): loss=0.5422385505781123\n",
      "Log Regression(4574/4999): loss=0.5420370811747895\n",
      "Log Regression(4575/4999): loss=0.5447992782402019\n",
      "Log Regression(4576/4999): loss=0.5406134526867618\n",
      "Log Regression(4577/4999): loss=0.5401029497843226\n",
      "Log Regression(4578/4999): loss=0.5410229556809029\n",
      "Log Regression(4579/4999): loss=0.5403581372215877\n",
      "Log Regression(4580/4999): loss=0.5409605559812692\n",
      "Log Regression(4581/4999): loss=0.5403084776154552\n",
      "Log Regression(4582/4999): loss=0.5403585353957859\n",
      "Log Regression(4583/4999): loss=0.541184112602735\n",
      "Log Regression(4584/4999): loss=0.5416619609600238\n",
      "Log Regression(4585/4999): loss=0.5411759544881838\n",
      "Log Regression(4586/4999): loss=0.5409476872148606\n",
      "Log Regression(4587/4999): loss=0.5405438201412173\n",
      "Log Regression(4588/4999): loss=0.5414776847926271\n",
      "Log Regression(4589/4999): loss=0.5403806363109815\n",
      "Log Regression(4590/4999): loss=0.5405360639830922\n",
      "Log Regression(4591/4999): loss=0.5410441326639011\n",
      "Log Regression(4592/4999): loss=0.5439194043244692\n",
      "Log Regression(4593/4999): loss=0.54089601068488\n",
      "Log Regression(4594/4999): loss=0.5431674865416755\n",
      "Log Regression(4595/4999): loss=0.5421116551500479\n",
      "Log Regression(4596/4999): loss=0.5407211392771722\n",
      "Log Regression(4597/4999): loss=0.5416436350553067\n",
      "Log Regression(4598/4999): loss=0.5407490269110041\n",
      "Log Regression(4599/4999): loss=0.5485309639346102\n",
      "Log Regression(4600/4999): loss=0.5477700432830964\n",
      "Log Regression(4601/4999): loss=0.5436339574561659\n",
      "Log Regression(4602/4999): loss=0.5461397522806736\n",
      "Log Regression(4603/4999): loss=0.5415795374595448\n",
      "Log Regression(4604/4999): loss=0.5402954607492487\n",
      "Log Regression(4605/4999): loss=0.5402447380280875\n",
      "Log Regression(4606/4999): loss=0.5402452382490563\n",
      "Log Regression(4607/4999): loss=0.543151752442341\n",
      "Log Regression(4608/4999): loss=0.5403768496209874\n",
      "Log Regression(4609/4999): loss=0.5401557617849093\n",
      "Log Regression(4610/4999): loss=0.540224547304816\n",
      "Log Regression(4611/4999): loss=0.5402146397708428\n",
      "Log Regression(4612/4999): loss=0.5401642502010613\n",
      "Log Regression(4613/4999): loss=0.5407694897120926\n",
      "Log Regression(4614/4999): loss=0.5401001849298874\n",
      "Log Regression(4615/4999): loss=0.5402636695267695\n",
      "Log Regression(4616/4999): loss=0.5400694703049468\n",
      "Log Regression(4617/4999): loss=0.5401286979881387\n",
      "Log Regression(4618/4999): loss=0.5412839748998962\n",
      "Log Regression(4619/4999): loss=0.5401797868831208\n",
      "Log Regression(4620/4999): loss=0.5399569316864437\n",
      "Log Regression(4621/4999): loss=0.5400333117515737\n",
      "Log Regression(4622/4999): loss=0.54071402815373\n",
      "Log Regression(4623/4999): loss=0.5403878107481892\n",
      "Log Regression(4624/4999): loss=0.5402510490355744\n",
      "Log Regression(4625/4999): loss=0.5434960087306968\n",
      "Log Regression(4626/4999): loss=0.5402642989071138\n",
      "Log Regression(4627/4999): loss=0.5415580838849516\n",
      "Log Regression(4628/4999): loss=0.5441867626507654\n",
      "Log Regression(4629/4999): loss=0.5400551666317641\n",
      "Log Regression(4630/4999): loss=0.5402074504631276\n",
      "Log Regression(4631/4999): loss=0.5405521551286858\n",
      "Log Regression(4632/4999): loss=0.5401219351563155\n",
      "Log Regression(4633/4999): loss=0.5399859274728469\n",
      "Log Regression(4634/4999): loss=0.5400035736686946\n",
      "Log Regression(4635/4999): loss=0.5400249070111147\n",
      "Log Regression(4636/4999): loss=0.5405893557273352\n",
      "Log Regression(4637/4999): loss=0.5401038946919068\n",
      "Log Regression(4638/4999): loss=0.5417091230054641\n",
      "Log Regression(4639/4999): loss=0.5400486835556821\n",
      "Log Regression(4640/4999): loss=0.5403989244917726\n",
      "Log Regression(4641/4999): loss=0.5426695144098398\n",
      "Log Regression(4642/4999): loss=0.5445652895434406\n",
      "Log Regression(4643/4999): loss=0.5423023862091554\n",
      "Log Regression(4644/4999): loss=0.5415629123492236\n",
      "Log Regression(4645/4999): loss=0.5433393653737203\n",
      "Log Regression(4646/4999): loss=0.5408359904386988\n",
      "Log Regression(4647/4999): loss=0.5406612462395518\n",
      "Log Regression(4648/4999): loss=0.5413238288244823\n",
      "Log Regression(4649/4999): loss=0.54212922802528\n",
      "Log Regression(4650/4999): loss=0.5432571805278065\n",
      "Log Regression(4651/4999): loss=0.5429336415897501\n",
      "Log Regression(4652/4999): loss=0.544171017342475\n",
      "Log Regression(4653/4999): loss=0.5430338042887747\n",
      "Log Regression(4654/4999): loss=0.5412057967459799\n",
      "Log Regression(4655/4999): loss=0.5414536987957469\n",
      "Log Regression(4656/4999): loss=0.5424924867569311\n",
      "Log Regression(4657/4999): loss=0.5471807374950028\n",
      "Log Regression(4658/4999): loss=0.5493526767224084\n",
      "Log Regression(4659/4999): loss=0.5452779292201049\n",
      "Log Regression(4660/4999): loss=0.550801200610961\n",
      "Log Regression(4661/4999): loss=0.5485263385003211\n",
      "Log Regression(4662/4999): loss=0.5433466662927627\n",
      "Log Regression(4663/4999): loss=0.5450898812897974\n",
      "Log Regression(4664/4999): loss=0.5406714550175887\n",
      "Log Regression(4665/4999): loss=0.5407686496062201\n",
      "Log Regression(4666/4999): loss=0.5407063542991659\n",
      "Log Regression(4667/4999): loss=0.5410420812552006\n",
      "Log Regression(4668/4999): loss=0.5413043116494872\n",
      "Log Regression(4669/4999): loss=0.5423868707703766\n",
      "Log Regression(4670/4999): loss=0.5409350752781873\n",
      "Log Regression(4671/4999): loss=0.5410695509371939\n",
      "Log Regression(4672/4999): loss=0.54348357658398\n",
      "Log Regression(4673/4999): loss=0.5427184440864466\n",
      "Log Regression(4674/4999): loss=0.54640468597827\n",
      "Log Regression(4675/4999): loss=0.5422775978499256\n",
      "Log Regression(4676/4999): loss=0.5425339308787726\n",
      "Log Regression(4677/4999): loss=0.540441028028375\n",
      "Log Regression(4678/4999): loss=0.5405796642841249\n",
      "Log Regression(4679/4999): loss=0.5421089838102121\n",
      "Log Regression(4680/4999): loss=0.5455182984663052\n",
      "Log Regression(4681/4999): loss=0.5402099745336187\n",
      "Log Regression(4682/4999): loss=0.5404570899902009\n",
      "Log Regression(4683/4999): loss=0.5409576066663261\n",
      "Log Regression(4684/4999): loss=0.5415631753931447\n",
      "Log Regression(4685/4999): loss=0.5426755013063206\n",
      "Log Regression(4686/4999): loss=0.5422035785691012\n",
      "Log Regression(4687/4999): loss=0.5413653890548856\n",
      "Log Regression(4688/4999): loss=0.5408436641852757\n",
      "Log Regression(4689/4999): loss=0.5436821784368496\n",
      "Log Regression(4690/4999): loss=0.543555668429305\n",
      "Log Regression(4691/4999): loss=0.5420008606545254\n",
      "Log Regression(4692/4999): loss=0.5409573976336067\n",
      "Log Regression(4693/4999): loss=0.5405092026081881\n",
      "Log Regression(4694/4999): loss=0.5426958716990666\n",
      "Log Regression(4695/4999): loss=0.5404802561927758\n",
      "Log Regression(4696/4999): loss=0.5410442941547072\n",
      "Log Regression(4697/4999): loss=0.5448669614250742\n",
      "Log Regression(4698/4999): loss=0.5427640852226895\n",
      "Log Regression(4699/4999): loss=0.5403862971927675\n",
      "Log Regression(4700/4999): loss=0.5409147431149609\n",
      "Log Regression(4701/4999): loss=0.5402516728659262\n",
      "Log Regression(4702/4999): loss=0.5413468164166906\n",
      "Log Regression(4703/4999): loss=0.5408801223954146\n",
      "Log Regression(4704/4999): loss=0.5412304735091664\n",
      "Log Regression(4705/4999): loss=0.5416600826109098\n",
      "Log Regression(4706/4999): loss=0.5427523472115802\n",
      "Log Regression(4707/4999): loss=0.5408473196590485\n",
      "Log Regression(4708/4999): loss=0.5401252953565456\n",
      "Log Regression(4709/4999): loss=0.5400521306839191\n",
      "Log Regression(4710/4999): loss=0.5402061954923834\n",
      "Log Regression(4711/4999): loss=0.5441222233573956\n",
      "Log Regression(4712/4999): loss=0.5419355350335877\n",
      "Log Regression(4713/4999): loss=0.5417167184476914\n",
      "Log Regression(4714/4999): loss=0.5423861062759009\n",
      "Log Regression(4715/4999): loss=0.5407296666313616\n",
      "Log Regression(4716/4999): loss=0.5406342913496027\n",
      "Log Regression(4717/4999): loss=0.5407821141791803\n",
      "Log Regression(4718/4999): loss=0.5410607140048906\n",
      "Log Regression(4719/4999): loss=0.5404747502426616\n",
      "Log Regression(4720/4999): loss=0.5402517257247763\n",
      "Log Regression(4721/4999): loss=0.5417727351999313\n",
      "Log Regression(4722/4999): loss=0.540187638091618\n",
      "Log Regression(4723/4999): loss=0.5422487230570907\n",
      "Log Regression(4724/4999): loss=0.5407833228306795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(4725/4999): loss=0.5429823624515151\n",
      "Log Regression(4726/4999): loss=0.54393710426243\n",
      "Log Regression(4727/4999): loss=0.5449477680614165\n",
      "Log Regression(4728/4999): loss=0.5402682012594097\n",
      "Log Regression(4729/4999): loss=0.5400283070980063\n",
      "Log Regression(4730/4999): loss=0.5410051074733527\n",
      "Log Regression(4731/4999): loss=0.5403186667105088\n",
      "Log Regression(4732/4999): loss=0.5400251878623503\n",
      "Log Regression(4733/4999): loss=0.5401649323342126\n",
      "Log Regression(4734/4999): loss=0.5407325125193421\n",
      "Log Regression(4735/4999): loss=0.5406477136683345\n",
      "Log Regression(4736/4999): loss=0.5405385672747391\n",
      "Log Regression(4737/4999): loss=0.5402045807536722\n",
      "Log Regression(4738/4999): loss=0.5400974036805627\n",
      "Log Regression(4739/4999): loss=0.5420112698713885\n",
      "Log Regression(4740/4999): loss=0.5408615153305306\n",
      "Log Regression(4741/4999): loss=0.540137208122748\n",
      "Log Regression(4742/4999): loss=0.5403393880841589\n",
      "Log Regression(4743/4999): loss=0.5404710318919508\n",
      "Log Regression(4744/4999): loss=0.5404438277593177\n",
      "Log Regression(4745/4999): loss=0.5404952778711343\n",
      "Log Regression(4746/4999): loss=0.5404632972963618\n",
      "Log Regression(4747/4999): loss=0.5430952689109428\n",
      "Log Regression(4748/4999): loss=0.5431073074033231\n",
      "Log Regression(4749/4999): loss=0.5405641310511798\n",
      "Log Regression(4750/4999): loss=0.5434609242419298\n",
      "Log Regression(4751/4999): loss=0.5439288608648707\n",
      "Log Regression(4752/4999): loss=0.545955589737945\n",
      "Log Regression(4753/4999): loss=0.543705991264202\n",
      "Log Regression(4754/4999): loss=0.5435622285072619\n",
      "Log Regression(4755/4999): loss=0.5411637546122136\n",
      "Log Regression(4756/4999): loss=0.5404267849832928\n",
      "Log Regression(4757/4999): loss=0.5404898676689782\n",
      "Log Regression(4758/4999): loss=0.5414411487146056\n",
      "Log Regression(4759/4999): loss=0.5439167923442094\n",
      "Log Regression(4760/4999): loss=0.5411723671360765\n",
      "Log Regression(4761/4999): loss=0.5407816821355897\n",
      "Log Regression(4762/4999): loss=0.5403562349429148\n",
      "Log Regression(4763/4999): loss=0.5424300796007498\n",
      "Log Regression(4764/4999): loss=0.5401254505876255\n",
      "Log Regression(4765/4999): loss=0.5402317771143845\n",
      "Log Regression(4766/4999): loss=0.5402876377817009\n",
      "Log Regression(4767/4999): loss=0.5409979554177392\n",
      "Log Regression(4768/4999): loss=0.5406134425803257\n",
      "Log Regression(4769/4999): loss=0.5402400459519983\n",
      "Log Regression(4770/4999): loss=0.5401528289210631\n",
      "Log Regression(4771/4999): loss=0.5416507533090458\n",
      "Log Regression(4772/4999): loss=0.5405610848391476\n",
      "Log Regression(4773/4999): loss=0.5404234635767553\n",
      "Log Regression(4774/4999): loss=0.5408952268534085\n",
      "Log Regression(4775/4999): loss=0.5400224661367284\n",
      "Log Regression(4776/4999): loss=0.5434160227891884\n",
      "Log Regression(4777/4999): loss=0.5416190422967758\n",
      "Log Regression(4778/4999): loss=0.541276980847108\n",
      "Log Regression(4779/4999): loss=0.5449185396483729\n",
      "Log Regression(4780/4999): loss=0.5444327997698118\n",
      "Log Regression(4781/4999): loss=0.5425696883135465\n",
      "Log Regression(4782/4999): loss=0.5421652601473798\n",
      "Log Regression(4783/4999): loss=0.5401088613699402\n",
      "Log Regression(4784/4999): loss=0.5403162270485045\n",
      "Log Regression(4785/4999): loss=0.5403965528701958\n",
      "Log Regression(4786/4999): loss=0.5409020868039441\n",
      "Log Regression(4787/4999): loss=0.5416819332043183\n",
      "Log Regression(4788/4999): loss=0.5442597185933487\n",
      "Log Regression(4789/4999): loss=0.5441477310656424\n",
      "Log Regression(4790/4999): loss=0.541199286780557\n",
      "Log Regression(4791/4999): loss=0.5407598789170269\n",
      "Log Regression(4792/4999): loss=0.540702391997083\n",
      "Log Regression(4793/4999): loss=0.5409014071462074\n",
      "Log Regression(4794/4999): loss=0.5402433386038026\n",
      "Log Regression(4795/4999): loss=0.5415588589203263\n",
      "Log Regression(4796/4999): loss=0.5424442484429617\n",
      "Log Regression(4797/4999): loss=0.5444303035909857\n",
      "Log Regression(4798/4999): loss=0.5403967943252929\n",
      "Log Regression(4799/4999): loss=0.5423992536565232\n",
      "Log Regression(4800/4999): loss=0.5438925231824836\n",
      "Log Regression(4801/4999): loss=0.542408097756559\n",
      "Log Regression(4802/4999): loss=0.540720733996301\n",
      "Log Regression(4803/4999): loss=0.5404589460720941\n",
      "Log Regression(4804/4999): loss=0.5427049436054375\n",
      "Log Regression(4805/4999): loss=0.5409225029444322\n",
      "Log Regression(4806/4999): loss=0.5406885383283814\n",
      "Log Regression(4807/4999): loss=0.5427372484968103\n",
      "Log Regression(4808/4999): loss=0.5414407170899768\n",
      "Log Regression(4809/4999): loss=0.5482270545357663\n",
      "Log Regression(4810/4999): loss=0.5419888375827318\n",
      "Log Regression(4811/4999): loss=0.5412195693765683\n",
      "Log Regression(4812/4999): loss=0.5456402808862293\n",
      "Log Regression(4813/4999): loss=0.5432688053017645\n",
      "Log Regression(4814/4999): loss=0.5413190398813404\n",
      "Log Regression(4815/4999): loss=0.5411309537335339\n",
      "Log Regression(4816/4999): loss=0.5413584993066201\n",
      "Log Regression(4817/4999): loss=0.5399934791969216\n",
      "Log Regression(4818/4999): loss=0.5400020373940331\n",
      "Log Regression(4819/4999): loss=0.5445430088951603\n",
      "Log Regression(4820/4999): loss=0.5418276223468734\n",
      "Log Regression(4821/4999): loss=0.5406715915744466\n",
      "Log Regression(4822/4999): loss=0.5399599442984705\n",
      "Log Regression(4823/4999): loss=0.5400969579307979\n",
      "Log Regression(4824/4999): loss=0.5404737577823359\n",
      "Log Regression(4825/4999): loss=0.5442415347882017\n",
      "Log Regression(4826/4999): loss=0.543283085062251\n",
      "Log Regression(4827/4999): loss=0.5408234268578509\n",
      "Log Regression(4828/4999): loss=0.5441849861403717\n",
      "Log Regression(4829/4999): loss=0.5440619813754882\n",
      "Log Regression(4830/4999): loss=0.5403498790730297\n",
      "Log Regression(4831/4999): loss=0.5413282823296008\n",
      "Log Regression(4832/4999): loss=0.5409326137867562\n",
      "Log Regression(4833/4999): loss=0.5405184456642843\n",
      "Log Regression(4834/4999): loss=0.5402847866338923\n",
      "Log Regression(4835/4999): loss=0.5402255926276079\n",
      "Log Regression(4836/4999): loss=0.5419873533310683\n",
      "Log Regression(4837/4999): loss=0.5441460037553861\n",
      "Log Regression(4838/4999): loss=0.5417671080073696\n",
      "Log Regression(4839/4999): loss=0.5405771730240306\n",
      "Log Regression(4840/4999): loss=0.5410191439948279\n",
      "Log Regression(4841/4999): loss=0.541599671730849\n",
      "Log Regression(4842/4999): loss=0.5404489199205331\n",
      "Log Regression(4843/4999): loss=0.540639284843395\n",
      "Log Regression(4844/4999): loss=0.5418398692375304\n",
      "Log Regression(4845/4999): loss=0.5421488020317605\n",
      "Log Regression(4846/4999): loss=0.5406530364119537\n",
      "Log Regression(4847/4999): loss=0.5403021192978886\n",
      "Log Regression(4848/4999): loss=0.5428215966773184\n",
      "Log Regression(4849/4999): loss=0.5417793203572354\n",
      "Log Regression(4850/4999): loss=0.5414362925921885\n",
      "Log Regression(4851/4999): loss=0.545774982806664\n",
      "Log Regression(4852/4999): loss=0.5406632838633774\n",
      "Log Regression(4853/4999): loss=0.5424738949120023\n",
      "Log Regression(4854/4999): loss=0.5426961611410972\n",
      "Log Regression(4855/4999): loss=0.5453378043150281\n",
      "Log Regression(4856/4999): loss=0.5440110017968928\n",
      "Log Regression(4857/4999): loss=0.5434637491500571\n",
      "Log Regression(4858/4999): loss=0.5417622972992515\n",
      "Log Regression(4859/4999): loss=0.5404947399094794\n",
      "Log Regression(4860/4999): loss=0.5402660484149955\n",
      "Log Regression(4861/4999): loss=0.5400900840575534\n",
      "Log Regression(4862/4999): loss=0.5401815047366387\n",
      "Log Regression(4863/4999): loss=0.5401430325800323\n",
      "Log Regression(4864/4999): loss=0.5419468436928898\n",
      "Log Regression(4865/4999): loss=0.5405129998059578\n",
      "Log Regression(4866/4999): loss=0.5407141931480545\n",
      "Log Regression(4867/4999): loss=0.540110325726168\n",
      "Log Regression(4868/4999): loss=0.5405549979928859\n",
      "Log Regression(4869/4999): loss=0.5429745876943957\n",
      "Log Regression(4870/4999): loss=0.5467129853061629\n",
      "Log Regression(4871/4999): loss=0.54118863831432\n",
      "Log Regression(4872/4999): loss=0.541745112576368\n",
      "Log Regression(4873/4999): loss=0.5429156960603772\n",
      "Log Regression(4874/4999): loss=0.540345682622722\n",
      "Log Regression(4875/4999): loss=0.5401775261365529\n",
      "Log Regression(4876/4999): loss=0.5411613244293934\n",
      "Log Regression(4877/4999): loss=0.5404641864498829\n",
      "Log Regression(4878/4999): loss=0.5403273043704299\n",
      "Log Regression(4879/4999): loss=0.5414400404741692\n",
      "Log Regression(4880/4999): loss=0.5400957290754038\n",
      "Log Regression(4881/4999): loss=0.5412465968061382\n",
      "Log Regression(4882/4999): loss=0.540778544017362\n",
      "Log Regression(4883/4999): loss=0.540673111346674\n",
      "Log Regression(4884/4999): loss=0.5431105712128635\n",
      "Log Regression(4885/4999): loss=0.5426013509450424\n",
      "Log Regression(4886/4999): loss=0.5407107281794502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(4887/4999): loss=0.5402863618921366\n",
      "Log Regression(4888/4999): loss=0.543441649674543\n",
      "Log Regression(4889/4999): loss=0.5414767785044818\n",
      "Log Regression(4890/4999): loss=0.5411180907993698\n",
      "Log Regression(4891/4999): loss=0.5402551408374878\n",
      "Log Regression(4892/4999): loss=0.5407124541725742\n",
      "Log Regression(4893/4999): loss=0.5414900652126782\n",
      "Log Regression(4894/4999): loss=0.5436570324355459\n",
      "Log Regression(4895/4999): loss=0.5424281085296577\n",
      "Log Regression(4896/4999): loss=0.5428855704832396\n",
      "Log Regression(4897/4999): loss=0.5414727448341016\n",
      "Log Regression(4898/4999): loss=0.5408970558928979\n",
      "Log Regression(4899/4999): loss=0.5404657294434442\n",
      "Log Regression(4900/4999): loss=0.5404423602719173\n",
      "Log Regression(4901/4999): loss=0.5430837714792239\n",
      "Log Regression(4902/4999): loss=0.5424359772281476\n",
      "Log Regression(4903/4999): loss=0.5438356371110523\n",
      "Log Regression(4904/4999): loss=0.5415968136364168\n",
      "Log Regression(4905/4999): loss=0.5402320373328016\n",
      "Log Regression(4906/4999): loss=0.5404091468048426\n",
      "Log Regression(4907/4999): loss=0.5409691275430509\n",
      "Log Regression(4908/4999): loss=0.5410637112745519\n",
      "Log Regression(4909/4999): loss=0.5435082180719014\n",
      "Log Regression(4910/4999): loss=0.5429895100629821\n",
      "Log Regression(4911/4999): loss=0.5440683207511426\n",
      "Log Regression(4912/4999): loss=0.5421989152382173\n",
      "Log Regression(4913/4999): loss=0.5402453928728912\n",
      "Log Regression(4914/4999): loss=0.5408267252048228\n",
      "Log Regression(4915/4999): loss=0.5402927070162967\n",
      "Log Regression(4916/4999): loss=0.5400085544596365\n",
      "Log Regression(4917/4999): loss=0.540289980866881\n",
      "Log Regression(4918/4999): loss=0.5404934459395194\n",
      "Log Regression(4919/4999): loss=0.5422604693923759\n",
      "Log Regression(4920/4999): loss=0.5411161817222299\n",
      "Log Regression(4921/4999): loss=0.5416800757728498\n",
      "Log Regression(4922/4999): loss=0.5406672572517894\n",
      "Log Regression(4923/4999): loss=0.542741216396301\n",
      "Log Regression(4924/4999): loss=0.545373935092092\n",
      "Log Regression(4925/4999): loss=0.5432992687974239\n",
      "Log Regression(4926/4999): loss=0.5400875617511959\n",
      "Log Regression(4927/4999): loss=0.5412256094337584\n",
      "Log Regression(4928/4999): loss=0.5451119275349635\n",
      "Log Regression(4929/4999): loss=0.5439967668390037\n",
      "Log Regression(4930/4999): loss=0.5426711276530642\n",
      "Log Regression(4931/4999): loss=0.5404632108230935\n",
      "Log Regression(4932/4999): loss=0.5402828880829879\n",
      "Log Regression(4933/4999): loss=0.5408158750997331\n",
      "Log Regression(4934/4999): loss=0.5408104916706498\n",
      "Log Regression(4935/4999): loss=0.5408620813064506\n",
      "Log Regression(4936/4999): loss=0.5402024475507117\n",
      "Log Regression(4937/4999): loss=0.5411158470745452\n",
      "Log Regression(4938/4999): loss=0.5414111451416611\n",
      "Log Regression(4939/4999): loss=0.5429552766626379\n",
      "Log Regression(4940/4999): loss=0.5405973819229423\n",
      "Log Regression(4941/4999): loss=0.5402245288004177\n",
      "Log Regression(4942/4999): loss=0.5403793058987236\n",
      "Log Regression(4943/4999): loss=0.543353111641774\n",
      "Log Regression(4944/4999): loss=0.5461234033234513\n",
      "Log Regression(4945/4999): loss=0.5426523194420861\n",
      "Log Regression(4946/4999): loss=0.5441212465585866\n",
      "Log Regression(4947/4999): loss=0.5425196251542964\n",
      "Log Regression(4948/4999): loss=0.5417521258323315\n",
      "Log Regression(4949/4999): loss=0.5424186931133587\n",
      "Log Regression(4950/4999): loss=0.5399170386473462\n",
      "Log Regression(4951/4999): loss=0.5399148815285325\n",
      "Log Regression(4952/4999): loss=0.5401432360960244\n",
      "Log Regression(4953/4999): loss=0.5399228254674358\n",
      "Log Regression(4954/4999): loss=0.5404890178194925\n",
      "Log Regression(4955/4999): loss=0.5404229924115093\n",
      "Log Regression(4956/4999): loss=0.5402153910509857\n",
      "Log Regression(4957/4999): loss=0.5421634189312675\n",
      "Log Regression(4958/4999): loss=0.5415500070657138\n",
      "Log Regression(4959/4999): loss=0.5413861829105765\n",
      "Log Regression(4960/4999): loss=0.5399161854587803\n",
      "Log Regression(4961/4999): loss=0.5399129215256733\n",
      "Log Regression(4962/4999): loss=0.5399052368027527\n",
      "Log Regression(4963/4999): loss=0.5401678994256283\n",
      "Log Regression(4964/4999): loss=0.5409504050530188\n",
      "Log Regression(4965/4999): loss=0.5409151596204597\n",
      "Log Regression(4966/4999): loss=0.5431625552082547\n",
      "Log Regression(4967/4999): loss=0.5424303132322194\n",
      "Log Regression(4968/4999): loss=0.5402608814169003\n",
      "Log Regression(4969/4999): loss=0.5409005077847477\n",
      "Log Regression(4970/4999): loss=0.5402008227624532\n",
      "Log Regression(4971/4999): loss=0.5421683871395194\n",
      "Log Regression(4972/4999): loss=0.5401886100697906\n",
      "Log Regression(4973/4999): loss=0.5468022966348309\n",
      "Log Regression(4974/4999): loss=0.5407482691230759\n",
      "Log Regression(4975/4999): loss=0.5410897090492214\n",
      "Log Regression(4976/4999): loss=0.5410293534489158\n",
      "Log Regression(4977/4999): loss=0.5408025545148399\n",
      "Log Regression(4978/4999): loss=0.5402090121068576\n",
      "Log Regression(4979/4999): loss=0.5400050371794891\n",
      "Log Regression(4980/4999): loss=0.5401267615546662\n",
      "Log Regression(4981/4999): loss=0.5407435679064396\n",
      "Log Regression(4982/4999): loss=0.5401009897570733\n",
      "Log Regression(4983/4999): loss=0.5399560971448677\n",
      "Log Regression(4984/4999): loss=0.5407795470512716\n",
      "Log Regression(4985/4999): loss=0.5444959599388619\n",
      "Log Regression(4986/4999): loss=0.5402306858149869\n",
      "Log Regression(4987/4999): loss=0.540516200869372\n",
      "Log Regression(4988/4999): loss=0.5405234825928713\n",
      "Log Regression(4989/4999): loss=0.5410451046082938\n",
      "Log Regression(4990/4999): loss=0.540497125973792\n",
      "Log Regression(4991/4999): loss=0.5409155291538336\n",
      "Log Regression(4992/4999): loss=0.5431860435859652\n",
      "Log Regression(4993/4999): loss=0.5453484249895293\n",
      "Log Regression(4994/4999): loss=0.5411504938163632\n",
      "Log Regression(4995/4999): loss=0.5430447844897225\n",
      "Log Regression(4996/4999): loss=0.541633286286059\n",
      "Log Regression(4997/4999): loss=0.5442955737918008\n",
      "Log Regression(4998/4999): loss=0.5459493751622291\n",
      "Log Regression(4999/4999): loss=0.5411215709682157\n"
     ]
    }
   ],
   "source": [
    "w, mse = reg_logistic_regression(y_train_lr, tX_train, lambda_, initial_w, max_iters, gamma, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7297333333333333"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict_labels(w, tX_train)\n",
    "np.mean(y_train.reshape(-1,1)==y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73176"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict_labels(w, tX_val)\n",
    "np.mean(y_val.reshape(-1,1)==y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lambda = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros(tX.shape[1])#np.random.normal(loc=0, scale=1, size=(tX.shape[1],1))\n",
    "max_iters = 5000\n",
    "gamma = 1e-7\n",
    "lambda_ = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training process here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grubdragon/.pyenv/versions/3.7.4/envs/pyML/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict_labels(w, tX_train)\n",
    "np.mean(y_train.reshape(-1,1)==y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5781918208"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict_labels(w, tX_val)\n",
    "np.mean(y_val.reshape(-1,1)==y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lambda = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros(tX.shape[1])#np.random.normal(loc=0, scale=1, size=(tX.shape[1],1))\n",
    "max_iters = 5000\n",
    "gamma = 1e-7\n",
    "lambda_ = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Run the training step here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training process here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.728385"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict_labels(w, tX_train)\n",
    "np.mean(y_train.reshape(-1,1)==y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7314"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict_labels(w, tX_val)\n",
    "np.mean(y_val.reshape(-1,1)==y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After the minimal training, we try to do some extra data processing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can begin with removing the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_processor as dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_train_new, outlier_thresh = dp.remove_outliers(tX_train, conf_int=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the same outlier threshold\n",
    "tX_val_new, _ = dp.remove_outliers(tX_val, outlier_thresh=outlier_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((225000, 30), (25000, 30))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX_train_new.shape, tX_val_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We add polynomial features to help with training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_train_aug, tX_val_aug = dp.poly_features(tX_train_new,3), dp.poly_features(tX_val_new,3)\n",
    "tX_train_aug, tX_val_aug = dp.add_ones(tX_train_aug), dp.add_ones(tX_val_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((225000, 91), (25000, 91))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX_train_aug.shape, tX_val_aug.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge Regression with polynomials, without outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, mse = ridge_regression(y_train, tX_train_aug, lambda_=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1. -1.  1. ...  1. -1. -1.]\n",
      "(225000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6872355555555556"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict_labels(w, tX_train_aug)\n",
    "print(y_pred)\n",
    "print(y_pred.shape)\n",
    "np.mean(y_train==y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1. -1. -1. ... -1.  1. -1.]\n",
      "(25000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.68652"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict_labels(w, tX_val_aug)\n",
    "print(y_pred)\n",
    "print(y_pred.shape)\n",
    "np.mean(y_val==y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly removing outliers isn't doing a better job on the data. So we don't do that yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We try to normalize the data, hoping that effect of certain features doesn't dull others'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *\n",
    "import data_processor as dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_train_new, norm_stats = dp.standardize(tX_train_f)\n",
    "# We use the same outlier threshold\n",
    "tX_val_new, _ = dp.standardize(tX_val_f, norm_stats=norm_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((225000, 30), (25000, 30))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX_train_new.shape, tX_val_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We add polynomial features to help with training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_train_aug_poly, tX_val_aug_poly = dp.poly_features(tX_train_new,13), dp.poly_features(tX_val_new,13)\n",
    "tX_train_aug_poly, norm_stats = dp.standardize(tX_train_aug_poly)\n",
    "# We use the same outlier threshold\n",
    "tX_val_aug_poly, _ = dp.standardize(tX_val_aug_poly, norm_stats=norm_stats)\n",
    "\n",
    "tX_train_poly_std, tX_val_poly_std = dp.add_ones(tX_train_aug_poly), dp.add_ones(tX_val_aug_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((225000, 391), (25000, 391))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX_train_poly_std.shape, tX_val_poly_std.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge Regression with polynomials, standardizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda=1e-09\n",
      "0.09902222222222222\n",
      "0.09748\n",
      "Lambda=1e-08\n",
      "0.09448888888888889\n",
      "0.09488\n",
      "Lambda=1e-07\n",
      "0.09209777777777778\n",
      "0.09164\n",
      "Lambda=1e-06\n",
      "0.08792888888888889\n",
      "0.0876\n",
      "Lambda=1e-05\n",
      "0.08610222222222222\n",
      "0.08576\n",
      "Lambda=0.0001\n",
      "0.08498222222222222\n",
      "0.08484\n",
      "Lambda=0.001\n",
      "0.0836\n",
      "0.08348\n",
      "Lambda=0.01\n",
      "0.08057777777777778\n",
      "0.081\n",
      "Lambda=0.1\n",
      "0.07503555555555555\n",
      "0.07524\n",
      "Lambda=1\n",
      "0.06924444444444444\n",
      "0.07008\n",
      "Lambda=10.0\n",
      "0.06581333333333333\n",
      "0.06644\n",
      "Lambda=100.0\n",
      "0.06420888888888888\n",
      "0.0638\n",
      "Lambda=1000.0\n",
      "0.061013333333333336\n",
      "0.06104\n",
      "Lambda=10000.0\n",
      "0.05280888888888889\n",
      "0.05192\n",
      "Lambda=100000.0\n",
      "0.04538222222222222\n",
      "0.04468\n",
      "Lambda=1000000.0\n",
      "0.03361777777777778\n",
      "0.03188\n",
      "Lambda=10000000.0\n",
      "0.0016044444444444444\n",
      "0.0012\n"
     ]
    }
   ],
   "source": [
    "ws_ridge_poly_std = []\n",
    "for l in [1e-9,1e-8, 1e-7,1e-6,1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4, 1e5, 1e6, 1e7]:\n",
    "    w, mse = ridge_regression(y_train, tX_train_poly_std, lambda_=l)\n",
    "    ws_ridge_poly_std.append(w)\n",
    "    #print(w)\n",
    "    y_pred = predict_labels(w, tX_train_poly_std)\n",
    "    #print(y_pred)\n",
    "    #print(y_pred.shape)\n",
    "    print(\"Lambda=\"+str(l))\n",
    "    print(np.mean(y_train==y_pred))\n",
    "    y_pred = predict_labels(w, tX_val_poly_std)\n",
    "    print(np.mean(y_val==y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So some good lambda values are 1e-4 to 1e4, each giving good range of normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can try using logistic regression here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_train_aug_poly, tX_val_aug_poly = dp.poly_features(tX_train_new,4), dp.poly_features(tX_val_new,4)\n",
    "tX_train_aug_poly, stats2 = dp.standardize(tX_train_aug_poly)\n",
    "tX_val_aug_poly, _ = dp.standardize(tX_val_aug_poly, norm_stats=stats2)\n",
    "tX_train_poly_std, tX_val_poly_std = dp.add_ones(tX_train_aug_poly), dp.add_ones(tX_val_aug_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((225000, 121), (25000, 121))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX_train_poly_std.shape, tX_val_poly_std.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regulaized Logistic Regression with polynomials, standardizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros((tX_train_poly_std.shape[1],1))\n",
    "#initial_w = np.random.normal(loc=0, scale=0.0001, size=(tX_train_poly_std.shape[1],1))\n",
    "max_iters = 300\n",
    "gamma = 3e-4\n",
    "lambda_ = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(225000,)\n"
     ]
    }
   ],
   "source": [
    "y_train_lr = 0.5*(y_train+1)\n",
    "print(y_train_lr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(0/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(1/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(2/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(3/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(4/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(5/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(6/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(7/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(8/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(9/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(10/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(11/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(12/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(13/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(14/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(15/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(16/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(17/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(18/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(19/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(20/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(21/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(22/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(23/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(24/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(25/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(26/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(27/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(28/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(29/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(30/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(31/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(32/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(33/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(34/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(35/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(36/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(37/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(38/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(39/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(40/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(41/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(42/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(43/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(44/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(45/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(46/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(47/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(48/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(49/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(50/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(51/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(52/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(53/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(54/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(55/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(56/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(57/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(58/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(59/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(60/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(61/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(62/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(63/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(64/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(65/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(66/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(67/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(68/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(69/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(70/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(71/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(72/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(73/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(74/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(75/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(76/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(77/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(78/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(79/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(80/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(81/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(82/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(83/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(84/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(85/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(86/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(87/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(88/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(89/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(90/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(91/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(92/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(93/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(94/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(95/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(96/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(97/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(98/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(99/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(100/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(101/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(102/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(103/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(104/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(105/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(106/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(107/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(108/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(109/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(110/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(111/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(112/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(113/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(114/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(115/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(116/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(117/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(118/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(119/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(120/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(121/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(122/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(123/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(124/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(125/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(126/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(127/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(128/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(129/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(130/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(131/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(132/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(133/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(134/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(135/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(136/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(137/299): loss=nan log_reg_loss=nan reg_l=0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(138/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(139/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(140/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(141/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(142/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(143/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(144/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(145/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(146/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(147/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(148/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(149/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(150/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(151/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(152/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(153/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(154/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(155/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(156/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(157/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(158/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(159/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(160/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(161/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(162/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(163/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(164/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(165/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(166/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(167/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(168/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(169/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(170/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(171/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(172/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(173/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(174/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(175/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(176/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(177/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(178/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(179/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(180/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(181/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(182/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(183/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(184/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(185/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(186/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(187/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(188/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(189/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(190/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(191/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(192/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(193/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(194/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(195/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(196/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(197/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(198/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(199/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(200/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(201/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(202/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(203/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(204/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(205/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(206/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(207/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(208/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(209/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(210/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(211/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(212/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(213/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(214/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(215/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(216/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(217/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(218/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(219/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(220/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(221/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(222/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(223/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(224/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(225/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(226/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(227/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(228/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(229/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(230/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(231/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(232/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(233/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(234/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(235/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(236/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(237/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(238/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(239/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(240/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(241/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(242/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(243/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(244/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(245/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(246/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(247/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(248/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(249/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(250/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(251/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(252/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(253/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(254/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(255/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(256/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(257/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(258/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(259/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(260/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(261/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(262/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(263/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(264/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(265/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(266/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(267/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(268/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(269/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(270/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(271/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(272/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(273/299): loss=nan log_reg_loss=nan reg_l=0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(274/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(275/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(276/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(277/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(278/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(279/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(280/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(281/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(282/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(283/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(284/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(285/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(286/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(287/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(288/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(289/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(290/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(291/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(292/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(293/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(294/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(295/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(296/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(297/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(298/299): loss=nan log_reg_loss=nan reg_l=0.0\n",
      "Log Regression(299/299): loss=nan log_reg_loss=nan reg_l=0.0\n"
     ]
    }
   ],
   "source": [
    "w, mse = reg_logistic_regression(y_train_lr, tX_train_poly_std, lambda_, w, max_iters, gamma, batch_size=64, lr_decay=True, lr_decay_rate=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7693111111111111\n"
     ]
    }
   ],
   "source": [
    "y_pred1 = predict_labels(w, tX_train_poly_std)\n",
    "print(np.mean(y_train.reshape(-1,1)==y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.77372\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict_labels(w, tX_val_poly_std)\n",
    "print(np.mean(y_val.reshape(-1,1)==y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_good = w.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('weights.pkl','wb') as f:\n",
    "    pickle.dump(w_good, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_wut = w.copy()\n",
    "import pickle\n",
    "with open('weights_naned.pkl','wb') as f:\n",
    "    pickle.dump(w_wut, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2361"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_train_f, norm=dp.standardize(tX_train_f)\n",
    "tX_val_f, _=dp.standardize(tX_val_f, norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_train_int,int_list = dp.interaction_terms(tX_train_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(435,)\n"
     ]
    }
   ],
   "source": [
    "tX_train_cap, int_list=dp.var_cap(tX_train_int, int_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_train_int=dp.generate_interactions(tX_train_f,int_list[:150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_val_int=dp.generate_interactions(tX_val_f,int_list[:150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train_int,int_list = dp.interaction_terms(y_train_f)\n",
    "#y_val_int,int_list = dp.interaction_terms(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tX_train_poly=dp.poly_features(tX_train_f,5)\n",
    "tX_val_poly=dp.poly_features(tX_val_f,5)\n",
    "                           \n",
    "tX_train_poly, norm=dp.standardize(tX_train_poly)\n",
    "tX_val_poly,_=dp.standardize(tX_val_poly, norm)\n",
    "tX_train_poly, tX_val_poly = dp.add_ones(tX_train_poly), dp.add_ones(tX_val_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 211)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX_val_poly.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1. ... 0. 1. 0.]\n",
      "[1. 1. 0. ... 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "y_train_l = 0.5*(y_train+1)\n",
    "y_val_l=0.5*(y_val+1)\n",
    "\n",
    "\n",
    "print(y_train_l)\n",
    "print(y_val_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regresion Regularized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-08\n",
      "Log Regression(0/1499): loss=0.6926725230091082\n",
      "Log Regression(100/1499): loss=0.6646696115496871\n",
      "Log Regression(200/1499): loss=0.6523643551717306\n",
      "Log Regression(300/1499): loss=0.645099254928256\n",
      "Log Regression(400/1499): loss=0.6406879637232095\n"
     ]
    }
   ],
   "source": [
    "max_iters=1500\n",
    "gamma= 1e-4\n",
    "\n",
    "initial_w = np.zeros((tX_train_int.shape[1],1))\n",
    "weights=[]\n",
    "for l in [1e-8, 1e-7,1e-6,1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4, 1e5, 1e6, 1e7]:\n",
    "    print(l)\n",
    "    ws, losses = reg_logistic_regression(y_train_l, tX_train_int, l, initial_w, max_iters, gamma, batch_size = 100)\n",
    "    y_pred_train=predict_labels(ws[1499],tX_train_int)\n",
    "    y_pred_val=predict_labels(ws[1499],tX_val_int)\n",
    "    print(np.mean(y_train_l==y_pred_train.T))\n",
    "    print(np.mean(y_val_l==y_pred_val.T))\n",
    "    weights.append(ws)\n",
    "    #y_pred_train = predict_labels(w, tX_train_f)\n",
    "    #y_pred_val = predict_labels(w, tX_val_f)\n",
    "    #print(str(l)+\": MSE_TRAIN=\"+str(mse)+ \" TRAIN=\"+str(np.mean(y_train==y_pred_train))+\" VAL=\"+str(np.mean(y_val==y_pred_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 0. ... 0. 1. 0.]\n",
      "[[0. 0. 0. ... 0. 1. 0.]]\n",
      "150\n",
      "0.67228\n",
      "0.67444\n"
     ]
    }
   ],
   "source": [
    "y_pred_train=predict_labels(ws[999],tX_train_poly)\n",
    "y_pred_val=predict_labels(ws[999],tX_val_poly)\n",
    "\n",
    "print(y_train_l)\n",
    "print(y_pred_train.T)\n",
    "print(len(ws[1499]))\n",
    "print(np.mean(y_train_l==y_pred_train.T))\n",
    "print(np.mean(y_val_l==y_pred_val.T))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 301)\n"
     ]
    }
   ],
   "source": [
    "print(tX_val_poly.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 211)\n",
      "(751, 1)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-f9b903d29286>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtX_val_poly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mws\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m899\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtX_val_poly\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "def plot(losses,ws,tX_val,y_val):\n",
    "    # Plot two plots:\n",
    "    # loss as a function of step\n",
    "    # accuracy as a function of step  (on validation set)\n",
    "\n",
    "    steps = [i for i in range(1,len(losses)+1)]\n",
    "\n",
    "    ax1 = plt.plot(steps,losses)\n",
    "    plt.title('Loss as a function of step')\n",
    "    plt.xlabel('number of steps')\n",
    "    plt.ylabel('loss (approximate)')\n",
    "    plt.show()\n",
    "    #compute accuracy\n",
    "    accuracy = []\n",
    "    for w in ws:\n",
    "        y_pred = predict_labels(w,tX_val.T)\n",
    "        accuracy.append(np.mean(y_val.reshape(-1,1)==y_pred))\n",
    "\n",
    "        \n",
    "    ax2 = plt.plot(steps,accuracy)\n",
    "    plt.title('Accuracy as a function of step')\n",
    "    plt.xlabel('number of steps')\n",
    "    plt.ylabel('accuracy on validation set')\n",
    "    plt.show()\n",
    "print(tX_val_poly.shape)\n",
    "print(ws[899].shape)\n",
    "plot(losses,weights[1],tX_val_poly,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vlad/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 VAL=0.66116\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = np.dot(tX_train_f,l2[0])\n",
    "y_pred_val = predict_labels(l2[0], tX_val_f)\n",
    "\n",
    "print(str(np.mean(y_train==y_pred_train))+\" VAL=\"+str(np.mean(y_val==y_pred_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "344"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_test, _ = dp.fill_nan(tX_test, nan_value=-999, method='use_filler', filler=filler)\n",
    "\n",
    "tX_test,int_list = dp.interaction_terms(tX_test)\n",
    "tX_test=dp.add_ones(tX_test)\n",
    "\n",
    "\n",
    "\n",
    "#tX_test_new, _ = dp.standardize(tX_test, norm_stats=norm_stats)\n",
    "#tX_test_aug_poly = dp.poly_features(tX_test_new,7)\n",
    "#tX_test_poly_std = dp.add_ones(tX_test_aug_poly)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-e6ca4e773b27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtX_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoly_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Homework/Semester 5/Machine Learning/CS433-Project1/data_processor.py\u001b[0m in \u001b[0;36mpoly_features\u001b[0;34m(x, degree)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mcurr_deg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0mx_poly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_deg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdegree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(a, order)\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m     \"\"\"\n\u001b[0;32m--> 792\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[0;31m# Basic operations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tX_test=dp.poly_features(tX_test,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict_labels(l1[0], tX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-3e433871914e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtX_val_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_nan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnan_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m999\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'use_filler'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiller\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfiller\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtX_val_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minteraction_terms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtX_val_f\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtX_val_int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoly_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtX_val_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtX_val_f\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Homework/Semester 5/Machine Learning/CS433-Project1/data_processor.py\u001b[0m in \u001b[0;36mpoly_features\u001b[0;34m(x, degree)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdegree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mcurr_deg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_deg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mx_poly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_poly\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_deg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx_poly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mhstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#tX_val_f, _ = dp.fill_nan(tX_test, nan_value=-999, method='use_filler', filler=filler)\n",
    "#tX_val_int, int_list = dp.interaction_terms(tX_val_f)\n",
    "#tX_val_int=dp.poly_features(tX_val_int, 5)\n",
    "#y_pred = predict_labels(l1[1], tX_val_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'out.csv' # TODO: fill in desired name of output file for submission\n",
    "#y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Logistic Regresion Regularized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regresion Regularized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
