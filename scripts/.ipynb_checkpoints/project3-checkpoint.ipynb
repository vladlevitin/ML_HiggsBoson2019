{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_processor as dp\n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *\n",
    "import matplotlib.pyplot as plt # for data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = np.arange(len(y))\n",
    "np.random.shuffle(ind)\n",
    "ind_train = ind[:int(len(ind)*0.9)]\n",
    "ind_test = ind[int(len(ind)*0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_train = tX[ind_train]\n",
    "y_train = y[ind_train]\n",
    "\n",
    "tX_val = tX[ind_test]\n",
    "y_val = y[ind_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_train, filler = dp.fill_nan(tX_train, nan_value=-999, method='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_val, _ = dp.fill_nan(tX_val, nan_value=-999, method='use_filler', filler=filler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((225000, 30), (25000, 30))"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX_train.shape,tX_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=  (10,10))\n",
    "ax = sns.heatmap(tX_train[10:20,:30])\n",
    "ax.set_yticklabels([i for i in range(10,20)]) \n",
    "plt.title('Correlation map 2')\n",
    "plt.ylabel('features')\n",
    "plt.xlabel('features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=  (10,10))\n",
    "ax = sns.heatmap(tX_train[20:30,:30])\n",
    "ax.set_yticklabels([i for i in range(20,30)]) \n",
    "plt.title('Correlation map 3')\n",
    "plt.ylabel('features')\n",
    "plt.xlabel('features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=  (10,10))\n",
    "ax = sns.heatmap(tX_train[0:10,:30])\n",
    "ax.set_yticklabels([i for i in range(0,10)]) \n",
    "plt.title('Correlation map 1')\n",
    "plt.ylabel('features')\n",
    "plt.xlabel('features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill NaN. Apply LS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, mse = least_squares(y_train, tX_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1. -1.  1. ...  1. -1. -1.]\n",
      "(225000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7404266666666667"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict_labels(w, tX_train)\n",
    "print(y_pred)\n",
    "print(y_pred.shape)\n",
    "np.mean(y_train==y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.  1.  1. ... -1. -1. -1.]\n",
      "(25000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.74284"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict_labels(w, tX_val)\n",
    "print(y_pred)\n",
    "print(y_pred.shape)\n",
    "np.mean(y_val==y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001: MSE_TRAIN=0.34383321387881927 TRAIN=0.7404311111111112 VAL=0.74284\n",
      "0.001: MSE_TRAIN=0.34383321491499214 TRAIN=0.7404355555555555 VAL=0.74284\n",
      "0.01: MSE_TRAIN=0.3438332573871147 TRAIN=0.7404444444444445 VAL=0.7428\n",
      "0.1: MSE_TRAIN=0.3438334437031598 TRAIN=0.7404088888888889 VAL=0.74268\n",
      "1: MSE_TRAIN=0.34383351744321655 TRAIN=0.7404044444444444 VAL=0.74268\n",
      "10.0: MSE_TRAIN=0.3438335333866524 TRAIN=0.7403866666666666 VAL=0.74268\n",
      "100.0: MSE_TRAIN=0.34383418712302466 TRAIN=0.7403511111111111 VAL=0.74276\n",
      "1000.0: MSE_TRAIN=0.3438868882829171 TRAIN=0.7398933333333333 VAL=0.7418\n",
      "10000.0: MSE_TRAIN=0.3455044343817568 TRAIN=0.7372133333333334 VAL=0.73792\n"
     ]
    }
   ],
   "source": [
    "for l in [1e-4, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4]:\n",
    "    w, mse = ridge_regression(y_train, tX_train, l)\n",
    "    y_pred_train = predict_labels(w, tX_train)\n",
    "    y_pred_val = predict_labels(w, tX_val)\n",
    "    print(str(l)+\": MSE_TRAIN=\"+str(mse)+ \" TRAIN=\"+str(np.mean(y_train==y_pred_train))+\" VAL=\"+str(np.mean(y_val==y_pred_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Squares GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros(tX.shape[1])#np.random.normal(loc=0, scale=1, size=(tX.shape[1],1))\n",
    "max_iters = 1000\n",
    "gamma = 3e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/999): loss=0.5000000000000003\n",
      "Gradient Descent(1/999): loss=0.47514767720269957\n",
      "Gradient Descent(2/999): loss=0.4723188235681864\n",
      "Gradient Descent(3/999): loss=0.4698163342630682\n",
      "Gradient Descent(4/999): loss=0.4674265566178349\n",
      "Gradient Descent(5/999): loss=0.46514164772825245\n",
      "Gradient Descent(6/999): loss=0.4629560507024571\n",
      "Gradient Descent(7/999): loss=0.4608645305822573\n",
      "Gradient Descent(8/999): loss=0.45886213900965495\n",
      "Gradient Descent(9/999): loss=0.45694419825845206\n",
      "Gradient Descent(10/999): loss=0.45510628631722694\n",
      "Gradient Descent(11/999): loss=0.4533442227997835\n",
      "Gradient Descent(12/999): loss=0.45165405563508226\n",
      "Gradient Descent(13/999): loss=0.4500320484930448\n",
      "Gradient Descent(14/999): loss=0.4484746689051728\n",
      "Gradient Descent(15/999): loss=0.4469785770412906\n",
      "Gradient Descent(16/999): loss=0.44554061510593923\n",
      "Gradient Descent(17/999): loss=0.4441577973200208\n",
      "Gradient Descent(18/999): loss=0.44282730045524227\n",
      "Gradient Descent(19/999): loss=0.44154645489074\n",
      "Gradient Descent(20/999): loss=0.44031273616297534\n",
      "Gradient Descent(21/999): loss=0.4391237569816263\n",
      "Gradient Descent(22/999): loss=0.43797725968570755\n",
      "Gradient Descent(23/999): loss=0.43687110911559857\n",
      "Gradient Descent(24/999): loss=0.4358032858780057\n",
      "Gradient Descent(25/999): loss=0.4347718799821655\n",
      "Gradient Descent(26/999): loss=0.43377508482679294\n",
      "Gradient Descent(27/999): loss=0.4328111915184234\n",
      "Gradient Descent(28/999): loss=0.4318785835028596\n",
      "Gradient Descent(29/999): loss=0.43097573149245133\n",
      "Gradient Descent(30/999): loss=0.4301011886728928\n",
      "Gradient Descent(31/999): loss=0.42925358617411685\n",
      "Gradient Descent(32/999): loss=0.4284316287907271\n",
      "Gradient Descent(33/999): loss=0.4276340909382027\n",
      "Gradient Descent(34/999): loss=0.42685981283187957\n",
      "Gradient Descent(35/999): loss=0.42610769687642247\n",
      "Gradient Descent(36/999): loss=0.4253767042541843\n",
      "Gradient Descent(37/999): loss=0.42466585170148724\n",
      "Gradient Descent(38/999): loss=0.4239742084624667\n",
      "Gradient Descent(39/999): loss=0.42330089341068766\n",
      "Gradient Descent(40/999): loss=0.4226450723292887\n",
      "Gradient Descent(41/999): loss=0.42200595534091023\n",
      "Gradient Descent(42/999): loss=0.4213827944791551\n",
      "Gradient Descent(43/999): loss=0.42077488139377617\n",
      "Gradient Descent(44/999): loss=0.4201815451822238\n",
      "Gradient Descent(45/999): loss=0.4196021503405844\n",
      "Gradient Descent(46/999): loss=0.4190360948273326\n",
      "Gradient Descent(47/999): loss=0.4184828082336778\n",
      "Gradient Descent(48/999): loss=0.41794175005462875\n",
      "Gradient Descent(49/999): loss=0.41741240805522917\n",
      "Gradient Descent(50/999): loss=0.41689429672671274\n",
      "Gradient Descent(51/999): loss=0.4163869558276283\n",
      "Gradient Descent(52/999): loss=0.4158899490052479\n",
      "Gradient Descent(53/999): loss=0.41540286249283515\n",
      "Gradient Descent(54/999): loss=0.41492530387859133\n",
      "Gradient Descent(55/999): loss=0.4144569009423325\n",
      "Gradient Descent(56/999): loss=0.4139973005561613\n",
      "Gradient Descent(57/999): loss=0.41354616764560825\n",
      "Gradient Descent(58/999): loss=0.41310318420791237\n",
      "Gradient Descent(59/999): loss=0.4126680483842875\n",
      "Gradient Descent(60/999): loss=0.41224047358320576\n",
      "Gradient Descent(61/999): loss=0.41182018765188144\n",
      "Gradient Descent(62/999): loss=0.4114069320933015\n",
      "Gradient Descent(63/999): loss=0.41100046132629436\n",
      "Gradient Descent(64/999): loss=0.4106005419862641\n",
      "Gradient Descent(65/999): loss=0.41020695226434856\n",
      "Gradient Descent(66/999): loss=0.4098194812828864\n",
      "Gradient Descent(67/999): loss=0.409437928505192\n",
      "Gradient Descent(68/999): loss=0.40906210317774677\n",
      "Gradient Descent(69/999): loss=0.4086918238030226\n",
      "Gradient Descent(70/999): loss=0.4083269176412503\n",
      "Gradient Descent(71/999): loss=0.4079672202395353\n",
      "Gradient Descent(72/999): loss=0.40761257498681847\n",
      "Gradient Descent(73/999): loss=0.40726283269325564\n",
      "Gradient Descent(74/999): loss=0.406917851192671\n",
      "Gradient Descent(75/999): loss=0.40657749496681506\n",
      "Gradient Descent(76/999): loss=0.4062416347902233\n",
      "Gradient Descent(77/999): loss=0.40591014739454523\n",
      "Gradient Descent(78/999): loss=0.4055829151512675\n",
      "Gradient Descent(79/999): loss=0.40525982577181896\n",
      "Gradient Descent(80/999): loss=0.4049407720241037\n",
      "Gradient Descent(81/999): loss=0.40462565146455315\n",
      "Gradient Descent(82/999): loss=0.40431436618484473\n",
      "Gradient Descent(83/999): loss=0.40400682257248033\n",
      "Gradient Descent(84/999): loss=0.4037029310844597\n",
      "Gradient Descent(85/999): loss=0.4034026060333301\n",
      "Gradient Descent(86/999): loss=0.40310576538492776\n",
      "Gradient Descent(87/999): loss=0.4028123305671719\n",
      "Gradient Descent(88/999): loss=0.40252222628929846\n",
      "Gradient Descent(89/999): loss=0.40223538037096235\n",
      "Gradient Descent(90/999): loss=0.401951723580664\n",
      "Gradient Descent(91/999): loss=0.4016711894829858\n",
      "Gradient Descent(92/999): loss=0.40139371429415716\n",
      "Gradient Descent(93/999): loss=0.4011192367454852\n",
      "Gradient Descent(94/999): loss=0.4008476979542232\n",
      "Gradient Descent(95/999): loss=0.40057904130146443\n",
      "Gradient Descent(96/999): loss=0.400313212316676\n",
      "Gradient Descent(97/999): loss=0.40005015856850773\n",
      "Gradient Descent(98/999): loss=0.3997898295615313\n",
      "Gradient Descent(99/999): loss=0.3995321766385824\n",
      "Gradient Descent(100/999): loss=0.3992771528883994\n",
      "Gradient Descent(101/999): loss=0.3990247130582663\n",
      "Gradient Descent(102/999): loss=0.39877481347138577\n",
      "Gradient Descent(103/999): loss=0.398527411948721\n",
      "Gradient Descent(104/999): loss=0.39828246773506304\n",
      "Gradient Descent(105/999): loss=0.3980399414290891\n",
      "Gradient Descent(106/999): loss=0.3977997949171949\n",
      "Gradient Descent(107/999): loss=0.3975619913108917\n",
      "Gradient Descent(108/999): loss=0.39732649488757393\n",
      "Gradient Descent(109/999): loss=0.3970932710344709\n",
      "Gradient Descent(110/999): loss=0.396862286195611\n",
      "Gradient Descent(111/999): loss=0.3966335078216271\n",
      "Gradient Descent(112/999): loss=0.39640690432225534\n",
      "Gradient Descent(113/999): loss=0.39618244502137157\n",
      "Gradient Descent(114/999): loss=0.3959601001144323\n",
      "Gradient Descent(115/999): loss=0.39573984062818524\n",
      "Gradient Descent(116/999): loss=0.3955216383825258\n",
      "Gradient Descent(117/999): loss=0.39530546595438265\n",
      "Gradient Descent(118/999): loss=0.3950912966435201\n",
      "Gradient Descent(119/999): loss=0.3948791044401531\n",
      "Gradient Descent(120/999): loss=0.39466886399427537\n",
      "Gradient Descent(121/999): loss=0.3944605505866072\n",
      "Gradient Descent(122/999): loss=0.39425414010107396\n",
      "Gradient Descent(123/999): loss=0.3940496089987318\n",
      "Gradient Descent(124/999): loss=0.3938469342930616\n",
      "Gradient Descent(125/999): loss=0.3936460935265561\n",
      "Gradient Descent(126/999): loss=0.3934470647485297\n",
      "Gradient Descent(127/999): loss=0.39324982649408496\n",
      "Gradient Descent(128/999): loss=0.3930543577641708\n",
      "Gradient Descent(129/999): loss=0.392860638006675\n",
      "Gradient Descent(130/999): loss=0.3926686470984934\n",
      "Gradient Descent(131/999): loss=0.3924783653285232\n",
      "Gradient Descent(132/999): loss=0.3922897733815292\n",
      "Gradient Descent(133/999): loss=0.3921028523228357\n",
      "Gradient Descent(134/999): loss=0.3919175835838015\n",
      "Gradient Descent(135/999): loss=0.391733948948031\n",
      "Gradient Descent(136/999): loss=0.3915519305382846\n",
      "Gradient Descent(137/999): loss=0.39137151080405175\n",
      "Gradient Descent(138/999): loss=0.39119267250974454\n",
      "Gradient Descent(139/999): loss=0.39101539872348756\n",
      "Gradient Descent(140/999): loss=0.3908396728064612\n",
      "Gradient Descent(141/999): loss=0.3906654784027781\n",
      "Gradient Descent(142/999): loss=0.39049279942985793\n",
      "Gradient Descent(143/999): loss=0.3903216200692745\n",
      "Gradient Descent(144/999): loss=0.39015192475805144\n",
      "Gradient Descent(145/999): loss=0.389983698180383\n",
      "Gradient Descent(146/999): loss=0.3898169252597513\n",
      "Gradient Descent(147/999): loss=0.3896515911514273\n",
      "Gradient Descent(148/999): loss=0.389487681235328\n",
      "Gradient Descent(149/999): loss=0.3893251811092136\n",
      "Gradient Descent(150/999): loss=0.3891640765822063\n",
      "Gradient Descent(151/999): loss=0.3890043536686132\n",
      "Gradient Descent(152/999): loss=0.38884599858203744\n",
      "Gradient Descent(153/999): loss=0.3886889977297611\n",
      "Gradient Descent(154/999): loss=0.38853333770738707\n",
      "Gradient Descent(155/999): loss=0.3883790052937244\n",
      "Gradient Descent(156/999): loss=0.38822598744590625\n",
      "Gradient Descent(157/999): loss=0.3880742712947254\n",
      "Gradient Descent(158/999): loss=0.3879238441401779\n",
      "Gradient Descent(159/999): loss=0.38777469344720444\n",
      "Gradient Descent(160/999): loss=0.3876268068416159\n",
      "Gradient Descent(161/999): loss=0.38748017210619634\n",
      "Gradient Descent(162/999): loss=0.38733477717697384\n",
      "Gradient Descent(163/999): loss=0.3871906101396484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(164/999): loss=0.38704765922617085\n",
      "Gradient Descent(165/999): loss=0.3869059128114636\n",
      "Gradient Descent(166/999): loss=0.386765359410277\n",
      "Gradient Descent(167/999): loss=0.38662598767417256\n",
      "Gradient Descent(168/999): loss=0.3864877863886289\n",
      "Gradient Descent(169/999): loss=0.386350744470261\n",
      "Gradient Descent(170/999): loss=0.38621485096415037\n",
      "Gradient Descent(171/999): loss=0.38608009504127805\n",
      "Gradient Descent(172/999): loss=0.38594646599605514\n",
      "Gradient Descent(173/999): loss=0.3858139532439485\n",
      "Gradient Descent(174/999): loss=0.3856825463191928\n",
      "Gradient Descent(175/999): loss=0.3855522348725884\n",
      "Gradient Descent(176/999): loss=0.3854230086693781\n",
      "Gradient Descent(177/999): loss=0.38529485758720106\n",
      "Gradient Descent(178/999): loss=0.38516777161411636\n",
      "Gradient Descent(179/999): loss=0.38504174084669796\n",
      "Gradient Descent(180/999): loss=0.38491675548819293\n",
      "Gradient Descent(181/999): loss=0.3847928058467415\n",
      "Gradient Descent(182/999): loss=0.384669882333658\n",
      "Gradient Descent(183/999): loss=0.38454797546176567\n",
      "Gradient Descent(184/999): loss=0.3844270758437854\n",
      "Gradient Descent(185/999): loss=0.3843071741907756\n",
      "Gradient Descent(186/999): loss=0.3841882613106208\n",
      "Gradient Descent(187/999): loss=0.38407032810656494\n",
      "Gradient Descent(188/999): loss=0.3839533655757897\n",
      "Gradient Descent(189/999): loss=0.3838373648080344\n",
      "Gradient Descent(190/999): loss=0.38372231698425485\n",
      "Gradient Descent(191/999): loss=0.3836082133753223\n",
      "Gradient Descent(192/999): loss=0.38349504534075574\n",
      "Gradient Descent(193/999): loss=0.3833828043274913\n",
      "Gradient Descent(194/999): loss=0.38327148186868365\n",
      "Gradient Descent(195/999): loss=0.38316106958253887\n",
      "Gradient Descent(196/999): loss=0.38305155917117817\n",
      "Gradient Descent(197/999): loss=0.38294294241952903\n",
      "Gradient Descent(198/999): loss=0.3828352111942456\n",
      "Gradient Descent(199/999): loss=0.38272835744265454\n",
      "Gradient Descent(200/999): loss=0.3826223731917264\n",
      "Gradient Descent(201/999): loss=0.38251725054707003\n",
      "Gradient Descent(202/999): loss=0.38241298169195276\n",
      "Gradient Descent(203/999): loss=0.38230955888633966\n",
      "Gradient Descent(204/999): loss=0.38220697446595653\n",
      "Gradient Descent(205/999): loss=0.3821052208413717\n",
      "Gradient Descent(206/999): loss=0.38200429049709855\n",
      "Gradient Descent(207/999): loss=0.38190417599071563\n",
      "Gradient Descent(208/999): loss=0.38180486995200646\n",
      "Gradient Descent(209/999): loss=0.38170636508211514\n",
      "Gradient Descent(210/999): loss=0.38160865415271916\n",
      "Gradient Descent(211/999): loss=0.3815117300052192\n",
      "Gradient Descent(212/999): loss=0.3814155855499428\n",
      "Gradient Descent(213/999): loss=0.381320213765364\n",
      "Gradient Descent(214/999): loss=0.38122560769733765\n",
      "Gradient Descent(215/999): loss=0.3811317604583461\n",
      "Gradient Descent(216/999): loss=0.3810386652267609\n",
      "Gradient Descent(217/999): loss=0.3809463152461159\n",
      "Gradient Descent(218/999): loss=0.38085470382439435\n",
      "Gradient Descent(219/999): loss=0.38076382433332684\n",
      "Gradient Descent(220/999): loss=0.38067367020770126\n",
      "Gradient Descent(221/999): loss=0.38058423494468385\n",
      "Gradient Descent(222/999): loss=0.38049551210315175\n",
      "Gradient Descent(223/999): loss=0.38040749530303547\n",
      "Gradient Descent(224/999): loss=0.38032017822467146\n",
      "Gradient Descent(225/999): loss=0.38023355460816527\n",
      "Gradient Descent(226/999): loss=0.38014761825276405\n",
      "Gradient Descent(227/999): loss=0.3800623630162377\n",
      "Gradient Descent(228/999): loss=0.37997778281427047\n",
      "Gradient Descent(229/999): loss=0.3798938716198599\n",
      "Gradient Descent(230/999): loss=0.37981062346272515\n",
      "Gradient Descent(231/999): loss=0.3797280324287235\n",
      "Gradient Descent(232/999): loss=0.3796460926592749\n",
      "Gradient Descent(233/999): loss=0.379564798350795\n",
      "Gradient Descent(234/999): loss=0.3794841437541337\n",
      "Gradient Descent(235/999): loss=0.3794041231740246\n",
      "Gradient Descent(236/999): loss=0.37932473096853886\n",
      "Gradient Descent(237/999): loss=0.3792459615485472\n",
      "Gradient Descent(238/999): loss=0.3791678093771894\n",
      "Gradient Descent(239/999): loss=0.3790902689693498\n",
      "Gradient Descent(240/999): loss=0.3790133348911394\n",
      "Gradient Descent(241/999): loss=0.37893700175938516\n",
      "Gradient Descent(242/999): loss=0.3788612642411248\n",
      "Gradient Descent(243/999): loss=0.3787861170531086\n",
      "Gradient Descent(244/999): loss=0.3787115549613063\n",
      "Gradient Descent(245/999): loss=0.37863757278042093\n",
      "Gradient Descent(246/999): loss=0.37856416537340787\n",
      "Gradient Descent(247/999): loss=0.3784913276509995\n",
      "Gradient Descent(248/999): loss=0.3784190545712356\n",
      "Gradient Descent(249/999): loss=0.37834734113900026\n",
      "Gradient Descent(250/999): loss=0.37827618240556116\n",
      "Gradient Descent(251/999): loss=0.3782055734681185\n",
      "Gradient Descent(252/999): loss=0.3781355094693545\n",
      "Gradient Descent(253/999): loss=0.37806598559699217\n",
      "Gradient Descent(254/999): loss=0.37799699708335516\n",
      "Gradient Descent(255/999): loss=0.37792853920493574\n",
      "Gradient Descent(256/999): loss=0.37786060728196613\n",
      "Gradient Descent(257/999): loss=0.377793196677994\n",
      "Gradient Descent(258/999): loss=0.37772630279946384\n",
      "Gradient Descent(259/999): loss=0.3776599210953025\n",
      "Gradient Descent(260/999): loss=0.3775940470565083\n",
      "Gradient Descent(261/999): loss=0.37752867621574654\n",
      "Gradient Descent(262/999): loss=0.3774638041469471\n",
      "Gradient Descent(263/999): loss=0.37739942646490815\n",
      "Gradient Descent(264/999): loss=0.3773355388249028\n",
      "Gradient Descent(265/999): loss=0.37727213692229117\n",
      "Gradient Descent(266/999): loss=0.3772092164921349\n",
      "Gradient Descent(267/999): loss=0.37714677330881763\n",
      "Gradient Descent(268/999): loss=0.3770848031856677\n",
      "Gradient Descent(269/999): loss=0.37702330197458644\n",
      "Gradient Descent(270/999): loss=0.376962265565678\n",
      "Gradient Descent(271/999): loss=0.3769016898868864\n",
      "Gradient Descent(272/999): loss=0.3768415709036326\n",
      "Gradient Descent(273/999): loss=0.37678190461845823\n",
      "Gradient Descent(274/999): loss=0.37672268707067114\n",
      "Gradient Descent(275/999): loss=0.3766639143359959\n",
      "Gradient Descent(276/999): loss=0.3766055825262266\n",
      "Gradient Descent(277/999): loss=0.37654768778888426\n",
      "Gradient Descent(278/999): loss=0.3764902263068769\n",
      "Gradient Descent(279/999): loss=0.37643319429816363\n",
      "Gradient Descent(280/999): loss=0.37637658801542223\n",
      "Gradient Descent(281/999): loss=0.37632040374571885\n",
      "Gradient Descent(282/999): loss=0.37626463781018277\n",
      "Gradient Descent(283/999): loss=0.3762092865636833\n",
      "Gradient Descent(284/999): loss=0.37615434639450973\n",
      "Gradient Descent(285/999): loss=0.3760998137240558\n",
      "Gradient Descent(286/999): loss=0.3760456850065058\n",
      "Gradient Descent(287/999): loss=0.37599195672852476\n",
      "Gradient Descent(288/999): loss=0.37593862540895123\n",
      "Gradient Descent(289/999): loss=0.37588568759849306\n",
      "Gradient Descent(290/999): loss=0.37583313987942685\n",
      "Gradient Descent(291/999): loss=0.3757809788652999\n",
      "Gradient Descent(292/999): loss=0.37572920120063424\n",
      "Gradient Descent(293/999): loss=0.3756778035606359\n",
      "Gradient Descent(294/999): loss=0.3756267826509043\n",
      "Gradient Descent(295/999): loss=0.37557613520714644\n",
      "Gradient Descent(296/999): loss=0.37552585799489313\n",
      "Gradient Descent(297/999): loss=0.3754759478092182\n",
      "Gradient Descent(298/999): loss=0.37542640147445944\n",
      "Gradient Descent(299/999): loss=0.37537721584394507\n",
      "Gradient Descent(300/999): loss=0.3753283877997192\n",
      "Gradient Descent(301/999): loss=0.37527991425227275\n",
      "Gradient Descent(302/999): loss=0.37523179214027585\n",
      "Gradient Descent(303/999): loss=0.37518401843031235\n",
      "Gradient Descent(304/999): loss=0.3751365901166183\n",
      "Gradient Descent(305/999): loss=0.37508950422082277\n",
      "Gradient Descent(306/999): loss=0.37504275779168894\n",
      "Gradient Descent(307/999): loss=0.374996347904861\n",
      "Gradient Descent(308/999): loss=0.3749502716626112\n",
      "Gradient Descent(309/999): loss=0.37490452619359\n",
      "Gradient Descent(310/999): loss=0.3748591086525793\n",
      "Gradient Descent(311/999): loss=0.37481401622024657\n",
      "Gradient Descent(312/999): loss=0.3747692461029032\n",
      "Gradient Descent(313/999): loss=0.3747247955322635\n",
      "Gradient Descent(314/999): loss=0.37468066176520703\n",
      "Gradient Descent(315/999): loss=0.3746368420835432\n",
      "Gradient Descent(316/999): loss=0.37459333379377735\n",
      "Gradient Descent(317/999): loss=0.3745501342268801\n",
      "Gradient Descent(318/999): loss=0.37450724073805797\n",
      "Gradient Descent(319/999): loss=0.37446465070652735\n",
      "Gradient Descent(320/999): loss=0.37442236153528935\n",
      "Gradient Descent(321/999): loss=0.37438037065090746\n",
      "Gradient Descent(322/999): loss=0.3743386755032878\n",
      "Gradient Descent(323/999): loss=0.37429727356546094\n",
      "Gradient Descent(324/999): loss=0.37425616233336495\n",
      "Gradient Descent(325/999): loss=0.3742153393256326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(326/999): loss=0.37417480208337905\n",
      "Gradient Descent(327/999): loss=0.3741345481699916\n",
      "Gradient Descent(328/999): loss=0.3740945751709227\n",
      "Gradient Descent(329/999): loss=0.3740548806934829\n",
      "Gradient Descent(330/999): loss=0.3740154623666381\n",
      "Gradient Descent(331/999): loss=0.37397631784080665\n",
      "Gradient Descent(332/999): loss=0.37393744478765994\n",
      "Gradient Descent(333/999): loss=0.3738988408999242\n",
      "Gradient Descent(334/999): loss=0.3738605038911842\n",
      "Gradient Descent(335/999): loss=0.3738224314956887\n",
      "Gradient Descent(336/999): loss=0.37378462146815833\n",
      "Gradient Descent(337/999): loss=0.373747071583595\n",
      "Gradient Descent(338/999): loss=0.3737097796370922\n",
      "Gradient Descent(339/999): loss=0.373672743443649\n",
      "Gradient Descent(340/999): loss=0.37363596083798484\n",
      "Gradient Descent(341/999): loss=0.3735994296743547\n",
      "Gradient Descent(342/999): loss=0.3735631478263694\n",
      "Gradient Descent(343/999): loss=0.3735271131868135\n",
      "Gradient Descent(344/999): loss=0.37349132366746873\n",
      "Gradient Descent(345/999): loss=0.3734557771989363\n",
      "Gradient Descent(346/999): loss=0.3734204717304625\n",
      "Gradient Descent(347/999): loss=0.3733854052297655\n",
      "Gradient Descent(348/999): loss=0.3733505756828632\n",
      "Gradient Descent(349/999): loss=0.37331598109390424\n",
      "Gradient Descent(350/999): loss=0.3732816194849984\n",
      "Gradient Descent(351/999): loss=0.3732474888960513\n",
      "Gradient Descent(352/999): loss=0.3732135873845979\n",
      "Gradient Descent(353/999): loss=0.3731799130256396\n",
      "Gradient Descent(354/999): loss=0.37314646391148193\n",
      "Gradient Descent(355/999): loss=0.37311323815157466\n",
      "Gradient Descent(356/999): loss=0.3730802338723521\n",
      "Gradient Descent(357/999): loss=0.37304744921707567\n",
      "Gradient Descent(358/999): loss=0.37301488234567837\n",
      "Gradient Descent(359/999): loss=0.37298253143461\n",
      "Gradient Descent(360/999): loss=0.3729503946766844\n",
      "Gradient Descent(361/999): loss=0.37291847028092723\n",
      "Gradient Descent(362/999): loss=0.37288675647242653\n",
      "Gradient Descent(363/999): loss=0.3728552514921839\n",
      "Gradient Descent(364/999): loss=0.3728239535969664\n",
      "Gradient Descent(365/999): loss=0.37279286105916193\n",
      "Gradient Descent(366/999): loss=0.3727619721666336\n",
      "Gradient Descent(367/999): loss=0.372731285222577\n",
      "Gradient Descent(368/999): loss=0.37270079854537874\n",
      "Gradient Descent(369/999): loss=0.37267051046847505\n",
      "Gradient Descent(370/999): loss=0.37264041934021347\n",
      "Gradient Descent(371/999): loss=0.37261052352371477\n",
      "Gradient Descent(372/999): loss=0.3725808213967365\n",
      "Gradient Descent(373/999): loss=0.3725513113515377\n",
      "Gradient Descent(374/999): loss=0.3725219917947443\n",
      "Gradient Descent(375/999): loss=0.372492861147218\n",
      "Gradient Descent(376/999): loss=0.37246391784392335\n",
      "Gradient Descent(377/999): loss=0.372435160333798\n",
      "Gradient Descent(378/999): loss=0.3724065870796244\n",
      "Gradient Descent(379/999): loss=0.3723781965579009\n",
      "Gradient Descent(380/999): loss=0.3723499872587162\n",
      "Gradient Descent(381/999): loss=0.3723219576856233\n",
      "Gradient Descent(382/999): loss=0.37229410635551624\n",
      "Gradient Descent(383/999): loss=0.372266431798506\n",
      "Gradient Descent(384/999): loss=0.37223893255779955\n",
      "Gradient Descent(385/999): loss=0.3722116071895785\n",
      "Gradient Descent(386/999): loss=0.3721844542628802\n",
      "Gradient Descent(387/999): loss=0.37215747235947894\n",
      "Gradient Descent(388/999): loss=0.372130660073769\n",
      "Gradient Descent(389/999): loss=0.3721040160126475\n",
      "Gradient Descent(390/999): loss=0.3720775387954004\n",
      "Gradient Descent(391/999): loss=0.3720512270535887\n",
      "Gradient Descent(392/999): loss=0.37202507943093377\n",
      "Gradient Descent(393/999): loss=0.37199909458320757\n",
      "Gradient Descent(394/999): loss=0.3719732711781202\n",
      "Gradient Descent(395/999): loss=0.3719476078952109\n",
      "Gradient Descent(396/999): loss=0.37192210342573956\n",
      "Gradient Descent(397/999): loss=0.3718967564725784\n",
      "Gradient Descent(398/999): loss=0.37187156575010516\n",
      "Gradient Descent(399/999): loss=0.3718465299840984\n",
      "Gradient Descent(400/999): loss=0.3718216479116314\n",
      "Gradient Descent(401/999): loss=0.3717969182809697\n",
      "Gradient Descent(402/999): loss=0.3717723398514673\n",
      "Gradient Descent(403/999): loss=0.3717479113934658\n",
      "Gradient Descent(404/999): loss=0.3717236316881928\n",
      "Gradient Descent(405/999): loss=0.3716994995276626\n",
      "Gradient Descent(406/999): loss=0.3716755137145767\n",
      "Gradient Descent(407/999): loss=0.3716516730622265\n",
      "Gradient Descent(408/999): loss=0.3716279763943956\n",
      "Gradient Descent(409/999): loss=0.37160442254526366\n",
      "Gradient Descent(410/999): loss=0.37158101035931157\n",
      "Gradient Descent(411/999): loss=0.3715577386912264\n",
      "Gradient Descent(412/999): loss=0.37153460640580854\n",
      "Gradient Descent(413/999): loss=0.37151161237787894\n",
      "Gradient Descent(414/999): loss=0.37148875549218674\n",
      "Gradient Descent(415/999): loss=0.3714660346433193\n",
      "Gradient Descent(416/999): loss=0.37144344873561136\n",
      "Gradient Descent(417/999): loss=0.37142099668305656\n",
      "Gradient Descent(418/999): loss=0.3713986774092184\n",
      "Gradient Descent(419/999): loss=0.37137648984714305\n",
      "Gradient Descent(420/999): loss=0.37135443293927284\n",
      "Gradient Descent(421/999): loss=0.3713325056373598\n",
      "Gradient Descent(422/999): loss=0.37131070690238105\n",
      "Gradient Descent(423/999): loss=0.3712890357044546\n",
      "Gradient Descent(424/999): loss=0.37126749102275447\n",
      "Gradient Descent(425/999): loss=0.3712460718454307\n",
      "Gradient Descent(426/999): loss=0.3712247771695241\n",
      "Gradient Descent(427/999): loss=0.3712036060008881\n",
      "Gradient Descent(428/999): loss=0.371182557354106\n",
      "Gradient Descent(429/999): loss=0.37116163025241244\n",
      "Gradient Descent(430/999): loss=0.3711408237276143\n",
      "Gradient Descent(431/999): loss=0.37112013682001266\n",
      "Gradient Descent(432/999): loss=0.3710995685783247\n",
      "Gradient Descent(433/999): loss=0.3710791180596076\n",
      "Gradient Descent(434/999): loss=0.3710587843291821\n",
      "Gradient Descent(435/999): loss=0.3710385664605576\n",
      "Gradient Descent(436/999): loss=0.3710184635353575\n",
      "Gradient Descent(437/999): loss=0.37099847464324465\n",
      "Gradient Descent(438/999): loss=0.3709785988818487\n",
      "Gradient Descent(439/999): loss=0.37095883535669355\n",
      "Gradient Descent(440/999): loss=0.3709391831811255\n",
      "Gradient Descent(441/999): loss=0.37091964147624173\n",
      "Gradient Descent(442/999): loss=0.37090020937082\n",
      "Gradient Descent(443/999): loss=0.37088088600124886\n",
      "Gradient Descent(444/999): loss=0.3708616705114581\n",
      "Gradient Descent(445/999): loss=0.3708425620528505\n",
      "Gradient Descent(446/999): loss=0.37082355978423404\n",
      "Gradient Descent(447/999): loss=0.3708046628717541\n",
      "Gradient Descent(448/999): loss=0.3707858704888271\n",
      "Gradient Descent(449/999): loss=0.37076718181607404\n",
      "Gradient Descent(450/999): loss=0.3707485960412562\n",
      "Gradient Descent(451/999): loss=0.3707301123592088\n",
      "Gradient Descent(452/999): loss=0.3707117299717776\n",
      "Gradient Descent(453/999): loss=0.3706934480877553\n",
      "Gradient Descent(454/999): loss=0.3706752659228185\n",
      "Gradient Descent(455/999): loss=0.3706571826994647\n",
      "Gradient Descent(456/999): loss=0.3706391976469512\n",
      "Gradient Descent(457/999): loss=0.37062131000123316\n",
      "Gradient Descent(458/999): loss=0.3706035190049034\n",
      "Gradient Descent(459/999): loss=0.3705858239071319\n",
      "Gradient Descent(460/999): loss=0.3705682239636063\n",
      "Gradient Descent(461/999): loss=0.3705507184364728\n",
      "Gradient Descent(462/999): loss=0.3705333065942779\n",
      "Gradient Descent(463/999): loss=0.3705159877119102\n",
      "Gradient Descent(464/999): loss=0.3704987610705429\n",
      "Gradient Descent(465/999): loss=0.370481625957577\n",
      "Gradient Descent(466/999): loss=0.3704645816665855\n",
      "Gradient Descent(467/999): loss=0.3704476274972567\n",
      "Gradient Descent(468/999): loss=0.37043076275533904\n",
      "Gradient Descent(469/999): loss=0.37041398675258713\n",
      "Gradient Descent(470/999): loss=0.3703972988067065\n",
      "Gradient Descent(471/999): loss=0.37038069824130027\n",
      "Gradient Descent(472/999): loss=0.37036418438581553\n",
      "Gradient Descent(473/999): loss=0.3703477565754913\n",
      "Gradient Descent(474/999): loss=0.370331414151305\n",
      "Gradient Descent(475/999): loss=0.37031515645992197\n",
      "Gradient Descent(476/999): loss=0.3702989828536433\n",
      "Gradient Descent(477/999): loss=0.37028289269035486\n",
      "Gradient Descent(478/999): loss=0.3702668853334777\n",
      "Gradient Descent(479/999): loss=0.3702509601519172\n",
      "Gradient Descent(480/999): loss=0.37023511652001423\n",
      "Gradient Descent(481/999): loss=0.37021935381749593\n",
      "Gradient Descent(482/999): loss=0.3702036714294274\n",
      "Gradient Descent(483/999): loss=0.37018806874616283\n",
      "Gradient Descent(484/999): loss=0.3701725451632991\n",
      "Gradient Descent(485/999): loss=0.3701571000816271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(486/999): loss=0.37014173290708646\n",
      "Gradient Descent(487/999): loss=0.3701264430507177\n",
      "Gradient Descent(488/999): loss=0.37011122992861756\n",
      "Gradient Descent(489/999): loss=0.3700960929618928\n",
      "Gradient Descent(490/999): loss=0.37008103157661537\n",
      "Gradient Descent(491/999): loss=0.3700660452037776\n",
      "Gradient Descent(492/999): loss=0.37005113327924855\n",
      "Gradient Descent(493/999): loss=0.37003629524372883\n",
      "Gradient Descent(494/999): loss=0.37002153054270864\n",
      "Gradient Descent(495/999): loss=0.3700068386264241\n",
      "Gradient Descent(496/999): loss=0.3699922189498144\n",
      "Gradient Descent(497/999): loss=0.3699776709724799\n",
      "Gradient Descent(498/999): loss=0.36996319415864004\n",
      "Gradient Descent(499/999): loss=0.3699487879770923\n",
      "Gradient Descent(500/999): loss=0.36993445190117064\n",
      "Gradient Descent(501/999): loss=0.36992018540870497\n",
      "Gradient Descent(502/999): loss=0.3699059879819809\n",
      "Gradient Descent(503/999): loss=0.3698918591076998\n",
      "Gradient Descent(504/999): loss=0.36987779827693884\n",
      "Gradient Descent(505/999): loss=0.3698638049851124\n",
      "Gradient Descent(506/999): loss=0.36984987873193287\n",
      "Gradient Descent(507/999): loss=0.36983601902137164\n",
      "Gradient Descent(508/999): loss=0.36982222536162207\n",
      "Gradient Descent(509/999): loss=0.36980849726506076\n",
      "Gradient Descent(510/999): loss=0.3697948342482105\n",
      "Gradient Descent(511/999): loss=0.36978123583170314\n",
      "Gradient Descent(512/999): loss=0.36976770154024263\n",
      "Gradient Descent(513/999): loss=0.3697542309025687\n",
      "Gradient Descent(514/999): loss=0.36974082345142106\n",
      "Gradient Descent(515/999): loss=0.3697274787235032\n",
      "Gradient Descent(516/999): loss=0.36971419625944724\n",
      "Gradient Descent(517/999): loss=0.3697009756037785\n",
      "Gradient Descent(518/999): loss=0.3696878163048813\n",
      "Gradient Descent(519/999): loss=0.36967471791496376\n",
      "Gradient Descent(520/999): loss=0.3696616799900246\n",
      "Gradient Descent(521/999): loss=0.36964870208981787\n",
      "Gradient Descent(522/999): loss=0.3696357837778213\n",
      "Gradient Descent(523/999): loss=0.36962292462120155\n",
      "Gradient Descent(524/999): loss=0.3696101241907816\n",
      "Gradient Descent(525/999): loss=0.3695973820610092\n",
      "Gradient Descent(526/999): loss=0.36958469780992304\n",
      "Gradient Descent(527/999): loss=0.36957207101912165\n",
      "Gradient Descent(528/999): loss=0.36955950127373155\n",
      "Gradient Descent(529/999): loss=0.36954698816237586\n",
      "Gradient Descent(530/999): loss=0.3695345312771429\n",
      "Gradient Descent(531/999): loss=0.36952213021355534\n",
      "Gradient Descent(532/999): loss=0.36950978457054023\n",
      "Gradient Descent(533/999): loss=0.369497493950398\n",
      "Gradient Descent(534/999): loss=0.3694852579587728\n",
      "Gradient Descent(535/999): loss=0.36947307620462244\n",
      "Gradient Descent(536/999): loss=0.3694609483001895\n",
      "Gradient Descent(537/999): loss=0.36944887386097175\n",
      "Gradient Descent(538/999): loss=0.3694368525056929\n",
      "Gradient Descent(539/999): loss=0.36942488385627487\n",
      "Gradient Descent(540/999): loss=0.3694129675378085\n",
      "Gradient Descent(541/999): loss=0.3694011031785257\n",
      "Gradient Descent(542/999): loss=0.36938929040977164\n",
      "Gradient Descent(543/999): loss=0.3693775288659774\n",
      "Gradient Descent(544/999): loss=0.3693658181846319\n",
      "Gradient Descent(545/999): loss=0.3693541580062552\n",
      "Gradient Descent(546/999): loss=0.3693425479743717\n",
      "Gradient Descent(547/999): loss=0.36933098773548334\n",
      "Gradient Descent(548/999): loss=0.3693194769390433\n",
      "Gradient Descent(549/999): loss=0.3693080152374298\n",
      "Gradient Descent(550/999): loss=0.36929660228592\n",
      "Gradient Descent(551/999): loss=0.3692852377426647\n",
      "Gradient Descent(552/999): loss=0.36927392126866293\n",
      "Gradient Descent(553/999): loss=0.3692626525277362\n",
      "Gradient Descent(554/999): loss=0.3692514311865038\n",
      "Gradient Descent(555/999): loss=0.3692402569143586\n",
      "Gradient Descent(556/999): loss=0.36922912938344143\n",
      "Gradient Descent(557/999): loss=0.36921804826861804\n",
      "Gradient Descent(558/999): loss=0.3692070132474539\n",
      "Gradient Descent(559/999): loss=0.3691960240001909\n",
      "Gradient Descent(560/999): loss=0.36918508020972374\n",
      "Gradient Descent(561/999): loss=0.3691741815615765\n",
      "Gradient Descent(562/999): loss=0.36916332774387894\n",
      "Gradient Descent(563/999): loss=0.36915251844734426\n",
      "Gradient Descent(564/999): loss=0.36914175336524574\n",
      "Gradient Descent(565/999): loss=0.3691310321933943\n",
      "Gradient Descent(566/999): loss=0.36912035463011655\n",
      "Gradient Descent(567/999): loss=0.36910972037623146\n",
      "Gradient Descent(568/999): loss=0.3690991291350302\n",
      "Gradient Descent(569/999): loss=0.36908858061225236\n",
      "Gradient Descent(570/999): loss=0.36907807451606606\n",
      "Gradient Descent(571/999): loss=0.36906761055704534\n",
      "Gradient Descent(572/999): loss=0.3690571884481498\n",
      "Gradient Descent(573/999): loss=0.3690468079047033\n",
      "Gradient Descent(574/999): loss=0.36903646864437295\n",
      "Gradient Descent(575/999): loss=0.3690261703871489\n",
      "Gradient Descent(576/999): loss=0.3690159128553235\n",
      "Gradient Descent(577/999): loss=0.36900569577347175\n",
      "Gradient Descent(578/999): loss=0.36899551886843024\n",
      "Gradient Descent(579/999): loss=0.36898538186927826\n",
      "Gradient Descent(580/999): loss=0.3689752845073172\n",
      "Gradient Descent(581/999): loss=0.36896522651605185\n",
      "Gradient Descent(582/999): loss=0.3689552076311706\n",
      "Gradient Descent(583/999): loss=0.3689452275905266\n",
      "Gradient Descent(584/999): loss=0.3689352861341182\n",
      "Gradient Descent(585/999): loss=0.36892538300407046\n",
      "Gradient Descent(586/999): loss=0.36891551794461724\n",
      "Gradient Descent(587/999): loss=0.3689056907020811\n",
      "Gradient Descent(588/999): loss=0.3688959010248566\n",
      "Gradient Descent(589/999): loss=0.3688861486633908\n",
      "Gradient Descent(590/999): loss=0.36887643337016696\n",
      "Gradient Descent(591/999): loss=0.3688667548996847\n",
      "Gradient Descent(592/999): loss=0.36885711300844404\n",
      "Gradient Descent(593/999): loss=0.36884750745492667\n",
      "Gradient Descent(594/999): loss=0.36883793799957976\n",
      "Gradient Descent(595/999): loss=0.3688284044047979\n",
      "Gradient Descent(596/999): loss=0.36881890643490606\n",
      "Gradient Descent(597/999): loss=0.3688094438561434\n",
      "Gradient Descent(598/999): loss=0.36880001643664656\n",
      "Gradient Descent(599/999): loss=0.3687906239464318\n",
      "Gradient Descent(600/999): loss=0.3687812661573802\n",
      "Gradient Descent(601/999): loss=0.3687719428432203\n",
      "Gradient Descent(602/999): loss=0.36876265377951245\n",
      "Gradient Descent(603/999): loss=0.36875339874363305\n",
      "Gradient Descent(604/999): loss=0.36874417751475785\n",
      "Gradient Descent(605/999): loss=0.3687349898738473\n",
      "Gradient Descent(606/999): loss=0.3687258356036304\n",
      "Gradient Descent(607/999): loss=0.36871671448858934\n",
      "Gradient Descent(608/999): loss=0.36870762631494425\n",
      "Gradient Descent(609/999): loss=0.36869857087063856\n",
      "Gradient Descent(610/999): loss=0.36868954794532266\n",
      "Gradient Descent(611/999): loss=0.36868055733034133\n",
      "Gradient Descent(612/999): loss=0.3686715988187162\n",
      "Gradient Descent(613/999): loss=0.3686626722051336\n",
      "Gradient Descent(614/999): loss=0.36865377728592874\n",
      "Gradient Descent(615/999): loss=0.3686449138590718\n",
      "Gradient Descent(616/999): loss=0.3686360817241534\n",
      "Gradient Descent(617/999): loss=0.3686272806823715\n",
      "Gradient Descent(618/999): loss=0.3686185105365158\n",
      "Gradient Descent(619/999): loss=0.36860977109095555\n",
      "Gradient Descent(620/999): loss=0.36860106215162497\n",
      "Gradient Descent(621/999): loss=0.3685923835260099\n",
      "Gradient Descent(622/999): loss=0.3685837350231344\n",
      "Gradient Descent(623/999): loss=0.3685751164535471\n",
      "Gradient Descent(624/999): loss=0.3685665276293083\n",
      "Gradient Descent(625/999): loss=0.36855796836397714\n",
      "Gradient Descent(626/999): loss=0.368549438472598\n",
      "Gradient Descent(627/999): loss=0.36854093777168784\n",
      "Gradient Descent(628/999): loss=0.3685324660792237\n",
      "Gradient Descent(629/999): loss=0.36852402321463007\n",
      "Gradient Descent(630/999): loss=0.3685156089987661\n",
      "Gradient Descent(631/999): loss=0.3685072232539132\n",
      "Gradient Descent(632/999): loss=0.36849886580376323\n",
      "Gradient Descent(633/999): loss=0.3684905364734056\n",
      "Gradient Descent(634/999): loss=0.36848223508931577\n",
      "Gradient Descent(635/999): loss=0.36847396147934286\n",
      "Gradient Descent(636/999): loss=0.36846571547269846\n",
      "Gradient Descent(637/999): loss=0.3684574968999441\n",
      "Gradient Descent(638/999): loss=0.36844930559297995\n",
      "Gradient Descent(639/999): loss=0.3684411413850335\n",
      "Gradient Descent(640/999): loss=0.3684330041106476\n",
      "Gradient Descent(641/999): loss=0.3684248936056696\n",
      "Gradient Descent(642/999): loss=0.36841680970724\n",
      "Gradient Descent(643/999): loss=0.36840875225378117\n",
      "Gradient Descent(644/999): loss=0.36840072108498645\n",
      "Gradient Descent(645/999): loss=0.3683927160418092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(646/999): loss=0.36838473696645174\n",
      "Gradient Descent(647/999): loss=0.3683767837023548\n",
      "Gradient Descent(648/999): loss=0.36836885609418757\n",
      "Gradient Descent(649/999): loss=0.36836095398783547\n",
      "Gradient Descent(650/999): loss=0.3683530772303914\n",
      "Gradient Descent(651/999): loss=0.36834522567014444\n",
      "Gradient Descent(652/999): loss=0.3683373991565697\n",
      "Gradient Descent(653/999): loss=0.3683295975403185\n",
      "Gradient Descent(654/999): loss=0.36832182067320807\n",
      "Gradient Descent(655/999): loss=0.3683140684082111\n",
      "Gradient Descent(656/999): loss=0.3683063405994469\n",
      "Gradient Descent(657/999): loss=0.36829863710217026\n",
      "Gradient Descent(658/999): loss=0.3682909577727631\n",
      "Gradient Descent(659/999): loss=0.3682833024687236\n",
      "Gradient Descent(660/999): loss=0.3682756710486576\n",
      "Gradient Descent(661/999): loss=0.36826806337226836\n",
      "Gradient Descent(662/999): loss=0.3682604793003478\n",
      "Gradient Descent(663/999): loss=0.3682529186947665\n",
      "Gradient Descent(664/999): loss=0.36824538141846536\n",
      "Gradient Descent(665/999): loss=0.36823786733544595\n",
      "Gradient Descent(666/999): loss=0.36823037631076116\n",
      "Gradient Descent(667/999): loss=0.3682229082105069\n",
      "Gradient Descent(668/999): loss=0.36821546290181284\n",
      "Gradient Descent(669/999): loss=0.36820804025283344\n",
      "Gradient Descent(670/999): loss=0.3682006401327394\n",
      "Gradient Descent(671/999): loss=0.3681932624117092\n",
      "Gradient Descent(672/999): loss=0.3681859069609201\n",
      "Gradient Descent(673/999): loss=0.36817857365254\n",
      "Gradient Descent(674/999): loss=0.3681712623597188\n",
      "Gradient Descent(675/999): loss=0.36816397295658\n",
      "Gradient Descent(676/999): loss=0.36815670531821276\n",
      "Gradient Descent(677/999): loss=0.3681494593206629\n",
      "Gradient Descent(678/999): loss=0.3681422348409258\n",
      "Gradient Descent(679/999): loss=0.3681350317569379\n",
      "Gradient Descent(680/999): loss=0.36812784994756825\n",
      "Gradient Descent(681/999): loss=0.36812068929261127\n",
      "Gradient Descent(682/999): loss=0.36811354967277826\n",
      "Gradient Descent(683/999): loss=0.36810643096969065\n",
      "Gradient Descent(684/999): loss=0.36809933306587084\n",
      "Gradient Descent(685/999): loss=0.3680922558447356\n",
      "Gradient Descent(686/999): loss=0.3680851991905886\n",
      "Gradient Descent(687/999): loss=0.36807816298861196\n",
      "Gradient Descent(688/999): loss=0.3680711471248598\n",
      "Gradient Descent(689/999): loss=0.3680641514862498\n",
      "Gradient Descent(690/999): loss=0.3680571759605575\n",
      "Gradient Descent(691/999): loss=0.36805022043640706\n",
      "Gradient Descent(692/999): loss=0.3680432848032656\n",
      "Gradient Descent(693/999): loss=0.3680363689514357\n",
      "Gradient Descent(694/999): loss=0.36802947277204784\n",
      "Gradient Descent(695/999): loss=0.36802259615705396\n",
      "Gradient Descent(696/999): loss=0.36801573899922024\n",
      "Gradient Descent(697/999): loss=0.3680089011921205\n",
      "Gradient Descent(698/999): loss=0.36800208263012907\n",
      "Gradient Descent(699/999): loss=0.36799528320841435\n",
      "Gradient Descent(700/999): loss=0.36798850282293183\n",
      "Gradient Descent(701/999): loss=0.3679817413704176\n",
      "Gradient Descent(702/999): loss=0.3679749987483819\n",
      "Gradient Descent(703/999): loss=0.36796827485510214\n",
      "Gradient Descent(704/999): loss=0.36796156958961707\n",
      "Gradient Descent(705/999): loss=0.36795488285172\n",
      "Gradient Descent(706/999): loss=0.36794821454195287\n",
      "Gradient Descent(707/999): loss=0.367941564561599\n",
      "Gradient Descent(708/999): loss=0.36793493281267786\n",
      "Gradient Descent(709/999): loss=0.3679283191979384\n",
      "Gradient Descent(710/999): loss=0.36792172362085324\n",
      "Gradient Descent(711/999): loss=0.3679151459856125\n",
      "Gradient Descent(712/999): loss=0.36790858619711714\n",
      "Gradient Descent(713/999): loss=0.36790204416097433\n",
      "Gradient Descent(714/999): loss=0.3678955197834901\n",
      "Gradient Descent(715/999): loss=0.3678890129716642\n",
      "Gradient Descent(716/999): loss=0.3678825236331849\n",
      "Gradient Descent(717/999): loss=0.3678760516764219\n",
      "Gradient Descent(718/999): loss=0.36786959701042127\n",
      "Gradient Descent(719/999): loss=0.3678631595449006\n",
      "Gradient Descent(720/999): loss=0.36785673919024175\n",
      "Gradient Descent(721/999): loss=0.3678503358574866\n",
      "Gradient Descent(722/999): loss=0.36784394945833093\n",
      "Gradient Descent(723/999): loss=0.36783757990511945\n",
      "Gradient Descent(724/999): loss=0.3678312271108395\n",
      "Gradient Descent(725/999): loss=0.36782489098911725\n",
      "Gradient Descent(726/999): loss=0.36781857145421\n",
      "Gradient Descent(727/999): loss=0.36781226842100356\n",
      "Gradient Descent(728/999): loss=0.3678059818050053\n",
      "Gradient Descent(729/999): loss=0.36779971152233937\n",
      "Gradient Descent(730/999): loss=0.3677934574897417\n",
      "Gradient Descent(731/999): loss=0.3677872196245552\n",
      "Gradient Descent(732/999): loss=0.3677809978447235\n",
      "Gradient Descent(733/999): loss=0.367774792068788\n",
      "Gradient Descent(734/999): loss=0.3677686022158807\n",
      "Gradient Descent(735/999): loss=0.3677624282057211\n",
      "Gradient Descent(736/999): loss=0.3677562699586103\n",
      "Gradient Descent(737/999): loss=0.3677501273954263\n",
      "Gradient Descent(738/999): loss=0.36774400043761934\n",
      "Gradient Descent(739/999): loss=0.3677378890072075\n",
      "Gradient Descent(740/999): loss=0.3677317930267713\n",
      "Gradient Descent(741/999): loss=0.36772571241945\n",
      "Gradient Descent(742/999): loss=0.3677196471089356\n",
      "Gradient Descent(743/999): loss=0.3677135970194697\n",
      "Gradient Descent(744/999): loss=0.36770756207583793\n",
      "Gradient Descent(745/999): loss=0.3677015422033663\n",
      "Gradient Descent(746/999): loss=0.3676955373279159\n",
      "Gradient Descent(747/999): loss=0.3676895473758786\n",
      "Gradient Descent(748/999): loss=0.3676835722741738\n",
      "Gradient Descent(749/999): loss=0.36767761195024273\n",
      "Gradient Descent(750/999): loss=0.36767166633204434\n",
      "Gradient Descent(751/999): loss=0.36766573534805197\n",
      "Gradient Descent(752/999): loss=0.36765981892724814\n",
      "Gradient Descent(753/999): loss=0.3676539169991206\n",
      "Gradient Descent(754/999): loss=0.3676480294936582\n",
      "Gradient Descent(755/999): loss=0.3676421563413476\n",
      "Gradient Descent(756/999): loss=0.3676362974731673\n",
      "Gradient Descent(757/999): loss=0.3676304528205856\n",
      "Gradient Descent(758/999): loss=0.3676246223155554\n",
      "Gradient Descent(759/999): loss=0.3676188058905104\n",
      "Gradient Descent(760/999): loss=0.3676130034783614\n",
      "Gradient Descent(761/999): loss=0.3676072150124924\n",
      "Gradient Descent(762/999): loss=0.36760144042675613\n",
      "Gradient Descent(763/999): loss=0.36759567965547113\n",
      "Gradient Descent(764/999): loss=0.3675899326334178\n",
      "Gradient Descent(765/999): loss=0.36758419929583336\n",
      "Gradient Descent(766/999): loss=0.3675784795784095\n",
      "Gradient Descent(767/999): loss=0.36757277341728845\n",
      "Gradient Descent(768/999): loss=0.367567080749059\n",
      "Gradient Descent(769/999): loss=0.3675614015107527\n",
      "Gradient Descent(770/999): loss=0.3675557356398404\n",
      "Gradient Descent(771/999): loss=0.3675500830742292\n",
      "Gradient Descent(772/999): loss=0.3675444437522581\n",
      "Gradient Descent(773/999): loss=0.3675388176126947\n",
      "Gradient Descent(774/999): loss=0.3675332045947321\n",
      "Gradient Descent(775/999): loss=0.3675276046379852\n",
      "Gradient Descent(776/999): loss=0.36752201768248677\n",
      "Gradient Descent(777/999): loss=0.367516443668685\n",
      "Gradient Descent(778/999): loss=0.3675108825374395\n",
      "Gradient Descent(779/999): loss=0.36750533423001813\n",
      "Gradient Descent(780/999): loss=0.3674997986880933\n",
      "Gradient Descent(781/999): loss=0.3674942758537394\n",
      "Gradient Descent(782/999): loss=0.36748876566942923\n",
      "Gradient Descent(783/999): loss=0.36748326807803033\n",
      "Gradient Descent(784/999): loss=0.3674777830228026\n",
      "Gradient Descent(785/999): loss=0.3674723104473945\n",
      "Gradient Descent(786/999): loss=0.36746685029584003\n",
      "Gradient Descent(787/999): loss=0.3674614025125557\n",
      "Gradient Descent(788/999): loss=0.3674559670423376\n",
      "Gradient Descent(789/999): loss=0.3674505438303581\n",
      "Gradient Descent(790/999): loss=0.3674451328221625\n",
      "Gradient Descent(791/999): loss=0.36743973396366686\n",
      "Gradient Descent(792/999): loss=0.36743434720115437\n",
      "Gradient Descent(793/999): loss=0.3674289724812725\n",
      "Gradient Descent(794/999): loss=0.3674236097510301\n",
      "Gradient Descent(795/999): loss=0.3674182589577947\n",
      "Gradient Descent(796/999): loss=0.36741292004928927\n",
      "Gradient Descent(797/999): loss=0.36740759297358977\n",
      "Gradient Descent(798/999): loss=0.36740227767912137\n",
      "Gradient Descent(799/999): loss=0.3673969741146573\n",
      "Gradient Descent(800/999): loss=0.36739168222931456\n",
      "Gradient Descent(801/999): loss=0.36738640197255157\n",
      "Gradient Descent(802/999): loss=0.36738113329416605\n",
      "Gradient Descent(803/999): loss=0.36737587614429146\n",
      "Gradient Descent(804/999): loss=0.3673706304733946\n",
      "Gradient Descent(805/999): loss=0.3673653962322734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(806/999): loss=0.3673601733720534\n",
      "Gradient Descent(807/999): loss=0.3673549618441861\n",
      "Gradient Descent(808/999): loss=0.36734976160044536\n",
      "Gradient Descent(809/999): loss=0.36734457259292586\n",
      "Gradient Descent(810/999): loss=0.3673393947740395\n",
      "Gradient Descent(811/999): loss=0.36733422809651384\n",
      "Gradient Descent(812/999): loss=0.3673290725133885\n",
      "Gradient Descent(813/999): loss=0.3673239279780141\n",
      "Gradient Descent(814/999): loss=0.3673187944440483\n",
      "Gradient Descent(815/999): loss=0.3673136718654545\n",
      "Gradient Descent(816/999): loss=0.3673085601964984\n",
      "Gradient Descent(817/999): loss=0.3673034593917468\n",
      "Gradient Descent(818/999): loss=0.3672983694060641\n",
      "Gradient Descent(819/999): loss=0.36729329019461054\n",
      "Gradient Descent(820/999): loss=0.3672882217128398\n",
      "Gradient Descent(821/999): loss=0.3672831639164961\n",
      "Gradient Descent(822/999): loss=0.3672781167616131\n",
      "Gradient Descent(823/999): loss=0.3672730802045103\n",
      "Gradient Descent(824/999): loss=0.3672680542017914\n",
      "Gradient Descent(825/999): loss=0.36726303871034255\n",
      "Gradient Descent(826/999): loss=0.3672580336873289\n",
      "Gradient Descent(827/999): loss=0.3672530390901934\n",
      "Gradient Descent(828/999): loss=0.3672480548766543\n",
      "Gradient Descent(829/999): loss=0.3672430810047028\n",
      "Gradient Descent(830/999): loss=0.3672381174326012\n",
      "Gradient Descent(831/999): loss=0.3672331641188805\n",
      "Gradient Descent(832/999): loss=0.36722822102233865\n",
      "Gradient Descent(833/999): loss=0.36722328810203775\n",
      "Gradient Descent(834/999): loss=0.3672183653173029\n",
      "Gradient Descent(835/999): loss=0.3672134526277194\n",
      "Gradient Descent(836/999): loss=0.3672085499931312\n",
      "Gradient Descent(837/999): loss=0.3672036573736381\n",
      "Gradient Descent(838/999): loss=0.36719877472959467\n",
      "Gradient Descent(839/999): loss=0.36719390202160807\n",
      "Gradient Descent(840/999): loss=0.36718903921053553\n",
      "Gradient Descent(841/999): loss=0.36718418625748267\n",
      "Gradient Descent(842/999): loss=0.36717934312380185\n",
      "Gradient Descent(843/999): loss=0.3671745097710902\n",
      "Gradient Descent(844/999): loss=0.3671696861611866\n",
      "Gradient Descent(845/999): loss=0.3671648722561716\n",
      "Gradient Descent(846/999): loss=0.3671600680183646\n",
      "Gradient Descent(847/999): loss=0.3671552734103215\n",
      "Gradient Descent(848/999): loss=0.36715048839483366\n",
      "Gradient Descent(849/999): loss=0.3671457129349259\n",
      "Gradient Descent(850/999): loss=0.3671409469938543\n",
      "Gradient Descent(851/999): loss=0.3671361905351052\n",
      "Gradient Descent(852/999): loss=0.3671314435223922\n",
      "Gradient Descent(853/999): loss=0.3671267059196556\n",
      "Gradient Descent(854/999): loss=0.3671219776910598\n",
      "Gradient Descent(855/999): loss=0.367117258800992\n",
      "Gradient Descent(856/999): loss=0.36711254921406056\n",
      "Gradient Descent(857/999): loss=0.36710784889509285\n",
      "Gradient Descent(858/999): loss=0.3671031578091336\n",
      "Gradient Descent(859/999): loss=0.36709847592144396\n",
      "Gradient Descent(860/999): loss=0.3670938031974988\n",
      "Gradient Descent(861/999): loss=0.36708913960298584\n",
      "Gradient Descent(862/999): loss=0.36708448510380337\n",
      "Gradient Descent(863/999): loss=0.36707983966605945\n",
      "Gradient Descent(864/999): loss=0.3670752032560694\n",
      "Gradient Descent(865/999): loss=0.3670705758403547\n",
      "Gradient Descent(866/999): loss=0.3670659573856414\n",
      "Gradient Descent(867/999): loss=0.36706134785885847\n",
      "Gradient Descent(868/999): loss=0.36705674722713616\n",
      "Gradient Descent(869/999): loss=0.3670521554578045\n",
      "Gradient Descent(870/999): loss=0.3670475725183918\n",
      "Gradient Descent(871/999): loss=0.3670429983766234\n",
      "Gradient Descent(872/999): loss=0.3670384330004195\n",
      "Gradient Descent(873/999): loss=0.36703387635789414\n",
      "Gradient Descent(874/999): loss=0.36702932841735403\n",
      "Gradient Descent(875/999): loss=0.36702478914729614\n",
      "Gradient Descent(876/999): loss=0.3670202585164069\n",
      "Gradient Descent(877/999): loss=0.367015736493561\n",
      "Gradient Descent(878/999): loss=0.3670112230478193\n",
      "Gradient Descent(879/999): loss=0.36700671814842756\n",
      "Gradient Descent(880/999): loss=0.36700222176481573\n",
      "Gradient Descent(881/999): loss=0.36699773386659545\n",
      "Gradient Descent(882/999): loss=0.36699325442355935\n",
      "Gradient Descent(883/999): loss=0.36698878340568\n",
      "Gradient Descent(884/999): loss=0.3669843207831073\n",
      "Gradient Descent(885/999): loss=0.36697986652616843\n",
      "Gradient Descent(886/999): loss=0.3669754206053658\n",
      "Gradient Descent(887/999): loss=0.36697098299137637\n",
      "Gradient Descent(888/999): loss=0.36696655365504943\n",
      "Gradient Descent(889/999): loss=0.36696213256740556\n",
      "Gradient Descent(890/999): loss=0.3669577196996361\n",
      "Gradient Descent(891/999): loss=0.36695331502310086\n",
      "Gradient Descent(892/999): loss=0.36694891850932765\n",
      "Gradient Descent(893/999): loss=0.36694453013001027\n",
      "Gradient Descent(894/999): loss=0.3669401498570078\n",
      "Gradient Descent(895/999): loss=0.36693577766234287\n",
      "Gradient Descent(896/999): loss=0.3669314135182017\n",
      "Gradient Descent(897/999): loss=0.3669270573969307\n",
      "Gradient Descent(898/999): loss=0.3669227092710371\n",
      "Gradient Descent(899/999): loss=0.36691836911318737\n",
      "Gradient Descent(900/999): loss=0.3669140368962052\n",
      "Gradient Descent(901/999): loss=0.3669097125930714\n",
      "Gradient Descent(902/999): loss=0.3669053961769219\n",
      "Gradient Descent(903/999): loss=0.36690108762104706\n",
      "Gradient Descent(904/999): loss=0.36689678689889055\n",
      "Gradient Descent(905/999): loss=0.36689249398404766\n",
      "Gradient Descent(906/999): loss=0.3668882088502648\n",
      "Gradient Descent(907/999): loss=0.36688393147143833\n",
      "Gradient Descent(908/999): loss=0.3668796618216126\n",
      "Gradient Descent(909/999): loss=0.3668753998749798\n",
      "Gradient Descent(910/999): loss=0.366871145605879\n",
      "Gradient Descent(911/999): loss=0.36686689898879415\n",
      "Gradient Descent(912/999): loss=0.36686265999835327\n",
      "Gradient Descent(913/999): loss=0.3668584286093279\n",
      "Gradient Descent(914/999): loss=0.36685420479663167\n",
      "Gradient Descent(915/999): loss=0.3668499885353192\n",
      "Gradient Descent(916/999): loss=0.36684577980058536\n",
      "Gradient Descent(917/999): loss=0.36684157856776356\n",
      "Gradient Descent(918/999): loss=0.3668373848123259\n",
      "Gradient Descent(919/999): loss=0.36683319850988044\n",
      "Gradient Descent(920/999): loss=0.3668290196361723\n",
      "Gradient Descent(921/999): loss=0.3668248481670806\n",
      "Gradient Descent(922/999): loss=0.3668206840786191\n",
      "Gradient Descent(923/999): loss=0.36681652734693415\n",
      "Gradient Descent(924/999): loss=0.36681237794830424\n",
      "Gradient Descent(925/999): loss=0.3668082358591387\n",
      "Gradient Descent(926/999): loss=0.36680410105597727\n",
      "Gradient Descent(927/999): loss=0.3667999735154883\n",
      "Gradient Descent(928/999): loss=0.3667958532144689\n",
      "Gradient Descent(929/999): loss=0.3667917401298433\n",
      "Gradient Descent(930/999): loss=0.36678763423866123\n",
      "Gradient Descent(931/999): loss=0.3667835355180989\n",
      "Gradient Descent(932/999): loss=0.3667794439454564\n",
      "Gradient Descent(933/999): loss=0.36677535949815765\n",
      "Gradient Descent(934/999): loss=0.3667712821537489\n",
      "Gradient Descent(935/999): loss=0.3667672118898984\n",
      "Gradient Descent(936/999): loss=0.3667631486843952\n",
      "Gradient Descent(937/999): loss=0.3667590925151485\n",
      "Gradient Descent(938/999): loss=0.36675504336018666\n",
      "Gradient Descent(939/999): loss=0.36675100119765586\n",
      "Gradient Descent(940/999): loss=0.3667469660058202\n",
      "Gradient Descent(941/999): loss=0.36674293776306044\n",
      "Gradient Descent(942/999): loss=0.3667389164478726\n",
      "Gradient Descent(943/999): loss=0.3667349020388678\n",
      "Gradient Descent(944/999): loss=0.36673089451477137\n",
      "Gradient Descent(945/999): loss=0.36672689385442153\n",
      "Gradient Descent(946/999): loss=0.36672290003676977\n",
      "Gradient Descent(947/999): loss=0.3667189130408781\n",
      "Gradient Descent(948/999): loss=0.36671493284592005\n",
      "Gradient Descent(949/999): loss=0.3667109594311788\n",
      "Gradient Descent(950/999): loss=0.3667069927760471\n",
      "Gradient Descent(951/999): loss=0.3667030328600263\n",
      "Gradient Descent(952/999): loss=0.3666990796627247\n",
      "Gradient Descent(953/999): loss=0.36669513316385827\n",
      "Gradient Descent(954/999): loss=0.36669119334324884\n",
      "Gradient Descent(955/999): loss=0.36668726018082365\n",
      "Gradient Descent(956/999): loss=0.36668333365661443\n",
      "Gradient Descent(957/999): loss=0.36667941375075686\n",
      "Gradient Descent(958/999): loss=0.3666755004434901\n",
      "Gradient Descent(959/999): loss=0.3666715937151553\n",
      "Gradient Descent(960/999): loss=0.3666676935461959\n",
      "Gradient Descent(961/999): loss=0.3666637999171554\n",
      "Gradient Descent(962/999): loss=0.36665991280867855\n",
      "Gradient Descent(963/999): loss=0.3666560322015089\n",
      "Gradient Descent(964/999): loss=0.36665215807648927\n",
      "Gradient Descent(965/999): loss=0.3666482904145606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(966/999): loss=0.3666444291967611\n",
      "Gradient Descent(967/999): loss=0.3666405744042259\n",
      "Gradient Descent(968/999): loss=0.3666367260181864\n",
      "Gradient Descent(969/999): loss=0.36663288401996885\n",
      "Gradient Descent(970/999): loss=0.366629048390995\n",
      "Gradient Descent(971/999): loss=0.36662521911277995\n",
      "Gradient Descent(972/999): loss=0.36662139616693284\n",
      "Gradient Descent(973/999): loss=0.3666175795351552\n",
      "Gradient Descent(974/999): loss=0.36661376919924094\n",
      "Gradient Descent(975/999): loss=0.366609965141075\n",
      "Gradient Descent(976/999): loss=0.3666061673426339\n",
      "Gradient Descent(977/999): loss=0.3666023757859838\n",
      "Gradient Descent(978/999): loss=0.36659859045328036\n",
      "Gradient Descent(979/999): loss=0.36659481132676863\n",
      "Gradient Descent(980/999): loss=0.36659103838878176\n",
      "Gradient Descent(981/999): loss=0.3665872716217407\n",
      "Gradient Descent(982/999): loss=0.36658351100815345\n",
      "Gradient Descent(983/999): loss=0.3665797565306145\n",
      "Gradient Descent(984/999): loss=0.3665760081718043\n",
      "Gradient Descent(985/999): loss=0.3665722659144887\n",
      "Gradient Descent(986/999): loss=0.3665685297415181\n",
      "Gradient Descent(987/999): loss=0.36656479963582744\n",
      "Gradient Descent(988/999): loss=0.3665610755804347\n",
      "Gradient Descent(989/999): loss=0.366557357558441\n",
      "Gradient Descent(990/999): loss=0.3665536455530303\n",
      "Gradient Descent(991/999): loss=0.3665499395474679\n",
      "Gradient Descent(992/999): loss=0.3665462395251008\n",
      "Gradient Descent(993/999): loss=0.36654254546935633\n",
      "Gradient Descent(994/999): loss=0.3665388573637422\n",
      "Gradient Descent(995/999): loss=0.36653517519184586\n",
      "Gradient Descent(996/999): loss=0.36653149893733394\n",
      "Gradient Descent(997/999): loss=0.36652782858395133\n",
      "Gradient Descent(998/999): loss=0.3665241641155211\n",
      "Gradient Descent(999/999): loss=0.36652050551594373\n"
     ]
    }
   ],
   "source": [
    "w, mse = least_squares_GD(y_train, tX_train, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7219155555555555"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict_labels(w, tX_train)\n",
    "np.mean(y_train.reshape(-1,1)==y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72184"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict_labels(w, tX_val)\n",
    "np.mean(y_val.reshape(-1,1)==y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Squares SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros(tX.shape[1])#np.random.normal(loc=0, scale=1, size=(tX.shape[1],1))\n",
    "max_iters = 5000\n",
    "gamma = 3e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/999): loss=0.4978445564319043\n",
      "Gradient Descent(1/999): loss=0.49123923762061833\n",
      "Gradient Descent(2/999): loss=0.4880131989135347\n",
      "Gradient Descent(3/999): loss=0.4845408339254324\n",
      "Gradient Descent(4/999): loss=0.4865696244391807\n",
      "Gradient Descent(5/999): loss=0.482804517193521\n",
      "Gradient Descent(6/999): loss=0.48129068967045385\n",
      "Gradient Descent(7/999): loss=0.4812130371856222\n",
      "Gradient Descent(8/999): loss=0.48232716417095733\n",
      "Gradient Descent(9/999): loss=0.4825403986654286\n",
      "Gradient Descent(10/999): loss=0.4812578237833007\n",
      "Gradient Descent(11/999): loss=0.4793890881185568\n",
      "Gradient Descent(12/999): loss=0.4778084057884684\n",
      "Gradient Descent(13/999): loss=0.4750920367974531\n",
      "Gradient Descent(14/999): loss=0.4758001275271161\n",
      "Gradient Descent(15/999): loss=0.47444480301903796\n",
      "Gradient Descent(16/999): loss=0.4738897010827102\n",
      "Gradient Descent(17/999): loss=0.47340607204806545\n",
      "Gradient Descent(18/999): loss=0.4733425343451806\n",
      "Gradient Descent(19/999): loss=0.47345163269696033\n",
      "Gradient Descent(20/999): loss=0.47290695759914236\n",
      "Gradient Descent(21/999): loss=0.47255634284234077\n",
      "Gradient Descent(22/999): loss=0.4719716198022061\n",
      "Gradient Descent(23/999): loss=0.4714204184225932\n",
      "Gradient Descent(24/999): loss=0.471133737702898\n",
      "Gradient Descent(25/999): loss=0.47082514080116494\n",
      "Gradient Descent(26/999): loss=0.4708862729771384\n",
      "Gradient Descent(27/999): loss=0.4699154639091567\n",
      "Gradient Descent(28/999): loss=0.4694480083326204\n",
      "Gradient Descent(29/999): loss=0.46923364155046915\n",
      "Gradient Descent(30/999): loss=0.46894434539500174\n",
      "Gradient Descent(31/999): loss=0.4687033191515611\n",
      "Gradient Descent(32/999): loss=0.4685554364048271\n",
      "Gradient Descent(33/999): loss=0.4684990575511305\n",
      "Gradient Descent(34/999): loss=0.46827952564946146\n",
      "Gradient Descent(35/999): loss=0.46806366971284163\n",
      "Gradient Descent(36/999): loss=0.4678247935046933\n",
      "Gradient Descent(37/999): loss=0.4676955296633225\n",
      "Gradient Descent(38/999): loss=0.4673323467572083\n",
      "Gradient Descent(39/999): loss=0.4673413635667016\n",
      "Gradient Descent(40/999): loss=0.46703945010947623\n",
      "Gradient Descent(41/999): loss=0.4668993116780129\n",
      "Gradient Descent(42/999): loss=0.4667721377574088\n",
      "Gradient Descent(43/999): loss=0.4671103762440999\n",
      "Gradient Descent(44/999): loss=0.4670332088446792\n",
      "Gradient Descent(45/999): loss=0.46706963097549453\n",
      "Gradient Descent(46/999): loss=0.467209739054033\n",
      "Gradient Descent(47/999): loss=0.467845815415532\n",
      "Gradient Descent(48/999): loss=0.4679255495139672\n",
      "Gradient Descent(49/999): loss=0.4667153379571502\n",
      "Gradient Descent(50/999): loss=0.46583517017619025\n",
      "Gradient Descent(51/999): loss=0.46580683239533527\n",
      "Gradient Descent(52/999): loss=0.465042910190008\n",
      "Gradient Descent(53/999): loss=0.4646431617351284\n",
      "Gradient Descent(54/999): loss=0.464400390801461\n",
      "Gradient Descent(55/999): loss=0.46388398089257965\n",
      "Gradient Descent(56/999): loss=0.463595213448514\n",
      "Gradient Descent(57/999): loss=0.46345012167225175\n",
      "Gradient Descent(58/999): loss=0.4634135372209672\n",
      "Gradient Descent(59/999): loss=0.4631052409886764\n",
      "Gradient Descent(60/999): loss=0.4629516016067651\n",
      "Gradient Descent(61/999): loss=0.46280607651124556\n",
      "Gradient Descent(62/999): loss=0.4626268126330189\n",
      "Gradient Descent(63/999): loss=0.4625989350854609\n",
      "Gradient Descent(64/999): loss=0.4630153065267691\n",
      "Gradient Descent(65/999): loss=0.46247141765399935\n",
      "Gradient Descent(66/999): loss=0.4630166176200995\n",
      "Gradient Descent(67/999): loss=0.4634337292784185\n",
      "Gradient Descent(68/999): loss=0.46287228938228053\n",
      "Gradient Descent(69/999): loss=0.463509404529731\n",
      "Gradient Descent(70/999): loss=0.46275490089194127\n",
      "Gradient Descent(71/999): loss=0.46148167905062787\n",
      "Gradient Descent(72/999): loss=0.4612809032646238\n",
      "Gradient Descent(73/999): loss=0.4620689467647663\n",
      "Gradient Descent(74/999): loss=0.46218452749872635\n",
      "Gradient Descent(75/999): loss=0.46161266265882095\n",
      "Gradient Descent(76/999): loss=0.4610398778249127\n",
      "Gradient Descent(77/999): loss=0.460086265750447\n",
      "Gradient Descent(78/999): loss=0.4595390663624854\n",
      "Gradient Descent(79/999): loss=0.4594019356816811\n",
      "Gradient Descent(80/999): loss=0.45903475667626636\n",
      "Gradient Descent(81/999): loss=0.45883639321286085\n",
      "Gradient Descent(82/999): loss=0.4586452958390403\n",
      "Gradient Descent(83/999): loss=0.45846325808816907\n",
      "Gradient Descent(84/999): loss=0.45847193312543305\n",
      "Gradient Descent(85/999): loss=0.4579347493866558\n",
      "Gradient Descent(86/999): loss=0.45779441538675075\n",
      "Gradient Descent(87/999): loss=0.457510051986463\n",
      "Gradient Descent(88/999): loss=0.45732451511576433\n",
      "Gradient Descent(89/999): loss=0.45708810510388587\n",
      "Gradient Descent(90/999): loss=0.4569665521709717\n",
      "Gradient Descent(91/999): loss=0.45739604830071134\n",
      "Gradient Descent(92/999): loss=0.4581912380321968\n",
      "Gradient Descent(93/999): loss=0.4570354885407032\n",
      "Gradient Descent(94/999): loss=0.45668886650583584\n",
      "Gradient Descent(95/999): loss=0.4563938440827609\n",
      "Gradient Descent(96/999): loss=0.45601771377051714\n",
      "Gradient Descent(97/999): loss=0.45586960008675786\n",
      "Gradient Descent(98/999): loss=0.4556586534721917\n",
      "Gradient Descent(99/999): loss=0.4554809931407441\n",
      "Gradient Descent(100/999): loss=0.4556603083142728\n",
      "Gradient Descent(101/999): loss=0.45526563224542144\n",
      "Gradient Descent(102/999): loss=0.4549794748995031\n",
      "Gradient Descent(103/999): loss=0.4548423036698165\n",
      "Gradient Descent(104/999): loss=0.4545023326071126\n",
      "Gradient Descent(105/999): loss=0.454241176085902\n",
      "Gradient Descent(106/999): loss=0.4540142623919829\n",
      "Gradient Descent(107/999): loss=0.45392780416832457\n",
      "Gradient Descent(108/999): loss=0.4541501596250837\n",
      "Gradient Descent(109/999): loss=0.45380420012299233\n",
      "Gradient Descent(110/999): loss=0.45394283719562767\n",
      "Gradient Descent(111/999): loss=0.45362313007144295\n",
      "Gradient Descent(112/999): loss=0.4532467940991028\n",
      "Gradient Descent(113/999): loss=0.4529245701031568\n",
      "Gradient Descent(114/999): loss=0.452868851326508\n",
      "Gradient Descent(115/999): loss=0.45252502441346476\n",
      "Gradient Descent(116/999): loss=0.4526079776374259\n",
      "Gradient Descent(117/999): loss=0.4526403737018846\n",
      "Gradient Descent(118/999): loss=0.4529290826678366\n",
      "Gradient Descent(119/999): loss=0.4525359544081379\n",
      "Gradient Descent(120/999): loss=0.4519654480256863\n",
      "Gradient Descent(121/999): loss=0.4526141828710783\n",
      "Gradient Descent(122/999): loss=0.4522581014872416\n",
      "Gradient Descent(123/999): loss=0.45194657992072335\n",
      "Gradient Descent(124/999): loss=0.45209727614294065\n",
      "Gradient Descent(125/999): loss=0.4514424974342194\n",
      "Gradient Descent(126/999): loss=0.4510565507442195\n",
      "Gradient Descent(127/999): loss=0.4508162350069529\n",
      "Gradient Descent(128/999): loss=0.4506508152037354\n",
      "Gradient Descent(129/999): loss=0.4502491072171185\n",
      "Gradient Descent(130/999): loss=0.45044232785890614\n",
      "Gradient Descent(131/999): loss=0.45088182048258\n",
      "Gradient Descent(132/999): loss=0.4506410710981693\n",
      "Gradient Descent(133/999): loss=0.4506606208850966\n",
      "Gradient Descent(134/999): loss=0.4509198479012757\n",
      "Gradient Descent(135/999): loss=0.4501949868793718\n",
      "Gradient Descent(136/999): loss=0.4503062932040171\n",
      "Gradient Descent(137/999): loss=0.44931911949099274\n",
      "Gradient Descent(138/999): loss=0.44933341036395263\n",
      "Gradient Descent(139/999): loss=0.44911479122019865\n",
      "Gradient Descent(140/999): loss=0.44915316691049084\n",
      "Gradient Descent(141/999): loss=0.44916300307114543\n",
      "Gradient Descent(142/999): loss=0.44891404127915013\n",
      "Gradient Descent(143/999): loss=0.4485504926370205\n",
      "Gradient Descent(144/999): loss=0.44824117225706345\n",
      "Gradient Descent(145/999): loss=0.44772205717774916\n",
      "Gradient Descent(146/999): loss=0.4476467971962212\n",
      "Gradient Descent(147/999): loss=0.44747082412870903\n",
      "Gradient Descent(148/999): loss=0.4472909881509993\n",
      "Gradient Descent(149/999): loss=0.44712445552167135\n",
      "Gradient Descent(150/999): loss=0.44715148361652707\n",
      "Gradient Descent(151/999): loss=0.44679092191149805\n",
      "Gradient Descent(152/999): loss=0.44674242469091396\n",
      "Gradient Descent(153/999): loss=0.44662811278646103\n",
      "Gradient Descent(154/999): loss=0.4464816089212643\n",
      "Gradient Descent(155/999): loss=0.4464985200792186\n",
      "Gradient Descent(156/999): loss=0.4462979609983535\n",
      "Gradient Descent(157/999): loss=0.4460108324712086\n",
      "Gradient Descent(158/999): loss=0.44589222982061033\n",
      "Gradient Descent(159/999): loss=0.44585812167399336\n",
      "Gradient Descent(160/999): loss=0.44577690077095544\n",
      "Gradient Descent(161/999): loss=0.44525334285762663\n",
      "Gradient Descent(162/999): loss=0.4451096009539626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(163/999): loss=0.44479296097341187\n",
      "Gradient Descent(164/999): loss=0.4450521803310013\n",
      "Gradient Descent(165/999): loss=0.44483408582899486\n",
      "Gradient Descent(166/999): loss=0.4450814962856125\n",
      "Gradient Descent(167/999): loss=0.4453018177058671\n",
      "Gradient Descent(168/999): loss=0.4447993223783432\n",
      "Gradient Descent(169/999): loss=0.44486231572298407\n",
      "Gradient Descent(170/999): loss=0.44440007150573485\n",
      "Gradient Descent(171/999): loss=0.44472924434548655\n",
      "Gradient Descent(172/999): loss=0.4447099156371695\n",
      "Gradient Descent(173/999): loss=0.4438578428253266\n",
      "Gradient Descent(174/999): loss=0.4435315866956234\n",
      "Gradient Descent(175/999): loss=0.44369885213643306\n",
      "Gradient Descent(176/999): loss=0.443308257692218\n",
      "Gradient Descent(177/999): loss=0.44300010268246703\n",
      "Gradient Descent(178/999): loss=0.44285566323550085\n",
      "Gradient Descent(179/999): loss=0.4426808287296232\n",
      "Gradient Descent(180/999): loss=0.44255065627128665\n",
      "Gradient Descent(181/999): loss=0.4427642856489929\n",
      "Gradient Descent(182/999): loss=0.44266579912400156\n",
      "Gradient Descent(183/999): loss=0.44263024051715516\n",
      "Gradient Descent(184/999): loss=0.44241387495734796\n",
      "Gradient Descent(185/999): loss=0.4423113133476265\n",
      "Gradient Descent(186/999): loss=0.44251802754213015\n",
      "Gradient Descent(187/999): loss=0.44234112268919556\n",
      "Gradient Descent(188/999): loss=0.4426730705766474\n",
      "Gradient Descent(189/999): loss=0.44214467030804694\n",
      "Gradient Descent(190/999): loss=0.4417414404397522\n",
      "Gradient Descent(191/999): loss=0.4413747003964426\n",
      "Gradient Descent(192/999): loss=0.441129522516274\n",
      "Gradient Descent(193/999): loss=0.4409920937538925\n",
      "Gradient Descent(194/999): loss=0.4407766033896517\n",
      "Gradient Descent(195/999): loss=0.44056780095741965\n",
      "Gradient Descent(196/999): loss=0.44065563063528396\n",
      "Gradient Descent(197/999): loss=0.4403559530449217\n",
      "Gradient Descent(198/999): loss=0.44015381124139696\n",
      "Gradient Descent(199/999): loss=0.4402733830494377\n",
      "Gradient Descent(200/999): loss=0.43997721334309353\n",
      "Gradient Descent(201/999): loss=0.43980714564120066\n",
      "Gradient Descent(202/999): loss=0.4397024557781551\n",
      "Gradient Descent(203/999): loss=0.4395182142717852\n",
      "Gradient Descent(204/999): loss=0.4393643506442928\n",
      "Gradient Descent(205/999): loss=0.43931995022864534\n",
      "Gradient Descent(206/999): loss=0.4393940795666321\n",
      "Gradient Descent(207/999): loss=0.43929686197701545\n",
      "Gradient Descent(208/999): loss=0.43905215833301603\n",
      "Gradient Descent(209/999): loss=0.43900708434030916\n",
      "Gradient Descent(210/999): loss=0.4392047481173299\n",
      "Gradient Descent(211/999): loss=0.43888237663226654\n",
      "Gradient Descent(212/999): loss=0.4386848692550268\n",
      "Gradient Descent(213/999): loss=0.438594320475972\n",
      "Gradient Descent(214/999): loss=0.4385314170203267\n",
      "Gradient Descent(215/999): loss=0.43844142349858445\n",
      "Gradient Descent(216/999): loss=0.43849472912463455\n",
      "Gradient Descent(217/999): loss=0.4385080700255276\n",
      "Gradient Descent(218/999): loss=0.4381307869688676\n",
      "Gradient Descent(219/999): loss=0.4381095438890168\n",
      "Gradient Descent(220/999): loss=0.43809793490435295\n",
      "Gradient Descent(221/999): loss=0.4379574098941761\n",
      "Gradient Descent(222/999): loss=0.4377348141013609\n",
      "Gradient Descent(223/999): loss=0.43756408817481746\n",
      "Gradient Descent(224/999): loss=0.437631802732865\n",
      "Gradient Descent(225/999): loss=0.4373723677813838\n",
      "Gradient Descent(226/999): loss=0.43725287065684576\n",
      "Gradient Descent(227/999): loss=0.4371373336088593\n",
      "Gradient Descent(228/999): loss=0.43714827689065894\n",
      "Gradient Descent(229/999): loss=0.43700318517304837\n",
      "Gradient Descent(230/999): loss=0.43689455208776334\n",
      "Gradient Descent(231/999): loss=0.4368562236432135\n",
      "Gradient Descent(232/999): loss=0.43681392042178546\n",
      "Gradient Descent(233/999): loss=0.43701928127441664\n",
      "Gradient Descent(234/999): loss=0.4367993071060096\n",
      "Gradient Descent(235/999): loss=0.4365522500356253\n",
      "Gradient Descent(236/999): loss=0.43639240435729815\n",
      "Gradient Descent(237/999): loss=0.4363324777623843\n",
      "Gradient Descent(238/999): loss=0.43620510980155586\n",
      "Gradient Descent(239/999): loss=0.43606358032621945\n",
      "Gradient Descent(240/999): loss=0.4363140262458733\n",
      "Gradient Descent(241/999): loss=0.4357954077345989\n",
      "Gradient Descent(242/999): loss=0.435629597580029\n",
      "Gradient Descent(243/999): loss=0.43566753438969785\n",
      "Gradient Descent(244/999): loss=0.43536973683778685\n",
      "Gradient Descent(245/999): loss=0.43523598982060013\n",
      "Gradient Descent(246/999): loss=0.4350755162150398\n",
      "Gradient Descent(247/999): loss=0.4349478862323907\n",
      "Gradient Descent(248/999): loss=0.43481415482772884\n",
      "Gradient Descent(249/999): loss=0.43473972141994344\n",
      "Gradient Descent(250/999): loss=0.43476729913148693\n",
      "Gradient Descent(251/999): loss=0.4346036274176066\n",
      "Gradient Descent(252/999): loss=0.4344749221892148\n",
      "Gradient Descent(253/999): loss=0.4344120614735768\n",
      "Gradient Descent(254/999): loss=0.4342958590828063\n",
      "Gradient Descent(255/999): loss=0.43421980388006093\n",
      "Gradient Descent(256/999): loss=0.43406022647013165\n",
      "Gradient Descent(257/999): loss=0.43397632157474575\n",
      "Gradient Descent(258/999): loss=0.4339369176533613\n",
      "Gradient Descent(259/999): loss=0.43379423156294317\n",
      "Gradient Descent(260/999): loss=0.4335884357481559\n",
      "Gradient Descent(261/999): loss=0.43357622441988114\n",
      "Gradient Descent(262/999): loss=0.4336353139255977\n",
      "Gradient Descent(263/999): loss=0.43363554567741236\n",
      "Gradient Descent(264/999): loss=0.433558458356916\n",
      "Gradient Descent(265/999): loss=0.43377290584750255\n",
      "Gradient Descent(266/999): loss=0.43310405680933395\n",
      "Gradient Descent(267/999): loss=0.43301839424885674\n",
      "Gradient Descent(268/999): loss=0.43293222678910737\n",
      "Gradient Descent(269/999): loss=0.43282186130731337\n",
      "Gradient Descent(270/999): loss=0.43278988485960385\n",
      "Gradient Descent(271/999): loss=0.4326329746545167\n",
      "Gradient Descent(272/999): loss=0.4326077829777607\n",
      "Gradient Descent(273/999): loss=0.4325147821531183\n",
      "Gradient Descent(274/999): loss=0.43246559470840434\n",
      "Gradient Descent(275/999): loss=0.43254312920609667\n",
      "Gradient Descent(276/999): loss=0.43258566349026684\n",
      "Gradient Descent(277/999): loss=0.4324347538494128\n",
      "Gradient Descent(278/999): loss=0.4321383465806456\n",
      "Gradient Descent(279/999): loss=0.4320129288884941\n",
      "Gradient Descent(280/999): loss=0.4318901985625639\n",
      "Gradient Descent(281/999): loss=0.43182149046090573\n",
      "Gradient Descent(282/999): loss=0.431729368177451\n",
      "Gradient Descent(283/999): loss=0.43165537053204206\n",
      "Gradient Descent(284/999): loss=0.43152130691409946\n",
      "Gradient Descent(285/999): loss=0.43140108311937697\n",
      "Gradient Descent(286/999): loss=0.4313729632002175\n",
      "Gradient Descent(287/999): loss=0.4314519520220319\n",
      "Gradient Descent(288/999): loss=0.4312058999283623\n",
      "Gradient Descent(289/999): loss=0.43116643520320097\n",
      "Gradient Descent(290/999): loss=0.43101836377878305\n",
      "Gradient Descent(291/999): loss=0.43089239895434717\n",
      "Gradient Descent(292/999): loss=0.4307767862630283\n",
      "Gradient Descent(293/999): loss=0.43091732616670847\n",
      "Gradient Descent(294/999): loss=0.43086086517114847\n",
      "Gradient Descent(295/999): loss=0.43049949762183215\n",
      "Gradient Descent(296/999): loss=0.4302864616468542\n",
      "Gradient Descent(297/999): loss=0.43013970945207297\n",
      "Gradient Descent(298/999): loss=0.43007662503701743\n",
      "Gradient Descent(299/999): loss=0.4300095088342779\n",
      "Gradient Descent(300/999): loss=0.4300008633555352\n",
      "Gradient Descent(301/999): loss=0.4301733028016119\n",
      "Gradient Descent(302/999): loss=0.42985080327824454\n",
      "Gradient Descent(303/999): loss=0.42981802332978153\n",
      "Gradient Descent(304/999): loss=0.42967079515542633\n",
      "Gradient Descent(305/999): loss=0.42938212302279855\n",
      "Gradient Descent(306/999): loss=0.4292457762586784\n",
      "Gradient Descent(307/999): loss=0.4292152740752582\n",
      "Gradient Descent(308/999): loss=0.4295760039638427\n",
      "Gradient Descent(309/999): loss=0.429365384075156\n",
      "Gradient Descent(310/999): loss=0.4294932091729923\n",
      "Gradient Descent(311/999): loss=0.42913587664078645\n",
      "Gradient Descent(312/999): loss=0.4299202123752144\n",
      "Gradient Descent(313/999): loss=0.42952503374196616\n",
      "Gradient Descent(314/999): loss=0.4296101742900926\n",
      "Gradient Descent(315/999): loss=0.4291859259634236\n",
      "Gradient Descent(316/999): loss=0.4285056381987604\n",
      "Gradient Descent(317/999): loss=0.42870390234846795\n",
      "Gradient Descent(318/999): loss=0.42903848615249923\n",
      "Gradient Descent(319/999): loss=0.42919009763175925\n",
      "Gradient Descent(320/999): loss=0.4286442296581669\n",
      "Gradient Descent(321/999): loss=0.4283041944938039\n",
      "Gradient Descent(322/999): loss=0.42865123036348574\n",
      "Gradient Descent(323/999): loss=0.42823304712504956\n",
      "Gradient Descent(324/999): loss=0.4280354226128468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(325/999): loss=0.4278609897521303\n",
      "Gradient Descent(326/999): loss=0.42779406528120495\n",
      "Gradient Descent(327/999): loss=0.4276754421324031\n",
      "Gradient Descent(328/999): loss=0.4276411017242782\n",
      "Gradient Descent(329/999): loss=0.4276320191109741\n",
      "Gradient Descent(330/999): loss=0.42761395913962785\n",
      "Gradient Descent(331/999): loss=0.42745753284331794\n",
      "Gradient Descent(332/999): loss=0.42774808290928823\n",
      "Gradient Descent(333/999): loss=0.427788650592864\n",
      "Gradient Descent(334/999): loss=0.4271120367898014\n",
      "Gradient Descent(335/999): loss=0.4270699318739413\n",
      "Gradient Descent(336/999): loss=0.427236418410412\n",
      "Gradient Descent(337/999): loss=0.4267612694181999\n",
      "Gradient Descent(338/999): loss=0.426704476657892\n",
      "Gradient Descent(339/999): loss=0.42671167884985506\n",
      "Gradient Descent(340/999): loss=0.42656256350692234\n",
      "Gradient Descent(341/999): loss=0.4266029614300968\n",
      "Gradient Descent(342/999): loss=0.4263973210817566\n",
      "Gradient Descent(343/999): loss=0.4263075124421799\n",
      "Gradient Descent(344/999): loss=0.426219492213541\n",
      "Gradient Descent(345/999): loss=0.4261481904398483\n",
      "Gradient Descent(346/999): loss=0.42600518884648986\n",
      "Gradient Descent(347/999): loss=0.4260019060130009\n",
      "Gradient Descent(348/999): loss=0.42591753460037657\n",
      "Gradient Descent(349/999): loss=0.42576730026387755\n",
      "Gradient Descent(350/999): loss=0.42572894047747295\n",
      "Gradient Descent(351/999): loss=0.4256602420897141\n",
      "Gradient Descent(352/999): loss=0.4256610537617265\n",
      "Gradient Descent(353/999): loss=0.42559358788160795\n",
      "Gradient Descent(354/999): loss=0.42585297258889954\n",
      "Gradient Descent(355/999): loss=0.4258345170743291\n",
      "Gradient Descent(356/999): loss=0.42576560625815174\n",
      "Gradient Descent(357/999): loss=0.42542446299525494\n",
      "Gradient Descent(358/999): loss=0.42589278732229074\n",
      "Gradient Descent(359/999): loss=0.425376274245315\n",
      "Gradient Descent(360/999): loss=0.42549102180087406\n",
      "Gradient Descent(361/999): loss=0.425668628516891\n",
      "Gradient Descent(362/999): loss=0.4259294226711321\n",
      "Gradient Descent(363/999): loss=0.4257837668393762\n",
      "Gradient Descent(364/999): loss=0.4258114300586656\n",
      "Gradient Descent(365/999): loss=0.4269079255663003\n",
      "Gradient Descent(366/999): loss=0.4263113790521518\n",
      "Gradient Descent(367/999): loss=0.4261229270810943\n",
      "Gradient Descent(368/999): loss=0.4265458891617961\n",
      "Gradient Descent(369/999): loss=0.4257864109400308\n",
      "Gradient Descent(370/999): loss=0.4249673779145391\n",
      "Gradient Descent(371/999): loss=0.4252257491642855\n",
      "Gradient Descent(372/999): loss=0.42477578017392487\n",
      "Gradient Descent(373/999): loss=0.4246044435733834\n",
      "Gradient Descent(374/999): loss=0.42516616100869076\n",
      "Gradient Descent(375/999): loss=0.4246935743411406\n",
      "Gradient Descent(376/999): loss=0.42467059554181524\n",
      "Gradient Descent(377/999): loss=0.4240624964521547\n",
      "Gradient Descent(378/999): loss=0.4238770930436347\n",
      "Gradient Descent(379/999): loss=0.42393512988694565\n",
      "Gradient Descent(380/999): loss=0.4238459694788167\n",
      "Gradient Descent(381/999): loss=0.42429339025289936\n",
      "Gradient Descent(382/999): loss=0.4242194047285764\n",
      "Gradient Descent(383/999): loss=0.4238408495218758\n",
      "Gradient Descent(384/999): loss=0.42435248027507094\n",
      "Gradient Descent(385/999): loss=0.42384249166168647\n",
      "Gradient Descent(386/999): loss=0.42394794820834314\n",
      "Gradient Descent(387/999): loss=0.4236121590845174\n",
      "Gradient Descent(388/999): loss=0.4235573200825438\n",
      "Gradient Descent(389/999): loss=0.42328673301539377\n",
      "Gradient Descent(390/999): loss=0.4230990591897362\n",
      "Gradient Descent(391/999): loss=0.4230047867531486\n",
      "Gradient Descent(392/999): loss=0.4229247024830935\n",
      "Gradient Descent(393/999): loss=0.4228642811234382\n",
      "Gradient Descent(394/999): loss=0.42291299310384456\n",
      "Gradient Descent(395/999): loss=0.42299393927056583\n",
      "Gradient Descent(396/999): loss=0.4228578145524541\n",
      "Gradient Descent(397/999): loss=0.42276510272853113\n",
      "Gradient Descent(398/999): loss=0.4225867288889931\n",
      "Gradient Descent(399/999): loss=0.4224944317291868\n",
      "Gradient Descent(400/999): loss=0.4224050321380583\n",
      "Gradient Descent(401/999): loss=0.4224284321001304\n",
      "Gradient Descent(402/999): loss=0.4223441754061196\n",
      "Gradient Descent(403/999): loss=0.42281036456577126\n",
      "Gradient Descent(404/999): loss=0.4225993858231266\n",
      "Gradient Descent(405/999): loss=0.42198860749769235\n",
      "Gradient Descent(406/999): loss=0.42187601461204377\n",
      "Gradient Descent(407/999): loss=0.4220632637435495\n",
      "Gradient Descent(408/999): loss=0.4218176218291626\n",
      "Gradient Descent(409/999): loss=0.4217677170550439\n",
      "Gradient Descent(410/999): loss=0.42159156797768577\n",
      "Gradient Descent(411/999): loss=0.42156502673390095\n",
      "Gradient Descent(412/999): loss=0.42150577781666043\n",
      "Gradient Descent(413/999): loss=0.4216503207524744\n",
      "Gradient Descent(414/999): loss=0.4216848641123594\n",
      "Gradient Descent(415/999): loss=0.42139227372916066\n",
      "Gradient Descent(416/999): loss=0.42126808250130227\n",
      "Gradient Descent(417/999): loss=0.42129882468507124\n",
      "Gradient Descent(418/999): loss=0.42112085559108187\n",
      "Gradient Descent(419/999): loss=0.42105647739254487\n",
      "Gradient Descent(420/999): loss=0.4210947031339948\n",
      "Gradient Descent(421/999): loss=0.4210516907607613\n",
      "Gradient Descent(422/999): loss=0.420893803800255\n",
      "Gradient Descent(423/999): loss=0.4208433533526907\n",
      "Gradient Descent(424/999): loss=0.42078211175445424\n",
      "Gradient Descent(425/999): loss=0.42074581745788525\n",
      "Gradient Descent(426/999): loss=0.4207257088647195\n",
      "Gradient Descent(427/999): loss=0.4209320522698444\n",
      "Gradient Descent(428/999): loss=0.42068832093650027\n",
      "Gradient Descent(429/999): loss=0.4205682923969189\n",
      "Gradient Descent(430/999): loss=0.4204822717544225\n",
      "Gradient Descent(431/999): loss=0.4206375784808433\n",
      "Gradient Descent(432/999): loss=0.42045606922498047\n",
      "Gradient Descent(433/999): loss=0.4204469742853985\n",
      "Gradient Descent(434/999): loss=0.42039109340243086\n",
      "Gradient Descent(435/999): loss=0.4203146057624375\n",
      "Gradient Descent(436/999): loss=0.42024294757534014\n",
      "Gradient Descent(437/999): loss=0.4202316246327125\n",
      "Gradient Descent(438/999): loss=0.42079157457281635\n",
      "Gradient Descent(439/999): loss=0.42030111566065526\n",
      "Gradient Descent(440/999): loss=0.4199821240073192\n",
      "Gradient Descent(441/999): loss=0.41991195282056976\n",
      "Gradient Descent(442/999): loss=0.41983747877532707\n",
      "Gradient Descent(443/999): loss=0.41977096845621537\n",
      "Gradient Descent(444/999): loss=0.41981008931433894\n",
      "Gradient Descent(445/999): loss=0.41973039422914754\n",
      "Gradient Descent(446/999): loss=0.4198992628442307\n",
      "Gradient Descent(447/999): loss=0.41982144551213635\n",
      "Gradient Descent(448/999): loss=0.42006797507571425\n",
      "Gradient Descent(449/999): loss=0.4197378752338443\n",
      "Gradient Descent(450/999): loss=0.41954023633755666\n",
      "Gradient Descent(451/999): loss=0.4196044916650453\n",
      "Gradient Descent(452/999): loss=0.4196478774936416\n",
      "Gradient Descent(453/999): loss=0.4194724420004253\n",
      "Gradient Descent(454/999): loss=0.4196045480285701\n",
      "Gradient Descent(455/999): loss=0.41938700106206367\n",
      "Gradient Descent(456/999): loss=0.4191972059302367\n",
      "Gradient Descent(457/999): loss=0.4191007968160574\n",
      "Gradient Descent(458/999): loss=0.41943347235788986\n",
      "Gradient Descent(459/999): loss=0.4192661338575193\n",
      "Gradient Descent(460/999): loss=0.41912122677057057\n",
      "Gradient Descent(461/999): loss=0.41948297433344856\n",
      "Gradient Descent(462/999): loss=0.4196535621173229\n",
      "Gradient Descent(463/999): loss=0.4188929761760039\n",
      "Gradient Descent(464/999): loss=0.4187617511580127\n",
      "Gradient Descent(465/999): loss=0.4186718769739174\n",
      "Gradient Descent(466/999): loss=0.4186095442252443\n",
      "Gradient Descent(467/999): loss=0.41869898156529317\n",
      "Gradient Descent(468/999): loss=0.4187167415178653\n",
      "Gradient Descent(469/999): loss=0.41852301335859543\n",
      "Gradient Descent(470/999): loss=0.41848015678929157\n",
      "Gradient Descent(471/999): loss=0.4184395653900498\n",
      "Gradient Descent(472/999): loss=0.4185774702554404\n",
      "Gradient Descent(473/999): loss=0.41865708929355067\n",
      "Gradient Descent(474/999): loss=0.4182584727213859\n",
      "Gradient Descent(475/999): loss=0.4182685259224025\n",
      "Gradient Descent(476/999): loss=0.4182410557352788\n",
      "Gradient Descent(477/999): loss=0.41815539226613224\n",
      "Gradient Descent(478/999): loss=0.4181207626115649\n",
      "Gradient Descent(479/999): loss=0.41800743328412215\n",
      "Gradient Descent(480/999): loss=0.4180665085619035\n",
      "Gradient Descent(481/999): loss=0.41814827739910587\n",
      "Gradient Descent(482/999): loss=0.4180849920330562\n",
      "Gradient Descent(483/999): loss=0.4179772566343976\n",
      "Gradient Descent(484/999): loss=0.4179474837806935\n",
      "Gradient Descent(485/999): loss=0.41785547105737164\n",
      "Gradient Descent(486/999): loss=0.4177612037174503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(487/999): loss=0.4177956887663558\n",
      "Gradient Descent(488/999): loss=0.41769621363244397\n",
      "Gradient Descent(489/999): loss=0.4179964946325642\n",
      "Gradient Descent(490/999): loss=0.4179759650692734\n",
      "Gradient Descent(491/999): loss=0.4178820222695865\n",
      "Gradient Descent(492/999): loss=0.4180289532707104\n",
      "Gradient Descent(493/999): loss=0.4183522268532387\n",
      "Gradient Descent(494/999): loss=0.41826402246754235\n",
      "Gradient Descent(495/999): loss=0.41765882827360834\n",
      "Gradient Descent(496/999): loss=0.4174765794940306\n",
      "Gradient Descent(497/999): loss=0.41753773504496255\n",
      "Gradient Descent(498/999): loss=0.4173037013342604\n",
      "Gradient Descent(499/999): loss=0.4172808343194396\n",
      "Gradient Descent(500/999): loss=0.4184548031033199\n",
      "Gradient Descent(501/999): loss=0.41820692025392964\n",
      "Gradient Descent(502/999): loss=0.41765841754852706\n",
      "Gradient Descent(503/999): loss=0.4175677639418083\n",
      "Gradient Descent(504/999): loss=0.417149716141366\n",
      "Gradient Descent(505/999): loss=0.4170116146030017\n",
      "Gradient Descent(506/999): loss=0.4168538382988895\n",
      "Gradient Descent(507/999): loss=0.4166824959146382\n",
      "Gradient Descent(508/999): loss=0.4166611569201151\n",
      "Gradient Descent(509/999): loss=0.4166208842985268\n",
      "Gradient Descent(510/999): loss=0.4164624262681466\n",
      "Gradient Descent(511/999): loss=0.41639537765810547\n",
      "Gradient Descent(512/999): loss=0.41650406585499966\n",
      "Gradient Descent(513/999): loss=0.4165160164081641\n",
      "Gradient Descent(514/999): loss=0.41642732413535\n",
      "Gradient Descent(515/999): loss=0.41620575077381355\n",
      "Gradient Descent(516/999): loss=0.41635566348274383\n",
      "Gradient Descent(517/999): loss=0.416212487757847\n",
      "Gradient Descent(518/999): loss=0.4164386565396298\n",
      "Gradient Descent(519/999): loss=0.4160226690390594\n",
      "Gradient Descent(520/999): loss=0.4160635785047541\n",
      "Gradient Descent(521/999): loss=0.415953582545532\n",
      "Gradient Descent(522/999): loss=0.4158559556059157\n",
      "Gradient Descent(523/999): loss=0.4158070558431953\n",
      "Gradient Descent(524/999): loss=0.41581152659829235\n",
      "Gradient Descent(525/999): loss=0.41587846367611364\n",
      "Gradient Descent(526/999): loss=0.4160175815633395\n",
      "Gradient Descent(527/999): loss=0.41571370924322315\n",
      "Gradient Descent(528/999): loss=0.41554945785992936\n",
      "Gradient Descent(529/999): loss=0.4155252686496721\n",
      "Gradient Descent(530/999): loss=0.4154106157830652\n",
      "Gradient Descent(531/999): loss=0.41537342077487643\n",
      "Gradient Descent(532/999): loss=0.41522363447315114\n",
      "Gradient Descent(533/999): loss=0.41515475726227336\n",
      "Gradient Descent(534/999): loss=0.41522618314901655\n",
      "Gradient Descent(535/999): loss=0.41513789347024604\n",
      "Gradient Descent(536/999): loss=0.41557365362448084\n",
      "Gradient Descent(537/999): loss=0.41495966766417297\n",
      "Gradient Descent(538/999): loss=0.41495788233436853\n",
      "Gradient Descent(539/999): loss=0.4149031859573862\n",
      "Gradient Descent(540/999): loss=0.41503211225908515\n",
      "Gradient Descent(541/999): loss=0.41488578871887893\n",
      "Gradient Descent(542/999): loss=0.4148130666129163\n",
      "Gradient Descent(543/999): loss=0.4146618286481094\n",
      "Gradient Descent(544/999): loss=0.4146427382795493\n",
      "Gradient Descent(545/999): loss=0.41477924081396333\n",
      "Gradient Descent(546/999): loss=0.4145433327062206\n",
      "Gradient Descent(547/999): loss=0.41442348902438003\n",
      "Gradient Descent(548/999): loss=0.4143123821363709\n",
      "Gradient Descent(549/999): loss=0.41429556082577135\n",
      "Gradient Descent(550/999): loss=0.4142401810065011\n",
      "Gradient Descent(551/999): loss=0.4141792426452719\n",
      "Gradient Descent(552/999): loss=0.4141332918492813\n",
      "Gradient Descent(553/999): loss=0.4140846260909725\n",
      "Gradient Descent(554/999): loss=0.4141119409734217\n",
      "Gradient Descent(555/999): loss=0.41399747645621654\n",
      "Gradient Descent(556/999): loss=0.41394775557141705\n",
      "Gradient Descent(557/999): loss=0.4139669953238367\n",
      "Gradient Descent(558/999): loss=0.41399584046796356\n",
      "Gradient Descent(559/999): loss=0.41386376972783706\n",
      "Gradient Descent(560/999): loss=0.413788327423512\n",
      "Gradient Descent(561/999): loss=0.4137754667044004\n",
      "Gradient Descent(562/999): loss=0.4137493427010808\n",
      "Gradient Descent(563/999): loss=0.41367681395794137\n",
      "Gradient Descent(564/999): loss=0.4136455548518288\n",
      "Gradient Descent(565/999): loss=0.41365085777705896\n",
      "Gradient Descent(566/999): loss=0.41361058623693525\n",
      "Gradient Descent(567/999): loss=0.41350360009948706\n",
      "Gradient Descent(568/999): loss=0.4134489963201988\n",
      "Gradient Descent(569/999): loss=0.4134198536418734\n",
      "Gradient Descent(570/999): loss=0.41334057589640577\n",
      "Gradient Descent(571/999): loss=0.4136649944349297\n",
      "Gradient Descent(572/999): loss=0.41365462245178714\n",
      "Gradient Descent(573/999): loss=0.41337343746445093\n",
      "Gradient Descent(574/999): loss=0.41318034940967585\n",
      "Gradient Descent(575/999): loss=0.41310679726944777\n",
      "Gradient Descent(576/999): loss=0.4130468282193088\n",
      "Gradient Descent(577/999): loss=0.412998337552181\n",
      "Gradient Descent(578/999): loss=0.4131754829270645\n",
      "Gradient Descent(579/999): loss=0.4131542273058056\n",
      "Gradient Descent(580/999): loss=0.4129599788651935\n",
      "Gradient Descent(581/999): loss=0.4132432360174915\n",
      "Gradient Descent(582/999): loss=0.41311612822415905\n",
      "Gradient Descent(583/999): loss=0.41301869276137143\n",
      "Gradient Descent(584/999): loss=0.41280623742060746\n",
      "Gradient Descent(585/999): loss=0.4127952931851515\n",
      "Gradient Descent(586/999): loss=0.41264286282022566\n",
      "Gradient Descent(587/999): loss=0.4126234042669985\n",
      "Gradient Descent(588/999): loss=0.4126691838305694\n",
      "Gradient Descent(589/999): loss=0.41252770716852694\n",
      "Gradient Descent(590/999): loss=0.41268053876187866\n",
      "Gradient Descent(591/999): loss=0.4125811860897569\n",
      "Gradient Descent(592/999): loss=0.4124220062026693\n",
      "Gradient Descent(593/999): loss=0.4126942687126772\n",
      "Gradient Descent(594/999): loss=0.4133428680057385\n",
      "Gradient Descent(595/999): loss=0.41338583004707136\n",
      "Gradient Descent(596/999): loss=0.4132281562082922\n",
      "Gradient Descent(597/999): loss=0.4129495498982477\n",
      "Gradient Descent(598/999): loss=0.4132742840431304\n",
      "Gradient Descent(599/999): loss=0.4125345199228493\n",
      "Gradient Descent(600/999): loss=0.4124143932205798\n",
      "Gradient Descent(601/999): loss=0.4121557125360465\n",
      "Gradient Descent(602/999): loss=0.41235348057507953\n",
      "Gradient Descent(603/999): loss=0.4124069533796096\n",
      "Gradient Descent(604/999): loss=0.4126878349137249\n",
      "Gradient Descent(605/999): loss=0.4128200180853948\n",
      "Gradient Descent(606/999): loss=0.4128671708258005\n",
      "Gradient Descent(607/999): loss=0.4127865995115272\n",
      "Gradient Descent(608/999): loss=0.4127493776072481\n",
      "Gradient Descent(609/999): loss=0.41313108964318956\n",
      "Gradient Descent(610/999): loss=0.4130111220043185\n",
      "Gradient Descent(611/999): loss=0.4132569642981369\n",
      "Gradient Descent(612/999): loss=0.41325773555092327\n",
      "Gradient Descent(613/999): loss=0.4127467310246033\n",
      "Gradient Descent(614/999): loss=0.4126829671572627\n",
      "Gradient Descent(615/999): loss=0.4121963421150002\n",
      "Gradient Descent(616/999): loss=0.4122387647103251\n",
      "Gradient Descent(617/999): loss=0.4119397914935132\n",
      "Gradient Descent(618/999): loss=0.41151225815164794\n",
      "Gradient Descent(619/999): loss=0.41141317821027085\n",
      "Gradient Descent(620/999): loss=0.4113334264414948\n",
      "Gradient Descent(621/999): loss=0.4111777703144768\n",
      "Gradient Descent(622/999): loss=0.4111397010310286\n",
      "Gradient Descent(623/999): loss=0.4111969346673078\n",
      "Gradient Descent(624/999): loss=0.4111388666504289\n",
      "Gradient Descent(625/999): loss=0.4112306310984299\n",
      "Gradient Descent(626/999): loss=0.41155048123221527\n",
      "Gradient Descent(627/999): loss=0.41128031945185073\n",
      "Gradient Descent(628/999): loss=0.4114970335231141\n",
      "Gradient Descent(629/999): loss=0.41128169973628065\n",
      "Gradient Descent(630/999): loss=0.4110766176812348\n",
      "Gradient Descent(631/999): loss=0.4109960269706618\n",
      "Gradient Descent(632/999): loss=0.4108341927138419\n",
      "Gradient Descent(633/999): loss=0.41077766297089785\n",
      "Gradient Descent(634/999): loss=0.41074881796812385\n",
      "Gradient Descent(635/999): loss=0.41071945666152604\n",
      "Gradient Descent(636/999): loss=0.41066456250285166\n",
      "Gradient Descent(637/999): loss=0.4106350095464998\n",
      "Gradient Descent(638/999): loss=0.4106827854438586\n",
      "Gradient Descent(639/999): loss=0.41049721076884343\n",
      "Gradient Descent(640/999): loss=0.41055114883472454\n",
      "Gradient Descent(641/999): loss=0.41043092857160923\n",
      "Gradient Descent(642/999): loss=0.4103974017428048\n",
      "Gradient Descent(643/999): loss=0.4103832687389135\n",
      "Gradient Descent(644/999): loss=0.4103346359640348\n",
      "Gradient Descent(645/999): loss=0.4103704992306072\n",
      "Gradient Descent(646/999): loss=0.4102910785783727\n",
      "Gradient Descent(647/999): loss=0.4101985999618157\n",
      "Gradient Descent(648/999): loss=0.4101757674452186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(649/999): loss=0.410148646797067\n",
      "Gradient Descent(650/999): loss=0.410062240962285\n",
      "Gradient Descent(651/999): loss=0.4099868485611347\n",
      "Gradient Descent(652/999): loss=0.40996956900807463\n",
      "Gradient Descent(653/999): loss=0.40997028183605916\n",
      "Gradient Descent(654/999): loss=0.40987434793461025\n",
      "Gradient Descent(655/999): loss=0.40990187924941207\n",
      "Gradient Descent(656/999): loss=0.40980453332226024\n",
      "Gradient Descent(657/999): loss=0.4096999101090274\n",
      "Gradient Descent(658/999): loss=0.40966767674698457\n",
      "Gradient Descent(659/999): loss=0.4096238413646568\n",
      "Gradient Descent(660/999): loss=0.4095764511922953\n",
      "Gradient Descent(661/999): loss=0.4095869760561112\n",
      "Gradient Descent(662/999): loss=0.40963625318995944\n",
      "Gradient Descent(663/999): loss=0.4094798084650021\n",
      "Gradient Descent(664/999): loss=0.40939146222628736\n",
      "Gradient Descent(665/999): loss=0.40974363312610707\n",
      "Gradient Descent(666/999): loss=0.409745612548248\n",
      "Gradient Descent(667/999): loss=0.4105115441205169\n",
      "Gradient Descent(668/999): loss=0.410145382173866\n",
      "Gradient Descent(669/999): loss=0.4102082198603383\n",
      "Gradient Descent(670/999): loss=0.40994649873793526\n",
      "Gradient Descent(671/999): loss=0.4106240451740131\n",
      "Gradient Descent(672/999): loss=0.41002679379238494\n",
      "Gradient Descent(673/999): loss=0.40934009473485017\n",
      "Gradient Descent(674/999): loss=0.4096042434747677\n",
      "Gradient Descent(675/999): loss=0.41019900916685864\n",
      "Gradient Descent(676/999): loss=0.40982899176750837\n",
      "Gradient Descent(677/999): loss=0.40974952778030377\n",
      "Gradient Descent(678/999): loss=0.41002316243061226\n",
      "Gradient Descent(679/999): loss=0.4115117079624093\n",
      "Gradient Descent(680/999): loss=0.4104910297557627\n",
      "Gradient Descent(681/999): loss=0.4094806185100477\n",
      "Gradient Descent(682/999): loss=0.40954121877315053\n",
      "Gradient Descent(683/999): loss=0.4097110762418203\n",
      "Gradient Descent(684/999): loss=0.40926800819576176\n",
      "Gradient Descent(685/999): loss=0.4092955800566662\n",
      "Gradient Descent(686/999): loss=0.40919818287936693\n",
      "Gradient Descent(687/999): loss=0.4096017543489093\n",
      "Gradient Descent(688/999): loss=0.41020094783209265\n",
      "Gradient Descent(689/999): loss=0.40941259528642476\n",
      "Gradient Descent(690/999): loss=0.40977969499996847\n",
      "Gradient Descent(691/999): loss=0.4096320279152268\n",
      "Gradient Descent(692/999): loss=0.409642595736889\n",
      "Gradient Descent(693/999): loss=0.4095624266452241\n",
      "Gradient Descent(694/999): loss=0.41016232542449205\n",
      "Gradient Descent(695/999): loss=0.41040814742252674\n",
      "Gradient Descent(696/999): loss=0.4091723919270635\n",
      "Gradient Descent(697/999): loss=0.409527683342771\n",
      "Gradient Descent(698/999): loss=0.40891896232941877\n",
      "Gradient Descent(699/999): loss=0.40919317703919533\n",
      "Gradient Descent(700/999): loss=0.4091102569240551\n",
      "Gradient Descent(701/999): loss=0.4088864219565489\n",
      "Gradient Descent(702/999): loss=0.40870128394553834\n",
      "Gradient Descent(703/999): loss=0.40845923538663503\n",
      "Gradient Descent(704/999): loss=0.4096565807577887\n",
      "Gradient Descent(705/999): loss=0.409458299306764\n",
      "Gradient Descent(706/999): loss=0.4088618178535289\n",
      "Gradient Descent(707/999): loss=0.40856463261002957\n",
      "Gradient Descent(708/999): loss=0.4089817278471778\n",
      "Gradient Descent(709/999): loss=0.4085027778372434\n",
      "Gradient Descent(710/999): loss=0.40861517837087175\n",
      "Gradient Descent(711/999): loss=0.40860996579761116\n",
      "Gradient Descent(712/999): loss=0.4085411656699289\n",
      "Gradient Descent(713/999): loss=0.4089099897852802\n",
      "Gradient Descent(714/999): loss=0.4095835164114429\n",
      "Gradient Descent(715/999): loss=0.4082189529904832\n",
      "Gradient Descent(716/999): loss=0.4083850587764835\n",
      "Gradient Descent(717/999): loss=0.4083229998223911\n",
      "Gradient Descent(718/999): loss=0.4077624809832645\n",
      "Gradient Descent(719/999): loss=0.4077246951172643\n",
      "Gradient Descent(720/999): loss=0.40770614946717576\n",
      "Gradient Descent(721/999): loss=0.4077415544268437\n",
      "Gradient Descent(722/999): loss=0.4079262658215339\n",
      "Gradient Descent(723/999): loss=0.40804067138518474\n",
      "Gradient Descent(724/999): loss=0.4077558294498997\n",
      "Gradient Descent(725/999): loss=0.4080238565258497\n",
      "Gradient Descent(726/999): loss=0.4076916838754622\n",
      "Gradient Descent(727/999): loss=0.40804135859212315\n",
      "Gradient Descent(728/999): loss=0.4082968692696695\n",
      "Gradient Descent(729/999): loss=0.40840361548023274\n",
      "Gradient Descent(730/999): loss=0.40838431053354396\n",
      "Gradient Descent(731/999): loss=0.4081397354640135\n",
      "Gradient Descent(732/999): loss=0.40824532192667334\n",
      "Gradient Descent(733/999): loss=0.4079203524787043\n",
      "Gradient Descent(734/999): loss=0.40801329142565956\n",
      "Gradient Descent(735/999): loss=0.4073473950213138\n",
      "Gradient Descent(736/999): loss=0.40718722494902865\n",
      "Gradient Descent(737/999): loss=0.4071710303727986\n",
      "Gradient Descent(738/999): loss=0.40711813787390055\n",
      "Gradient Descent(739/999): loss=0.40718582115189367\n",
      "Gradient Descent(740/999): loss=0.4072954728885853\n",
      "Gradient Descent(741/999): loss=0.40710180124159956\n",
      "Gradient Descent(742/999): loss=0.4071368921610407\n",
      "Gradient Descent(743/999): loss=0.40710354967047857\n",
      "Gradient Descent(744/999): loss=0.40682351388926963\n",
      "Gradient Descent(745/999): loss=0.40678276231669963\n",
      "Gradient Descent(746/999): loss=0.4067621461164926\n",
      "Gradient Descent(747/999): loss=0.4067439189452806\n",
      "Gradient Descent(748/999): loss=0.4068343291896187\n",
      "Gradient Descent(749/999): loss=0.4068742674023521\n",
      "Gradient Descent(750/999): loss=0.40728220859231673\n",
      "Gradient Descent(751/999): loss=0.4068121797989256\n",
      "Gradient Descent(752/999): loss=0.40702153472644553\n",
      "Gradient Descent(753/999): loss=0.4082195040873979\n",
      "Gradient Descent(754/999): loss=0.4074372301283644\n",
      "Gradient Descent(755/999): loss=0.40768233153408723\n",
      "Gradient Descent(756/999): loss=0.4077189984903705\n",
      "Gradient Descent(757/999): loss=0.4076937747627095\n",
      "Gradient Descent(758/999): loss=0.4079637934321867\n",
      "Gradient Descent(759/999): loss=0.407348687805409\n",
      "Gradient Descent(760/999): loss=0.40755408837407436\n",
      "Gradient Descent(761/999): loss=0.4079375835118441\n",
      "Gradient Descent(762/999): loss=0.4084984701984562\n",
      "Gradient Descent(763/999): loss=0.40704593057796645\n",
      "Gradient Descent(764/999): loss=0.40723983471876607\n",
      "Gradient Descent(765/999): loss=0.40705377854798014\n",
      "Gradient Descent(766/999): loss=0.40707543964904813\n",
      "Gradient Descent(767/999): loss=0.40647525163013787\n",
      "Gradient Descent(768/999): loss=0.4064738124720279\n",
      "Gradient Descent(769/999): loss=0.40655094914586204\n",
      "Gradient Descent(770/999): loss=0.40612009664628146\n",
      "Gradient Descent(771/999): loss=0.4060039967321247\n",
      "Gradient Descent(772/999): loss=0.4059191499332729\n",
      "Gradient Descent(773/999): loss=0.40587469153835676\n",
      "Gradient Descent(774/999): loss=0.4058320494139203\n",
      "Gradient Descent(775/999): loss=0.40586106531012067\n",
      "Gradient Descent(776/999): loss=0.40578449938150185\n",
      "Gradient Descent(777/999): loss=0.4057750867097832\n",
      "Gradient Descent(778/999): loss=0.4058092478815996\n",
      "Gradient Descent(779/999): loss=0.40572967836648177\n",
      "Gradient Descent(780/999): loss=0.40569660285164927\n",
      "Gradient Descent(781/999): loss=0.4061012256076458\n",
      "Gradient Descent(782/999): loss=0.4058321344044497\n",
      "Gradient Descent(783/999): loss=0.4057604795530823\n",
      "Gradient Descent(784/999): loss=0.40563738960655077\n",
      "Gradient Descent(785/999): loss=0.40560713491753225\n",
      "Gradient Descent(786/999): loss=0.4056009612891944\n",
      "Gradient Descent(787/999): loss=0.4057366090722744\n",
      "Gradient Descent(788/999): loss=0.4057557592925408\n",
      "Gradient Descent(789/999): loss=0.40552943979537165\n",
      "Gradient Descent(790/999): loss=0.4054561660882877\n",
      "Gradient Descent(791/999): loss=0.40550054752708725\n",
      "Gradient Descent(792/999): loss=0.4057064421390119\n",
      "Gradient Descent(793/999): loss=0.4055598436792744\n",
      "Gradient Descent(794/999): loss=0.4054170265186595\n",
      "Gradient Descent(795/999): loss=0.4054684084857471\n",
      "Gradient Descent(796/999): loss=0.405191527962186\n",
      "Gradient Descent(797/999): loss=0.40516760154291637\n",
      "Gradient Descent(798/999): loss=0.4051442151138092\n",
      "Gradient Descent(799/999): loss=0.4051685743508801\n",
      "Gradient Descent(800/999): loss=0.4051988782818515\n",
      "Gradient Descent(801/999): loss=0.40530123896595627\n",
      "Gradient Descent(802/999): loss=0.40508155178677985\n",
      "Gradient Descent(803/999): loss=0.4051611408007156\n",
      "Gradient Descent(804/999): loss=0.4050366707595973\n",
      "Gradient Descent(805/999): loss=0.4049256585081778\n",
      "Gradient Descent(806/999): loss=0.4048917658409152\n",
      "Gradient Descent(807/999): loss=0.4048815283618812\n",
      "Gradient Descent(808/999): loss=0.40499418035051715\n",
      "Gradient Descent(809/999): loss=0.40485277660213204\n",
      "Gradient Descent(810/999): loss=0.404823736725305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(811/999): loss=0.40475525729244444\n",
      "Gradient Descent(812/999): loss=0.4046635449411427\n",
      "Gradient Descent(813/999): loss=0.4046172779220924\n",
      "Gradient Descent(814/999): loss=0.4046749651045595\n",
      "Gradient Descent(815/999): loss=0.4046346785925506\n",
      "Gradient Descent(816/999): loss=0.4045372115585433\n",
      "Gradient Descent(817/999): loss=0.404492050851792\n",
      "Gradient Descent(818/999): loss=0.404507379660633\n",
      "Gradient Descent(819/999): loss=0.4047839027677783\n",
      "Gradient Descent(820/999): loss=0.40440659763661024\n",
      "Gradient Descent(821/999): loss=0.4043849523605578\n",
      "Gradient Descent(822/999): loss=0.4045285057510985\n",
      "Gradient Descent(823/999): loss=0.4044100862038184\n",
      "Gradient Descent(824/999): loss=0.4043327675691818\n",
      "Gradient Descent(825/999): loss=0.40441389293964053\n",
      "Gradient Descent(826/999): loss=0.4043913385939749\n",
      "Gradient Descent(827/999): loss=0.40453568992761013\n",
      "Gradient Descent(828/999): loss=0.4047707480440102\n",
      "Gradient Descent(829/999): loss=0.40470588051498857\n",
      "Gradient Descent(830/999): loss=0.40541017852366584\n",
      "Gradient Descent(831/999): loss=0.40612304522018267\n",
      "Gradient Descent(832/999): loss=0.40593900126192917\n",
      "Gradient Descent(833/999): loss=0.4054459401679128\n",
      "Gradient Descent(834/999): loss=0.4046935318407904\n",
      "Gradient Descent(835/999): loss=0.40425623508293623\n",
      "Gradient Descent(836/999): loss=0.4039935029066094\n",
      "Gradient Descent(837/999): loss=0.40396037948508456\n",
      "Gradient Descent(838/999): loss=0.40396662363802227\n",
      "Gradient Descent(839/999): loss=0.4039946977739436\n",
      "Gradient Descent(840/999): loss=0.4039181612144223\n",
      "Gradient Descent(841/999): loss=0.4038738003396878\n",
      "Gradient Descent(842/999): loss=0.40384467495682425\n",
      "Gradient Descent(843/999): loss=0.40383991443549877\n",
      "Gradient Descent(844/999): loss=0.4038809841956087\n",
      "Gradient Descent(845/999): loss=0.40402256056853486\n",
      "Gradient Descent(846/999): loss=0.4041373492800244\n",
      "Gradient Descent(847/999): loss=0.40382228751582483\n",
      "Gradient Descent(848/999): loss=0.40384692453528026\n",
      "Gradient Descent(849/999): loss=0.40389739753134224\n",
      "Gradient Descent(850/999): loss=0.4037033374888431\n",
      "Gradient Descent(851/999): loss=0.4035403637296793\n",
      "Gradient Descent(852/999): loss=0.40350048822770523\n",
      "Gradient Descent(853/999): loss=0.40353984355089234\n",
      "Gradient Descent(854/999): loss=0.4038887402601007\n",
      "Gradient Descent(855/999): loss=0.40358505509550774\n",
      "Gradient Descent(856/999): loss=0.4034845885377621\n",
      "Gradient Descent(857/999): loss=0.40450821003072723\n",
      "Gradient Descent(858/999): loss=0.4043033283564786\n",
      "Gradient Descent(859/999): loss=0.4038413424280202\n",
      "Gradient Descent(860/999): loss=0.4043005898590411\n",
      "Gradient Descent(861/999): loss=0.40428996711824594\n",
      "Gradient Descent(862/999): loss=0.4038185934476188\n",
      "Gradient Descent(863/999): loss=0.4035434463964753\n",
      "Gradient Descent(864/999): loss=0.40347951593357734\n",
      "Gradient Descent(865/999): loss=0.40334884639178553\n",
      "Gradient Descent(866/999): loss=0.40324533625899833\n",
      "Gradient Descent(867/999): loss=0.4032796925152363\n",
      "Gradient Descent(868/999): loss=0.4033897923978159\n",
      "Gradient Descent(869/999): loss=0.4031286502317643\n",
      "Gradient Descent(870/999): loss=0.40313674376190556\n",
      "Gradient Descent(871/999): loss=0.40310850118863994\n",
      "Gradient Descent(872/999): loss=0.4031269748177214\n",
      "Gradient Descent(873/999): loss=0.40315639092065514\n",
      "Gradient Descent(874/999): loss=0.4031675329497058\n",
      "Gradient Descent(875/999): loss=0.40293431133357965\n",
      "Gradient Descent(876/999): loss=0.4029036137894023\n",
      "Gradient Descent(877/999): loss=0.4028677884569854\n",
      "Gradient Descent(878/999): loss=0.4028302788948974\n",
      "Gradient Descent(879/999): loss=0.40282285701664255\n",
      "Gradient Descent(880/999): loss=0.4028024114230589\n",
      "Gradient Descent(881/999): loss=0.40292720538681087\n",
      "Gradient Descent(882/999): loss=0.40297748928492433\n",
      "Gradient Descent(883/999): loss=0.40284677204508984\n",
      "Gradient Descent(884/999): loss=0.4028490241550969\n",
      "Gradient Descent(885/999): loss=0.4027361185796886\n",
      "Gradient Descent(886/999): loss=0.4026205614402624\n",
      "Gradient Descent(887/999): loss=0.4026094457957201\n",
      "Gradient Descent(888/999): loss=0.40268528950523547\n",
      "Gradient Descent(889/999): loss=0.40264562847123914\n",
      "Gradient Descent(890/999): loss=0.4026892811854493\n",
      "Gradient Descent(891/999): loss=0.4030705157243349\n",
      "Gradient Descent(892/999): loss=0.40293339575879206\n",
      "Gradient Descent(893/999): loss=0.40256004805103573\n",
      "Gradient Descent(894/999): loss=0.4025038318703142\n",
      "Gradient Descent(895/999): loss=0.4024546554105922\n",
      "Gradient Descent(896/999): loss=0.4024626373213611\n",
      "Gradient Descent(897/999): loss=0.40244382691772285\n",
      "Gradient Descent(898/999): loss=0.4027143301697934\n",
      "Gradient Descent(899/999): loss=0.40267404083779595\n",
      "Gradient Descent(900/999): loss=0.40233660264930216\n",
      "Gradient Descent(901/999): loss=0.40247513115933425\n",
      "Gradient Descent(902/999): loss=0.40228230252104996\n",
      "Gradient Descent(903/999): loss=0.40229970877928156\n",
      "Gradient Descent(904/999): loss=0.40231727255424354\n",
      "Gradient Descent(905/999): loss=0.4021539205422594\n",
      "Gradient Descent(906/999): loss=0.40245443759930893\n",
      "Gradient Descent(907/999): loss=0.4026177116881835\n",
      "Gradient Descent(908/999): loss=0.40250376037370744\n",
      "Gradient Descent(909/999): loss=0.40243413861159183\n",
      "Gradient Descent(910/999): loss=0.4023238936857633\n",
      "Gradient Descent(911/999): loss=0.4021691025391681\n",
      "Gradient Descent(912/999): loss=0.40232701901892975\n",
      "Gradient Descent(913/999): loss=0.40202398752117907\n",
      "Gradient Descent(914/999): loss=0.40223798022098056\n",
      "Gradient Descent(915/999): loss=0.4027716313458678\n",
      "Gradient Descent(916/999): loss=0.4026299951375451\n",
      "Gradient Descent(917/999): loss=0.40297223646027125\n",
      "Gradient Descent(918/999): loss=0.40215461002716557\n",
      "Gradient Descent(919/999): loss=0.4018736115808545\n",
      "Gradient Descent(920/999): loss=0.40180368858498267\n",
      "Gradient Descent(921/999): loss=0.4016734471146583\n",
      "Gradient Descent(922/999): loss=0.40166686222840325\n",
      "Gradient Descent(923/999): loss=0.401584775215491\n",
      "Gradient Descent(924/999): loss=0.401642098460388\n",
      "Gradient Descent(925/999): loss=0.40151977925901594\n",
      "Gradient Descent(926/999): loss=0.4016553517815761\n",
      "Gradient Descent(927/999): loss=0.4016513888723099\n",
      "Gradient Descent(928/999): loss=0.4015562075345792\n",
      "Gradient Descent(929/999): loss=0.4014694213878545\n",
      "Gradient Descent(930/999): loss=0.4015527275581152\n",
      "Gradient Descent(931/999): loss=0.4014232730221056\n",
      "Gradient Descent(932/999): loss=0.4013733991877322\n",
      "Gradient Descent(933/999): loss=0.401379645688566\n",
      "Gradient Descent(934/999): loss=0.4013060216633465\n",
      "Gradient Descent(935/999): loss=0.4014486306914698\n",
      "Gradient Descent(936/999): loss=0.4012824358565212\n",
      "Gradient Descent(937/999): loss=0.40128040609263643\n",
      "Gradient Descent(938/999): loss=0.4012375147436706\n",
      "Gradient Descent(939/999): loss=0.40119106755371514\n",
      "Gradient Descent(940/999): loss=0.4011564353449616\n",
      "Gradient Descent(941/999): loss=0.4012799532221007\n",
      "Gradient Descent(942/999): loss=0.4011457542085956\n",
      "Gradient Descent(943/999): loss=0.4012751801210613\n",
      "Gradient Descent(944/999): loss=0.40106448676422607\n",
      "Gradient Descent(945/999): loss=0.40105057398211624\n",
      "Gradient Descent(946/999): loss=0.40106657061373596\n",
      "Gradient Descent(947/999): loss=0.40106790616431415\n",
      "Gradient Descent(948/999): loss=0.40094294554519727\n",
      "Gradient Descent(949/999): loss=0.4009262590117108\n",
      "Gradient Descent(950/999): loss=0.4009472102451196\n",
      "Gradient Descent(951/999): loss=0.4009106949241998\n",
      "Gradient Descent(952/999): loss=0.40089505898222405\n",
      "Gradient Descent(953/999): loss=0.40102151202746134\n",
      "Gradient Descent(954/999): loss=0.4009464825271043\n",
      "Gradient Descent(955/999): loss=0.40101428601646466\n",
      "Gradient Descent(956/999): loss=0.4008993329355697\n",
      "Gradient Descent(957/999): loss=0.40074128195339676\n",
      "Gradient Descent(958/999): loss=0.400785552755708\n",
      "Gradient Descent(959/999): loss=0.4007433336706329\n",
      "Gradient Descent(960/999): loss=0.4007071734142863\n",
      "Gradient Descent(961/999): loss=0.40076422304881854\n",
      "Gradient Descent(962/999): loss=0.4006342960902234\n",
      "Gradient Descent(963/999): loss=0.40080918918852354\n",
      "Gradient Descent(964/999): loss=0.40084995308440224\n",
      "Gradient Descent(965/999): loss=0.40069090810598895\n",
      "Gradient Descent(966/999): loss=0.40064034672823634\n",
      "Gradient Descent(967/999): loss=0.40077078028725754\n",
      "Gradient Descent(968/999): loss=0.400959739717342\n",
      "Gradient Descent(969/999): loss=0.40049955015490435\n",
      "Gradient Descent(970/999): loss=0.40045102111077757\n",
      "Gradient Descent(971/999): loss=0.4004093203399401\n",
      "Gradient Descent(972/999): loss=0.4003836248547666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(973/999): loss=0.40032485522469\n",
      "Gradient Descent(974/999): loss=0.4004700702046661\n",
      "Gradient Descent(975/999): loss=0.4002649992546339\n",
      "Gradient Descent(976/999): loss=0.4002024784358968\n",
      "Gradient Descent(977/999): loss=0.4005074822956666\n",
      "Gradient Descent(978/999): loss=0.4001717596099878\n",
      "Gradient Descent(979/999): loss=0.4001173744643487\n",
      "Gradient Descent(980/999): loss=0.40020159142598005\n",
      "Gradient Descent(981/999): loss=0.4000763310875739\n",
      "Gradient Descent(982/999): loss=0.400128355062376\n",
      "Gradient Descent(983/999): loss=0.4000477518601404\n",
      "Gradient Descent(984/999): loss=0.4000102611611286\n",
      "Gradient Descent(985/999): loss=0.3999947193680409\n",
      "Gradient Descent(986/999): loss=0.3999846126166332\n",
      "Gradient Descent(987/999): loss=0.39994242982539996\n",
      "Gradient Descent(988/999): loss=0.3999495622167426\n",
      "Gradient Descent(989/999): loss=0.39993773167350893\n",
      "Gradient Descent(990/999): loss=0.39997973309878604\n",
      "Gradient Descent(991/999): loss=0.39987431870917145\n",
      "Gradient Descent(992/999): loss=0.39986852994259353\n",
      "Gradient Descent(993/999): loss=0.3998798981813895\n",
      "Gradient Descent(994/999): loss=0.3999389584185791\n",
      "Gradient Descent(995/999): loss=0.39992666779476593\n",
      "Gradient Descent(996/999): loss=0.3997665898676055\n",
      "Gradient Descent(997/999): loss=0.39999293189161916\n",
      "Gradient Descent(998/999): loss=0.3999362148668711\n",
      "Gradient Descent(999/999): loss=0.39980683443192233\n"
     ]
    }
   ],
   "source": [
    "w, mse = least_squares_SGD(y_train, tX_train, initial_w, max_iters, gamma, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.682"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict_labels(w, tX_train)\n",
    "np.mean(y_train.reshape(-1,1)==y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68116"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict_labels(w, tX_val)\n",
    "np.mean(y_val.reshape(-1,1)==y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros(tX.shape[1])#np.random.normal(loc=0, scale=1, size=(tX.shape[1],1))\n",
    "max_iters = 1000\n",
    "gamma = 1.5e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_lr = y_train>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(0/999): loss=0.6669003579525978\n",
      "Log Regression(1/999): loss=0.6633099924185696\n",
      "Log Regression(2/999): loss=0.6671924049787051\n",
      "Log Regression(3/999): loss=0.6625946626484256\n",
      "Log Regression(4/999): loss=0.6555295559377617\n",
      "Log Regression(5/999): loss=0.6537296167985533\n",
      "Log Regression(6/999): loss=0.6512040560434509\n",
      "Log Regression(7/999): loss=0.669039467151037\n",
      "Log Regression(8/999): loss=0.6545967006305298\n",
      "Log Regression(9/999): loss=0.6662910205997788\n",
      "Log Regression(10/999): loss=0.671063695532839\n",
      "Log Regression(11/999): loss=0.6534930625490328\n",
      "Log Regression(12/999): loss=0.6442113514190045\n",
      "Log Regression(13/999): loss=0.6502131136760555\n",
      "Log Regression(14/999): loss=0.6811939006131291\n",
      "Log Regression(15/999): loss=0.6503067418237227\n",
      "Log Regression(16/999): loss=0.6391423637090622\n",
      "Log Regression(17/999): loss=0.6362725304905471\n",
      "Log Regression(18/999): loss=0.653626634791935\n",
      "Log Regression(19/999): loss=0.6486101647910342\n",
      "Log Regression(20/999): loss=0.6315788207595077\n",
      "Log Regression(21/999): loss=0.6358913911358474\n",
      "Log Regression(22/999): loss=0.627898217798026\n",
      "Log Regression(23/999): loss=0.6299929895864896\n",
      "Log Regression(24/999): loss=0.6318283209448347\n",
      "Log Regression(25/999): loss=0.626699201203951\n",
      "Log Regression(26/999): loss=0.6248678619308665\n",
      "Log Regression(27/999): loss=0.6204256811618978\n",
      "Log Regression(28/999): loss=0.6199913424145859\n",
      "Log Regression(29/999): loss=0.628206762903143\n",
      "Log Regression(30/999): loss=0.6287067585937707\n",
      "Log Regression(31/999): loss=0.6197695560558639\n",
      "Log Regression(32/999): loss=0.6176889849642475\n",
      "Log Regression(33/999): loss=0.6204636789383455\n",
      "Log Regression(34/999): loss=0.6200304500487184\n",
      "Log Regression(35/999): loss=0.6156913488595539\n",
      "Log Regression(36/999): loss=0.6262996857594639\n",
      "Log Regression(37/999): loss=0.6172999694424577\n",
      "Log Regression(38/999): loss=0.6179716797058626\n",
      "Log Regression(39/999): loss=0.6146232916396371\n",
      "Log Regression(40/999): loss=0.6109307238345438\n",
      "Log Regression(41/999): loss=0.6124622404029463\n",
      "Log Regression(42/999): loss=0.6209696007065213\n",
      "Log Regression(43/999): loss=0.6454379442817207\n",
      "Log Regression(44/999): loss=0.6083362569629255\n",
      "Log Regression(45/999): loss=0.6150656817522204\n",
      "Log Regression(46/999): loss=0.6245954625628258\n",
      "Log Regression(47/999): loss=0.6189726297475281\n",
      "Log Regression(48/999): loss=0.6064535411204816\n",
      "Log Regression(49/999): loss=0.605872783266224\n",
      "Log Regression(50/999): loss=0.6051914470274973\n",
      "Log Regression(51/999): loss=0.6058234133774067\n",
      "Log Regression(52/999): loss=0.6036726856301857\n",
      "Log Regression(53/999): loss=0.6055236456899153\n",
      "Log Regression(54/999): loss=0.6410230673829747\n",
      "Log Regression(55/999): loss=0.6226031121342825\n",
      "Log Regression(56/999): loss=0.60710470155118\n",
      "Log Regression(57/999): loss=0.6023896447691784\n",
      "Log Regression(58/999): loss=0.6191517203492901\n",
      "Log Regression(59/999): loss=0.6024597995782486\n",
      "Log Regression(60/999): loss=0.6015656929365328\n",
      "Log Regression(61/999): loss=0.6358459807878767\n",
      "Log Regression(62/999): loss=0.6002524828166271\n",
      "Log Regression(63/999): loss=0.5996469223252326\n",
      "Log Regression(64/999): loss=0.5981712002824159\n",
      "Log Regression(65/999): loss=0.5981296886170279\n",
      "Log Regression(66/999): loss=0.5988482909434155\n",
      "Log Regression(67/999): loss=0.5983070443631001\n",
      "Log Regression(68/999): loss=0.5972398745546699\n",
      "Log Regression(69/999): loss=0.6083964680195447\n",
      "Log Regression(70/999): loss=0.6128338129390675\n",
      "Log Regression(71/999): loss=0.5993908973015711\n",
      "Log Regression(72/999): loss=0.5948363086891538\n",
      "Log Regression(73/999): loss=0.6037257209309411\n",
      "Log Regression(74/999): loss=0.5958737506875489\n",
      "Log Regression(75/999): loss=0.6161094000794756\n",
      "Log Regression(76/999): loss=0.605185601660873\n",
      "Log Regression(77/999): loss=0.5931730369299569\n",
      "Log Regression(78/999): loss=0.5923278688091442\n",
      "Log Regression(79/999): loss=0.6092629203331833\n",
      "Log Regression(80/999): loss=0.59178286480362\n",
      "Log Regression(81/999): loss=0.6027736439297938\n",
      "Log Regression(82/999): loss=0.5902680046062604\n",
      "Log Regression(83/999): loss=0.5898250110873018\n",
      "Log Regression(84/999): loss=0.589816711750753\n",
      "Log Regression(85/999): loss=0.5894700566101359\n",
      "Log Regression(86/999): loss=0.5911520858077417\n",
      "Log Regression(87/999): loss=0.5927916006792234\n",
      "Log Regression(88/999): loss=0.588666452214789\n",
      "Log Regression(89/999): loss=0.5877900597869391\n",
      "Log Regression(90/999): loss=0.5948082887942882\n",
      "Log Regression(91/999): loss=0.5870049900894789\n",
      "Log Regression(92/999): loss=0.5868589807894696\n",
      "Log Regression(93/999): loss=0.5910648637133813\n",
      "Log Regression(94/999): loss=0.5936844363169472\n",
      "Log Regression(95/999): loss=0.5883435697180176\n",
      "Log Regression(96/999): loss=0.5853775601835726\n",
      "Log Regression(97/999): loss=0.5914972814611404\n",
      "Log Regression(98/999): loss=0.5867104063101237\n",
      "Log Regression(99/999): loss=0.5859443068186119\n",
      "Log Regression(100/999): loss=0.589623564519277\n",
      "Log Regression(101/999): loss=0.5844985939917351\n",
      "Log Regression(102/999): loss=0.5850057976668535\n",
      "Log Regression(103/999): loss=0.6038701808146123\n",
      "Log Regression(104/999): loss=0.5915125622729966\n",
      "Log Regression(105/999): loss=0.5847937957111968\n",
      "Log Regression(106/999): loss=0.5832673966819271\n",
      "Log Regression(107/999): loss=0.5840019686961047\n",
      "Log Regression(108/999): loss=0.5836202417483988\n",
      "Log Regression(109/999): loss=0.5864383415989262\n",
      "Log Regression(110/999): loss=0.5824071359628914\n",
      "Log Regression(111/999): loss=0.5938077461910644\n",
      "Log Regression(112/999): loss=0.5842440250425834\n",
      "Log Regression(113/999): loss=0.5829153519301323\n",
      "Log Regression(114/999): loss=0.5891349293438943\n",
      "Log Regression(115/999): loss=0.581500771463861\n",
      "Log Regression(116/999): loss=0.5864147724953954\n",
      "Log Regression(117/999): loss=0.5833158883572418\n",
      "Log Regression(118/999): loss=0.6332463465069155\n",
      "Log Regression(119/999): loss=0.5943179511172755\n",
      "Log Regression(120/999): loss=0.5953715665062385\n",
      "Log Regression(121/999): loss=0.5815286834791326\n",
      "Log Regression(122/999): loss=0.5825578223890384\n",
      "Log Regression(123/999): loss=0.5818511738997867\n",
      "Log Regression(124/999): loss=0.5853559551827127\n",
      "Log Regression(125/999): loss=0.5829959413294089\n",
      "Log Regression(126/999): loss=0.579908262117042\n",
      "Log Regression(127/999): loss=0.5815156775396392\n",
      "Log Regression(128/999): loss=0.5998737077946366\n",
      "Log Regression(129/999): loss=0.5787155143449371\n",
      "Log Regression(130/999): loss=0.5832988895821202\n",
      "Log Regression(131/999): loss=0.5834944253140637\n",
      "Log Regression(132/999): loss=0.5786345537181494\n",
      "Log Regression(133/999): loss=0.588222672490228\n",
      "Log Regression(134/999): loss=0.5833282239845727\n",
      "Log Regression(135/999): loss=0.5795071604426605\n",
      "Log Regression(136/999): loss=0.5793830734579107\n",
      "Log Regression(137/999): loss=0.5958947779052929\n",
      "Log Regression(138/999): loss=0.5898132457247239\n",
      "Log Regression(139/999): loss=0.5825033188427193\n",
      "Log Regression(140/999): loss=0.5873634522906868\n",
      "Log Regression(141/999): loss=0.5764250472340597\n",
      "Log Regression(142/999): loss=0.5750115277391264\n",
      "Log Regression(143/999): loss=0.5764877492546773\n",
      "Log Regression(144/999): loss=0.5758485559302757\n",
      "Log Regression(145/999): loss=0.5747513525919663\n",
      "Log Regression(146/999): loss=0.5744745376593933\n",
      "Log Regression(147/999): loss=0.5744938723273458\n",
      "Log Regression(148/999): loss=0.5755953386997582\n",
      "Log Regression(149/999): loss=0.5798676548792246\n",
      "Log Regression(150/999): loss=0.5737711123283915\n",
      "Log Regression(151/999): loss=0.5734106665075107\n",
      "Log Regression(152/999): loss=0.5793358250891427\n",
      "Log Regression(153/999): loss=0.5747429850068198\n",
      "Log Regression(154/999): loss=0.579831294085958\n",
      "Log Regression(155/999): loss=0.5899133187260763\n",
      "Log Regression(156/999): loss=0.5722255080843502\n",
      "Log Regression(157/999): loss=0.5726455664867105\n",
      "Log Regression(158/999): loss=0.5724008858017245\n",
      "Log Regression(159/999): loss=0.5732218160484226\n",
      "Log Regression(160/999): loss=0.5721523162244708\n",
      "Log Regression(161/999): loss=0.5714211349333171\n",
      "Log Regression(162/999): loss=0.5710022389472373\n",
      "Log Regression(163/999): loss=0.5736345929153626\n",
      "Log Regression(164/999): loss=0.5819810791741852\n",
      "Log Regression(165/999): loss=0.5719472109528663\n",
      "Log Regression(166/999): loss=0.5719443373614872\n",
      "Log Regression(167/999): loss=0.5757896460603021\n",
      "Log Regression(168/999): loss=0.5724515707480473\n",
      "Log Regression(169/999): loss=0.5720431643273678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(170/999): loss=0.5756956018932959\n",
      "Log Regression(171/999): loss=0.5951193760522699\n",
      "Log Regression(172/999): loss=0.5727527707600798\n",
      "Log Regression(173/999): loss=0.5694921814001845\n",
      "Log Regression(174/999): loss=0.5755948804533964\n",
      "Log Regression(175/999): loss=0.5923561218540812\n",
      "Log Regression(176/999): loss=0.5715374797465191\n",
      "Log Regression(177/999): loss=0.5743034042865179\n",
      "Log Regression(178/999): loss=0.5694402342310692\n",
      "Log Regression(179/999): loss=0.5821561537038866\n",
      "Log Regression(180/999): loss=0.5858227375795596\n",
      "Log Regression(181/999): loss=0.5709957685153568\n",
      "Log Regression(182/999): loss=0.5766654130162381\n",
      "Log Regression(183/999): loss=0.5692215456407501\n",
      "Log Regression(184/999): loss=0.5694268649071155\n",
      "Log Regression(185/999): loss=0.5674342357603802\n",
      "Log Regression(186/999): loss=0.5862894127663627\n",
      "Log Regression(187/999): loss=0.5786403954398477\n",
      "Log Regression(188/999): loss=0.5686122643535161\n",
      "Log Regression(189/999): loss=0.5881487475307083\n",
      "Log Regression(190/999): loss=0.5949723022941962\n",
      "Log Regression(191/999): loss=0.6011197640970126\n",
      "Log Regression(192/999): loss=0.5773939604379564\n",
      "Log Regression(193/999): loss=0.5656330244523923\n",
      "Log Regression(194/999): loss=0.5707861365647924\n",
      "Log Regression(195/999): loss=0.569347203352858\n",
      "Log Regression(196/999): loss=0.5657381745894349\n",
      "Log Regression(197/999): loss=0.568434864348652\n",
      "Log Regression(198/999): loss=0.5696614620221602\n",
      "Log Regression(199/999): loss=0.5669330161537043\n",
      "Log Regression(200/999): loss=0.5704599263963465\n",
      "Log Regression(201/999): loss=0.5745021609387259\n",
      "Log Regression(202/999): loss=0.5834698184374506\n",
      "Log Regression(203/999): loss=0.5651889630267511\n",
      "Log Regression(204/999): loss=0.5652889690401889\n",
      "Log Regression(205/999): loss=0.5665344886689416\n",
      "Log Regression(206/999): loss=0.5658723165190263\n",
      "Log Regression(207/999): loss=0.5637634555662522\n",
      "Log Regression(208/999): loss=0.5638434676125496\n",
      "Log Regression(209/999): loss=0.5664575227508275\n",
      "Log Regression(210/999): loss=0.5728883700743194\n",
      "Log Regression(211/999): loss=0.5632736501459367\n",
      "Log Regression(212/999): loss=0.5627181189853465\n",
      "Log Regression(213/999): loss=0.5634426082546524\n",
      "Log Regression(214/999): loss=0.5657974142333413\n",
      "Log Regression(215/999): loss=0.5634429231577203\n",
      "Log Regression(216/999): loss=0.5793511296368822\n",
      "Log Regression(217/999): loss=0.5820330301502493\n",
      "Log Regression(218/999): loss=0.5662402512595077\n",
      "Log Regression(219/999): loss=0.5807713565656037\n",
      "Log Regression(220/999): loss=0.5686743681904577\n",
      "Log Regression(221/999): loss=0.5766117150468587\n",
      "Log Regression(222/999): loss=0.5652329103109831\n",
      "Log Regression(223/999): loss=0.5675883113986734\n",
      "Log Regression(224/999): loss=0.5614766402181322\n",
      "Log Regression(225/999): loss=0.5660527402468355\n",
      "Log Regression(226/999): loss=0.5613472763001988\n",
      "Log Regression(227/999): loss=0.5784344824693802\n",
      "Log Regression(228/999): loss=0.5613182420049855\n",
      "Log Regression(229/999): loss=0.5650005640819166\n",
      "Log Regression(230/999): loss=0.5730206584865157\n",
      "Log Regression(231/999): loss=0.570649400541101\n",
      "Log Regression(232/999): loss=0.5642098880863335\n",
      "Log Regression(233/999): loss=0.5619323356535173\n",
      "Log Regression(234/999): loss=0.5616728583020639\n",
      "Log Regression(235/999): loss=0.560984395146893\n",
      "Log Regression(236/999): loss=0.5598298538234904\n",
      "Log Regression(237/999): loss=0.5616131753331012\n",
      "Log Regression(238/999): loss=0.5605592596391089\n",
      "Log Regression(239/999): loss=0.5604946764112592\n",
      "Log Regression(240/999): loss=0.5714434205032142\n",
      "Log Regression(241/999): loss=0.5595853248181442\n",
      "Log Regression(242/999): loss=0.5597117387084881\n",
      "Log Regression(243/999): loss=0.5634805607283359\n",
      "Log Regression(244/999): loss=0.5592924212233084\n",
      "Log Regression(245/999): loss=0.5611820634066111\n",
      "Log Regression(246/999): loss=0.5587115069224144\n",
      "Log Regression(247/999): loss=0.5586940923272873\n",
      "Log Regression(248/999): loss=0.5627971550106269\n",
      "Log Regression(249/999): loss=0.5582924485526185\n",
      "Log Regression(250/999): loss=0.5581408985557239\n",
      "Log Regression(251/999): loss=0.5613540310921933\n",
      "Log Regression(252/999): loss=0.558821369940778\n",
      "Log Regression(253/999): loss=0.5584990364915559\n",
      "Log Regression(254/999): loss=0.5574889440571186\n",
      "Log Regression(255/999): loss=0.5591453821596376\n",
      "Log Regression(256/999): loss=0.5581848185810604\n",
      "Log Regression(257/999): loss=0.5715498493995459\n",
      "Log Regression(258/999): loss=0.5579888269116201\n",
      "Log Regression(259/999): loss=0.5668397814547645\n",
      "Log Regression(260/999): loss=0.561888393966926\n",
      "Log Regression(261/999): loss=0.5575374844689386\n",
      "Log Regression(262/999): loss=0.5614483641037656\n",
      "Log Regression(263/999): loss=0.5569808691013921\n",
      "Log Regression(264/999): loss=0.5571339967734618\n",
      "Log Regression(265/999): loss=0.5565743667861683\n",
      "Log Regression(266/999): loss=0.5573165516056376\n",
      "Log Regression(267/999): loss=0.5593622344696437\n",
      "Log Regression(268/999): loss=0.562275620812841\n",
      "Log Regression(269/999): loss=0.556723427835113\n",
      "Log Regression(270/999): loss=0.5608256635377314\n",
      "Log Regression(271/999): loss=0.5658725353798408\n",
      "Log Regression(272/999): loss=0.5559653152827226\n",
      "Log Regression(273/999): loss=0.556704765438135\n",
      "Log Regression(274/999): loss=0.5616427193615223\n",
      "Log Regression(275/999): loss=0.5582976403560055\n",
      "Log Regression(276/999): loss=0.5699631084196419\n",
      "Log Regression(277/999): loss=0.5551639532723646\n",
      "Log Regression(278/999): loss=0.5557111314813322\n",
      "Log Regression(279/999): loss=0.555776564232058\n",
      "Log Regression(280/999): loss=0.5551406504822256\n",
      "Log Regression(281/999): loss=0.5555560853895005\n",
      "Log Regression(282/999): loss=0.5568798696158902\n",
      "Log Regression(283/999): loss=0.5729933858352947\n",
      "Log Regression(284/999): loss=0.5735483081546896\n",
      "Log Regression(285/999): loss=0.5750121096039311\n",
      "Log Regression(286/999): loss=0.5638342524440465\n",
      "Log Regression(287/999): loss=0.5545000991305726\n",
      "Log Regression(288/999): loss=0.5542047700196523\n",
      "Log Regression(289/999): loss=0.5628698886498034\n",
      "Log Regression(290/999): loss=0.5539501630231914\n",
      "Log Regression(291/999): loss=0.5626546573114768\n",
      "Log Regression(292/999): loss=0.5561111969512538\n",
      "Log Regression(293/999): loss=0.5569914769718817\n",
      "Log Regression(294/999): loss=0.5538127531263789\n",
      "Log Regression(295/999): loss=0.5550294280355477\n",
      "Log Regression(296/999): loss=0.5836205456048521\n",
      "Log Regression(297/999): loss=0.5558913695076686\n",
      "Log Regression(298/999): loss=0.5579366466308754\n",
      "Log Regression(299/999): loss=0.5539380171035173\n",
      "Log Regression(300/999): loss=0.5618858893677028\n",
      "Log Regression(301/999): loss=0.5565138307294205\n",
      "Log Regression(302/999): loss=0.5639061468488671\n",
      "Log Regression(303/999): loss=0.5546108186467397\n",
      "Log Regression(304/999): loss=0.5553880762763173\n",
      "Log Regression(305/999): loss=0.5560598620823404\n",
      "Log Regression(306/999): loss=0.5718705123325544\n",
      "Log Regression(307/999): loss=0.571438957235369\n",
      "Log Regression(308/999): loss=0.5570472793651154\n",
      "Log Regression(309/999): loss=0.5559375756728011\n",
      "Log Regression(310/999): loss=0.564491105962908\n",
      "Log Regression(311/999): loss=0.5565781614728814\n",
      "Log Regression(312/999): loss=0.5597372694378308\n",
      "Log Regression(313/999): loss=0.5637524488830933\n",
      "Log Regression(314/999): loss=0.5580466461453332\n",
      "Log Regression(315/999): loss=0.5567232417988222\n",
      "Log Regression(316/999): loss=0.551944227759044\n",
      "Log Regression(317/999): loss=0.5517775811693145\n",
      "Log Regression(318/999): loss=0.5596630918466272\n",
      "Log Regression(319/999): loss=0.5559818532805507\n",
      "Log Regression(320/999): loss=0.604319125191887\n",
      "Log Regression(321/999): loss=0.5562885657350067\n",
      "Log Regression(322/999): loss=0.5528246424846345\n",
      "Log Regression(323/999): loss=0.5665233151421168\n",
      "Log Regression(324/999): loss=0.5587302891879755\n",
      "Log Regression(325/999): loss=0.5515414329130016\n",
      "Log Regression(326/999): loss=0.5511512160519905\n",
      "Log Regression(327/999): loss=0.551623437055306\n",
      "Log Regression(328/999): loss=0.550960092980999\n",
      "Log Regression(329/999): loss=0.5523547159781734\n",
      "Log Regression(330/999): loss=0.5507914348470224\n",
      "Log Regression(331/999): loss=0.5507113154154591\n",
      "Log Regression(332/999): loss=0.5556573429831762\n",
      "Log Regression(333/999): loss=0.5522259514157571\n",
      "Log Regression(334/999): loss=0.5558999201439008\n",
      "Log Regression(335/999): loss=0.5568101731892866\n",
      "Log Regression(336/999): loss=0.5866901081877793\n",
      "Log Regression(337/999): loss=0.60707525993992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(338/999): loss=0.5507694166378119\n",
      "Log Regression(339/999): loss=0.5529003783780055\n",
      "Log Regression(340/999): loss=0.5522306479733862\n",
      "Log Regression(341/999): loss=0.5535085384676557\n",
      "Log Regression(342/999): loss=0.5516356195404456\n",
      "Log Regression(343/999): loss=0.551980499101624\n",
      "Log Regression(344/999): loss=0.5656792345289345\n",
      "Log Regression(345/999): loss=0.5751878511371592\n",
      "Log Regression(346/999): loss=0.5506807716745037\n",
      "Log Regression(347/999): loss=0.5510272250199534\n",
      "Log Regression(348/999): loss=0.5513022909092896\n",
      "Log Regression(349/999): loss=0.5502512059005009\n",
      "Log Regression(350/999): loss=0.5591084226523675\n",
      "Log Regression(351/999): loss=0.5511578783568112\n",
      "Log Regression(352/999): loss=0.5553077436919296\n",
      "Log Regression(353/999): loss=0.5523258700441149\n",
      "Log Regression(354/999): loss=0.5571658584450149\n",
      "Log Regression(355/999): loss=0.554000776022056\n",
      "Log Regression(356/999): loss=0.5641749816349942\n",
      "Log Regression(357/999): loss=0.5633548246254152\n",
      "Log Regression(358/999): loss=0.5495646285273267\n",
      "Log Regression(359/999): loss=0.5512408943683884\n",
      "Log Regression(360/999): loss=0.5918699126163754\n",
      "Log Regression(361/999): loss=0.5517055657655965\n",
      "Log Regression(362/999): loss=0.5524690934595978\n",
      "Log Regression(363/999): loss=0.5521092912261468\n",
      "Log Regression(364/999): loss=0.5565347460195796\n",
      "Log Regression(365/999): loss=0.5609752530973457\n",
      "Log Regression(366/999): loss=0.5540479276259647\n",
      "Log Regression(367/999): loss=0.5505360868026806\n",
      "Log Regression(368/999): loss=0.5493638720327035\n",
      "Log Regression(369/999): loss=0.5495363460415548\n",
      "Log Regression(370/999): loss=0.5525361201898703\n",
      "Log Regression(371/999): loss=0.5501646971578986\n",
      "Log Regression(372/999): loss=0.5512162024564027\n",
      "Log Regression(373/999): loss=0.5510367743877852\n",
      "Log Regression(374/999): loss=0.5511722050149473\n",
      "Log Regression(375/999): loss=0.5498862103103945\n",
      "Log Regression(376/999): loss=0.558055140840679\n",
      "Log Regression(377/999): loss=0.5506857502520424\n",
      "Log Regression(378/999): loss=0.5560831660164557\n",
      "Log Regression(379/999): loss=0.5517877973903828\n",
      "Log Regression(380/999): loss=0.552867239773709\n",
      "Log Regression(381/999): loss=0.5490899141475433\n",
      "Log Regression(382/999): loss=0.5487976256736636\n",
      "Log Regression(383/999): loss=0.5615599501551791\n",
      "Log Regression(384/999): loss=0.5546668848315472\n",
      "Log Regression(385/999): loss=0.5621894598173596\n",
      "Log Regression(386/999): loss=0.5483642244522944\n",
      "Log Regression(387/999): loss=0.5525409376744095\n",
      "Log Regression(388/999): loss=0.5486505685651792\n",
      "Log Regression(389/999): loss=0.5505064241623767\n",
      "Log Regression(390/999): loss=0.5474986082079659\n",
      "Log Regression(391/999): loss=0.5473554819185035\n",
      "Log Regression(392/999): loss=0.5477578901152739\n",
      "Log Regression(393/999): loss=0.5482395154367884\n",
      "Log Regression(394/999): loss=0.5603064518427632\n",
      "Log Regression(395/999): loss=0.5508131368513354\n",
      "Log Regression(396/999): loss=0.5470604220467326\n",
      "Log Regression(397/999): loss=0.5470545315756832\n",
      "Log Regression(398/999): loss=0.5487695978151295\n",
      "Log Regression(399/999): loss=0.5493129082549563\n",
      "Log Regression(400/999): loss=0.5469006448963052\n",
      "Log Regression(401/999): loss=0.561142328146855\n",
      "Log Regression(402/999): loss=0.5469528631666994\n",
      "Log Regression(403/999): loss=0.5504715544292296\n",
      "Log Regression(404/999): loss=0.5514601606031192\n",
      "Log Regression(405/999): loss=0.5471731129718019\n",
      "Log Regression(406/999): loss=0.5567230212154255\n",
      "Log Regression(407/999): loss=0.549061295287405\n",
      "Log Regression(408/999): loss=0.5527892114468604\n",
      "Log Regression(409/999): loss=0.5474574692279957\n",
      "Log Regression(410/999): loss=0.5551245430396703\n",
      "Log Regression(411/999): loss=0.5478854922083304\n",
      "Log Regression(412/999): loss=0.5604342101252945\n",
      "Log Regression(413/999): loss=0.5478394917550381\n",
      "Log Regression(414/999): loss=0.5526758061210025\n",
      "Log Regression(415/999): loss=0.5508149967154264\n",
      "Log Regression(416/999): loss=0.5516160717789202\n",
      "Log Regression(417/999): loss=0.5477783258642629\n",
      "Log Regression(418/999): loss=0.5470048240283978\n",
      "Log Regression(419/999): loss=0.5473473330061436\n",
      "Log Regression(420/999): loss=0.548646844159823\n",
      "Log Regression(421/999): loss=0.5468158730675163\n",
      "Log Regression(422/999): loss=0.5480377647215704\n",
      "Log Regression(423/999): loss=0.559114613148306\n",
      "Log Regression(424/999): loss=0.5643429348034308\n",
      "Log Regression(425/999): loss=0.5556924831539645\n",
      "Log Regression(426/999): loss=0.5468463275478782\n",
      "Log Regression(427/999): loss=0.5459382524274123\n",
      "Log Regression(428/999): loss=0.5465143939093304\n",
      "Log Regression(429/999): loss=0.5465718590880412\n",
      "Log Regression(430/999): loss=0.5460650990890794\n",
      "Log Regression(431/999): loss=0.5496611377509182\n",
      "Log Regression(432/999): loss=0.5468736097438802\n",
      "Log Regression(433/999): loss=0.5487462870706087\n",
      "Log Regression(434/999): loss=0.5466813555058635\n",
      "Log Regression(435/999): loss=0.5456374395701272\n",
      "Log Regression(436/999): loss=0.5528629000792398\n",
      "Log Regression(437/999): loss=0.5459759473952649\n",
      "Log Regression(438/999): loss=0.5456049712762758\n",
      "Log Regression(439/999): loss=0.5455331675639278\n",
      "Log Regression(440/999): loss=0.5488438557269247\n",
      "Log Regression(441/999): loss=0.548465197127936\n",
      "Log Regression(442/999): loss=0.5479024721429441\n",
      "Log Regression(443/999): loss=0.547132425101632\n",
      "Log Regression(444/999): loss=0.547563537344703\n",
      "Log Regression(445/999): loss=0.5464667218600429\n",
      "Log Regression(446/999): loss=0.5455416470272652\n",
      "Log Regression(447/999): loss=0.5449524297035885\n",
      "Log Regression(448/999): loss=0.550429362988251\n",
      "Log Regression(449/999): loss=0.546442882261094\n",
      "Log Regression(450/999): loss=0.549747480278952\n",
      "Log Regression(451/999): loss=0.5483062373124663\n",
      "Log Regression(452/999): loss=0.565320663713159\n",
      "Log Regression(453/999): loss=0.5508663707891696\n",
      "Log Regression(454/999): loss=0.5458093323240952\n",
      "Log Regression(455/999): loss=0.5459383148923173\n",
      "Log Regression(456/999): loss=0.5544987797585317\n",
      "Log Regression(457/999): loss=0.5448647697223734\n",
      "Log Regression(458/999): loss=0.5457270105168159\n",
      "Log Regression(459/999): loss=0.5449850941842946\n",
      "Log Regression(460/999): loss=0.5448959549273205\n",
      "Log Regression(461/999): loss=0.5477401043051837\n",
      "Log Regression(462/999): loss=0.5489773773045498\n",
      "Log Regression(463/999): loss=0.5492410944169644\n",
      "Log Regression(464/999): loss=0.5475834963879929\n",
      "Log Regression(465/999): loss=0.5502050188737534\n",
      "Log Regression(466/999): loss=0.553287587850988\n",
      "Log Regression(467/999): loss=0.5445050074119063\n",
      "Log Regression(468/999): loss=0.5467979716154904\n",
      "Log Regression(469/999): loss=0.5489144533069548\n",
      "Log Regression(470/999): loss=0.5445000388169984\n",
      "Log Regression(471/999): loss=0.5444260567365775\n",
      "Log Regression(472/999): loss=0.5456212505116869\n",
      "Log Regression(473/999): loss=0.5454543270632188\n",
      "Log Regression(474/999): loss=0.5522332276536872\n",
      "Log Regression(475/999): loss=0.5502756863040412\n",
      "Log Regression(476/999): loss=0.5453747172943326\n",
      "Log Regression(477/999): loss=0.5456629390194542\n",
      "Log Regression(478/999): loss=0.5476717041417813\n",
      "Log Regression(479/999): loss=0.5454067489753411\n",
      "Log Regression(480/999): loss=0.5461702563468194\n",
      "Log Regression(481/999): loss=0.5458555808449276\n",
      "Log Regression(482/999): loss=0.5462595533609583\n",
      "Log Regression(483/999): loss=0.5493492932844868\n",
      "Log Regression(484/999): loss=0.5455303152621924\n",
      "Log Regression(485/999): loss=0.5470107524810476\n",
      "Log Regression(486/999): loss=0.5463144527083402\n",
      "Log Regression(487/999): loss=0.5454303164224543\n",
      "Log Regression(488/999): loss=0.545241326486941\n",
      "Log Regression(489/999): loss=0.5660455534752209\n",
      "Log Regression(490/999): loss=0.5534617628217344\n",
      "Log Regression(491/999): loss=0.5452999658741574\n",
      "Log Regression(492/999): loss=0.5489468661755887\n",
      "Log Regression(493/999): loss=0.5488307887547735\n",
      "Log Regression(494/999): loss=0.5458699741494946\n",
      "Log Regression(495/999): loss=0.5437192672528677\n",
      "Log Regression(496/999): loss=0.5451761869296201\n",
      "Log Regression(497/999): loss=0.5510988431801142\n",
      "Log Regression(498/999): loss=0.5459524104696738\n",
      "Log Regression(499/999): loss=0.5477348335382979\n",
      "Log Regression(500/999): loss=0.5512673174081842\n",
      "Log Regression(501/999): loss=0.5478610195322969\n",
      "Log Regression(502/999): loss=0.5477496408225762\n",
      "Log Regression(503/999): loss=0.5547088606211207\n",
      "Log Regression(504/999): loss=0.5748613996646391\n",
      "Log Regression(505/999): loss=0.5474336150501551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(506/999): loss=0.5604156682213456\n",
      "Log Regression(507/999): loss=0.5439440019308672\n",
      "Log Regression(508/999): loss=0.5456442473149347\n",
      "Log Regression(509/999): loss=0.5580595975377055\n",
      "Log Regression(510/999): loss=0.5476082132709089\n",
      "Log Regression(511/999): loss=0.5436789007672072\n",
      "Log Regression(512/999): loss=0.543491752256352\n",
      "Log Regression(513/999): loss=0.5646121768787361\n",
      "Log Regression(514/999): loss=0.5447078496614186\n",
      "Log Regression(515/999): loss=0.5436576973476529\n",
      "Log Regression(516/999): loss=0.5442288206391822\n",
      "Log Regression(517/999): loss=0.5517763871436627\n",
      "Log Regression(518/999): loss=0.5444760411240853\n",
      "Log Regression(519/999): loss=0.5433443334122119\n",
      "Log Regression(520/999): loss=0.543841158687368\n",
      "Log Regression(521/999): loss=0.5464316021983273\n",
      "Log Regression(522/999): loss=0.5463025816565533\n",
      "Log Regression(523/999): loss=0.5432956873008453\n",
      "Log Regression(524/999): loss=0.5496647339000572\n",
      "Log Regression(525/999): loss=0.543810480195864\n",
      "Log Regression(526/999): loss=0.5443378089072136\n",
      "Log Regression(527/999): loss=0.5435711607177658\n",
      "Log Regression(528/999): loss=0.5427293527917665\n",
      "Log Regression(529/999): loss=0.5436091566486109\n",
      "Log Regression(530/999): loss=0.5464260120799812\n",
      "Log Regression(531/999): loss=0.5439240753300242\n",
      "Log Regression(532/999): loss=0.548756767597371\n",
      "Log Regression(533/999): loss=0.5428031639926466\n",
      "Log Regression(534/999): loss=0.542679470212931\n",
      "Log Regression(535/999): loss=0.543713279712701\n",
      "Log Regression(536/999): loss=0.5432682033035047\n",
      "Log Regression(537/999): loss=0.5480983290173382\n",
      "Log Regression(538/999): loss=0.5540360735642713\n",
      "Log Regression(539/999): loss=0.5453169194842671\n",
      "Log Regression(540/999): loss=0.5442012374662889\n",
      "Log Regression(541/999): loss=0.5426043090364444\n",
      "Log Regression(542/999): loss=0.5470206882151789\n",
      "Log Regression(543/999): loss=0.543302514602849\n",
      "Log Regression(544/999): loss=0.5428101214683597\n",
      "Log Regression(545/999): loss=0.5440031723051266\n",
      "Log Regression(546/999): loss=0.5454170603571704\n",
      "Log Regression(547/999): loss=0.5495573321081826\n",
      "Log Regression(548/999): loss=0.5492516758277369\n",
      "Log Regression(549/999): loss=0.5444597236250557\n",
      "Log Regression(550/999): loss=0.5445326590807654\n",
      "Log Regression(551/999): loss=0.5436946147254759\n",
      "Log Regression(552/999): loss=0.5430864274796773\n",
      "Log Regression(553/999): loss=0.5463011293861068\n",
      "Log Regression(554/999): loss=0.5448346259404658\n",
      "Log Regression(555/999): loss=0.5460155249611827\n",
      "Log Regression(556/999): loss=0.5452780690760879\n",
      "Log Regression(557/999): loss=0.5448232416008087\n",
      "Log Regression(558/999): loss=0.5511280027479627\n",
      "Log Regression(559/999): loss=0.5426272277522619\n",
      "Log Regression(560/999): loss=0.5451634429559985\n",
      "Log Regression(561/999): loss=0.5420326811329552\n",
      "Log Regression(562/999): loss=0.5468217287898204\n",
      "Log Regression(563/999): loss=0.5468392340450126\n",
      "Log Regression(564/999): loss=0.5468316072566836\n",
      "Log Regression(565/999): loss=0.5581944517667067\n",
      "Log Regression(566/999): loss=0.5444886079710464\n",
      "Log Regression(567/999): loss=0.5530240258638939\n",
      "Log Regression(568/999): loss=0.5426505960056796\n",
      "Log Regression(569/999): loss=0.5438276028807494\n",
      "Log Regression(570/999): loss=0.5421243303192604\n",
      "Log Regression(571/999): loss=0.5418981479930722\n",
      "Log Regression(572/999): loss=0.5482999717536677\n",
      "Log Regression(573/999): loss=0.5558104659202161\n",
      "Log Regression(574/999): loss=0.5424219726520436\n",
      "Log Regression(575/999): loss=0.5437714437742045\n",
      "Log Regression(576/999): loss=0.5488424296401937\n",
      "Log Regression(577/999): loss=0.5428173648273933\n",
      "Log Regression(578/999): loss=0.5417520832229435\n",
      "Log Regression(579/999): loss=0.5437293157516963\n",
      "Log Regression(580/999): loss=0.5629064082041965\n",
      "Log Regression(581/999): loss=0.5689257024911338\n",
      "Log Regression(582/999): loss=0.5434557144254603\n",
      "Log Regression(583/999): loss=0.5449043868505838\n",
      "Log Regression(584/999): loss=0.5423122898124592\n",
      "Log Regression(585/999): loss=0.5422958704327638\n",
      "Log Regression(586/999): loss=0.5498897411128135\n",
      "Log Regression(587/999): loss=0.5598249421680821\n",
      "Log Regression(588/999): loss=0.542764344579284\n",
      "Log Regression(589/999): loss=0.5438949539663066\n",
      "Log Regression(590/999): loss=0.542724641514558\n",
      "Log Regression(591/999): loss=0.5496012268515157\n",
      "Log Regression(592/999): loss=0.5433797016122268\n",
      "Log Regression(593/999): loss=0.5431307769294638\n",
      "Log Regression(594/999): loss=0.5447314393601822\n",
      "Log Regression(595/999): loss=0.5423994840240391\n",
      "Log Regression(596/999): loss=0.5568692962106551\n",
      "Log Regression(597/999): loss=0.5496136725131217\n",
      "Log Regression(598/999): loss=0.5465690284311523\n",
      "Log Regression(599/999): loss=0.5417889996299785\n",
      "Log Regression(600/999): loss=0.5416542403069649\n",
      "Log Regression(601/999): loss=0.5444704947058712\n",
      "Log Regression(602/999): loss=0.5458369328848212\n",
      "Log Regression(603/999): loss=0.5414658142195702\n",
      "Log Regression(604/999): loss=0.547083518776887\n",
      "Log Regression(605/999): loss=0.5441865351103904\n",
      "Log Regression(606/999): loss=0.5429578716694372\n",
      "Log Regression(607/999): loss=0.5464934742047782\n",
      "Log Regression(608/999): loss=0.542939046305887\n",
      "Log Regression(609/999): loss=0.5448236682593676\n",
      "Log Regression(610/999): loss=0.5458372077803052\n",
      "Log Regression(611/999): loss=0.5444400068913868\n",
      "Log Regression(612/999): loss=0.549481008798064\n",
      "Log Regression(613/999): loss=0.5434254091663799\n",
      "Log Regression(614/999): loss=0.5452946533207988\n",
      "Log Regression(615/999): loss=0.5435042073794756\n",
      "Log Regression(616/999): loss=0.5439296097906127\n",
      "Log Regression(617/999): loss=0.54316762704496\n",
      "Log Regression(618/999): loss=0.5431079392066626\n",
      "Log Regression(619/999): loss=0.5467071064125737\n",
      "Log Regression(620/999): loss=0.5432903346917535\n",
      "Log Regression(621/999): loss=0.553232558246558\n",
      "Log Regression(622/999): loss=0.5421764901203355\n",
      "Log Regression(623/999): loss=0.5420659175628064\n",
      "Log Regression(624/999): loss=0.5416729336291365\n",
      "Log Regression(625/999): loss=0.5435121654753857\n",
      "Log Regression(626/999): loss=0.5415966611520168\n",
      "Log Regression(627/999): loss=0.5568234858361528\n",
      "Log Regression(628/999): loss=0.5411229411063322\n",
      "Log Regression(629/999): loss=0.5413893208264124\n",
      "Log Regression(630/999): loss=0.5432880979418218\n",
      "Log Regression(631/999): loss=0.543977485600371\n",
      "Log Regression(632/999): loss=0.541534864297667\n",
      "Log Regression(633/999): loss=0.5412255654514191\n",
      "Log Regression(634/999): loss=0.5416482532020946\n",
      "Log Regression(635/999): loss=0.5434790511158\n",
      "Log Regression(636/999): loss=0.5433990143770365\n",
      "Log Regression(637/999): loss=0.5440754613122888\n",
      "Log Regression(638/999): loss=0.5496769669045795\n",
      "Log Regression(639/999): loss=0.541762579944455\n",
      "Log Regression(640/999): loss=0.5477591880808588\n",
      "Log Regression(641/999): loss=0.5418096723960362\n",
      "Log Regression(642/999): loss=0.5414261031426276\n",
      "Log Regression(643/999): loss=0.5458565176991921\n",
      "Log Regression(644/999): loss=0.540644420610334\n",
      "Log Regression(645/999): loss=0.5459686990382724\n",
      "Log Regression(646/999): loss=0.5404876661256462\n",
      "Log Regression(647/999): loss=0.5424426911488237\n",
      "Log Regression(648/999): loss=0.5447160550530828\n",
      "Log Regression(649/999): loss=0.5419408628228563\n",
      "Log Regression(650/999): loss=0.5499610816574574\n",
      "Log Regression(651/999): loss=0.5432894908652443\n",
      "Log Regression(652/999): loss=0.5407989070500044\n",
      "Log Regression(653/999): loss=0.5433750963229809\n",
      "Log Regression(654/999): loss=0.5414393080764848\n",
      "Log Regression(655/999): loss=0.5490375322849069\n",
      "Log Regression(656/999): loss=0.5411558793411505\n",
      "Log Regression(657/999): loss=0.541119431766715\n",
      "Log Regression(658/999): loss=0.5417843575182479\n",
      "Log Regression(659/999): loss=0.541132099677415\n",
      "Log Regression(660/999): loss=0.5416693702727773\n",
      "Log Regression(661/999): loss=0.5426948447809179\n",
      "Log Regression(662/999): loss=0.5503037551033407\n",
      "Log Regression(663/999): loss=0.5524820116239274\n",
      "Log Regression(664/999): loss=0.5425555428179136\n",
      "Log Regression(665/999): loss=0.5414477815208472\n",
      "Log Regression(666/999): loss=0.5406799058627487\n",
      "Log Regression(667/999): loss=0.5419956630164923\n",
      "Log Regression(668/999): loss=0.5413912979451349\n",
      "Log Regression(669/999): loss=0.5405688674187491\n",
      "Log Regression(670/999): loss=0.5424787942164091\n",
      "Log Regression(671/999): loss=0.5494693145288939\n",
      "Log Regression(672/999): loss=0.5499078134550717\n",
      "Log Regression(673/999): loss=0.5439246774101275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(674/999): loss=0.5428736767007154\n",
      "Log Regression(675/999): loss=0.551038394278235\n",
      "Log Regression(676/999): loss=0.5516699626371613\n",
      "Log Regression(677/999): loss=0.5409041826971458\n",
      "Log Regression(678/999): loss=0.5445383818937107\n",
      "Log Regression(679/999): loss=0.5405644974128889\n",
      "Log Regression(680/999): loss=0.5521384626566209\n",
      "Log Regression(681/999): loss=0.54039874981917\n",
      "Log Regression(682/999): loss=0.5409340254443054\n",
      "Log Regression(683/999): loss=0.5407752946696782\n",
      "Log Regression(684/999): loss=0.5451025401595405\n",
      "Log Regression(685/999): loss=0.5430722208924603\n",
      "Log Regression(686/999): loss=0.5527040920235816\n",
      "Log Regression(687/999): loss=0.5503944723286819\n",
      "Log Regression(688/999): loss=0.5473269969324154\n",
      "Log Regression(689/999): loss=0.541712452018906\n",
      "Log Regression(690/999): loss=0.5696246244597589\n",
      "Log Regression(691/999): loss=0.5487913074491185\n",
      "Log Regression(692/999): loss=0.541791507935627\n",
      "Log Regression(693/999): loss=0.5471340422909883\n",
      "Log Regression(694/999): loss=0.5494441040306711\n",
      "Log Regression(695/999): loss=0.565345813757983\n",
      "Log Regression(696/999): loss=0.5437215231382547\n",
      "Log Regression(697/999): loss=0.5497115514998362\n",
      "Log Regression(698/999): loss=0.5421736848127036\n",
      "Log Regression(699/999): loss=0.5416010810976517\n",
      "Log Regression(700/999): loss=0.5414318619000512\n",
      "Log Regression(701/999): loss=0.5412675805967923\n",
      "Log Regression(702/999): loss=0.5492515933742644\n",
      "Log Regression(703/999): loss=0.5425112090294977\n",
      "Log Regression(704/999): loss=0.559371805679583\n",
      "Log Regression(705/999): loss=0.540644359008574\n",
      "Log Regression(706/999): loss=0.5406685003374899\n",
      "Log Regression(707/999): loss=0.5411121412043376\n",
      "Log Regression(708/999): loss=0.5422842203263076\n",
      "Log Regression(709/999): loss=0.5416192202860861\n",
      "Log Regression(710/999): loss=0.5407556858166662\n",
      "Log Regression(711/999): loss=0.5409218418716628\n",
      "Log Regression(712/999): loss=0.5440213151505137\n",
      "Log Regression(713/999): loss=0.5433913770609673\n",
      "Log Regression(714/999): loss=0.5412147825693189\n",
      "Log Regression(715/999): loss=0.541139792520153\n",
      "Log Regression(716/999): loss=0.5469951269286975\n",
      "Log Regression(717/999): loss=0.5412116097677868\n",
      "Log Regression(718/999): loss=0.5396937073129477\n",
      "Log Regression(719/999): loss=0.5663992950166364\n",
      "Log Regression(720/999): loss=0.5501844783886244\n",
      "Log Regression(721/999): loss=0.5412864764338615\n",
      "Log Regression(722/999): loss=0.5403903937804352\n",
      "Log Regression(723/999): loss=0.5602832471207163\n",
      "Log Regression(724/999): loss=0.5429030103844908\n",
      "Log Regression(725/999): loss=0.540861652375316\n",
      "Log Regression(726/999): loss=0.5402799926505764\n",
      "Log Regression(727/999): loss=0.5424160899966829\n",
      "Log Regression(728/999): loss=0.5435536838184885\n",
      "Log Regression(729/999): loss=0.5705370890806633\n",
      "Log Regression(730/999): loss=0.5400144837430242\n",
      "Log Regression(731/999): loss=0.5554869543901404\n",
      "Log Regression(732/999): loss=0.5485281991605624\n",
      "Log Regression(733/999): loss=0.5410420843531661\n",
      "Log Regression(734/999): loss=0.5400288241679032\n",
      "Log Regression(735/999): loss=0.5445011069304637\n",
      "Log Regression(736/999): loss=0.5511733799576058\n",
      "Log Regression(737/999): loss=0.5524935551167548\n",
      "Log Regression(738/999): loss=0.5553302062852769\n",
      "Log Regression(739/999): loss=0.5395786591377314\n",
      "Log Regression(740/999): loss=0.5413886356105562\n",
      "Log Regression(741/999): loss=0.5431340618196326\n",
      "Log Regression(742/999): loss=0.5406018584948635\n",
      "Log Regression(743/999): loss=0.5418245874925475\n",
      "Log Regression(744/999): loss=0.5511936391721621\n",
      "Log Regression(745/999): loss=0.5402945343058082\n",
      "Log Regression(746/999): loss=0.5395812863975886\n",
      "Log Regression(747/999): loss=0.5446714912778909\n",
      "Log Regression(748/999): loss=0.5439163527522332\n",
      "Log Regression(749/999): loss=0.5407601714008158\n",
      "Log Regression(750/999): loss=0.5523121487249181\n",
      "Log Regression(751/999): loss=0.5402377241540314\n",
      "Log Regression(752/999): loss=0.5417695895678771\n",
      "Log Regression(753/999): loss=0.5421936385193187\n",
      "Log Regression(754/999): loss=0.5409044299040734\n",
      "Log Regression(755/999): loss=0.5631964002287418\n",
      "Log Regression(756/999): loss=0.542335132372769\n",
      "Log Regression(757/999): loss=0.5416268904434752\n",
      "Log Regression(758/999): loss=0.5406055250166452\n",
      "Log Regression(759/999): loss=0.5408976107318993\n",
      "Log Regression(760/999): loss=0.5449085421452131\n",
      "Log Regression(761/999): loss=0.5462364365919318\n",
      "Log Regression(762/999): loss=0.5456684935094264\n",
      "Log Regression(763/999): loss=0.5426676020310374\n",
      "Log Regression(764/999): loss=0.5449595395870357\n",
      "Log Regression(765/999): loss=0.5404593972360615\n",
      "Log Regression(766/999): loss=0.5400123752763487\n",
      "Log Regression(767/999): loss=0.5428802244136922\n",
      "Log Regression(768/999): loss=0.5396343992193436\n",
      "Log Regression(769/999): loss=0.5429696354328086\n",
      "Log Regression(770/999): loss=0.5410855028525328\n",
      "Log Regression(771/999): loss=0.5502781138654623\n",
      "Log Regression(772/999): loss=0.5409942394858958\n",
      "Log Regression(773/999): loss=0.5464195098288439\n",
      "Log Regression(774/999): loss=0.5456555863051249\n",
      "Log Regression(775/999): loss=0.5594532710971032\n",
      "Log Regression(776/999): loss=0.5411847664203843\n",
      "Log Regression(777/999): loss=0.558072186234006\n",
      "Log Regression(778/999): loss=0.5435873223823392\n",
      "Log Regression(779/999): loss=0.5398456335432619\n",
      "Log Regression(780/999): loss=0.5396698106407636\n",
      "Log Regression(781/999): loss=0.5397521171717884\n",
      "Log Regression(782/999): loss=0.5397287988510839\n",
      "Log Regression(783/999): loss=0.5393478704757123\n",
      "Log Regression(784/999): loss=0.5616430716299012\n",
      "Log Regression(785/999): loss=0.5422151998974228\n",
      "Log Regression(786/999): loss=0.5399772859845816\n",
      "Log Regression(787/999): loss=0.539878849536583\n",
      "Log Regression(788/999): loss=0.5431782329847052\n",
      "Log Regression(789/999): loss=0.5402909309004593\n",
      "Log Regression(790/999): loss=0.5452393626007204\n",
      "Log Regression(791/999): loss=0.539412251608462\n",
      "Log Regression(792/999): loss=0.5397816542349823\n",
      "Log Regression(793/999): loss=0.5393157087781444\n",
      "Log Regression(794/999): loss=0.5421033930919007\n",
      "Log Regression(795/999): loss=0.5392201252832078\n",
      "Log Regression(796/999): loss=0.5397659958994694\n",
      "Log Regression(797/999): loss=0.539605341406937\n",
      "Log Regression(798/999): loss=0.5407993361134804\n",
      "Log Regression(799/999): loss=0.5413818312171201\n",
      "Log Regression(800/999): loss=0.5402358729813755\n",
      "Log Regression(801/999): loss=0.5391434591469194\n",
      "Log Regression(802/999): loss=0.5391262629979657\n",
      "Log Regression(803/999): loss=0.5396566317290483\n",
      "Log Regression(804/999): loss=0.5397605758233583\n",
      "Log Regression(805/999): loss=0.5391364438848271\n",
      "Log Regression(806/999): loss=0.5396823842401652\n",
      "Log Regression(807/999): loss=0.5391438402713856\n",
      "Log Regression(808/999): loss=0.539370554969016\n",
      "Log Regression(809/999): loss=0.5391701331292427\n",
      "Log Regression(810/999): loss=0.5434403210275806\n",
      "Log Regression(811/999): loss=0.5414513330965369\n",
      "Log Regression(812/999): loss=0.5391878163984481\n",
      "Log Regression(813/999): loss=0.5392064719056592\n",
      "Log Regression(814/999): loss=0.5396190962125582\n",
      "Log Regression(815/999): loss=0.5465554687460105\n",
      "Log Regression(816/999): loss=0.5404506234763158\n",
      "Log Regression(817/999): loss=0.5397875234327629\n",
      "Log Regression(818/999): loss=0.5392520949831666\n",
      "Log Regression(819/999): loss=0.5391298644732172\n",
      "Log Regression(820/999): loss=0.5392096949463061\n",
      "Log Regression(821/999): loss=0.5397111138958681\n",
      "Log Regression(822/999): loss=0.5405802444587176\n",
      "Log Regression(823/999): loss=0.5390851991738427\n",
      "Log Regression(824/999): loss=0.5418635642974432\n",
      "Log Regression(825/999): loss=0.543460068797871\n",
      "Log Regression(826/999): loss=0.5429374401314805\n",
      "Log Regression(827/999): loss=0.5391409780028412\n",
      "Log Regression(828/999): loss=0.5396499289591487\n",
      "Log Regression(829/999): loss=0.556258024265767\n",
      "Log Regression(830/999): loss=0.5527780543377703\n",
      "Log Regression(831/999): loss=0.5493581882322215\n",
      "Log Regression(832/999): loss=0.541220837987145\n",
      "Log Regression(833/999): loss=0.5486119125901431\n",
      "Log Regression(834/999): loss=0.5397591305384126\n",
      "Log Regression(835/999): loss=0.5403510357006491\n",
      "Log Regression(836/999): loss=0.5393405067032546\n",
      "Log Regression(837/999): loss=0.5419694434715159\n",
      "Log Regression(838/999): loss=0.5389119940512757\n",
      "Log Regression(839/999): loss=0.5449481520271182\n",
      "Log Regression(840/999): loss=0.5400192312666587\n",
      "Log Regression(841/999): loss=0.5388552746002403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(842/999): loss=0.5389072391609611\n",
      "Log Regression(843/999): loss=0.5405467987482644\n",
      "Log Regression(844/999): loss=0.5467330514441289\n",
      "Log Regression(845/999): loss=0.538877098388668\n",
      "Log Regression(846/999): loss=0.5401601630839546\n",
      "Log Regression(847/999): loss=0.5459100567189282\n",
      "Log Regression(848/999): loss=0.5596430998786941\n",
      "Log Regression(849/999): loss=0.5415876190572702\n",
      "Log Regression(850/999): loss=0.5423994656876077\n",
      "Log Regression(851/999): loss=0.5524695358459355\n",
      "Log Regression(852/999): loss=0.5524755180429668\n",
      "Log Regression(853/999): loss=0.5445148082996143\n",
      "Log Regression(854/999): loss=0.5413455899699575\n",
      "Log Regression(855/999): loss=0.5395703366310561\n",
      "Log Regression(856/999): loss=0.5398988266518367\n",
      "Log Regression(857/999): loss=0.5437558541888022\n",
      "Log Regression(858/999): loss=0.5389673090269185\n",
      "Log Regression(859/999): loss=0.5389993746099844\n",
      "Log Regression(860/999): loss=0.5397761052756934\n",
      "Log Regression(861/999): loss=0.549665323240502\n",
      "Log Regression(862/999): loss=0.5394461213980002\n",
      "Log Regression(863/999): loss=0.5438493330797163\n",
      "Log Regression(864/999): loss=0.540497349060337\n",
      "Log Regression(865/999): loss=0.5395719730052068\n",
      "Log Regression(866/999): loss=0.5394679124155883\n",
      "Log Regression(867/999): loss=0.5427819926835018\n",
      "Log Regression(868/999): loss=0.5543675482084961\n",
      "Log Regression(869/999): loss=0.539036824106121\n",
      "Log Regression(870/999): loss=0.539635784665401\n",
      "Log Regression(871/999): loss=0.5402508880214226\n",
      "Log Regression(872/999): loss=0.5397305092493065\n",
      "Log Regression(873/999): loss=0.5422121449054182\n",
      "Log Regression(874/999): loss=0.5415580241551805\n",
      "Log Regression(875/999): loss=0.5392426018473259\n",
      "Log Regression(876/999): loss=0.5390970013091201\n",
      "Log Regression(877/999): loss=0.5460426190207767\n",
      "Log Regression(878/999): loss=0.5536005060918696\n",
      "Log Regression(879/999): loss=0.5421617890754316\n",
      "Log Regression(880/999): loss=0.539506951700115\n",
      "Log Regression(881/999): loss=0.5410576144342168\n",
      "Log Regression(882/999): loss=0.5443233564451857\n",
      "Log Regression(883/999): loss=0.5404407289312625\n",
      "Log Regression(884/999): loss=0.5393487247346415\n",
      "Log Regression(885/999): loss=0.5393148228654\n",
      "Log Regression(886/999): loss=0.5395253062174582\n",
      "Log Regression(887/999): loss=0.5403181445816393\n",
      "Log Regression(888/999): loss=0.5417021924500893\n",
      "Log Regression(889/999): loss=0.5400427397025805\n",
      "Log Regression(890/999): loss=0.5385694202298417\n",
      "Log Regression(891/999): loss=0.538829011401907\n",
      "Log Regression(892/999): loss=0.5422257318285119\n",
      "Log Regression(893/999): loss=0.5393088743486495\n",
      "Log Regression(894/999): loss=0.5384945215197486\n",
      "Log Regression(895/999): loss=0.5387194683591036\n",
      "Log Regression(896/999): loss=0.5387843539595527\n",
      "Log Regression(897/999): loss=0.5389986053178141\n",
      "Log Regression(898/999): loss=0.5548262478376395\n",
      "Log Regression(899/999): loss=0.5461476607146393\n",
      "Log Regression(900/999): loss=0.5404544005118174\n",
      "Log Regression(901/999): loss=0.5417846985773176\n",
      "Log Regression(902/999): loss=0.5390826929493724\n",
      "Log Regression(903/999): loss=0.5402543399905421\n",
      "Log Regression(904/999): loss=0.538578372925337\n",
      "Log Regression(905/999): loss=0.5416845661592715\n",
      "Log Regression(906/999): loss=0.5412592422881153\n",
      "Log Regression(907/999): loss=0.5430564811173162\n",
      "Log Regression(908/999): loss=0.5432024988766537\n",
      "Log Regression(909/999): loss=0.5384964366350543\n",
      "Log Regression(910/999): loss=0.5395599058125313\n",
      "Log Regression(911/999): loss=0.5388195784530411\n",
      "Log Regression(912/999): loss=0.5500844496851653\n",
      "Log Regression(913/999): loss=0.547398614600833\n",
      "Log Regression(914/999): loss=0.5562589368241304\n",
      "Log Regression(915/999): loss=0.5418053778354928\n",
      "Log Regression(916/999): loss=0.5390946657722917\n",
      "Log Regression(917/999): loss=0.5397783685425788\n",
      "Log Regression(918/999): loss=0.5422791222892362\n",
      "Log Regression(919/999): loss=0.5390672888232055\n",
      "Log Regression(920/999): loss=0.5392947777475241\n",
      "Log Regression(921/999): loss=0.5394809158016852\n",
      "Log Regression(922/999): loss=0.542187888411709\n",
      "Log Regression(923/999): loss=0.5383423095158232\n",
      "Log Regression(924/999): loss=0.5383669762613861\n",
      "Log Regression(925/999): loss=0.5493585557877139\n",
      "Log Regression(926/999): loss=0.5406245202114598\n",
      "Log Regression(927/999): loss=0.5384895391443424\n",
      "Log Regression(928/999): loss=0.5395365697975557\n",
      "Log Regression(929/999): loss=0.5438907369932188\n",
      "Log Regression(930/999): loss=0.5406009383355045\n",
      "Log Regression(931/999): loss=0.5383969511026511\n",
      "Log Regression(932/999): loss=0.5400444131252121\n",
      "Log Regression(933/999): loss=0.5457273670598913\n",
      "Log Regression(934/999): loss=0.5419265988007522\n",
      "Log Regression(935/999): loss=0.5417588075550159\n",
      "Log Regression(936/999): loss=0.5406619684050659\n",
      "Log Regression(937/999): loss=0.5393810492325377\n",
      "Log Regression(938/999): loss=0.5407218932406168\n",
      "Log Regression(939/999): loss=0.5388723632230987\n",
      "Log Regression(940/999): loss=0.5387718954173218\n",
      "Log Regression(941/999): loss=0.5413047083626811\n",
      "Log Regression(942/999): loss=0.538392416411645\n",
      "Log Regression(943/999): loss=0.5422717362505597\n",
      "Log Regression(944/999): loss=0.5381963152200243\n",
      "Log Regression(945/999): loss=0.5450749127866603\n",
      "Log Regression(946/999): loss=0.5415563386289958\n",
      "Log Regression(947/999): loss=0.5424530333801726\n",
      "Log Regression(948/999): loss=0.5384709135609036\n",
      "Log Regression(949/999): loss=0.5395941420186297\n",
      "Log Regression(950/999): loss=0.5454159870158005\n",
      "Log Regression(951/999): loss=0.5392720587366522\n",
      "Log Regression(952/999): loss=0.5597722567239567\n",
      "Log Regression(953/999): loss=0.5395566983244431\n",
      "Log Regression(954/999): loss=0.5405238253724031\n",
      "Log Regression(955/999): loss=0.5394064761396015\n",
      "Log Regression(956/999): loss=0.5386181571007469\n",
      "Log Regression(957/999): loss=0.5386755013818099\n",
      "Log Regression(958/999): loss=0.5382718438376837\n",
      "Log Regression(959/999): loss=0.5457652926333143\n",
      "Log Regression(960/999): loss=0.5403731690776384\n",
      "Log Regression(961/999): loss=0.5438140559074631\n",
      "Log Regression(962/999): loss=0.540494002692039\n",
      "Log Regression(963/999): loss=0.5381732871345483\n",
      "Log Regression(964/999): loss=0.5387097485183282\n",
      "Log Regression(965/999): loss=0.5417133428983318\n",
      "Log Regression(966/999): loss=0.5396472932025442\n",
      "Log Regression(967/999): loss=0.5402693395974111\n",
      "Log Regression(968/999): loss=0.5395305753847168\n",
      "Log Regression(969/999): loss=0.538414460783243\n",
      "Log Regression(970/999): loss=0.5426519569253474\n",
      "Log Regression(971/999): loss=0.5492577717512356\n",
      "Log Regression(972/999): loss=0.5385786157110997\n",
      "Log Regression(973/999): loss=0.5381046765462394\n",
      "Log Regression(974/999): loss=0.5386419872303728\n",
      "Log Regression(975/999): loss=0.5518346863245397\n",
      "Log Regression(976/999): loss=0.5452461751753295\n",
      "Log Regression(977/999): loss=0.5515407529421377\n",
      "Log Regression(978/999): loss=0.5382917934923245\n",
      "Log Regression(979/999): loss=0.5402879679773872\n",
      "Log Regression(980/999): loss=0.5386663458179825\n",
      "Log Regression(981/999): loss=0.5500927890187671\n",
      "Log Regression(982/999): loss=0.5441497054146637\n",
      "Log Regression(983/999): loss=0.5585451087243809\n",
      "Log Regression(984/999): loss=0.5385389013768521\n",
      "Log Regression(985/999): loss=0.5394227810120255\n",
      "Log Regression(986/999): loss=0.5482285388523109\n",
      "Log Regression(987/999): loss=0.5381753254037547\n",
      "Log Regression(988/999): loss=0.5454615923621202\n",
      "Log Regression(989/999): loss=0.5383160624992107\n",
      "Log Regression(990/999): loss=0.5399386037900418\n",
      "Log Regression(991/999): loss=0.5484621033073158\n",
      "Log Regression(992/999): loss=0.5431122874770193\n",
      "Log Regression(993/999): loss=0.5389771625570898\n",
      "Log Regression(994/999): loss=0.5482746892815414\n",
      "Log Regression(995/999): loss=0.5462074649646275\n",
      "Log Regression(996/999): loss=0.5419709176801609\n",
      "Log Regression(997/999): loss=0.5429062382393366\n",
      "Log Regression(998/999): loss=0.5382624963682421\n",
      "Log Regression(999/999): loss=0.5383841747448588\n"
     ]
    }
   ],
   "source": [
    "w, mse = logistic_regression(y_train_lr, tX_train, initial_w, max_iters, gamma, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7269155555555555"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict_labels(w, tX_train)\n",
    "np.mean(y_train.reshape(-1,1)==y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.727"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict_labels(w, tX_val)\n",
    "np.mean(y_val.reshape(-1,1)==y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Regularized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_lr = y_train>0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lambda = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros(tX.shape[1])#np.random.normal(loc=0, scale=1, size=(tX.shape[1],1))\n",
    "max_iters = 5000\n",
    "gamma = 2e-7\n",
    "lambda_ = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(0/4999): loss=0.6784966228564583\n",
      "Log Regression(1/4999): loss=0.6728777545647245\n",
      "Log Regression(2/4999): loss=0.6708599776250701\n",
      "Log Regression(3/4999): loss=0.6660818263001932\n",
      "Log Regression(4/4999): loss=0.6649078174277324\n",
      "Log Regression(5/4999): loss=0.6704532069215893\n",
      "Log Regression(6/4999): loss=0.6678752254563738\n",
      "Log Regression(7/4999): loss=0.6694979935716667\n",
      "Log Regression(8/4999): loss=0.6665048256311488\n",
      "Log Regression(9/4999): loss=0.6864026823746251\n",
      "Log Regression(10/4999): loss=0.6587512639394648\n",
      "Log Regression(11/4999): loss=0.6565441552690753\n",
      "Log Regression(12/4999): loss=0.6578398256117568\n",
      "Log Regression(13/4999): loss=0.6547958507810951\n",
      "Log Regression(14/4999): loss=0.6677523108259908\n",
      "Log Regression(15/4999): loss=0.6528702864391277\n",
      "Log Regression(16/4999): loss=0.6535367575972096\n",
      "Log Regression(17/4999): loss=0.6505342941981145\n",
      "Log Regression(18/4999): loss=0.6531550543678706\n",
      "Log Regression(19/4999): loss=0.6540315843090969\n",
      "Log Regression(20/4999): loss=0.6489394236679269\n",
      "Log Regression(21/4999): loss=0.6490824096941529\n",
      "Log Regression(22/4999): loss=0.651276137736324\n",
      "Log Regression(23/4999): loss=0.6456239637007866\n",
      "Log Regression(24/4999): loss=0.6456052419981767\n",
      "Log Regression(25/4999): loss=0.6534449890546066\n",
      "Log Regression(26/4999): loss=0.6440263420817224\n",
      "Log Regression(27/4999): loss=0.6444055822828884\n",
      "Log Regression(28/4999): loss=0.643059479187097\n",
      "Log Regression(29/4999): loss=0.6401539780494095\n",
      "Log Regression(30/4999): loss=0.6394595626262891\n",
      "Log Regression(31/4999): loss=0.6387759234779574\n",
      "Log Regression(32/4999): loss=0.6382237619289396\n",
      "Log Regression(33/4999): loss=0.6400165631704912\n",
      "Log Regression(34/4999): loss=0.6374325194945734\n",
      "Log Regression(35/4999): loss=0.6362906271065307\n",
      "Log Regression(36/4999): loss=0.6401213899153638\n",
      "Log Regression(37/4999): loss=0.6346018004996713\n",
      "Log Regression(38/4999): loss=0.6390735729769104\n",
      "Log Regression(39/4999): loss=0.6364832176627543\n",
      "Log Regression(40/4999): loss=0.6336726353596902\n",
      "Log Regression(41/4999): loss=0.6362902328084031\n",
      "Log Regression(42/4999): loss=0.6323844994011975\n",
      "Log Regression(43/4999): loss=0.6324120970008054\n",
      "Log Regression(44/4999): loss=0.6307274191895748\n",
      "Log Regression(45/4999): loss=0.6312145051748963\n",
      "Log Regression(46/4999): loss=0.6319250611890116\n",
      "Log Regression(47/4999): loss=0.6348861458191906\n",
      "Log Regression(48/4999): loss=0.6295424649920222\n",
      "Log Regression(49/4999): loss=0.6275286159975628\n",
      "Log Regression(50/4999): loss=0.6264349459692561\n",
      "Log Regression(51/4999): loss=0.6262707963391282\n",
      "Log Regression(52/4999): loss=0.6262757422027193\n",
      "Log Regression(53/4999): loss=0.6254900497065856\n",
      "Log Regression(54/4999): loss=0.6264139945883361\n",
      "Log Regression(55/4999): loss=0.6242710097439809\n",
      "Log Regression(56/4999): loss=0.6243334398660019\n",
      "Log Regression(57/4999): loss=0.6226107503421434\n",
      "Log Regression(58/4999): loss=0.6311765680692027\n",
      "Log Regression(59/4999): loss=0.6347724158483065\n",
      "Log Regression(60/4999): loss=0.6265958475971173\n",
      "Log Regression(61/4999): loss=0.6216488900475295\n",
      "Log Regression(62/4999): loss=0.6219499356030381\n",
      "Log Regression(63/4999): loss=0.6206497441261108\n",
      "Log Regression(64/4999): loss=0.6233959286874278\n",
      "Log Regression(65/4999): loss=0.6221465789841077\n",
      "Log Regression(66/4999): loss=0.6227173232251445\n",
      "Log Regression(67/4999): loss=0.6250357765096655\n",
      "Log Regression(68/4999): loss=0.6312131133562502\n",
      "Log Regression(69/4999): loss=0.6240946749816071\n",
      "Log Regression(70/4999): loss=0.6360522332953708\n",
      "Log Regression(71/4999): loss=0.6205502593576974\n",
      "Log Regression(72/4999): loss=0.6166864385097781\n",
      "Log Regression(73/4999): loss=0.6161557362172916\n",
      "Log Regression(74/4999): loss=0.6193070704686381\n",
      "Log Regression(75/4999): loss=0.6276517245370297\n",
      "Log Regression(76/4999): loss=0.6154860309160419\n",
      "Log Regression(77/4999): loss=0.6147767381572159\n",
      "Log Regression(78/4999): loss=0.6173547257524434\n",
      "Log Regression(79/4999): loss=0.6144650549964051\n",
      "Log Regression(80/4999): loss=0.6178613471692959\n",
      "Log Regression(81/4999): loss=0.616596259790468\n",
      "Log Regression(82/4999): loss=0.6122987661503807\n",
      "Log Regression(83/4999): loss=0.6119182972434234\n",
      "Log Regression(84/4999): loss=0.6118867020648596\n",
      "Log Regression(85/4999): loss=0.6149285110495983\n",
      "Log Regression(86/4999): loss=0.6115469318532702\n",
      "Log Regression(87/4999): loss=0.6115736013448103\n",
      "Log Regression(88/4999): loss=0.6100089120471365\n",
      "Log Regression(89/4999): loss=0.6098034881675873\n",
      "Log Regression(90/4999): loss=0.6097009661684687\n",
      "Log Regression(91/4999): loss=0.6098227234166285\n",
      "Log Regression(92/4999): loss=0.6089773016049688\n",
      "Log Regression(93/4999): loss=0.6107735353688258\n",
      "Log Regression(94/4999): loss=0.6095290821347511\n",
      "Log Regression(95/4999): loss=0.6080857038645425\n",
      "Log Regression(96/4999): loss=0.6085566162853532\n",
      "Log Regression(97/4999): loss=0.6085373259358683\n",
      "Log Regression(98/4999): loss=0.6138275937311071\n",
      "Log Regression(99/4999): loss=0.6099296843209396\n",
      "Log Regression(100/4999): loss=0.6068478781338755\n",
      "Log Regression(101/4999): loss=0.6066416640886514\n",
      "Log Regression(102/4999): loss=0.6057478827223383\n",
      "Log Regression(103/4999): loss=0.6061584979863387\n",
      "Log Regression(104/4999): loss=0.6057629322005308\n",
      "Log Regression(105/4999): loss=0.6049650877317475\n",
      "Log Regression(106/4999): loss=0.6058760704461518\n",
      "Log Regression(107/4999): loss=0.6066261756622023\n",
      "Log Regression(108/4999): loss=0.6072807152365964\n",
      "Log Regression(109/4999): loss=0.604432921557384\n",
      "Log Regression(110/4999): loss=0.6046518154871571\n",
      "Log Regression(111/4999): loss=0.6043545668529563\n",
      "Log Regression(112/4999): loss=0.6037041276683949\n",
      "Log Regression(113/4999): loss=0.603315881446286\n",
      "Log Regression(114/4999): loss=0.6056369361878025\n",
      "Log Regression(115/4999): loss=0.6039993755010006\n",
      "Log Regression(116/4999): loss=0.6020570203991737\n",
      "Log Regression(117/4999): loss=0.6031422685712948\n",
      "Log Regression(118/4999): loss=0.6025927743049916\n",
      "Log Regression(119/4999): loss=0.6028050154998655\n",
      "Log Regression(120/4999): loss=0.6008940256735644\n",
      "Log Regression(121/4999): loss=0.6025371847282829\n",
      "Log Regression(122/4999): loss=0.6006280495219358\n",
      "Log Regression(123/4999): loss=0.6002826150103898\n",
      "Log Regression(124/4999): loss=0.6002854135420016\n",
      "Log Regression(125/4999): loss=0.6007476867005354\n",
      "Log Regression(126/4999): loss=0.5992892262412077\n",
      "Log Regression(127/4999): loss=0.6016086507418371\n",
      "Log Regression(128/4999): loss=0.5989291408973675\n",
      "Log Regression(129/4999): loss=0.598944490242082\n",
      "Log Regression(130/4999): loss=0.5990117943528138\n",
      "Log Regression(131/4999): loss=0.5982989667852493\n",
      "Log Regression(132/4999): loss=0.5979414220170034\n",
      "Log Regression(133/4999): loss=0.6040984827591026\n",
      "Log Regression(134/4999): loss=0.6061697707077075\n",
      "Log Regression(135/4999): loss=0.6149005131353243\n",
      "Log Regression(136/4999): loss=0.5994128802181141\n",
      "Log Regression(137/4999): loss=0.5966949286238545\n",
      "Log Regression(138/4999): loss=0.5962724526625268\n",
      "Log Regression(139/4999): loss=0.5966052918331808\n",
      "Log Regression(140/4999): loss=0.5978883304906414\n",
      "Log Regression(141/4999): loss=0.5998483516019655\n",
      "Log Regression(142/4999): loss=0.5962478022233627\n",
      "Log Regression(143/4999): loss=0.5987594046956516\n",
      "Log Regression(144/4999): loss=0.5982236805799261\n",
      "Log Regression(145/4999): loss=0.5946310054718217\n",
      "Log Regression(146/4999): loss=0.5951098612739775\n",
      "Log Regression(147/4999): loss=0.5997059548727702\n",
      "Log Regression(148/4999): loss=0.5963413818190576\n",
      "Log Regression(149/4999): loss=0.5944448880801745\n",
      "Log Regression(150/4999): loss=0.5954934993901508\n",
      "Log Regression(151/4999): loss=0.5965073567248601\n",
      "Log Regression(152/4999): loss=0.5939400138803005\n",
      "Log Regression(153/4999): loss=0.593362738548627\n",
      "Log Regression(154/4999): loss=0.5934494399770718\n",
      "Log Regression(155/4999): loss=0.5942131034370715\n",
      "Log Regression(156/4999): loss=0.5932717930676594\n",
      "Log Regression(157/4999): loss=0.5957538209064448\n",
      "Log Regression(158/4999): loss=0.5943611573729753\n",
      "Log Regression(159/4999): loss=0.5935994807041007\n",
      "Log Regression(160/4999): loss=0.5943750947765576\n",
      "Log Regression(161/4999): loss=0.595391339271113\n",
      "Log Regression(162/4999): loss=0.5954818062138925\n",
      "Log Regression(163/4999): loss=0.596530855728369\n",
      "Log Regression(164/4999): loss=0.5917313820362193\n",
      "Log Regression(165/4999): loss=0.5962121924439291\n",
      "Log Regression(166/4999): loss=0.5952995788328328\n",
      "Log Regression(167/4999): loss=0.5913118561339814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(168/4999): loss=0.5913439115681086\n",
      "Log Regression(169/4999): loss=0.5909536986259676\n",
      "Log Regression(170/4999): loss=0.5937213943878525\n",
      "Log Regression(171/4999): loss=0.5902936894654216\n",
      "Log Regression(172/4999): loss=0.5901597914087361\n",
      "Log Regression(173/4999): loss=0.5905063737293693\n",
      "Log Regression(174/4999): loss=0.589952027658257\n",
      "Log Regression(175/4999): loss=0.5903084911694756\n",
      "Log Regression(176/4999): loss=0.590204336695589\n",
      "Log Regression(177/4999): loss=0.5907896204674562\n",
      "Log Regression(178/4999): loss=0.5908814362408069\n",
      "Log Regression(179/4999): loss=0.5928667646624776\n",
      "Log Regression(180/4999): loss=0.594538289215381\n",
      "Log Regression(181/4999): loss=0.5892790135638756\n",
      "Log Regression(182/4999): loss=0.5906767139308614\n",
      "Log Regression(183/4999): loss=0.5885797999049345\n",
      "Log Regression(184/4999): loss=0.5884338620044091\n",
      "Log Regression(185/4999): loss=0.5882080438819548\n",
      "Log Regression(186/4999): loss=0.5889101640622264\n",
      "Log Regression(187/4999): loss=0.5925123731764742\n",
      "Log Regression(188/4999): loss=0.596735062120155\n",
      "Log Regression(189/4999): loss=0.5916252579289234\n",
      "Log Regression(190/4999): loss=0.5884574509890396\n",
      "Log Regression(191/4999): loss=0.588883372807831\n",
      "Log Regression(192/4999): loss=0.5878695217808829\n",
      "Log Regression(193/4999): loss=0.5898871614482765\n",
      "Log Regression(194/4999): loss=0.5890942363893346\n",
      "Log Regression(195/4999): loss=0.5881714984615367\n",
      "Log Regression(196/4999): loss=0.5867749290796318\n",
      "Log Regression(197/4999): loss=0.5917176075759273\n",
      "Log Regression(198/4999): loss=0.5884161046240474\n",
      "Log Regression(199/4999): loss=0.5910882801646103\n",
      "Log Regression(200/4999): loss=0.5882450793492768\n",
      "Log Regression(201/4999): loss=0.5896570490784676\n",
      "Log Regression(202/4999): loss=0.5869442560074474\n",
      "Log Regression(203/4999): loss=0.5866585993394411\n",
      "Log Regression(204/4999): loss=0.5856419192803315\n",
      "Log Regression(205/4999): loss=0.5883967225671608\n",
      "Log Regression(206/4999): loss=0.5891504366689347\n",
      "Log Regression(207/4999): loss=0.5858423829016648\n",
      "Log Regression(208/4999): loss=0.5861036915061942\n",
      "Log Regression(209/4999): loss=0.586691472392425\n",
      "Log Regression(210/4999): loss=0.5855866677366601\n",
      "Log Regression(211/4999): loss=0.5894565758828911\n",
      "Log Regression(212/4999): loss=0.5874875615631877\n",
      "Log Regression(213/4999): loss=0.5853137666806101\n",
      "Log Regression(214/4999): loss=0.5847498526167342\n",
      "Log Regression(215/4999): loss=0.5847122236562631\n",
      "Log Regression(216/4999): loss=0.5853365558201005\n",
      "Log Regression(217/4999): loss=0.5848688424903872\n",
      "Log Regression(218/4999): loss=0.5856682671454563\n",
      "Log Regression(219/4999): loss=0.584752399820629\n",
      "Log Regression(220/4999): loss=0.5835596952691816\n",
      "Log Regression(221/4999): loss=0.5902055799195375\n",
      "Log Regression(222/4999): loss=0.5829997361725174\n",
      "Log Regression(223/4999): loss=0.5834278847126233\n",
      "Log Regression(224/4999): loss=0.5834276714469502\n",
      "Log Regression(225/4999): loss=0.5924899772443987\n",
      "Log Regression(226/4999): loss=0.5933339771094238\n",
      "Log Regression(227/4999): loss=0.5858857482438953\n",
      "Log Regression(228/4999): loss=0.5837080625650741\n",
      "Log Regression(229/4999): loss=0.5826060143996643\n",
      "Log Regression(230/4999): loss=0.5827641961729421\n",
      "Log Regression(231/4999): loss=0.5819627371151787\n",
      "Log Regression(232/4999): loss=0.5817113534454964\n",
      "Log Regression(233/4999): loss=0.5840665078001246\n",
      "Log Regression(234/4999): loss=0.5857323597101229\n",
      "Log Regression(235/4999): loss=0.5823378196430987\n",
      "Log Regression(236/4999): loss=0.5825883502382176\n",
      "Log Regression(237/4999): loss=0.5809038105016764\n",
      "Log Regression(238/4999): loss=0.5810682623179481\n",
      "Log Regression(239/4999): loss=0.5806851136278204\n",
      "Log Regression(240/4999): loss=0.5836834408277461\n",
      "Log Regression(241/4999): loss=0.580745485790204\n",
      "Log Regression(242/4999): loss=0.5817573783419621\n",
      "Log Regression(243/4999): loss=0.5802704533322844\n",
      "Log Regression(244/4999): loss=0.580044033054634\n",
      "Log Regression(245/4999): loss=0.5827295145630691\n",
      "Log Regression(246/4999): loss=0.5807934783120307\n",
      "Log Regression(247/4999): loss=0.5795230524976198\n",
      "Log Regression(248/4999): loss=0.5855348599902448\n",
      "Log Regression(249/4999): loss=0.5802024885468049\n",
      "Log Regression(250/4999): loss=0.5803895056237491\n",
      "Log Regression(251/4999): loss=0.5795892953744379\n",
      "Log Regression(252/4999): loss=0.5805910537446283\n",
      "Log Regression(253/4999): loss=0.5797205072209126\n",
      "Log Regression(254/4999): loss=0.5817448202895124\n",
      "Log Regression(255/4999): loss=0.5818108678220798\n",
      "Log Regression(256/4999): loss=0.5822489748720057\n",
      "Log Regression(257/4999): loss=0.5797079512827307\n",
      "Log Regression(258/4999): loss=0.5796576175634521\n",
      "Log Regression(259/4999): loss=0.5788261560672242\n",
      "Log Regression(260/4999): loss=0.5794793050189251\n",
      "Log Regression(261/4999): loss=0.5792173432444935\n",
      "Log Regression(262/4999): loss=0.5792236362798104\n",
      "Log Regression(263/4999): loss=0.5785181817833424\n",
      "Log Regression(264/4999): loss=0.5797973032893966\n",
      "Log Regression(265/4999): loss=0.5788876096546903\n",
      "Log Regression(266/4999): loss=0.5824789618926823\n",
      "Log Regression(267/4999): loss=0.579227081747716\n",
      "Log Regression(268/4999): loss=0.5846907203468278\n",
      "Log Regression(269/4999): loss=0.5778340342506609\n",
      "Log Regression(270/4999): loss=0.5776746202950808\n",
      "Log Regression(271/4999): loss=0.5806779440687072\n",
      "Log Regression(272/4999): loss=0.5777703929370481\n",
      "Log Regression(273/4999): loss=0.5812172349182715\n",
      "Log Regression(274/4999): loss=0.577159962715852\n",
      "Log Regression(275/4999): loss=0.5774140690767605\n",
      "Log Regression(276/4999): loss=0.5783653442241409\n",
      "Log Regression(277/4999): loss=0.5779650313709577\n",
      "Log Regression(278/4999): loss=0.5799347478811853\n",
      "Log Regression(279/4999): loss=0.5866270579215068\n",
      "Log Regression(280/4999): loss=0.5782343433491637\n",
      "Log Regression(281/4999): loss=0.5764760022868595\n",
      "Log Regression(282/4999): loss=0.5763664995487718\n",
      "Log Regression(283/4999): loss=0.5780708856338118\n",
      "Log Regression(284/4999): loss=0.5772998239557937\n",
      "Log Regression(285/4999): loss=0.5774384472452274\n",
      "Log Regression(286/4999): loss=0.5774486319881806\n",
      "Log Regression(287/4999): loss=0.5767113862405564\n",
      "Log Regression(288/4999): loss=0.5759249818188901\n",
      "Log Regression(289/4999): loss=0.5756056491681275\n",
      "Log Regression(290/4999): loss=0.5757418679014361\n",
      "Log Regression(291/4999): loss=0.5756583010602988\n",
      "Log Regression(292/4999): loss=0.5751775420562116\n",
      "Log Regression(293/4999): loss=0.5750523525255233\n",
      "Log Regression(294/4999): loss=0.5751374608916175\n",
      "Log Regression(295/4999): loss=0.5748698716095341\n",
      "Log Regression(296/4999): loss=0.5756084072739289\n",
      "Log Regression(297/4999): loss=0.5746625659756411\n",
      "Log Regression(298/4999): loss=0.5745663264282611\n",
      "Log Regression(299/4999): loss=0.5746654875212845\n",
      "Log Regression(300/4999): loss=0.5744846579763755\n",
      "Log Regression(301/4999): loss=0.5772973563385672\n",
      "Log Regression(302/4999): loss=0.5740883846819146\n",
      "Log Regression(303/4999): loss=0.574568760647332\n",
      "Log Regression(304/4999): loss=0.5775609721882036\n",
      "Log Regression(305/4999): loss=0.5752926323911114\n",
      "Log Regression(306/4999): loss=0.5738914943138759\n",
      "Log Regression(307/4999): loss=0.5735277388733415\n",
      "Log Regression(308/4999): loss=0.5738134408579912\n",
      "Log Regression(309/4999): loss=0.5734609361890806\n",
      "Log Regression(310/4999): loss=0.5733110261089026\n",
      "Log Regression(311/4999): loss=0.5741949983483987\n",
      "Log Regression(312/4999): loss=0.5754756055289895\n",
      "Log Regression(313/4999): loss=0.5761273415649348\n",
      "Log Regression(314/4999): loss=0.5802554789904026\n",
      "Log Regression(315/4999): loss=0.5817834281081067\n",
      "Log Regression(316/4999): loss=0.5735567304213975\n",
      "Log Regression(317/4999): loss=0.5731369392948834\n",
      "Log Regression(318/4999): loss=0.5789258978680799\n",
      "Log Regression(319/4999): loss=0.573694824169869\n",
      "Log Regression(320/4999): loss=0.5731799955850146\n",
      "Log Regression(321/4999): loss=0.5726918133369493\n",
      "Log Regression(322/4999): loss=0.5756681906799982\n",
      "Log Regression(323/4999): loss=0.5735085235702317\n",
      "Log Regression(324/4999): loss=0.5743540776310189\n",
      "Log Regression(325/4999): loss=0.58063522284714\n",
      "Log Regression(326/4999): loss=0.572532252058996\n",
      "Log Regression(327/4999): loss=0.5734782741980147\n",
      "Log Regression(328/4999): loss=0.5741798189882372\n",
      "Log Regression(329/4999): loss=0.5719470068393206\n",
      "Log Regression(330/4999): loss=0.5749284116908919\n",
      "Log Regression(331/4999): loss=0.5719801242266909\n",
      "Log Regression(332/4999): loss=0.5725578596305984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(333/4999): loss=0.571252259314384\n",
      "Log Regression(334/4999): loss=0.5720077486159174\n",
      "Log Regression(335/4999): loss=0.5723709556180596\n",
      "Log Regression(336/4999): loss=0.5757301222377004\n",
      "Log Regression(337/4999): loss=0.5711117668922824\n",
      "Log Regression(338/4999): loss=0.5728015430116175\n",
      "Log Regression(339/4999): loss=0.570675002841907\n",
      "Log Regression(340/4999): loss=0.5707911015097233\n",
      "Log Regression(341/4999): loss=0.5705621632101079\n",
      "Log Regression(342/4999): loss=0.570464487800399\n",
      "Log Regression(343/4999): loss=0.5709135343718872\n",
      "Log Regression(344/4999): loss=0.5702127804404351\n",
      "Log Regression(345/4999): loss=0.5706047499878729\n",
      "Log Regression(346/4999): loss=0.5700222104557217\n",
      "Log Regression(347/4999): loss=0.5710026654459901\n",
      "Log Regression(348/4999): loss=0.5700534882019314\n",
      "Log Regression(349/4999): loss=0.5699120661677908\n",
      "Log Regression(350/4999): loss=0.571142427381001\n",
      "Log Regression(351/4999): loss=0.5705791227849579\n",
      "Log Regression(352/4999): loss=0.5707069530198827\n",
      "Log Regression(353/4999): loss=0.569822883225359\n",
      "Log Regression(354/4999): loss=0.569496066787324\n",
      "Log Regression(355/4999): loss=0.5765084923961423\n",
      "Log Regression(356/4999): loss=0.5765715493848989\n",
      "Log Regression(357/4999): loss=0.5690207754228324\n",
      "Log Regression(358/4999): loss=0.5692711315686017\n",
      "Log Regression(359/4999): loss=0.5688348232611413\n",
      "Log Regression(360/4999): loss=0.5691907487350204\n",
      "Log Regression(361/4999): loss=0.5736996669600447\n",
      "Log Regression(362/4999): loss=0.5707248638950106\n",
      "Log Regression(363/4999): loss=0.5686526920782597\n",
      "Log Regression(364/4999): loss=0.570292497658523\n",
      "Log Regression(365/4999): loss=0.5697682965393596\n",
      "Log Regression(366/4999): loss=0.5684153385737553\n",
      "Log Regression(367/4999): loss=0.5688469993320429\n",
      "Log Regression(368/4999): loss=0.5728311289812905\n",
      "Log Regression(369/4999): loss=0.5688479650913728\n",
      "Log Regression(370/4999): loss=0.5682064628539087\n",
      "Log Regression(371/4999): loss=0.5694971060520821\n",
      "Log Regression(372/4999): loss=0.569791093530355\n",
      "Log Regression(373/4999): loss=0.5704281381562906\n",
      "Log Regression(374/4999): loss=0.5679089271211739\n",
      "Log Regression(375/4999): loss=0.5711584346504008\n",
      "Log Regression(376/4999): loss=0.570114573160629\n",
      "Log Regression(377/4999): loss=0.5694378082206238\n",
      "Log Regression(378/4999): loss=0.5691217202071818\n",
      "Log Regression(379/4999): loss=0.5730471213935735\n",
      "Log Regression(380/4999): loss=0.5673543403887058\n",
      "Log Regression(381/4999): loss=0.5676182219032726\n",
      "Log Regression(382/4999): loss=0.5672448803365219\n",
      "Log Regression(383/4999): loss=0.5693933677940954\n",
      "Log Regression(384/4999): loss=0.5674118702897567\n",
      "Log Regression(385/4999): loss=0.5670633347578827\n",
      "Log Regression(386/4999): loss=0.5691861221589025\n",
      "Log Regression(387/4999): loss=0.5688666765128754\n",
      "Log Regression(388/4999): loss=0.5675967273840968\n",
      "Log Regression(389/4999): loss=0.5677376978221298\n",
      "Log Regression(390/4999): loss=0.5665133066510606\n",
      "Log Regression(391/4999): loss=0.5664244203252574\n",
      "Log Regression(392/4999): loss=0.5663495291939131\n",
      "Log Regression(393/4999): loss=0.5664074353647651\n",
      "Log Regression(394/4999): loss=0.5660451899629166\n",
      "Log Regression(395/4999): loss=0.5661052811756919\n",
      "Log Regression(396/4999): loss=0.5733928751484292\n",
      "Log Regression(397/4999): loss=0.5680837181514309\n",
      "Log Regression(398/4999): loss=0.5684099045258676\n",
      "Log Regression(399/4999): loss=0.569073203028512\n",
      "Log Regression(400/4999): loss=0.5713751565748856\n",
      "Log Regression(401/4999): loss=0.5662142944715319\n",
      "Log Regression(402/4999): loss=0.5663547525457153\n",
      "Log Regression(403/4999): loss=0.5687352088678269\n",
      "Log Regression(404/4999): loss=0.5678904957356355\n",
      "Log Regression(405/4999): loss=0.5657386423368289\n",
      "Log Regression(406/4999): loss=0.5654429277768234\n",
      "Log Regression(407/4999): loss=0.5686046822474857\n",
      "Log Regression(408/4999): loss=0.5659160783952703\n",
      "Log Regression(409/4999): loss=0.5653634062604013\n",
      "Log Regression(410/4999): loss=0.5650711022192361\n",
      "Log Regression(411/4999): loss=0.5659399007036287\n",
      "Log Regression(412/4999): loss=0.5652945043093138\n",
      "Log Regression(413/4999): loss=0.5650639352821304\n",
      "Log Regression(414/4999): loss=0.5647145343556974\n",
      "Log Regression(415/4999): loss=0.5652926978437938\n",
      "Log Regression(416/4999): loss=0.5661880954701171\n",
      "Log Regression(417/4999): loss=0.564994930567119\n",
      "Log Regression(418/4999): loss=0.5705688344277526\n",
      "Log Regression(419/4999): loss=0.5643820004217436\n",
      "Log Regression(420/4999): loss=0.5642619816337139\n",
      "Log Regression(421/4999): loss=0.5649692192778659\n",
      "Log Regression(422/4999): loss=0.5663188049864561\n",
      "Log Regression(423/4999): loss=0.5642235194678052\n",
      "Log Regression(424/4999): loss=0.564255407317255\n",
      "Log Regression(425/4999): loss=0.5645925810182849\n",
      "Log Regression(426/4999): loss=0.5644380969685976\n",
      "Log Regression(427/4999): loss=0.565917158306182\n",
      "Log Regression(428/4999): loss=0.5646496493494016\n",
      "Log Regression(429/4999): loss=0.5640308296459863\n",
      "Log Regression(430/4999): loss=0.5640625528403468\n",
      "Log Regression(431/4999): loss=0.5637881901809703\n",
      "Log Regression(432/4999): loss=0.5640685704657844\n",
      "Log Regression(433/4999): loss=0.5721135546755386\n",
      "Log Regression(434/4999): loss=0.5635711145957232\n",
      "Log Regression(435/4999): loss=0.5634460811284926\n",
      "Log Regression(436/4999): loss=0.5643003040768405\n",
      "Log Regression(437/4999): loss=0.5653596746423579\n",
      "Log Regression(438/4999): loss=0.5673323263334294\n",
      "Log Regression(439/4999): loss=0.5636589108856476\n",
      "Log Regression(440/4999): loss=0.5636430282750238\n",
      "Log Regression(441/4999): loss=0.564200297873191\n",
      "Log Regression(442/4999): loss=0.5631661389348958\n",
      "Log Regression(443/4999): loss=0.5631998332685937\n",
      "Log Regression(444/4999): loss=0.5733210939570685\n",
      "Log Regression(445/4999): loss=0.575949360717833\n",
      "Log Regression(446/4999): loss=0.5650595736226699\n",
      "Log Regression(447/4999): loss=0.5636742015925102\n",
      "Log Regression(448/4999): loss=0.5653997068331283\n",
      "Log Regression(449/4999): loss=0.5629619929448639\n",
      "Log Regression(450/4999): loss=0.5628660181947859\n",
      "Log Regression(451/4999): loss=0.5626467471163438\n",
      "Log Regression(452/4999): loss=0.5682035401748264\n",
      "Log Regression(453/4999): loss=0.5624775171612315\n",
      "Log Regression(454/4999): loss=0.565558986504745\n",
      "Log Regression(455/4999): loss=0.5628441624026669\n",
      "Log Regression(456/4999): loss=0.5624979271587894\n",
      "Log Regression(457/4999): loss=0.5637202339702516\n",
      "Log Regression(458/4999): loss=0.56302446769161\n",
      "Log Regression(459/4999): loss=0.56346125090904\n",
      "Log Regression(460/4999): loss=0.5630383331609369\n",
      "Log Regression(461/4999): loss=0.5639248548735919\n",
      "Log Regression(462/4999): loss=0.5624675810155316\n",
      "Log Regression(463/4999): loss=0.5622148607398813\n",
      "Log Regression(464/4999): loss=0.5622926729913919\n",
      "Log Regression(465/4999): loss=0.5637014397067466\n",
      "Log Regression(466/4999): loss=0.561965046157574\n",
      "Log Regression(467/4999): loss=0.5637505884537766\n",
      "Log Regression(468/4999): loss=0.5619500060734225\n",
      "Log Regression(469/4999): loss=0.561806688268154\n",
      "Log Regression(470/4999): loss=0.5617838002334903\n",
      "Log Regression(471/4999): loss=0.5617778094710368\n",
      "Log Regression(472/4999): loss=0.5633794282233986\n",
      "Log Regression(473/4999): loss=0.5618491689180534\n",
      "Log Regression(474/4999): loss=0.5666181082375746\n",
      "Log Regression(475/4999): loss=0.5638963649162881\n",
      "Log Regression(476/4999): loss=0.5636929763543777\n",
      "Log Regression(477/4999): loss=0.562610158953844\n",
      "Log Regression(478/4999): loss=0.5671006679977456\n",
      "Log Regression(479/4999): loss=0.5671869085483001\n",
      "Log Regression(480/4999): loss=0.5640869766470199\n",
      "Log Regression(481/4999): loss=0.5621588040180219\n",
      "Log Regression(482/4999): loss=0.5615555718413489\n",
      "Log Regression(483/4999): loss=0.5623213201319822\n",
      "Log Regression(484/4999): loss=0.5628372085489268\n",
      "Log Regression(485/4999): loss=0.5639148277763831\n",
      "Log Regression(486/4999): loss=0.5632312106695393\n",
      "Log Regression(487/4999): loss=0.5609298854180129\n",
      "Log Regression(488/4999): loss=0.5609166201592535\n",
      "Log Regression(489/4999): loss=0.5610794034104446\n",
      "Log Regression(490/4999): loss=0.562048680782427\n",
      "Log Regression(491/4999): loss=0.5612695088596288\n",
      "Log Regression(492/4999): loss=0.5612439615588435\n",
      "Log Regression(493/4999): loss=0.5610602403810313\n",
      "Log Regression(494/4999): loss=0.5633869094437878\n",
      "Log Regression(495/4999): loss=0.5619940063894566\n",
      "Log Regression(496/4999): loss=0.5612515337796523\n",
      "Log Regression(497/4999): loss=0.5631005494997412\n",
      "Log Regression(498/4999): loss=0.5630407834129958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(499/4999): loss=0.5637242267113239\n",
      "Log Regression(500/4999): loss=0.560898344407372\n",
      "Log Regression(501/4999): loss=0.5601668831195317\n",
      "Log Regression(502/4999): loss=0.5605058887371627\n",
      "Log Regression(503/4999): loss=0.5605171938980315\n",
      "Log Regression(504/4999): loss=0.5600223472760419\n",
      "Log Regression(505/4999): loss=0.5612766512714116\n",
      "Log Regression(506/4999): loss=0.5621813029103014\n",
      "Log Regression(507/4999): loss=0.5625395578953822\n",
      "Log Regression(508/4999): loss=0.5606370686192802\n",
      "Log Regression(509/4999): loss=0.5598064069667685\n",
      "Log Regression(510/4999): loss=0.5599460660278602\n",
      "Log Regression(511/4999): loss=0.5600354911005548\n",
      "Log Regression(512/4999): loss=0.5660438805362955\n",
      "Log Regression(513/4999): loss=0.5604976708842115\n",
      "Log Regression(514/4999): loss=0.5603675711175682\n",
      "Log Regression(515/4999): loss=0.5699975575439977\n",
      "Log Regression(516/4999): loss=0.5778670361674876\n",
      "Log Regression(517/4999): loss=0.5676115700491734\n",
      "Log Regression(518/4999): loss=0.5610090342173989\n",
      "Log Regression(519/4999): loss=0.5592750414872272\n",
      "Log Regression(520/4999): loss=0.5634736622610353\n",
      "Log Regression(521/4999): loss=0.5603870474346302\n",
      "Log Regression(522/4999): loss=0.5612943602807378\n",
      "Log Regression(523/4999): loss=0.559392428574064\n",
      "Log Regression(524/4999): loss=0.5592032211605718\n",
      "Log Regression(525/4999): loss=0.5616775965370752\n",
      "Log Regression(526/4999): loss=0.5651588041199358\n",
      "Log Regression(527/4999): loss=0.5624500060309885\n",
      "Log Regression(528/4999): loss=0.5601719194650489\n",
      "Log Regression(529/4999): loss=0.5633114586671932\n",
      "Log Regression(530/4999): loss=0.5599332236634189\n",
      "Log Regression(531/4999): loss=0.5589533007186271\n",
      "Log Regression(532/4999): loss=0.5595157463733303\n",
      "Log Regression(533/4999): loss=0.5617813379109158\n",
      "Log Regression(534/4999): loss=0.5602332345987969\n",
      "Log Regression(535/4999): loss=0.5617429708058108\n",
      "Log Regression(536/4999): loss=0.5609068415660715\n",
      "Log Regression(537/4999): loss=0.5592976466365137\n",
      "Log Regression(538/4999): loss=0.5635901648255093\n",
      "Log Regression(539/4999): loss=0.5587471881222943\n",
      "Log Regression(540/4999): loss=0.5590545799775046\n",
      "Log Regression(541/4999): loss=0.5592080233290512\n",
      "Log Regression(542/4999): loss=0.5584976277338752\n",
      "Log Regression(543/4999): loss=0.5600847745116205\n",
      "Log Regression(544/4999): loss=0.5590720406349514\n",
      "Log Regression(545/4999): loss=0.5584164564089616\n",
      "Log Regression(546/4999): loss=0.55885585411515\n",
      "Log Regression(547/4999): loss=0.5650624918229933\n",
      "Log Regression(548/4999): loss=0.5597887603636752\n",
      "Log Regression(549/4999): loss=0.5583145050312719\n",
      "Log Regression(550/4999): loss=0.5594180083927693\n",
      "Log Regression(551/4999): loss=0.5589627069612967\n",
      "Log Regression(552/4999): loss=0.5608774450404126\n",
      "Log Regression(553/4999): loss=0.5582108863811218\n",
      "Log Regression(554/4999): loss=0.5621562645896389\n",
      "Log Regression(555/4999): loss=0.5587848945755248\n",
      "Log Regression(556/4999): loss=0.5597785883516778\n",
      "Log Regression(557/4999): loss=0.5621362239230295\n",
      "Log Regression(558/4999): loss=0.5583608393735666\n",
      "Log Regression(559/4999): loss=0.5582605895370667\n",
      "Log Regression(560/4999): loss=0.5583306005214042\n",
      "Log Regression(561/4999): loss=0.5584330967053722\n",
      "Log Regression(562/4999): loss=0.5603693075795596\n",
      "Log Regression(563/4999): loss=0.5579351844753965\n",
      "Log Regression(564/4999): loss=0.557995461366013\n",
      "Log Regression(565/4999): loss=0.5578862412250345\n",
      "Log Regression(566/4999): loss=0.5583709471232868\n",
      "Log Regression(567/4999): loss=0.5576587915839856\n",
      "Log Regression(568/4999): loss=0.5579725325719114\n",
      "Log Regression(569/4999): loss=0.5635821672285053\n",
      "Log Regression(570/4999): loss=0.5582999811613658\n",
      "Log Regression(571/4999): loss=0.557578283380217\n",
      "Log Regression(572/4999): loss=0.5577651376127413\n",
      "Log Regression(573/4999): loss=0.5579482459691351\n",
      "Log Regression(574/4999): loss=0.5586254006099614\n",
      "Log Regression(575/4999): loss=0.5594308794792621\n",
      "Log Regression(576/4999): loss=0.5587471928746335\n",
      "Log Regression(577/4999): loss=0.5586393859830968\n",
      "Log Regression(578/4999): loss=0.5584322553817271\n",
      "Log Regression(579/4999): loss=0.5573106747741808\n",
      "Log Regression(580/4999): loss=0.5590768095812522\n",
      "Log Regression(581/4999): loss=0.5569590836629128\n",
      "Log Regression(582/4999): loss=0.5570456000820247\n",
      "Log Regression(583/4999): loss=0.5568654877259444\n",
      "Log Regression(584/4999): loss=0.5567976903348163\n",
      "Log Regression(585/4999): loss=0.5598548330831923\n",
      "Log Regression(586/4999): loss=0.5567743554314181\n",
      "Log Regression(587/4999): loss=0.5566639247404396\n",
      "Log Regression(588/4999): loss=0.5584894700864809\n",
      "Log Regression(589/4999): loss=0.5588432190755134\n",
      "Log Regression(590/4999): loss=0.5587606541321982\n",
      "Log Regression(591/4999): loss=0.556442522994356\n",
      "Log Regression(592/4999): loss=0.5567608540241769\n",
      "Log Regression(593/4999): loss=0.5564733266251132\n",
      "Log Regression(594/4999): loss=0.5590180281382945\n",
      "Log Regression(595/4999): loss=0.5569130978860777\n",
      "Log Regression(596/4999): loss=0.5577372299516793\n",
      "Log Regression(597/4999): loss=0.5565932775503132\n",
      "Log Regression(598/4999): loss=0.556745154803992\n",
      "Log Regression(599/4999): loss=0.5576601682672221\n",
      "Log Regression(600/4999): loss=0.5561175698389008\n",
      "Log Regression(601/4999): loss=0.5561375768897175\n",
      "Log Regression(602/4999): loss=0.5567912221189156\n",
      "Log Regression(603/4999): loss=0.5562251111813079\n",
      "Log Regression(604/4999): loss=0.5571975568233379\n",
      "Log Regression(605/4999): loss=0.5566777758437108\n",
      "Log Regression(606/4999): loss=0.5598592392698859\n",
      "Log Regression(607/4999): loss=0.5569751154179358\n",
      "Log Regression(608/4999): loss=0.5561318225114676\n",
      "Log Regression(609/4999): loss=0.5559788126448646\n",
      "Log Regression(610/4999): loss=0.555955928627096\n",
      "Log Regression(611/4999): loss=0.5563086766888918\n",
      "Log Regression(612/4999): loss=0.5553955032870774\n",
      "Log Regression(613/4999): loss=0.5554729053627286\n",
      "Log Regression(614/4999): loss=0.5567646394627992\n",
      "Log Regression(615/4999): loss=0.5586585833674561\n",
      "Log Regression(616/4999): loss=0.5596117780802362\n",
      "Log Regression(617/4999): loss=0.5556425075928326\n",
      "Log Regression(618/4999): loss=0.5606446571520793\n",
      "Log Regression(619/4999): loss=0.5586628857328967\n",
      "Log Regression(620/4999): loss=0.555547431837055\n",
      "Log Regression(621/4999): loss=0.5622133835991364\n",
      "Log Regression(622/4999): loss=0.5555360963059868\n",
      "Log Regression(623/4999): loss=0.5553178445277287\n",
      "Log Regression(624/4999): loss=0.5555494822594116\n",
      "Log Regression(625/4999): loss=0.5559752038611087\n",
      "Log Regression(626/4999): loss=0.5557263441252815\n",
      "Log Regression(627/4999): loss=0.5552540612901916\n",
      "Log Regression(628/4999): loss=0.555226360160359\n",
      "Log Regression(629/4999): loss=0.5551031584129857\n",
      "Log Regression(630/4999): loss=0.5575770130756167\n",
      "Log Regression(631/4999): loss=0.5557832407207431\n",
      "Log Regression(632/4999): loss=0.5554807331161078\n",
      "Log Regression(633/4999): loss=0.5656163273517001\n",
      "Log Regression(634/4999): loss=0.5586787330746154\n",
      "Log Regression(635/4999): loss=0.5579481182668231\n",
      "Log Regression(636/4999): loss=0.5561007842437408\n",
      "Log Regression(637/4999): loss=0.557659414984435\n",
      "Log Regression(638/4999): loss=0.554738155709128\n",
      "Log Regression(639/4999): loss=0.5547085379622222\n",
      "Log Regression(640/4999): loss=0.5550651998707963\n",
      "Log Regression(641/4999): loss=0.5558990145869405\n",
      "Log Regression(642/4999): loss=0.554591386405591\n",
      "Log Regression(643/4999): loss=0.5555940612541491\n",
      "Log Regression(644/4999): loss=0.5629395344941177\n",
      "Log Regression(645/4999): loss=0.555992957086692\n",
      "Log Regression(646/4999): loss=0.5554850693095623\n",
      "Log Regression(647/4999): loss=0.5548683539133665\n",
      "Log Regression(648/4999): loss=0.5548921012825898\n",
      "Log Regression(649/4999): loss=0.5554673924531971\n",
      "Log Regression(650/4999): loss=0.5543017158058519\n",
      "Log Regression(651/4999): loss=0.5546015144246409\n",
      "Log Regression(652/4999): loss=0.5557867143721973\n",
      "Log Regression(653/4999): loss=0.5543542498235454\n",
      "Log Regression(654/4999): loss=0.5545860118545735\n",
      "Log Regression(655/4999): loss=0.5541289830208133\n",
      "Log Regression(656/4999): loss=0.5540557078694612\n",
      "Log Regression(657/4999): loss=0.5541105537025486\n",
      "Log Regression(658/4999): loss=0.5542356883675417\n",
      "Log Regression(659/4999): loss=0.5566175363790541\n",
      "Log Regression(660/4999): loss=0.5562759704507033\n",
      "Log Regression(661/4999): loss=0.562010322103955\n",
      "Log Regression(662/4999): loss=0.5566035931199896\n",
      "Log Regression(663/4999): loss=0.5571359997118766\n",
      "Log Regression(664/4999): loss=0.5552907047511166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(665/4999): loss=0.5602341627163508\n",
      "Log Regression(666/4999): loss=0.5553961935063999\n",
      "Log Regression(667/4999): loss=0.5539017214747416\n",
      "Log Regression(668/4999): loss=0.5539187887560787\n",
      "Log Regression(669/4999): loss=0.5545400229843725\n",
      "Log Regression(670/4999): loss=0.5552219488423459\n",
      "Log Regression(671/4999): loss=0.5579205443548472\n",
      "Log Regression(672/4999): loss=0.5536761401415047\n",
      "Log Regression(673/4999): loss=0.5536931758008755\n",
      "Log Regression(674/4999): loss=0.553513779481798\n",
      "Log Regression(675/4999): loss=0.5538362921290907\n",
      "Log Regression(676/4999): loss=0.5538429252799129\n",
      "Log Regression(677/4999): loss=0.5537105280928998\n",
      "Log Regression(678/4999): loss=0.5552863597272875\n",
      "Log Regression(679/4999): loss=0.5533513090002976\n",
      "Log Regression(680/4999): loss=0.5557320840939914\n",
      "Log Regression(681/4999): loss=0.5543382306943101\n",
      "Log Regression(682/4999): loss=0.5537848103909838\n",
      "Log Regression(683/4999): loss=0.5538487412461147\n",
      "Log Regression(684/4999): loss=0.5601593131265589\n",
      "Log Regression(685/4999): loss=0.5660662319035992\n",
      "Log Regression(686/4999): loss=0.5541306860006825\n",
      "Log Regression(687/4999): loss=0.5546004213223995\n",
      "Log Regression(688/4999): loss=0.5541764270614503\n",
      "Log Regression(689/4999): loss=0.5559398043665726\n",
      "Log Regression(690/4999): loss=0.5566089859956064\n",
      "Log Regression(691/4999): loss=0.5555976914588874\n",
      "Log Regression(692/4999): loss=0.5529854410264583\n",
      "Log Regression(693/4999): loss=0.5549411105307925\n",
      "Log Regression(694/4999): loss=0.5604467826029726\n",
      "Log Regression(695/4999): loss=0.561164809238738\n",
      "Log Regression(696/4999): loss=0.5563118186741881\n",
      "Log Regression(697/4999): loss=0.5568508445505\n",
      "Log Regression(698/4999): loss=0.557355040864287\n",
      "Log Regression(699/4999): loss=0.5530938675367446\n",
      "Log Regression(700/4999): loss=0.5529558928348254\n",
      "Log Regression(701/4999): loss=0.553425219922737\n",
      "Log Regression(702/4999): loss=0.552908435330786\n",
      "Log Regression(703/4999): loss=0.5529240654609844\n",
      "Log Regression(704/4999): loss=0.55288717317107\n",
      "Log Regression(705/4999): loss=0.5548650475136672\n",
      "Log Regression(706/4999): loss=0.5537226525668546\n",
      "Log Regression(707/4999): loss=0.5528930830935048\n",
      "Log Regression(708/4999): loss=0.5615984148905272\n",
      "Log Regression(709/4999): loss=0.5543310011901186\n",
      "Log Regression(710/4999): loss=0.5576779699903631\n",
      "Log Regression(711/4999): loss=0.5535727821930262\n",
      "Log Regression(712/4999): loss=0.5569633560260474\n",
      "Log Regression(713/4999): loss=0.5542754977427893\n",
      "Log Regression(714/4999): loss=0.5536928801470576\n",
      "Log Regression(715/4999): loss=0.5531103627165085\n",
      "Log Regression(716/4999): loss=0.5553350355326221\n",
      "Log Regression(717/4999): loss=0.5634965592811\n",
      "Log Regression(718/4999): loss=0.5548260681909415\n",
      "Log Regression(719/4999): loss=0.5537986076647218\n",
      "Log Regression(720/4999): loss=0.5548512045443655\n",
      "Log Regression(721/4999): loss=0.5634008298384424\n",
      "Log Regression(722/4999): loss=0.5598576681437907\n",
      "Log Regression(723/4999): loss=0.556624970070365\n",
      "Log Regression(724/4999): loss=0.5523777480663358\n",
      "Log Regression(725/4999): loss=0.55236598111512\n",
      "Log Regression(726/4999): loss=0.5537108159596128\n",
      "Log Regression(727/4999): loss=0.5547225449257278\n",
      "Log Regression(728/4999): loss=0.5555507595784427\n",
      "Log Regression(729/4999): loss=0.5554458438400962\n",
      "Log Regression(730/4999): loss=0.5524445173811942\n",
      "Log Regression(731/4999): loss=0.5531286225053159\n",
      "Log Regression(732/4999): loss=0.5553132452824251\n",
      "Log Regression(733/4999): loss=0.5525258200795654\n",
      "Log Regression(734/4999): loss=0.5535993814815818\n",
      "Log Regression(735/4999): loss=0.5534721958609473\n",
      "Log Regression(736/4999): loss=0.5578584668691516\n",
      "Log Regression(737/4999): loss=0.5524662280778357\n",
      "Log Regression(738/4999): loss=0.5538750644993204\n",
      "Log Regression(739/4999): loss=0.5519237300709955\n",
      "Log Regression(740/4999): loss=0.5530877327937938\n",
      "Log Regression(741/4999): loss=0.552059944512131\n",
      "Log Regression(742/4999): loss=0.5525044932531905\n",
      "Log Regression(743/4999): loss=0.5526144341110708\n",
      "Log Regression(744/4999): loss=0.5519419597506972\n",
      "Log Regression(745/4999): loss=0.5523123318688642\n",
      "Log Regression(746/4999): loss=0.5523539747507736\n",
      "Log Regression(747/4999): loss=0.5566955793630837\n",
      "Log Regression(748/4999): loss=0.5562231113382028\n",
      "Log Regression(749/4999): loss=0.5546549369762662\n",
      "Log Regression(750/4999): loss=0.5535308656352285\n",
      "Log Regression(751/4999): loss=0.5536656943740297\n",
      "Log Regression(752/4999): loss=0.5516998840360473\n",
      "Log Regression(753/4999): loss=0.5516167235862559\n",
      "Log Regression(754/4999): loss=0.5536376954042996\n",
      "Log Regression(755/4999): loss=0.5530370897125679\n",
      "Log Regression(756/4999): loss=0.5517264816323357\n",
      "Log Regression(757/4999): loss=0.5515055992409005\n",
      "Log Regression(758/4999): loss=0.5525130196386754\n",
      "Log Regression(759/4999): loss=0.5530989176320581\n",
      "Log Regression(760/4999): loss=0.5513421465464169\n",
      "Log Regression(761/4999): loss=0.5514382521775435\n",
      "Log Regression(762/4999): loss=0.5517067017281138\n",
      "Log Regression(763/4999): loss=0.5516202314237442\n",
      "Log Regression(764/4999): loss=0.551667792545908\n",
      "Log Regression(765/4999): loss=0.5513500939535192\n",
      "Log Regression(766/4999): loss=0.5519340545730109\n",
      "Log Regression(767/4999): loss=0.5515390356288644\n",
      "Log Regression(768/4999): loss=0.5519254558321207\n",
      "Log Regression(769/4999): loss=0.5566819433492679\n",
      "Log Regression(770/4999): loss=0.5515551402694057\n",
      "Log Regression(771/4999): loss=0.5571325893350503\n",
      "Log Regression(772/4999): loss=0.5518407521380602\n",
      "Log Regression(773/4999): loss=0.552420190555589\n",
      "Log Regression(774/4999): loss=0.5514367946111636\n",
      "Log Regression(775/4999): loss=0.5511081305499839\n",
      "Log Regression(776/4999): loss=0.5524080726725268\n",
      "Log Regression(777/4999): loss=0.5513159910705155\n",
      "Log Regression(778/4999): loss=0.5538551208340368\n",
      "Log Regression(779/4999): loss=0.5525711654271677\n",
      "Log Regression(780/4999): loss=0.5508745810781859\n",
      "Log Regression(781/4999): loss=0.5515552254486145\n",
      "Log Regression(782/4999): loss=0.5536248800021698\n",
      "Log Regression(783/4999): loss=0.5605011076060232\n",
      "Log Regression(784/4999): loss=0.5555718554171953\n",
      "Log Regression(785/4999): loss=0.5510856598600106\n",
      "Log Regression(786/4999): loss=0.5506775481974792\n",
      "Log Regression(787/4999): loss=0.5546312512214241\n",
      "Log Regression(788/4999): loss=0.55144476081513\n",
      "Log Regression(789/4999): loss=0.5508608488166803\n",
      "Log Regression(790/4999): loss=0.550769476134495\n",
      "Log Regression(791/4999): loss=0.5505877868657099\n",
      "Log Regression(792/4999): loss=0.5515539092167123\n",
      "Log Regression(793/4999): loss=0.5519050615099808\n",
      "Log Regression(794/4999): loss=0.5506971868655799\n",
      "Log Regression(795/4999): loss=0.5505462398162488\n",
      "Log Regression(796/4999): loss=0.5506779640528792\n",
      "Log Regression(797/4999): loss=0.5513498768483256\n",
      "Log Regression(798/4999): loss=0.5504513091824339\n",
      "Log Regression(799/4999): loss=0.5530397477428993\n",
      "Log Regression(800/4999): loss=0.5535091281377558\n",
      "Log Regression(801/4999): loss=0.5506140811114238\n",
      "Log Regression(802/4999): loss=0.5505664651114461\n",
      "Log Regression(803/4999): loss=0.5506384927138698\n",
      "Log Regression(804/4999): loss=0.5505320526872015\n",
      "Log Regression(805/4999): loss=0.5508861335878088\n",
      "Log Regression(806/4999): loss=0.5521406703767954\n",
      "Log Regression(807/4999): loss=0.5509755443323302\n",
      "Log Regression(808/4999): loss=0.5587786560723046\n",
      "Log Regression(809/4999): loss=0.5565951940884989\n",
      "Log Regression(810/4999): loss=0.553491518684629\n",
      "Log Regression(811/4999): loss=0.5554824103020284\n",
      "Log Regression(812/4999): loss=0.5529818873812054\n",
      "Log Regression(813/4999): loss=0.5549156007023217\n",
      "Log Regression(814/4999): loss=0.5518678502677324\n",
      "Log Regression(815/4999): loss=0.5517983375312642\n",
      "Log Regression(816/4999): loss=0.5501587547573762\n",
      "Log Regression(817/4999): loss=0.5656090793643042\n",
      "Log Regression(818/4999): loss=0.5705956438146973\n",
      "Log Regression(819/4999): loss=0.5603341873904851\n",
      "Log Regression(820/4999): loss=0.5533596331188909\n",
      "Log Regression(821/4999): loss=0.5605113932573579\n",
      "Log Regression(822/4999): loss=0.5566007161900088\n",
      "Log Regression(823/4999): loss=0.5538102183970894\n",
      "Log Regression(824/4999): loss=0.5519534116589784\n",
      "Log Regression(825/4999): loss=0.5514476934694552\n",
      "Log Regression(826/4999): loss=0.5497311353598058\n",
      "Log Regression(827/4999): loss=0.550095741157803\n",
      "Log Regression(828/4999): loss=0.5496928840613092\n",
      "Log Regression(829/4999): loss=0.5519578175716637\n",
      "Log Regression(830/4999): loss=0.5499235523630149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(831/4999): loss=0.5496711792064161\n",
      "Log Regression(832/4999): loss=0.5500987188978191\n",
      "Log Regression(833/4999): loss=0.5503905063085334\n",
      "Log Regression(834/4999): loss=0.5495951728890114\n",
      "Log Regression(835/4999): loss=0.5500044646923857\n",
      "Log Regression(836/4999): loss=0.5543901209839704\n",
      "Log Regression(837/4999): loss=0.5529786340179351\n",
      "Log Regression(838/4999): loss=0.5502764597274238\n",
      "Log Regression(839/4999): loss=0.5532393967631901\n",
      "Log Regression(840/4999): loss=0.5494233372099698\n",
      "Log Regression(841/4999): loss=0.5496546051585562\n",
      "Log Regression(842/4999): loss=0.5494073189114573\n",
      "Log Regression(843/4999): loss=0.554936968408945\n",
      "Log Regression(844/4999): loss=0.5551534660851126\n",
      "Log Regression(845/4999): loss=0.5551763955205227\n",
      "Log Regression(846/4999): loss=0.5537651052081507\n",
      "Log Regression(847/4999): loss=0.549716078256842\n",
      "Log Regression(848/4999): loss=0.5500850048822601\n",
      "Log Regression(849/4999): loss=0.5519893020028582\n",
      "Log Regression(850/4999): loss=0.549253897087549\n",
      "Log Regression(851/4999): loss=0.5497207690883995\n",
      "Log Regression(852/4999): loss=0.5497402609660736\n",
      "Log Regression(853/4999): loss=0.5496465252758784\n",
      "Log Regression(854/4999): loss=0.5490908893042146\n",
      "Log Regression(855/4999): loss=0.5491219705858064\n",
      "Log Regression(856/4999): loss=0.5495326021726563\n",
      "Log Regression(857/4999): loss=0.5494336198125246\n",
      "Log Regression(858/4999): loss=0.5496621957551932\n",
      "Log Regression(859/4999): loss=0.5527287978230502\n",
      "Log Regression(860/4999): loss=0.551066948605096\n",
      "Log Regression(861/4999): loss=0.5491860465418522\n",
      "Log Regression(862/4999): loss=0.5499368164767549\n",
      "Log Regression(863/4999): loss=0.5496195127566166\n",
      "Log Regression(864/4999): loss=0.5491533432129746\n",
      "Log Regression(865/4999): loss=0.5504731883113312\n",
      "Log Regression(866/4999): loss=0.5522530509484497\n",
      "Log Regression(867/4999): loss=0.5489184810802196\n",
      "Log Regression(868/4999): loss=0.5546162016635067\n",
      "Log Regression(869/4999): loss=0.5506226152523308\n",
      "Log Regression(870/4999): loss=0.5490974953819192\n",
      "Log Regression(871/4999): loss=0.5499382990805078\n",
      "Log Regression(872/4999): loss=0.5488624851269944\n",
      "Log Regression(873/4999): loss=0.5498215907349838\n",
      "Log Regression(874/4999): loss=0.5489377015523875\n",
      "Log Regression(875/4999): loss=0.5500267162794503\n",
      "Log Regression(876/4999): loss=0.5488745185585504\n",
      "Log Regression(877/4999): loss=0.5496998227856639\n",
      "Log Regression(878/4999): loss=0.5495004013847872\n",
      "Log Regression(879/4999): loss=0.5498967649140772\n",
      "Log Regression(880/4999): loss=0.5493615093824827\n",
      "Log Regression(881/4999): loss=0.5507577659871719\n",
      "Log Regression(882/4999): loss=0.5523885679777242\n",
      "Log Regression(883/4999): loss=0.551073259378432\n",
      "Log Regression(884/4999): loss=0.5487327693980396\n",
      "Log Regression(885/4999): loss=0.550153703357828\n",
      "Log Regression(886/4999): loss=0.5495295013607328\n",
      "Log Regression(887/4999): loss=0.5527121109782375\n",
      "Log Regression(888/4999): loss=0.5489191912423225\n",
      "Log Regression(889/4999): loss=0.5485867469826651\n",
      "Log Regression(890/4999): loss=0.549010159029162\n",
      "Log Regression(891/4999): loss=0.5487139323732578\n",
      "Log Regression(892/4999): loss=0.549246430606464\n",
      "Log Regression(893/4999): loss=0.5495861002775276\n",
      "Log Regression(894/4999): loss=0.550544590115632\n",
      "Log Regression(895/4999): loss=0.548965273421867\n",
      "Log Regression(896/4999): loss=0.550262949497283\n",
      "Log Regression(897/4999): loss=0.5499282198987682\n",
      "Log Regression(898/4999): loss=0.5553946429077299\n",
      "Log Regression(899/4999): loss=0.5495726346389351\n",
      "Log Regression(900/4999): loss=0.548309740191833\n",
      "Log Regression(901/4999): loss=0.5487104860615385\n",
      "Log Regression(902/4999): loss=0.5502236151867361\n",
      "Log Regression(903/4999): loss=0.5494763048138422\n",
      "Log Regression(904/4999): loss=0.5484969087258085\n",
      "Log Regression(905/4999): loss=0.5481673150826591\n",
      "Log Regression(906/4999): loss=0.5482132247453578\n",
      "Log Regression(907/4999): loss=0.548641609116701\n",
      "Log Regression(908/4999): loss=0.5483093407407196\n",
      "Log Regression(909/4999): loss=0.5481642664636177\n",
      "Log Regression(910/4999): loss=0.548508562249341\n",
      "Log Regression(911/4999): loss=0.5480764804853925\n",
      "Log Regression(912/4999): loss=0.5496096824183898\n",
      "Log Regression(913/4999): loss=0.5486295444433117\n",
      "Log Regression(914/4999): loss=0.5480931967899029\n",
      "Log Regression(915/4999): loss=0.5483459381732521\n",
      "Log Regression(916/4999): loss=0.5481642494656457\n",
      "Log Regression(917/4999): loss=0.5482955362590578\n",
      "Log Regression(918/4999): loss=0.5498617609491793\n",
      "Log Regression(919/4999): loss=0.5480635047974016\n",
      "Log Regression(920/4999): loss=0.5483180570029949\n",
      "Log Regression(921/4999): loss=0.5482353890826931\n",
      "Log Regression(922/4999): loss=0.5479373421268463\n",
      "Log Regression(923/4999): loss=0.5504060822201493\n",
      "Log Regression(924/4999): loss=0.548022501992404\n",
      "Log Regression(925/4999): loss=0.5478384440502131\n",
      "Log Regression(926/4999): loss=0.5478566516722648\n",
      "Log Regression(927/4999): loss=0.5499995013176668\n",
      "Log Regression(928/4999): loss=0.5479356714908723\n",
      "Log Regression(929/4999): loss=0.549385335258368\n",
      "Log Regression(930/4999): loss=0.5488566728742019\n",
      "Log Regression(931/4999): loss=0.549292093646222\n",
      "Log Regression(932/4999): loss=0.5477061398558122\n",
      "Log Regression(933/4999): loss=0.5481940223246015\n",
      "Log Regression(934/4999): loss=0.5476946102589245\n",
      "Log Regression(935/4999): loss=0.5476551352515723\n",
      "Log Regression(936/4999): loss=0.5487093085086582\n",
      "Log Regression(937/4999): loss=0.5479768309409107\n",
      "Log Regression(938/4999): loss=0.5478890900881714\n",
      "Log Regression(939/4999): loss=0.547894199257478\n",
      "Log Regression(940/4999): loss=0.5475900442288976\n",
      "Log Regression(941/4999): loss=0.5485983623523991\n",
      "Log Regression(942/4999): loss=0.548862130334417\n",
      "Log Regression(943/4999): loss=0.5476824514794806\n",
      "Log Regression(944/4999): loss=0.5483122927112816\n",
      "Log Regression(945/4999): loss=0.5527743308094683\n",
      "Log Regression(946/4999): loss=0.5508167445533955\n",
      "Log Regression(947/4999): loss=0.5524253207982192\n",
      "Log Regression(948/4999): loss=0.5508772295770862\n",
      "Log Regression(949/4999): loss=0.5474634916542845\n",
      "Log Regression(950/4999): loss=0.54742889647778\n",
      "Log Regression(951/4999): loss=0.5474696395912507\n",
      "Log Regression(952/4999): loss=0.5495213413993965\n",
      "Log Regression(953/4999): loss=0.5478066834291129\n",
      "Log Regression(954/4999): loss=0.5482899441364334\n",
      "Log Regression(955/4999): loss=0.5490645005397181\n",
      "Log Regression(956/4999): loss=0.5474377827992966\n",
      "Log Regression(957/4999): loss=0.5498290019974267\n",
      "Log Regression(958/4999): loss=0.5478203968159044\n",
      "Log Regression(959/4999): loss=0.5480202977171474\n",
      "Log Regression(960/4999): loss=0.5474864795712365\n",
      "Log Regression(961/4999): loss=0.5483384202050775\n",
      "Log Regression(962/4999): loss=0.5485260003366982\n",
      "Log Regression(963/4999): loss=0.5491182853997518\n",
      "Log Regression(964/4999): loss=0.5482126298429106\n",
      "Log Regression(965/4999): loss=0.5472584113526855\n",
      "Log Regression(966/4999): loss=0.5481488123362517\n",
      "Log Regression(967/4999): loss=0.5482633056939978\n",
      "Log Regression(968/4999): loss=0.5478656555854752\n",
      "Log Regression(969/4999): loss=0.5477423214354321\n",
      "Log Regression(970/4999): loss=0.5477520036445993\n",
      "Log Regression(971/4999): loss=0.547188545466084\n",
      "Log Regression(972/4999): loss=0.5531970412363979\n",
      "Log Regression(973/4999): loss=0.5519685853136751\n",
      "Log Regression(974/4999): loss=0.5480095350915458\n",
      "Log Regression(975/4999): loss=0.548020876736304\n",
      "Log Regression(976/4999): loss=0.5479920166975286\n",
      "Log Regression(977/4999): loss=0.553844765927774\n",
      "Log Regression(978/4999): loss=0.547891504099473\n",
      "Log Regression(979/4999): loss=0.5478717340815616\n",
      "Log Regression(980/4999): loss=0.5507629608678367\n",
      "Log Regression(981/4999): loss=0.5476117877486028\n",
      "Log Regression(982/4999): loss=0.5477512353404032\n",
      "Log Regression(983/4999): loss=0.5501140702672954\n",
      "Log Regression(984/4999): loss=0.5509411956069303\n",
      "Log Regression(985/4999): loss=0.5492176098680077\n",
      "Log Regression(986/4999): loss=0.5529258502014484\n",
      "Log Regression(987/4999): loss=0.5510097979818392\n",
      "Log Regression(988/4999): loss=0.5496228859152902\n",
      "Log Regression(989/4999): loss=0.5491577735329761\n",
      "Log Regression(990/4999): loss=0.5513623847326488\n",
      "Log Regression(991/4999): loss=0.550959734925336\n",
      "Log Regression(992/4999): loss=0.5511066239447254\n",
      "Log Regression(993/4999): loss=0.5491249695476197\n",
      "Log Regression(994/4999): loss=0.5484614902139283\n",
      "Log Regression(995/4999): loss=0.5483658270772307\n",
      "Log Regression(996/4999): loss=0.5495687195811779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(997/4999): loss=0.5489670847356679\n",
      "Log Regression(998/4999): loss=0.5510808263857452\n",
      "Log Regression(999/4999): loss=0.5505558116310918\n",
      "Log Regression(1000/4999): loss=0.5484326146855707\n",
      "Log Regression(1001/4999): loss=0.5482150904413872\n",
      "Log Regression(1002/4999): loss=0.549240430456087\n",
      "Log Regression(1003/4999): loss=0.5494108437399274\n",
      "Log Regression(1004/4999): loss=0.5496944741082016\n",
      "Log Regression(1005/4999): loss=0.5475073638619259\n",
      "Log Regression(1006/4999): loss=0.5471550819875318\n",
      "Log Regression(1007/4999): loss=0.5471121939556366\n",
      "Log Regression(1008/4999): loss=0.5479134482149142\n",
      "Log Regression(1009/4999): loss=0.5500765910108603\n",
      "Log Regression(1010/4999): loss=0.5479839245357511\n",
      "Log Regression(1011/4999): loss=0.5481864530430532\n",
      "Log Regression(1012/4999): loss=0.5512518024485734\n",
      "Log Regression(1013/4999): loss=0.5474574057850329\n",
      "Log Regression(1014/4999): loss=0.5478076014617571\n",
      "Log Regression(1015/4999): loss=0.5501992332386777\n",
      "Log Regression(1016/4999): loss=0.5468729807731606\n",
      "Log Regression(1017/4999): loss=0.546809243923294\n",
      "Log Regression(1018/4999): loss=0.5465933223960787\n",
      "Log Regression(1019/4999): loss=0.5466720418815633\n",
      "Log Regression(1020/4999): loss=0.5486581254830866\n",
      "Log Regression(1021/4999): loss=0.5484237681159998\n",
      "Log Regression(1022/4999): loss=0.5465756418302532\n",
      "Log Regression(1023/4999): loss=0.5475539340890252\n",
      "Log Regression(1024/4999): loss=0.5483278381437929\n",
      "Log Regression(1025/4999): loss=0.5466374096878076\n",
      "Log Regression(1026/4999): loss=0.5467921496326734\n",
      "Log Regression(1027/4999): loss=0.5477002570374658\n",
      "Log Regression(1028/4999): loss=0.5465902368591067\n",
      "Log Regression(1029/4999): loss=0.5497757450581774\n",
      "Log Regression(1030/4999): loss=0.5469557462467816\n",
      "Log Regression(1031/4999): loss=0.5467904567162197\n",
      "Log Regression(1032/4999): loss=0.5470539343435814\n",
      "Log Regression(1033/4999): loss=0.5468045391585501\n",
      "Log Regression(1034/4999): loss=0.5478485331365567\n",
      "Log Regression(1035/4999): loss=0.5469525402002722\n",
      "Log Regression(1036/4999): loss=0.5472993967076236\n",
      "Log Regression(1037/4999): loss=0.5475494260093343\n",
      "Log Regression(1038/4999): loss=0.5464936559997756\n",
      "Log Regression(1039/4999): loss=0.5472320151832201\n",
      "Log Regression(1040/4999): loss=0.5488650554869094\n",
      "Log Regression(1041/4999): loss=0.5464497869118841\n",
      "Log Regression(1042/4999): loss=0.5486691626087344\n",
      "Log Regression(1043/4999): loss=0.5463636770541721\n",
      "Log Regression(1044/4999): loss=0.5488288057134237\n",
      "Log Regression(1045/4999): loss=0.5474840206512869\n",
      "Log Regression(1046/4999): loss=0.5466950208959919\n",
      "Log Regression(1047/4999): loss=0.549115972613582\n",
      "Log Regression(1048/4999): loss=0.549822835557389\n",
      "Log Regression(1049/4999): loss=0.5463266281126102\n",
      "Log Regression(1050/4999): loss=0.5463067955659406\n",
      "Log Regression(1051/4999): loss=0.5510065409118878\n",
      "Log Regression(1052/4999): loss=0.5469115773697005\n",
      "Log Regression(1053/4999): loss=0.5461728418734436\n",
      "Log Regression(1054/4999): loss=0.5472170397266867\n",
      "Log Regression(1055/4999): loss=0.5472467216387761\n",
      "Log Regression(1056/4999): loss=0.5465425731556902\n",
      "Log Regression(1057/4999): loss=0.5462691544589257\n",
      "Log Regression(1058/4999): loss=0.5462532007118938\n",
      "Log Regression(1059/4999): loss=0.5460543514300202\n",
      "Log Regression(1060/4999): loss=0.5468491606886201\n",
      "Log Regression(1061/4999): loss=0.5473597285195363\n",
      "Log Regression(1062/4999): loss=0.5460392588429449\n",
      "Log Regression(1063/4999): loss=0.5474749620628149\n",
      "Log Regression(1064/4999): loss=0.5472853188960554\n",
      "Log Regression(1065/4999): loss=0.5515986567966332\n",
      "Log Regression(1066/4999): loss=0.5471721030962592\n",
      "Log Regression(1067/4999): loss=0.5461940485046509\n",
      "Log Regression(1068/4999): loss=0.5479377014543991\n",
      "Log Regression(1069/4999): loss=0.5470482175096574\n",
      "Log Regression(1070/4999): loss=0.5459552844743505\n",
      "Log Regression(1071/4999): loss=0.5485894464335919\n",
      "Log Regression(1072/4999): loss=0.5467161856665517\n",
      "Log Regression(1073/4999): loss=0.5484832452419955\n",
      "Log Regression(1074/4999): loss=0.5511657112741161\n",
      "Log Regression(1075/4999): loss=0.5497938648392404\n",
      "Log Regression(1076/4999): loss=0.5520362955059548\n",
      "Log Regression(1077/4999): loss=0.5459221395044036\n",
      "Log Regression(1078/4999): loss=0.5468365705276521\n",
      "Log Regression(1079/4999): loss=0.5468059255089731\n",
      "Log Regression(1080/4999): loss=0.5465867121166447\n",
      "Log Regression(1081/4999): loss=0.5461774963268305\n",
      "Log Regression(1082/4999): loss=0.5483912289853469\n",
      "Log Regression(1083/4999): loss=0.5459407140166253\n",
      "Log Regression(1084/4999): loss=0.5458356571368067\n",
      "Log Regression(1085/4999): loss=0.546260982277697\n",
      "Log Regression(1086/4999): loss=0.5478651828832183\n",
      "Log Regression(1087/4999): loss=0.5482243951860543\n",
      "Log Regression(1088/4999): loss=0.5457857503947091\n",
      "Log Regression(1089/4999): loss=0.5459558066332652\n",
      "Log Regression(1090/4999): loss=0.5476598813122008\n",
      "Log Regression(1091/4999): loss=0.5458220331507742\n",
      "Log Regression(1092/4999): loss=0.548379595427473\n",
      "Log Regression(1093/4999): loss=0.5475676665315364\n",
      "Log Regression(1094/4999): loss=0.545831442619481\n",
      "Log Regression(1095/4999): loss=0.5466167331893876\n",
      "Log Regression(1096/4999): loss=0.5479122187985287\n",
      "Log Regression(1097/4999): loss=0.5457658865047263\n",
      "Log Regression(1098/4999): loss=0.5541802401840142\n",
      "Log Regression(1099/4999): loss=0.5497280201207112\n",
      "Log Regression(1100/4999): loss=0.5493506085789565\n",
      "Log Regression(1101/4999): loss=0.5458847235610568\n",
      "Log Regression(1102/4999): loss=0.5461208013939307\n",
      "Log Regression(1103/4999): loss=0.5460280543846624\n",
      "Log Regression(1104/4999): loss=0.5467326259751792\n",
      "Log Regression(1105/4999): loss=0.5458217476299119\n",
      "Log Regression(1106/4999): loss=0.5457842801335161\n",
      "Log Regression(1107/4999): loss=0.5502660396367227\n",
      "Log Regression(1108/4999): loss=0.5464692527622111\n",
      "Log Regression(1109/4999): loss=0.5457339765101517\n",
      "Log Regression(1110/4999): loss=0.5455683333468259\n",
      "Log Regression(1111/4999): loss=0.5458100516373833\n",
      "Log Regression(1112/4999): loss=0.5481288361838833\n",
      "Log Regression(1113/4999): loss=0.5455319712607483\n",
      "Log Regression(1114/4999): loss=0.5455447397398913\n",
      "Log Regression(1115/4999): loss=0.5455633112955888\n",
      "Log Regression(1116/4999): loss=0.5461781543422994\n",
      "Log Regression(1117/4999): loss=0.5455016763433332\n",
      "Log Regression(1118/4999): loss=0.5456261096180304\n",
      "Log Regression(1119/4999): loss=0.5461756537203586\n",
      "Log Regression(1120/4999): loss=0.5463381278647317\n",
      "Log Regression(1121/4999): loss=0.546261123114602\n",
      "Log Regression(1122/4999): loss=0.5467353748403325\n",
      "Log Regression(1123/4999): loss=0.5463343980328776\n",
      "Log Regression(1124/4999): loss=0.5461457770389091\n",
      "Log Regression(1125/4999): loss=0.5459068352906272\n",
      "Log Regression(1126/4999): loss=0.5475586954231842\n",
      "Log Regression(1127/4999): loss=0.5462613192675676\n",
      "Log Regression(1128/4999): loss=0.5458594022459763\n",
      "Log Regression(1129/4999): loss=0.5468869307683991\n",
      "Log Regression(1130/4999): loss=0.5465819053993178\n",
      "Log Regression(1131/4999): loss=0.5454919254682025\n",
      "Log Regression(1132/4999): loss=0.5455316258808591\n",
      "Log Regression(1133/4999): loss=0.5471175362313001\n",
      "Log Regression(1134/4999): loss=0.5455703148063225\n",
      "Log Regression(1135/4999): loss=0.5473654682029236\n",
      "Log Regression(1136/4999): loss=0.5526578580557626\n",
      "Log Regression(1137/4999): loss=0.5454032075969373\n",
      "Log Regression(1138/4999): loss=0.5455475693084632\n",
      "Log Regression(1139/4999): loss=0.5455732343178911\n",
      "Log Regression(1140/4999): loss=0.5459833943765977\n",
      "Log Regression(1141/4999): loss=0.5454177318766765\n",
      "Log Regression(1142/4999): loss=0.5490156313339394\n",
      "Log Regression(1143/4999): loss=0.5488065844679795\n",
      "Log Regression(1144/4999): loss=0.5524961324796916\n",
      "Log Regression(1145/4999): loss=0.545954450384546\n",
      "Log Regression(1146/4999): loss=0.5457288239352001\n",
      "Log Regression(1147/4999): loss=0.545914953969768\n",
      "Log Regression(1148/4999): loss=0.5454361824836541\n",
      "Log Regression(1149/4999): loss=0.545991448408783\n",
      "Log Regression(1150/4999): loss=0.5452814371423208\n",
      "Log Regression(1151/4999): loss=0.5458509264411159\n",
      "Log Regression(1152/4999): loss=0.5456394594868237\n",
      "Log Regression(1153/4999): loss=0.5455002378857657\n",
      "Log Regression(1154/4999): loss=0.5468841898704486\n",
      "Log Regression(1155/4999): loss=0.5523867322056683\n",
      "Log Regression(1156/4999): loss=0.548295772852267\n",
      "Log Regression(1157/4999): loss=0.5455046340115024\n",
      "Log Regression(1158/4999): loss=0.5456528327525475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(1159/4999): loss=0.545787650385867\n",
      "Log Regression(1160/4999): loss=0.5461666079939301\n",
      "Log Regression(1161/4999): loss=0.5495745402303325\n",
      "Log Regression(1162/4999): loss=0.5496531570190849\n",
      "Log Regression(1163/4999): loss=0.5474240035103046\n",
      "Log Regression(1164/4999): loss=0.5456108288624152\n",
      "Log Regression(1165/4999): loss=0.5454800948441723\n",
      "Log Regression(1166/4999): loss=0.5463937321283888\n",
      "Log Regression(1167/4999): loss=0.5478239288272\n",
      "Log Regression(1168/4999): loss=0.5454884878675017\n",
      "Log Regression(1169/4999): loss=0.5456633459930224\n",
      "Log Regression(1170/4999): loss=0.5467255588863774\n",
      "Log Regression(1171/4999): loss=0.5458905505137147\n",
      "Log Regression(1172/4999): loss=0.5473629633062504\n",
      "Log Regression(1173/4999): loss=0.5458682482375803\n",
      "Log Regression(1174/4999): loss=0.5457913415528953\n",
      "Log Regression(1175/4999): loss=0.5456930191103688\n",
      "Log Regression(1176/4999): loss=0.5454586179826976\n",
      "Log Regression(1177/4999): loss=0.5455267754143096\n",
      "Log Regression(1178/4999): loss=0.5477457120693554\n",
      "Log Regression(1179/4999): loss=0.546111759964574\n",
      "Log Regression(1180/4999): loss=0.5458794039470138\n",
      "Log Regression(1181/4999): loss=0.5453734427619688\n",
      "Log Regression(1182/4999): loss=0.5458431468079863\n",
      "Log Regression(1183/4999): loss=0.545372486512444\n",
      "Log Regression(1184/4999): loss=0.5469391973256527\n",
      "Log Regression(1185/4999): loss=0.5471515844781059\n",
      "Log Regression(1186/4999): loss=0.5453599590567325\n",
      "Log Regression(1187/4999): loss=0.5477250327110484\n",
      "Log Regression(1188/4999): loss=0.5455929506538609\n",
      "Log Regression(1189/4999): loss=0.5453503789556527\n",
      "Log Regression(1190/4999): loss=0.5452553357560254\n",
      "Log Regression(1191/4999): loss=0.545582349541277\n",
      "Log Regression(1192/4999): loss=0.545498716679621\n",
      "Log Regression(1193/4999): loss=0.5456126299102884\n",
      "Log Regression(1194/4999): loss=0.5452343026243653\n",
      "Log Regression(1195/4999): loss=0.5453694531640662\n",
      "Log Regression(1196/4999): loss=0.5452292061375762\n",
      "Log Regression(1197/4999): loss=0.545922061833509\n",
      "Log Regression(1198/4999): loss=0.5459825944978102\n",
      "Log Regression(1199/4999): loss=0.5454131108253488\n",
      "Log Regression(1200/4999): loss=0.5464954405203313\n",
      "Log Regression(1201/4999): loss=0.5454634860684154\n",
      "Log Regression(1202/4999): loss=0.5460150636151647\n",
      "Log Regression(1203/4999): loss=0.5490055756550073\n",
      "Log Regression(1204/4999): loss=0.5452391354418793\n",
      "Log Regression(1205/4999): loss=0.5453084829326398\n",
      "Log Regression(1206/4999): loss=0.5451801105173287\n",
      "Log Regression(1207/4999): loss=0.5458900778771887\n",
      "Log Regression(1208/4999): loss=0.5451545232892568\n",
      "Log Regression(1209/4999): loss=0.5570482078021186\n",
      "Log Regression(1210/4999): loss=0.5515945110851959\n",
      "Log Regression(1211/4999): loss=0.5507315703006228\n",
      "Log Regression(1212/4999): loss=0.5471628342712583\n",
      "Log Regression(1213/4999): loss=0.5469473706704979\n",
      "Log Regression(1214/4999): loss=0.5450542875505965\n",
      "Log Regression(1215/4999): loss=0.5519731812494761\n",
      "Log Regression(1216/4999): loss=0.5567853298310164\n",
      "Log Regression(1217/4999): loss=0.5465251348722817\n",
      "Log Regression(1218/4999): loss=0.5457706597522337\n",
      "Log Regression(1219/4999): loss=0.5459012209152215\n",
      "Log Regression(1220/4999): loss=0.5459733724723341\n",
      "Log Regression(1221/4999): loss=0.5459878306968035\n",
      "Log Regression(1222/4999): loss=0.5465593583003667\n",
      "Log Regression(1223/4999): loss=0.5457266866591125\n",
      "Log Regression(1224/4999): loss=0.5507849317636684\n",
      "Log Regression(1225/4999): loss=0.5503753188606125\n",
      "Log Regression(1226/4999): loss=0.5449232478736357\n",
      "Log Regression(1227/4999): loss=0.5447310884958388\n",
      "Log Regression(1228/4999): loss=0.5449887084702828\n",
      "Log Regression(1229/4999): loss=0.5447594753796409\n",
      "Log Regression(1230/4999): loss=0.5450325641500889\n",
      "Log Regression(1231/4999): loss=0.5494405257110216\n",
      "Log Regression(1232/4999): loss=0.5446739948326722\n",
      "Log Regression(1233/4999): loss=0.5447700899118355\n",
      "Log Regression(1234/4999): loss=0.5455582033642412\n",
      "Log Regression(1235/4999): loss=0.5467979516734087\n",
      "Log Regression(1236/4999): loss=0.54686069310692\n",
      "Log Regression(1237/4999): loss=0.544936053887707\n",
      "Log Regression(1238/4999): loss=0.5459370094059909\n",
      "Log Regression(1239/4999): loss=0.5516258167823616\n",
      "Log Regression(1240/4999): loss=0.5491538745948544\n",
      "Log Regression(1241/4999): loss=0.553333355935361\n",
      "Log Regression(1242/4999): loss=0.5508557718245677\n",
      "Log Regression(1243/4999): loss=0.5516064918566728\n",
      "Log Regression(1244/4999): loss=0.5475821950077824\n",
      "Log Regression(1245/4999): loss=0.5463948412000996\n",
      "Log Regression(1246/4999): loss=0.5449978421218439\n",
      "Log Regression(1247/4999): loss=0.5450364184902952\n",
      "Log Regression(1248/4999): loss=0.5473399377924574\n",
      "Log Regression(1249/4999): loss=0.5458562227637607\n",
      "Log Regression(1250/4999): loss=0.546582859878292\n",
      "Log Regression(1251/4999): loss=0.5446996794209975\n",
      "Log Regression(1252/4999): loss=0.5444561230601553\n",
      "Log Regression(1253/4999): loss=0.5447145593720772\n",
      "Log Regression(1254/4999): loss=0.5444626836599107\n",
      "Log Regression(1255/4999): loss=0.5444905056144558\n",
      "Log Regression(1256/4999): loss=0.5445720718504995\n",
      "Log Regression(1257/4999): loss=0.5446075524712539\n",
      "Log Regression(1258/4999): loss=0.5448741202778227\n",
      "Log Regression(1259/4999): loss=0.5489254898213473\n",
      "Log Regression(1260/4999): loss=0.5450164454353893\n",
      "Log Regression(1261/4999): loss=0.5502042767991114\n",
      "Log Regression(1262/4999): loss=0.5522377689138528\n",
      "Log Regression(1263/4999): loss=0.5519373720324992\n",
      "Log Regression(1264/4999): loss=0.547236770624857\n",
      "Log Regression(1265/4999): loss=0.5471389536839337\n",
      "Log Regression(1266/4999): loss=0.5445443209310844\n",
      "Log Regression(1267/4999): loss=0.5455229528659526\n",
      "Log Regression(1268/4999): loss=0.5453765069847403\n",
      "Log Regression(1269/4999): loss=0.5490351146601027\n",
      "Log Regression(1270/4999): loss=0.5446677152542633\n",
      "Log Regression(1271/4999): loss=0.5445688312259632\n",
      "Log Regression(1272/4999): loss=0.5446909159261792\n",
      "Log Regression(1273/4999): loss=0.5448631812796217\n",
      "Log Regression(1274/4999): loss=0.5449166619749449\n",
      "Log Regression(1275/4999): loss=0.5451497168588384\n",
      "Log Regression(1276/4999): loss=0.5454484630725972\n",
      "Log Regression(1277/4999): loss=0.5476959841826629\n",
      "Log Regression(1278/4999): loss=0.5448159734418214\n",
      "Log Regression(1279/4999): loss=0.5442596173171754\n",
      "Log Regression(1280/4999): loss=0.5482665952233521\n",
      "Log Regression(1281/4999): loss=0.5464995371683831\n",
      "Log Regression(1282/4999): loss=0.5474676892827619\n",
      "Log Regression(1283/4999): loss=0.5463195535116087\n",
      "Log Regression(1284/4999): loss=0.5447032313304123\n",
      "Log Regression(1285/4999): loss=0.5459930630862978\n",
      "Log Regression(1286/4999): loss=0.544538975416047\n",
      "Log Regression(1287/4999): loss=0.5445325955558792\n",
      "Log Regression(1288/4999): loss=0.5461489691682684\n",
      "Log Regression(1289/4999): loss=0.5465620699246999\n",
      "Log Regression(1290/4999): loss=0.5528310765688801\n",
      "Log Regression(1291/4999): loss=0.5461434574575001\n",
      "Log Regression(1292/4999): loss=0.5455469027261544\n",
      "Log Regression(1293/4999): loss=0.5471848823294014\n",
      "Log Regression(1294/4999): loss=0.548671819338064\n",
      "Log Regression(1295/4999): loss=0.5543232680437592\n",
      "Log Regression(1296/4999): loss=0.5456194937736976\n",
      "Log Regression(1297/4999): loss=0.5491434916471597\n",
      "Log Regression(1298/4999): loss=0.5448782398117294\n",
      "Log Regression(1299/4999): loss=0.548657577644095\n",
      "Log Regression(1300/4999): loss=0.5464067022820797\n",
      "Log Regression(1301/4999): loss=0.5443915973698983\n",
      "Log Regression(1302/4999): loss=0.5442228507569096\n",
      "Log Regression(1303/4999): loss=0.5441968187273999\n",
      "Log Regression(1304/4999): loss=0.5449349423587562\n",
      "Log Regression(1305/4999): loss=0.544255000933756\n",
      "Log Regression(1306/4999): loss=0.5441426110010604\n",
      "Log Regression(1307/4999): loss=0.5486109578089804\n",
      "Log Regression(1308/4999): loss=0.5466130963229856\n",
      "Log Regression(1309/4999): loss=0.5440642394360834\n",
      "Log Regression(1310/4999): loss=0.5473155616656217\n",
      "Log Regression(1311/4999): loss=0.5490185642108871\n",
      "Log Regression(1312/4999): loss=0.5542417637703796\n",
      "Log Regression(1313/4999): loss=0.5481880444869309\n",
      "Log Regression(1314/4999): loss=0.5452556935326712\n",
      "Log Regression(1315/4999): loss=0.5450821819979865\n",
      "Log Regression(1316/4999): loss=0.544603556736806\n",
      "Log Regression(1317/4999): loss=0.5441498807734051\n",
      "Log Regression(1318/4999): loss=0.5444995562740662\n",
      "Log Regression(1319/4999): loss=0.5492583941387983\n",
      "Log Regression(1320/4999): loss=0.5476742630006339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(1321/4999): loss=0.5475567314862018\n",
      "Log Regression(1322/4999): loss=0.5481986250978664\n",
      "Log Regression(1323/4999): loss=0.5498915884692421\n",
      "Log Regression(1324/4999): loss=0.5442330333934642\n",
      "Log Regression(1325/4999): loss=0.54630872834265\n",
      "Log Regression(1326/4999): loss=0.5446039259025917\n",
      "Log Regression(1327/4999): loss=0.5441891148518238\n",
      "Log Regression(1328/4999): loss=0.5440226589845009\n",
      "Log Regression(1329/4999): loss=0.5439750689403238\n",
      "Log Regression(1330/4999): loss=0.5446981773964197\n",
      "Log Regression(1331/4999): loss=0.5518572810832788\n",
      "Log Regression(1332/4999): loss=0.5524841121299108\n",
      "Log Regression(1333/4999): loss=0.5455958001413855\n",
      "Log Regression(1334/4999): loss=0.5446219639792904\n",
      "Log Regression(1335/4999): loss=0.5492590067289007\n",
      "Log Regression(1336/4999): loss=0.5468526653877224\n",
      "Log Regression(1337/4999): loss=0.5453474387382841\n",
      "Log Regression(1338/4999): loss=0.5456442294396284\n",
      "Log Regression(1339/4999): loss=0.5500542910549533\n",
      "Log Regression(1340/4999): loss=0.5460584602973209\n",
      "Log Regression(1341/4999): loss=0.5450493531309311\n",
      "Log Regression(1342/4999): loss=0.544616697733646\n",
      "Log Regression(1343/4999): loss=0.5443341400810953\n",
      "Log Regression(1344/4999): loss=0.5459806625534069\n",
      "Log Regression(1345/4999): loss=0.5489177739081087\n",
      "Log Regression(1346/4999): loss=0.5495721574805916\n",
      "Log Regression(1347/4999): loss=0.5471109228624859\n",
      "Log Regression(1348/4999): loss=0.553459801506098\n",
      "Log Regression(1349/4999): loss=0.5455389567194554\n",
      "Log Regression(1350/4999): loss=0.5471780317448453\n",
      "Log Regression(1351/4999): loss=0.5448115742643745\n",
      "Log Regression(1352/4999): loss=0.5443762096712186\n",
      "Log Regression(1353/4999): loss=0.5441006714037495\n",
      "Log Regression(1354/4999): loss=0.5442321776422349\n",
      "Log Regression(1355/4999): loss=0.5445893337910318\n",
      "Log Regression(1356/4999): loss=0.54545472898529\n",
      "Log Regression(1357/4999): loss=0.5500953947834071\n",
      "Log Regression(1358/4999): loss=0.5461029326016666\n",
      "Log Regression(1359/4999): loss=0.5521690269606273\n",
      "Log Regression(1360/4999): loss=0.5468690445362299\n",
      "Log Regression(1361/4999): loss=0.5448435961292912\n",
      "Log Regression(1362/4999): loss=0.544929038056858\n",
      "Log Regression(1363/4999): loss=0.5455411685519443\n",
      "Log Regression(1364/4999): loss=0.5469688915866852\n",
      "Log Regression(1365/4999): loss=0.5467063468128733\n",
      "Log Regression(1366/4999): loss=0.5443161913943757\n",
      "Log Regression(1367/4999): loss=0.5450309715633653\n",
      "Log Regression(1368/4999): loss=0.5441614873707901\n",
      "Log Regression(1369/4999): loss=0.5438913557029267\n",
      "Log Regression(1370/4999): loss=0.5439360885760943\n",
      "Log Regression(1371/4999): loss=0.5458558453572421\n",
      "Log Regression(1372/4999): loss=0.543811524366626\n",
      "Log Regression(1373/4999): loss=0.5441186913647913\n",
      "Log Regression(1374/4999): loss=0.5449069556498282\n",
      "Log Regression(1375/4999): loss=0.5460077648119633\n",
      "Log Regression(1376/4999): loss=0.5437845145098771\n",
      "Log Regression(1377/4999): loss=0.544539598207582\n",
      "Log Regression(1378/4999): loss=0.5438038406403832\n",
      "Log Regression(1379/4999): loss=0.5439905226972014\n",
      "Log Regression(1380/4999): loss=0.5439409616903238\n",
      "Log Regression(1381/4999): loss=0.5437665310031734\n",
      "Log Regression(1382/4999): loss=0.543873416334493\n",
      "Log Regression(1383/4999): loss=0.5443064293357803\n",
      "Log Regression(1384/4999): loss=0.5448997304732314\n",
      "Log Regression(1385/4999): loss=0.5438620956416347\n",
      "Log Regression(1386/4999): loss=0.5442396016337381\n",
      "Log Regression(1387/4999): loss=0.544350031286982\n",
      "Log Regression(1388/4999): loss=0.5440697905565908\n",
      "Log Regression(1389/4999): loss=0.5474993263040021\n",
      "Log Regression(1390/4999): loss=0.5448071206113194\n",
      "Log Regression(1391/4999): loss=0.5452759143596069\n",
      "Log Regression(1392/4999): loss=0.5437835487541754\n",
      "Log Regression(1393/4999): loss=0.5438355671135535\n",
      "Log Regression(1394/4999): loss=0.5447594530310672\n",
      "Log Regression(1395/4999): loss=0.5465530275169909\n",
      "Log Regression(1396/4999): loss=0.5444387031131861\n",
      "Log Regression(1397/4999): loss=0.5437293388634532\n",
      "Log Regression(1398/4999): loss=0.5438095865743523\n",
      "Log Regression(1399/4999): loss=0.5478751842074594\n",
      "Log Regression(1400/4999): loss=0.54501043656954\n",
      "Log Regression(1401/4999): loss=0.5437498461100456\n",
      "Log Regression(1402/4999): loss=0.5440965412767874\n",
      "Log Regression(1403/4999): loss=0.543689370178025\n",
      "Log Regression(1404/4999): loss=0.5442015597887353\n",
      "Log Regression(1405/4999): loss=0.5441272605352934\n",
      "Log Regression(1406/4999): loss=0.5439460045424763\n",
      "Log Regression(1407/4999): loss=0.5455628541469415\n",
      "Log Regression(1408/4999): loss=0.5443418727258025\n",
      "Log Regression(1409/4999): loss=0.546183970013418\n",
      "Log Regression(1410/4999): loss=0.550452069336708\n",
      "Log Regression(1411/4999): loss=0.5445572399580446\n",
      "Log Regression(1412/4999): loss=0.5435127640867397\n",
      "Log Regression(1413/4999): loss=0.5460885883876182\n",
      "Log Regression(1414/4999): loss=0.5450758633780661\n",
      "Log Regression(1415/4999): loss=0.5464832642865918\n",
      "Log Regression(1416/4999): loss=0.5454047324597129\n",
      "Log Regression(1417/4999): loss=0.5498093437480773\n",
      "Log Regression(1418/4999): loss=0.5454784699432741\n",
      "Log Regression(1419/4999): loss=0.5441129347561792\n",
      "Log Regression(1420/4999): loss=0.544748573480449\n",
      "Log Regression(1421/4999): loss=0.5434951757451019\n",
      "Log Regression(1422/4999): loss=0.5471318784351524\n",
      "Log Regression(1423/4999): loss=0.5519272964348912\n",
      "Log Regression(1424/4999): loss=0.5495493608152276\n",
      "Log Regression(1425/4999): loss=0.5470064235215623\n",
      "Log Regression(1426/4999): loss=0.5463906325837946\n",
      "Log Regression(1427/4999): loss=0.5436151161173847\n",
      "Log Regression(1428/4999): loss=0.5469422677369056\n",
      "Log Regression(1429/4999): loss=0.5475115033741154\n",
      "Log Regression(1430/4999): loss=0.5442596520687079\n",
      "Log Regression(1431/4999): loss=0.5445712430538758\n",
      "Log Regression(1432/4999): loss=0.5437009591405277\n",
      "Log Regression(1433/4999): loss=0.5436738928144053\n",
      "Log Regression(1434/4999): loss=0.544318761234135\n",
      "Log Regression(1435/4999): loss=0.5435641782142491\n",
      "Log Regression(1436/4999): loss=0.5438337481582503\n",
      "Log Regression(1437/4999): loss=0.5443063870976312\n",
      "Log Regression(1438/4999): loss=0.5454498153563047\n",
      "Log Regression(1439/4999): loss=0.5504929246133237\n",
      "Log Regression(1440/4999): loss=0.5525234937584066\n",
      "Log Regression(1441/4999): loss=0.5441554475253785\n",
      "Log Regression(1442/4999): loss=0.5457902979800161\n",
      "Log Regression(1443/4999): loss=0.5496572797441585\n",
      "Log Regression(1444/4999): loss=0.5475031369742681\n",
      "Log Regression(1445/4999): loss=0.54396387784535\n",
      "Log Regression(1446/4999): loss=0.5436274779447694\n",
      "Log Regression(1447/4999): loss=0.5436027009754718\n",
      "Log Regression(1448/4999): loss=0.5434905138980528\n",
      "Log Regression(1449/4999): loss=0.5438955408491372\n",
      "Log Regression(1450/4999): loss=0.5457648807816652\n",
      "Log Regression(1451/4999): loss=0.5446321303743004\n",
      "Log Regression(1452/4999): loss=0.5435405250175305\n",
      "Log Regression(1453/4999): loss=0.5452345353926168\n",
      "Log Regression(1454/4999): loss=0.5439062469070624\n",
      "Log Regression(1455/4999): loss=0.5434341816391155\n",
      "Log Regression(1456/4999): loss=0.5442675349232904\n",
      "Log Regression(1457/4999): loss=0.5440456355424778\n",
      "Log Regression(1458/4999): loss=0.5473105336046807\n",
      "Log Regression(1459/4999): loss=0.5440529821701806\n",
      "Log Regression(1460/4999): loss=0.545015206292672\n",
      "Log Regression(1461/4999): loss=0.5446489757454999\n",
      "Log Regression(1462/4999): loss=0.5451284174844472\n",
      "Log Regression(1463/4999): loss=0.5442037394928506\n",
      "Log Regression(1464/4999): loss=0.544203251044416\n",
      "Log Regression(1465/4999): loss=0.5448393878981054\n",
      "Log Regression(1466/4999): loss=0.547113747061331\n",
      "Log Regression(1467/4999): loss=0.5437476366731646\n",
      "Log Regression(1468/4999): loss=0.5455524221809778\n",
      "Log Regression(1469/4999): loss=0.5453339059716003\n",
      "Log Regression(1470/4999): loss=0.547123185638562\n",
      "Log Regression(1471/4999): loss=0.5506245885403249\n",
      "Log Regression(1472/4999): loss=0.5465800158095476\n",
      "Log Regression(1473/4999): loss=0.5438050957754765\n",
      "Log Regression(1474/4999): loss=0.5450429724481854\n",
      "Log Regression(1475/4999): loss=0.5441029350535156\n",
      "Log Regression(1476/4999): loss=0.5436697873283142\n",
      "Log Regression(1477/4999): loss=0.5438438846915019\n",
      "Log Regression(1478/4999): loss=0.5460270952485727\n",
      "Log Regression(1479/4999): loss=0.5437240733322941\n",
      "Log Regression(1480/4999): loss=0.543191419266321\n",
      "Log Regression(1481/4999): loss=0.5433493619109486\n",
      "Log Regression(1482/4999): loss=0.5433585049062793\n",
      "Log Regression(1483/4999): loss=0.5437338074218242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(1484/4999): loss=0.5451372848011964\n",
      "Log Regression(1485/4999): loss=0.5433136654122434\n",
      "Log Regression(1486/4999): loss=0.5439165262977989\n",
      "Log Regression(1487/4999): loss=0.5435638867719212\n",
      "Log Regression(1488/4999): loss=0.5448818100406757\n",
      "Log Regression(1489/4999): loss=0.5463490137957496\n",
      "Log Regression(1490/4999): loss=0.5607543576151693\n",
      "Log Regression(1491/4999): loss=0.5495716472399713\n",
      "Log Regression(1492/4999): loss=0.5467793730018339\n",
      "Log Regression(1493/4999): loss=0.5453748837213621\n",
      "Log Regression(1494/4999): loss=0.5472610232522819\n",
      "Log Regression(1495/4999): loss=0.5448568517279594\n",
      "Log Regression(1496/4999): loss=0.543418649307963\n",
      "Log Regression(1497/4999): loss=0.5441330413111084\n",
      "Log Regression(1498/4999): loss=0.5435478019871882\n",
      "Log Regression(1499/4999): loss=0.545108445431921\n",
      "Log Regression(1500/4999): loss=0.5432990391697522\n",
      "Log Regression(1501/4999): loss=0.5463524489427874\n",
      "Log Regression(1502/4999): loss=0.5432229421699072\n",
      "Log Regression(1503/4999): loss=0.5431377680519117\n",
      "Log Regression(1504/4999): loss=0.5434410135559498\n",
      "Log Regression(1505/4999): loss=0.5430580631937002\n",
      "Log Regression(1506/4999): loss=0.5431350315774747\n",
      "Log Regression(1507/4999): loss=0.5431333039433947\n",
      "Log Regression(1508/4999): loss=0.5435614734887204\n",
      "Log Regression(1509/4999): loss=0.5439003817626554\n",
      "Log Regression(1510/4999): loss=0.5435245016036077\n",
      "Log Regression(1511/4999): loss=0.5431559808158325\n",
      "Log Regression(1512/4999): loss=0.5432992861543654\n",
      "Log Regression(1513/4999): loss=0.5430718590039391\n",
      "Log Regression(1514/4999): loss=0.5467993929244067\n",
      "Log Regression(1515/4999): loss=0.5466922253337329\n",
      "Log Regression(1516/4999): loss=0.5452767141898984\n",
      "Log Regression(1517/4999): loss=0.5443918957034297\n",
      "Log Regression(1518/4999): loss=0.5450727299396033\n",
      "Log Regression(1519/4999): loss=0.5488677861246009\n",
      "Log Regression(1520/4999): loss=0.5437937296561168\n",
      "Log Regression(1521/4999): loss=0.5436867049285339\n",
      "Log Regression(1522/4999): loss=0.5436242312555426\n",
      "Log Regression(1523/4999): loss=0.5435646163018696\n",
      "Log Regression(1524/4999): loss=0.5435858449164703\n",
      "Log Regression(1525/4999): loss=0.5444395053014756\n",
      "Log Regression(1526/4999): loss=0.5432447860164192\n",
      "Log Regression(1527/4999): loss=0.5444914806321406\n",
      "Log Regression(1528/4999): loss=0.5433450057351211\n",
      "Log Regression(1529/4999): loss=0.5430599119269637\n",
      "Log Regression(1530/4999): loss=0.5435094092418502\n",
      "Log Regression(1531/4999): loss=0.5435267053391959\n",
      "Log Regression(1532/4999): loss=0.5437542132495716\n",
      "Log Regression(1533/4999): loss=0.5491331716259269\n",
      "Log Regression(1534/4999): loss=0.5446157645269466\n",
      "Log Regression(1535/4999): loss=0.5479817753772079\n",
      "Log Regression(1536/4999): loss=0.5446785550568183\n",
      "Log Regression(1537/4999): loss=0.5456894268235585\n",
      "Log Regression(1538/4999): loss=0.5466729510536193\n",
      "Log Regression(1539/4999): loss=0.5447144023550582\n",
      "Log Regression(1540/4999): loss=0.5440724676634406\n",
      "Log Regression(1541/4999): loss=0.5474555163003662\n",
      "Log Regression(1542/4999): loss=0.5433223275383051\n",
      "Log Regression(1543/4999): loss=0.5440774803836301\n",
      "Log Regression(1544/4999): loss=0.5440752391397324\n",
      "Log Regression(1545/4999): loss=0.5432105290914858\n",
      "Log Regression(1546/4999): loss=0.5445743851762318\n",
      "Log Regression(1547/4999): loss=0.5470467196874618\n",
      "Log Regression(1548/4999): loss=0.5473299387195739\n",
      "Log Regression(1549/4999): loss=0.5440294354526607\n",
      "Log Regression(1550/4999): loss=0.5492840391147061\n",
      "Log Regression(1551/4999): loss=0.5469135469626852\n",
      "Log Regression(1552/4999): loss=0.5475887170857899\n",
      "Log Regression(1553/4999): loss=0.5441369615704558\n",
      "Log Regression(1554/4999): loss=0.5467218248079462\n",
      "Log Regression(1555/4999): loss=0.5494043930764282\n",
      "Log Regression(1556/4999): loss=0.5428778422742329\n",
      "Log Regression(1557/4999): loss=0.5428622512041913\n",
      "Log Regression(1558/4999): loss=0.5461476322155653\n",
      "Log Regression(1559/4999): loss=0.5448296077762365\n",
      "Log Regression(1560/4999): loss=0.5448202980433983\n",
      "Log Regression(1561/4999): loss=0.5442040518823582\n",
      "Log Regression(1562/4999): loss=0.5428298283875947\n",
      "Log Regression(1563/4999): loss=0.5433679355658693\n",
      "Log Regression(1564/4999): loss=0.543921886148146\n",
      "Log Regression(1565/4999): loss=0.5444488098987983\n",
      "Log Regression(1566/4999): loss=0.5428854866556011\n",
      "Log Regression(1567/4999): loss=0.5429558561776401\n",
      "Log Regression(1568/4999): loss=0.5430036975451248\n",
      "Log Regression(1569/4999): loss=0.5440303909230102\n",
      "Log Regression(1570/4999): loss=0.5465791800341211\n",
      "Log Regression(1571/4999): loss=0.543477829236243\n",
      "Log Regression(1572/4999): loss=0.5446991935847413\n",
      "Log Regression(1573/4999): loss=0.5431651404622435\n",
      "Log Regression(1574/4999): loss=0.5448641413152787\n",
      "Log Regression(1575/4999): loss=0.5441452527778508\n",
      "Log Regression(1576/4999): loss=0.5442605288895562\n",
      "Log Regression(1577/4999): loss=0.5428487590418067\n",
      "Log Regression(1578/4999): loss=0.5461697095311627\n",
      "Log Regression(1579/4999): loss=0.5448485908533613\n",
      "Log Regression(1580/4999): loss=0.5436615616161531\n",
      "Log Regression(1581/4999): loss=0.5436557642477889\n",
      "Log Regression(1582/4999): loss=0.5470069955686013\n",
      "Log Regression(1583/4999): loss=0.5446806143365849\n",
      "Log Regression(1584/4999): loss=0.543290114972995\n",
      "Log Regression(1585/4999): loss=0.5429301272013096\n",
      "Log Regression(1586/4999): loss=0.5437033153772758\n",
      "Log Regression(1587/4999): loss=0.5427775068311025\n",
      "Log Regression(1588/4999): loss=0.5428018923703766\n",
      "Log Regression(1589/4999): loss=0.5427745829149678\n",
      "Log Regression(1590/4999): loss=0.5430391644523217\n",
      "Log Regression(1591/4999): loss=0.5436801588337575\n",
      "Log Regression(1592/4999): loss=0.544502653311154\n",
      "Log Regression(1593/4999): loss=0.5427833539272221\n",
      "Log Regression(1594/4999): loss=0.5442131726875294\n",
      "Log Regression(1595/4999): loss=0.5441449924889087\n",
      "Log Regression(1596/4999): loss=0.5471422826445668\n",
      "Log Regression(1597/4999): loss=0.5444243641215005\n",
      "Log Regression(1598/4999): loss=0.5447233520804009\n",
      "Log Regression(1599/4999): loss=0.5469927733510643\n",
      "Log Regression(1600/4999): loss=0.5485153692811923\n",
      "Log Regression(1601/4999): loss=0.5438946599005366\n",
      "Log Regression(1602/4999): loss=0.5445347749366591\n",
      "Log Regression(1603/4999): loss=0.5427245252909\n",
      "Log Regression(1604/4999): loss=0.543930073090117\n",
      "Log Regression(1605/4999): loss=0.5430454094823055\n",
      "Log Regression(1606/4999): loss=0.5443297106453093\n",
      "Log Regression(1607/4999): loss=0.542706143645257\n",
      "Log Regression(1608/4999): loss=0.5427917299157088\n",
      "Log Regression(1609/4999): loss=0.5431595816878377\n",
      "Log Regression(1610/4999): loss=0.5436029121547253\n",
      "Log Regression(1611/4999): loss=0.5426931818771397\n",
      "Log Regression(1612/4999): loss=0.5428318188796238\n",
      "Log Regression(1613/4999): loss=0.5427474895238344\n",
      "Log Regression(1614/4999): loss=0.5489234848198347\n",
      "Log Regression(1615/4999): loss=0.5439814604344887\n",
      "Log Regression(1616/4999): loss=0.5456334759527516\n",
      "Log Regression(1617/4999): loss=0.5467442382460059\n",
      "Log Regression(1618/4999): loss=0.5476263958531753\n",
      "Log Regression(1619/4999): loss=0.5480195781354397\n",
      "Log Regression(1620/4999): loss=0.5483875580362557\n",
      "Log Regression(1621/4999): loss=0.5429610795412758\n",
      "Log Regression(1622/4999): loss=0.543066890884623\n",
      "Log Regression(1623/4999): loss=0.5428544027625778\n",
      "Log Regression(1624/4999): loss=0.5443092203817618\n",
      "Log Regression(1625/4999): loss=0.5425894763894332\n",
      "Log Regression(1626/4999): loss=0.5430565089375299\n",
      "Log Regression(1627/4999): loss=0.5467226621181742\n",
      "Log Regression(1628/4999): loss=0.5462385823028725\n",
      "Log Regression(1629/4999): loss=0.5432693780198254\n",
      "Log Regression(1630/4999): loss=0.5460239845084536\n",
      "Log Regression(1631/4999): loss=0.5429642440042285\n",
      "Log Regression(1632/4999): loss=0.5428381021467932\n",
      "Log Regression(1633/4999): loss=0.5458941945526268\n",
      "Log Regression(1634/4999): loss=0.5437377817912067\n",
      "Log Regression(1635/4999): loss=0.5427056008999811\n",
      "Log Regression(1636/4999): loss=0.5435180024616153\n",
      "Log Regression(1637/4999): loss=0.5428397625347567\n",
      "Log Regression(1638/4999): loss=0.545153004996947\n",
      "Log Regression(1639/4999): loss=0.5474522185621243\n",
      "Log Regression(1640/4999): loss=0.5497323326012918\n",
      "Log Regression(1641/4999): loss=0.544502454639429\n",
      "Log Regression(1642/4999): loss=0.546989535624834\n",
      "Log Regression(1643/4999): loss=0.5438803223811784\n",
      "Log Regression(1644/4999): loss=0.5443134875996393\n",
      "Log Regression(1645/4999): loss=0.5429726506484435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(1646/4999): loss=0.5457718174712617\n",
      "Log Regression(1647/4999): loss=0.5444084072667349\n",
      "Log Regression(1648/4999): loss=0.54298308288193\n",
      "Log Regression(1649/4999): loss=0.5425578682665539\n",
      "Log Regression(1650/4999): loss=0.5446994545899146\n",
      "Log Regression(1651/4999): loss=0.5448128271136129\n",
      "Log Regression(1652/4999): loss=0.5429451918512943\n",
      "Log Regression(1653/4999): loss=0.5425487060528079\n",
      "Log Regression(1654/4999): loss=0.5434146795599509\n",
      "Log Regression(1655/4999): loss=0.5425353277189573\n",
      "Log Regression(1656/4999): loss=0.5426548552349083\n",
      "Log Regression(1657/4999): loss=0.5433850051224098\n",
      "Log Regression(1658/4999): loss=0.5478307774772786\n",
      "Log Regression(1659/4999): loss=0.5438029266077613\n",
      "Log Regression(1660/4999): loss=0.5511444716000761\n",
      "Log Regression(1661/4999): loss=0.5458775899360534\n",
      "Log Regression(1662/4999): loss=0.5466946983589956\n",
      "Log Regression(1663/4999): loss=0.5428853992397483\n",
      "Log Regression(1664/4999): loss=0.542527309523876\n",
      "Log Regression(1665/4999): loss=0.542559371033161\n",
      "Log Regression(1666/4999): loss=0.5426798809966757\n",
      "Log Regression(1667/4999): loss=0.5429373040037667\n",
      "Log Regression(1668/4999): loss=0.5425146574820696\n",
      "Log Regression(1669/4999): loss=0.542530918649394\n",
      "Log Regression(1670/4999): loss=0.5456357359325948\n",
      "Log Regression(1671/4999): loss=0.5462203047506763\n",
      "Log Regression(1672/4999): loss=0.5444571067437249\n",
      "Log Regression(1673/4999): loss=0.542560612146741\n",
      "Log Regression(1674/4999): loss=0.542875567675866\n",
      "Log Regression(1675/4999): loss=0.5427721615227916\n",
      "Log Regression(1676/4999): loss=0.543367259304016\n",
      "Log Regression(1677/4999): loss=0.5427210130497744\n",
      "Log Regression(1678/4999): loss=0.5425343082325487\n",
      "Log Regression(1679/4999): loss=0.5430158911682114\n",
      "Log Regression(1680/4999): loss=0.5430827880443582\n",
      "Log Regression(1681/4999): loss=0.5431181074436126\n",
      "Log Regression(1682/4999): loss=0.5424339344930362\n",
      "Log Regression(1683/4999): loss=0.5424686731026723\n",
      "Log Regression(1684/4999): loss=0.5426018947500946\n",
      "Log Regression(1685/4999): loss=0.5468368947392048\n",
      "Log Regression(1686/4999): loss=0.546516237377975\n",
      "Log Regression(1687/4999): loss=0.5424262015491053\n",
      "Log Regression(1688/4999): loss=0.5424386106223759\n",
      "Log Regression(1689/4999): loss=0.5425961819865953\n",
      "Log Regression(1690/4999): loss=0.5426780949849513\n",
      "Log Regression(1691/4999): loss=0.543786518948735\n",
      "Log Regression(1692/4999): loss=0.5429611808652\n",
      "Log Regression(1693/4999): loss=0.5444206130554556\n",
      "Log Regression(1694/4999): loss=0.5487062321384435\n",
      "Log Regression(1695/4999): loss=0.5537103531797145\n",
      "Log Regression(1696/4999): loss=0.5479013070496469\n",
      "Log Regression(1697/4999): loss=0.5425559443864926\n",
      "Log Regression(1698/4999): loss=0.5455398963948219\n",
      "Log Regression(1699/4999): loss=0.5423420600332208\n",
      "Log Regression(1700/4999): loss=0.5423781719558962\n",
      "Log Regression(1701/4999): loss=0.5433680902729519\n",
      "Log Regression(1702/4999): loss=0.5430411163970649\n",
      "Log Regression(1703/4999): loss=0.5462049115417359\n",
      "Log Regression(1704/4999): loss=0.5439887545571916\n",
      "Log Regression(1705/4999): loss=0.5424110846707118\n",
      "Log Regression(1706/4999): loss=0.5424214942243222\n",
      "Log Regression(1707/4999): loss=0.5425902082634597\n",
      "Log Regression(1708/4999): loss=0.542821861140052\n",
      "Log Regression(1709/4999): loss=0.5427142565670673\n",
      "Log Regression(1710/4999): loss=0.5424841071634501\n",
      "Log Regression(1711/4999): loss=0.5434794317251307\n",
      "Log Regression(1712/4999): loss=0.5423784586969751\n",
      "Log Regression(1713/4999): loss=0.5424155712070646\n",
      "Log Regression(1714/4999): loss=0.5428379843951159\n",
      "Log Regression(1715/4999): loss=0.5443144224010837\n",
      "Log Regression(1716/4999): loss=0.5430633081292462\n",
      "Log Regression(1717/4999): loss=0.5432858131521972\n",
      "Log Regression(1718/4999): loss=0.5434869630915311\n",
      "Log Regression(1719/4999): loss=0.5466940027320885\n",
      "Log Regression(1720/4999): loss=0.5449697207247066\n",
      "Log Regression(1721/4999): loss=0.5453640511264514\n",
      "Log Regression(1722/4999): loss=0.5457314299039377\n",
      "Log Regression(1723/4999): loss=0.5429122976983045\n",
      "Log Regression(1724/4999): loss=0.5427861474617309\n",
      "Log Regression(1725/4999): loss=0.5423166277444387\n",
      "Log Regression(1726/4999): loss=0.5427167370072763\n",
      "Log Regression(1727/4999): loss=0.5423011885042731\n",
      "Log Regression(1728/4999): loss=0.5425244285112651\n",
      "Log Regression(1729/4999): loss=0.5427774862415256\n",
      "Log Regression(1730/4999): loss=0.5422860780392514\n",
      "Log Regression(1731/4999): loss=0.5424443393814302\n",
      "Log Regression(1732/4999): loss=0.5429127957494897\n",
      "Log Regression(1733/4999): loss=0.5486886432909078\n",
      "Log Regression(1734/4999): loss=0.5448646523656098\n",
      "Log Regression(1735/4999): loss=0.5440471215257282\n",
      "Log Regression(1736/4999): loss=0.5424619005696343\n",
      "Log Regression(1737/4999): loss=0.54246410663515\n",
      "Log Regression(1738/4999): loss=0.5440500453521505\n",
      "Log Regression(1739/4999): loss=0.547090257367714\n",
      "Log Regression(1740/4999): loss=0.5468138138182196\n",
      "Log Regression(1741/4999): loss=0.5485408875568483\n",
      "Log Regression(1742/4999): loss=0.5498372160630713\n",
      "Log Regression(1743/4999): loss=0.5557448450636592\n",
      "Log Regression(1744/4999): loss=0.5495728593046629\n",
      "Log Regression(1745/4999): loss=0.5445538531882183\n",
      "Log Regression(1746/4999): loss=0.5442792272238873\n",
      "Log Regression(1747/4999): loss=0.542456480236632\n",
      "Log Regression(1748/4999): loss=0.542271333960258\n",
      "Log Regression(1749/4999): loss=0.5423474545159515\n",
      "Log Regression(1750/4999): loss=0.5424073205844312\n",
      "Log Regression(1751/4999): loss=0.54345747052518\n",
      "Log Regression(1752/4999): loss=0.5439389573830704\n",
      "Log Regression(1753/4999): loss=0.5467701333377499\n",
      "Log Regression(1754/4999): loss=0.5428574193387243\n",
      "Log Regression(1755/4999): loss=0.5429565328871144\n",
      "Log Regression(1756/4999): loss=0.5425963109461435\n",
      "Log Regression(1757/4999): loss=0.543113364615115\n",
      "Log Regression(1758/4999): loss=0.5432905370504423\n",
      "Log Regression(1759/4999): loss=0.5465375220566357\n",
      "Log Regression(1760/4999): loss=0.5425325197185302\n",
      "Log Regression(1761/4999): loss=0.5430950886284717\n",
      "Log Regression(1762/4999): loss=0.5423380497387914\n",
      "Log Regression(1763/4999): loss=0.5431064261765431\n",
      "Log Regression(1764/4999): loss=0.5457697642221306\n",
      "Log Regression(1765/4999): loss=0.5429828689854062\n",
      "Log Regression(1766/4999): loss=0.5428366835147059\n",
      "Log Regression(1767/4999): loss=0.542437004812238\n",
      "Log Regression(1768/4999): loss=0.542739672995077\n",
      "Log Regression(1769/4999): loss=0.5427121220211523\n",
      "Log Regression(1770/4999): loss=0.5455908071969278\n",
      "Log Regression(1771/4999): loss=0.5426403699954631\n",
      "Log Regression(1772/4999): loss=0.5463340067960751\n",
      "Log Regression(1773/4999): loss=0.5470000655309406\n",
      "Log Regression(1774/4999): loss=0.5489155450588569\n",
      "Log Regression(1775/4999): loss=0.5449215806017395\n",
      "Log Regression(1776/4999): loss=0.5460480532097435\n",
      "Log Regression(1777/4999): loss=0.5502465615846908\n",
      "Log Regression(1778/4999): loss=0.5448578453053542\n",
      "Log Regression(1779/4999): loss=0.5430681884411254\n",
      "Log Regression(1780/4999): loss=0.542461480506852\n",
      "Log Regression(1781/4999): loss=0.5437278051656764\n",
      "Log Regression(1782/4999): loss=0.5442533478741336\n",
      "Log Regression(1783/4999): loss=0.5426098320553715\n",
      "Log Regression(1784/4999): loss=0.5424436779620977\n",
      "Log Regression(1785/4999): loss=0.5422532077150607\n",
      "Log Regression(1786/4999): loss=0.5475845755460489\n",
      "Log Regression(1787/4999): loss=0.543870390550169\n",
      "Log Regression(1788/4999): loss=0.5461797867284703\n",
      "Log Regression(1789/4999): loss=0.5432915777067472\n",
      "Log Regression(1790/4999): loss=0.5426092399249403\n",
      "Log Regression(1791/4999): loss=0.5429438428641593\n",
      "Log Regression(1792/4999): loss=0.5430169756656781\n",
      "Log Regression(1793/4999): loss=0.5452438355700046\n",
      "Log Regression(1794/4999): loss=0.5445467967636646\n",
      "Log Regression(1795/4999): loss=0.5441859562562995\n",
      "Log Regression(1796/4999): loss=0.54572391571463\n",
      "Log Regression(1797/4999): loss=0.5452633896830489\n",
      "Log Regression(1798/4999): loss=0.5458681905495333\n",
      "Log Regression(1799/4999): loss=0.5424145729641074\n",
      "Log Regression(1800/4999): loss=0.5424431368324546\n",
      "Log Regression(1801/4999): loss=0.5458005358311885\n",
      "Log Regression(1802/4999): loss=0.5515279932179994\n",
      "Log Regression(1803/4999): loss=0.5432326210082451\n",
      "Log Regression(1804/4999): loss=0.5427464757108635\n",
      "Log Regression(1805/4999): loss=0.5427823523050863\n",
      "Log Regression(1806/4999): loss=0.5439010164337752\n",
      "Log Regression(1807/4999): loss=0.5435086736883782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(1808/4999): loss=0.5451721012455576\n",
      "Log Regression(1809/4999): loss=0.5513311755486996\n",
      "Log Regression(1810/4999): loss=0.5483335817734512\n",
      "Log Regression(1811/4999): loss=0.5488804673392723\n",
      "Log Regression(1812/4999): loss=0.5432017378657991\n",
      "Log Regression(1813/4999): loss=0.5424440474033688\n",
      "Log Regression(1814/4999): loss=0.5423170652916111\n",
      "Log Regression(1815/4999): loss=0.5423453341753116\n",
      "Log Regression(1816/4999): loss=0.5422741316894325\n",
      "Log Regression(1817/4999): loss=0.5424793986881559\n",
      "Log Regression(1818/4999): loss=0.5422411833195894\n",
      "Log Regression(1819/4999): loss=0.542531752897721\n",
      "Log Regression(1820/4999): loss=0.5429988494924691\n",
      "Log Regression(1821/4999): loss=0.5422061584104246\n",
      "Log Regression(1822/4999): loss=0.5454157570642875\n",
      "Log Regression(1823/4999): loss=0.547787301777442\n",
      "Log Regression(1824/4999): loss=0.5428907073848972\n",
      "Log Regression(1825/4999): loss=0.5439886040136238\n",
      "Log Regression(1826/4999): loss=0.5424732486438574\n",
      "Log Regression(1827/4999): loss=0.5421342641581646\n",
      "Log Regression(1828/4999): loss=0.5421807957919029\n",
      "Log Regression(1829/4999): loss=0.5421383541685877\n",
      "Log Regression(1830/4999): loss=0.5435769471475909\n",
      "Log Regression(1831/4999): loss=0.5452423511376034\n",
      "Log Regression(1832/4999): loss=0.5442475283614651\n",
      "Log Regression(1833/4999): loss=0.5447000028374981\n",
      "Log Regression(1834/4999): loss=0.546577440500752\n",
      "Log Regression(1835/4999): loss=0.5450343663391221\n",
      "Log Regression(1836/4999): loss=0.5429548485367532\n",
      "Log Regression(1837/4999): loss=0.5428104153238821\n",
      "Log Regression(1838/4999): loss=0.5426175200541522\n",
      "Log Regression(1839/4999): loss=0.5429396419093229\n",
      "Log Regression(1840/4999): loss=0.5421898739570677\n",
      "Log Regression(1841/4999): loss=0.5422540911583239\n",
      "Log Regression(1842/4999): loss=0.5422792752084329\n",
      "Log Regression(1843/4999): loss=0.5431777650131961\n",
      "Log Regression(1844/4999): loss=0.5435016061807043\n",
      "Log Regression(1845/4999): loss=0.543313607567422\n",
      "Log Regression(1846/4999): loss=0.5430635758482232\n",
      "Log Regression(1847/4999): loss=0.5420726997400532\n",
      "Log Regression(1848/4999): loss=0.542061726943517\n",
      "Log Regression(1849/4999): loss=0.5420166844412628\n",
      "Log Regression(1850/4999): loss=0.5422474386475497\n",
      "Log Regression(1851/4999): loss=0.5434450367273089\n",
      "Log Regression(1852/4999): loss=0.5432400180567735\n",
      "Log Regression(1853/4999): loss=0.5429571882551241\n",
      "Log Regression(1854/4999): loss=0.5429521879774939\n",
      "Log Regression(1855/4999): loss=0.542068539975402\n",
      "Log Regression(1856/4999): loss=0.5449008462045497\n",
      "Log Regression(1857/4999): loss=0.5423786715629042\n",
      "Log Regression(1858/4999): loss=0.542403742104718\n",
      "Log Regression(1859/4999): loss=0.5422419209388764\n",
      "Log Regression(1860/4999): loss=0.5445528734465601\n",
      "Log Regression(1861/4999): loss=0.5434442536744811\n",
      "Log Regression(1862/4999): loss=0.5424680294565447\n",
      "Log Regression(1863/4999): loss=0.5422998268838285\n",
      "Log Regression(1864/4999): loss=0.5459330475808776\n",
      "Log Regression(1865/4999): loss=0.5425863351686696\n",
      "Log Regression(1866/4999): loss=0.5421972816368432\n",
      "Log Regression(1867/4999): loss=0.5435589549602197\n",
      "Log Regression(1868/4999): loss=0.5428611383301482\n",
      "Log Regression(1869/4999): loss=0.5420164110043423\n",
      "Log Regression(1870/4999): loss=0.5420407146503152\n",
      "Log Regression(1871/4999): loss=0.542499899099621\n",
      "Log Regression(1872/4999): loss=0.5421112097540688\n",
      "Log Regression(1873/4999): loss=0.5424959099733565\n",
      "Log Regression(1874/4999): loss=0.5422588512410149\n",
      "Log Regression(1875/4999): loss=0.5441766677976096\n",
      "Log Regression(1876/4999): loss=0.5432176587706393\n",
      "Log Regression(1877/4999): loss=0.5421120873807497\n",
      "Log Regression(1878/4999): loss=0.5419466241191521\n",
      "Log Regression(1879/4999): loss=0.5425668728685774\n",
      "Log Regression(1880/4999): loss=0.5429841807053736\n",
      "Log Regression(1881/4999): loss=0.5419874292806274\n",
      "Log Regression(1882/4999): loss=0.5419732239252216\n",
      "Log Regression(1883/4999): loss=0.5421012701502035\n",
      "Log Regression(1884/4999): loss=0.5419452197188871\n",
      "Log Regression(1885/4999): loss=0.5427579526173665\n",
      "Log Regression(1886/4999): loss=0.5428623658926182\n",
      "Log Regression(1887/4999): loss=0.5520182084380817\n",
      "Log Regression(1888/4999): loss=0.5431283234369901\n",
      "Log Regression(1889/4999): loss=0.542058758234069\n",
      "Log Regression(1890/4999): loss=0.5421120394487312\n",
      "Log Regression(1891/4999): loss=0.5434241670380068\n",
      "Log Regression(1892/4999): loss=0.5443140584046934\n",
      "Log Regression(1893/4999): loss=0.5422640923067248\n",
      "Log Regression(1894/4999): loss=0.5432713563440043\n",
      "Log Regression(1895/4999): loss=0.5421013867893942\n",
      "Log Regression(1896/4999): loss=0.5420106308533362\n",
      "Log Regression(1897/4999): loss=0.5428344841149048\n",
      "Log Regression(1898/4999): loss=0.5435027144296867\n",
      "Log Regression(1899/4999): loss=0.5421124593157287\n",
      "Log Regression(1900/4999): loss=0.5425364320008041\n",
      "Log Regression(1901/4999): loss=0.5428916585375377\n",
      "Log Regression(1902/4999): loss=0.5420410589612117\n",
      "Log Regression(1903/4999): loss=0.5440398538458773\n",
      "Log Regression(1904/4999): loss=0.544397981211878\n",
      "Log Regression(1905/4999): loss=0.543673036700321\n",
      "Log Regression(1906/4999): loss=0.5436443745173424\n",
      "Log Regression(1907/4999): loss=0.5432113823608332\n",
      "Log Regression(1908/4999): loss=0.541978753250343\n",
      "Log Regression(1909/4999): loss=0.5423276077566164\n",
      "Log Regression(1910/4999): loss=0.5435425311748934\n",
      "Log Regression(1911/4999): loss=0.5436644674845068\n",
      "Log Regression(1912/4999): loss=0.5421164171525104\n",
      "Log Regression(1913/4999): loss=0.5419393798579906\n",
      "Log Regression(1914/4999): loss=0.5420736860214114\n",
      "Log Regression(1915/4999): loss=0.5435194878831137\n",
      "Log Regression(1916/4999): loss=0.5423962997651742\n",
      "Log Regression(1917/4999): loss=0.5427640498795426\n",
      "Log Regression(1918/4999): loss=0.5424338260434355\n",
      "Log Regression(1919/4999): loss=0.5423184586635437\n",
      "Log Regression(1920/4999): loss=0.5446431175383818\n",
      "Log Regression(1921/4999): loss=0.5424830580682117\n",
      "Log Regression(1922/4999): loss=0.5431582071862318\n",
      "Log Regression(1923/4999): loss=0.5420936017087054\n",
      "Log Regression(1924/4999): loss=0.5460546224449683\n",
      "Log Regression(1925/4999): loss=0.5423993416462118\n",
      "Log Regression(1926/4999): loss=0.5422675586209249\n",
      "Log Regression(1927/4999): loss=0.5428197875131752\n",
      "Log Regression(1928/4999): loss=0.5425508800286359\n",
      "Log Regression(1929/4999): loss=0.5427605320084458\n",
      "Log Regression(1930/4999): loss=0.5437039503413287\n",
      "Log Regression(1931/4999): loss=0.5435214382070722\n",
      "Log Regression(1932/4999): loss=0.5422611685158546\n",
      "Log Regression(1933/4999): loss=0.5436435778931563\n",
      "Log Regression(1934/4999): loss=0.542413538305752\n",
      "Log Regression(1935/4999): loss=0.5436607285047124\n",
      "Log Regression(1936/4999): loss=0.544378377081452\n",
      "Log Regression(1937/4999): loss=0.5432990875021798\n",
      "Log Regression(1938/4999): loss=0.5434172401956745\n",
      "Log Regression(1939/4999): loss=0.5454661813621159\n",
      "Log Regression(1940/4999): loss=0.5453001316102385\n",
      "Log Regression(1941/4999): loss=0.5429439184846803\n",
      "Log Regression(1942/4999): loss=0.5425771305715444\n",
      "Log Regression(1943/4999): loss=0.5445198121074443\n",
      "Log Regression(1944/4999): loss=0.5470220171108489\n",
      "Log Regression(1945/4999): loss=0.5433966720178387\n",
      "Log Regression(1946/4999): loss=0.5437244554948578\n",
      "Log Regression(1947/4999): loss=0.5444067111370339\n",
      "Log Regression(1948/4999): loss=0.5441786368460158\n",
      "Log Regression(1949/4999): loss=0.5432314166965405\n",
      "Log Regression(1950/4999): loss=0.5420959649400211\n",
      "Log Regression(1951/4999): loss=0.5473186826350491\n",
      "Log Regression(1952/4999): loss=0.5444799211408314\n",
      "Log Regression(1953/4999): loss=0.5454578669965102\n",
      "Log Regression(1954/4999): loss=0.5466582499681002\n",
      "Log Regression(1955/4999): loss=0.5444460869661467\n",
      "Log Regression(1956/4999): loss=0.5420120336998235\n",
      "Log Regression(1957/4999): loss=0.5429207079845559\n",
      "Log Regression(1958/4999): loss=0.5430857481058683\n",
      "Log Regression(1959/4999): loss=0.5419271255255449\n",
      "Log Regression(1960/4999): loss=0.5428186025953771\n",
      "Log Regression(1961/4999): loss=0.5423370759929418\n",
      "Log Regression(1962/4999): loss=0.5430277608102054\n",
      "Log Regression(1963/4999): loss=0.545013798050254\n",
      "Log Regression(1964/4999): loss=0.5434409380020385\n",
      "Log Regression(1965/4999): loss=0.5418011152983143\n",
      "Log Regression(1966/4999): loss=0.5427288590112664\n",
      "Log Regression(1967/4999): loss=0.5419080676502983\n",
      "Log Regression(1968/4999): loss=0.5418646360732067\n",
      "Log Regression(1969/4999): loss=0.5421138822765275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(1970/4999): loss=0.5447306095407446\n",
      "Log Regression(1971/4999): loss=0.5440708141660722\n",
      "Log Regression(1972/4999): loss=0.5420749834860887\n",
      "Log Regression(1973/4999): loss=0.5426112917841309\n",
      "Log Regression(1974/4999): loss=0.5417862082434349\n",
      "Log Regression(1975/4999): loss=0.5423076461078433\n",
      "Log Regression(1976/4999): loss=0.5417900435335147\n",
      "Log Regression(1977/4999): loss=0.5419468817550696\n",
      "Log Regression(1978/4999): loss=0.5418313323449636\n",
      "Log Regression(1979/4999): loss=0.5419723855951579\n",
      "Log Regression(1980/4999): loss=0.5418173465356454\n",
      "Log Regression(1981/4999): loss=0.5439155669306065\n",
      "Log Regression(1982/4999): loss=0.5506233915704812\n",
      "Log Regression(1983/4999): loss=0.547603979615032\n",
      "Log Regression(1984/4999): loss=0.5453777855227909\n",
      "Log Regression(1985/4999): loss=0.5438855627431743\n",
      "Log Regression(1986/4999): loss=0.5459629068586312\n",
      "Log Regression(1987/4999): loss=0.5485147787922064\n",
      "Log Regression(1988/4999): loss=0.5452109516109386\n",
      "Log Regression(1989/4999): loss=0.5421441210433893\n",
      "Log Regression(1990/4999): loss=0.5418109288788697\n",
      "Log Regression(1991/4999): loss=0.5441481189919332\n",
      "Log Regression(1992/4999): loss=0.5422378127811666\n",
      "Log Regression(1993/4999): loss=0.5418556557905427\n",
      "Log Regression(1994/4999): loss=0.5428731520784352\n",
      "Log Regression(1995/4999): loss=0.5427950305301813\n",
      "Log Regression(1996/4999): loss=0.5420136065799269\n",
      "Log Regression(1997/4999): loss=0.5449724908497157\n",
      "Log Regression(1998/4999): loss=0.5418818323862464\n",
      "Log Regression(1999/4999): loss=0.5419877246541203\n",
      "Log Regression(2000/4999): loss=0.5421993663842853\n",
      "Log Regression(2001/4999): loss=0.542512746010252\n",
      "Log Regression(2002/4999): loss=0.543063264877512\n",
      "Log Regression(2003/4999): loss=0.5429016783360199\n",
      "Log Regression(2004/4999): loss=0.5433047010127993\n",
      "Log Regression(2005/4999): loss=0.5423046409886292\n",
      "Log Regression(2006/4999): loss=0.5422040604068592\n",
      "Log Regression(2007/4999): loss=0.5419198213412786\n",
      "Log Regression(2008/4999): loss=0.5420329938641367\n",
      "Log Regression(2009/4999): loss=0.5450041616428525\n",
      "Log Regression(2010/4999): loss=0.5461314854841716\n",
      "Log Regression(2011/4999): loss=0.5437167850212308\n",
      "Log Regression(2012/4999): loss=0.5425720629628196\n",
      "Log Regression(2013/4999): loss=0.541904606053446\n",
      "Log Regression(2014/4999): loss=0.542238689291858\n",
      "Log Regression(2015/4999): loss=0.5418729115236003\n",
      "Log Regression(2016/4999): loss=0.5417867275101736\n",
      "Log Regression(2017/4999): loss=0.542054233282298\n",
      "Log Regression(2018/4999): loss=0.541868869428138\n",
      "Log Regression(2019/4999): loss=0.5418517757819631\n",
      "Log Regression(2020/4999): loss=0.5417153418621945\n",
      "Log Regression(2021/4999): loss=0.5438043858350537\n",
      "Log Regression(2022/4999): loss=0.5417924879112573\n",
      "Log Regression(2023/4999): loss=0.5420159634002172\n",
      "Log Regression(2024/4999): loss=0.5442638518080505\n",
      "Log Regression(2025/4999): loss=0.5428739357539656\n",
      "Log Regression(2026/4999): loss=0.5419556106783497\n",
      "Log Regression(2027/4999): loss=0.5419301752756442\n",
      "Log Regression(2028/4999): loss=0.5456950680013761\n",
      "Log Regression(2029/4999): loss=0.5472327947279662\n",
      "Log Regression(2030/4999): loss=0.5419221910085311\n",
      "Log Regression(2031/4999): loss=0.5438188150131239\n",
      "Log Regression(2032/4999): loss=0.545851731418265\n",
      "Log Regression(2033/4999): loss=0.5455457924008795\n",
      "Log Regression(2034/4999): loss=0.5445543128978311\n",
      "Log Regression(2035/4999): loss=0.5456270644390333\n",
      "Log Regression(2036/4999): loss=0.5421120162656773\n",
      "Log Regression(2037/4999): loss=0.5419645258793873\n",
      "Log Regression(2038/4999): loss=0.54205138334268\n",
      "Log Regression(2039/4999): loss=0.5419430758855223\n",
      "Log Regression(2040/4999): loss=0.5421162544188513\n",
      "Log Regression(2041/4999): loss=0.5427217011237108\n",
      "Log Regression(2042/4999): loss=0.5475485478861203\n",
      "Log Regression(2043/4999): loss=0.5517250999456901\n",
      "Log Regression(2044/4999): loss=0.5473302082515756\n",
      "Log Regression(2045/4999): loss=0.5476272578797847\n",
      "Log Regression(2046/4999): loss=0.5450557470333751\n",
      "Log Regression(2047/4999): loss=0.5447946207561315\n",
      "Log Regression(2048/4999): loss=0.5447574787088392\n",
      "Log Regression(2049/4999): loss=0.5432800062535739\n",
      "Log Regression(2050/4999): loss=0.5420931305837893\n",
      "Log Regression(2051/4999): loss=0.5418887421568679\n",
      "Log Regression(2052/4999): loss=0.5447231270178154\n",
      "Log Regression(2053/4999): loss=0.5418622112384233\n",
      "Log Regression(2054/4999): loss=0.5418443354083755\n",
      "Log Regression(2055/4999): loss=0.5421209716489165\n",
      "Log Regression(2056/4999): loss=0.5419665129234225\n",
      "Log Regression(2057/4999): loss=0.5435722248595996\n",
      "Log Regression(2058/4999): loss=0.5423420304940472\n",
      "Log Regression(2059/4999): loss=0.543561282224995\n",
      "Log Regression(2060/4999): loss=0.543694068607622\n",
      "Log Regression(2061/4999): loss=0.5442783468463664\n",
      "Log Regression(2062/4999): loss=0.5439723267972372\n",
      "Log Regression(2063/4999): loss=0.546178336902662\n",
      "Log Regression(2064/4999): loss=0.5419621032261913\n",
      "Log Regression(2065/4999): loss=0.5419652473178995\n",
      "Log Regression(2066/4999): loss=0.5424855005313196\n",
      "Log Regression(2067/4999): loss=0.5418903779064729\n",
      "Log Regression(2068/4999): loss=0.5440838536863721\n",
      "Log Regression(2069/4999): loss=0.5477890570846712\n",
      "Log Regression(2070/4999): loss=0.5471267952070387\n",
      "Log Regression(2071/4999): loss=0.5428133832741889\n",
      "Log Regression(2072/4999): loss=0.5424180220684935\n",
      "Log Regression(2073/4999): loss=0.5416604348838965\n",
      "Log Regression(2074/4999): loss=0.5416579277989899\n",
      "Log Regression(2075/4999): loss=0.5420375456765417\n",
      "Log Regression(2076/4999): loss=0.5440155412788507\n",
      "Log Regression(2077/4999): loss=0.5461345385720009\n",
      "Log Regression(2078/4999): loss=0.5456059922778093\n",
      "Log Regression(2079/4999): loss=0.5444551639181718\n",
      "Log Regression(2080/4999): loss=0.5444466282239359\n",
      "Log Regression(2081/4999): loss=0.5460524237659106\n",
      "Log Regression(2082/4999): loss=0.5424389254949484\n",
      "Log Regression(2083/4999): loss=0.5423634582358259\n",
      "Log Regression(2084/4999): loss=0.5418539064798423\n",
      "Log Regression(2085/4999): loss=0.5419926419389024\n",
      "Log Regression(2086/4999): loss=0.5416960521653822\n",
      "Log Regression(2087/4999): loss=0.5421142318898531\n",
      "Log Regression(2088/4999): loss=0.5416797944839447\n",
      "Log Regression(2089/4999): loss=0.5438433019108485\n",
      "Log Regression(2090/4999): loss=0.544220995934645\n",
      "Log Regression(2091/4999): loss=0.5429284996913284\n",
      "Log Regression(2092/4999): loss=0.5430737825950579\n",
      "Log Regression(2093/4999): loss=0.5450645238125358\n",
      "Log Regression(2094/4999): loss=0.5430992096175864\n",
      "Log Regression(2095/4999): loss=0.5418264010863444\n",
      "Log Regression(2096/4999): loss=0.5435783977283001\n",
      "Log Regression(2097/4999): loss=0.5436616971252505\n",
      "Log Regression(2098/4999): loss=0.5429819480417201\n",
      "Log Regression(2099/4999): loss=0.5426038350754484\n",
      "Log Regression(2100/4999): loss=0.5420451023720041\n",
      "Log Regression(2101/4999): loss=0.5428387042349898\n",
      "Log Regression(2102/4999): loss=0.5417411482151293\n",
      "Log Regression(2103/4999): loss=0.5427142081784866\n",
      "Log Regression(2104/4999): loss=0.542041983915712\n",
      "Log Regression(2105/4999): loss=0.5417322646650112\n",
      "Log Regression(2106/4999): loss=0.5426266788257881\n",
      "Log Regression(2107/4999): loss=0.5425857852829198\n",
      "Log Regression(2108/4999): loss=0.5419015521951518\n",
      "Log Regression(2109/4999): loss=0.5433728993551171\n",
      "Log Regression(2110/4999): loss=0.54797446877123\n",
      "Log Regression(2111/4999): loss=0.5463506852778325\n",
      "Log Regression(2112/4999): loss=0.5419914867026703\n",
      "Log Regression(2113/4999): loss=0.5424616877128352\n",
      "Log Regression(2114/4999): loss=0.5482193345659744\n",
      "Log Regression(2115/4999): loss=0.5417919383446373\n",
      "Log Regression(2116/4999): loss=0.5422754898128134\n",
      "Log Regression(2117/4999): loss=0.5420690226914788\n",
      "Log Regression(2118/4999): loss=0.5417627766510672\n",
      "Log Regression(2119/4999): loss=0.5422258667298525\n",
      "Log Regression(2120/4999): loss=0.5465408559197868\n",
      "Log Regression(2121/4999): loss=0.5435672599806733\n",
      "Log Regression(2122/4999): loss=0.5430567734028958\n",
      "Log Regression(2123/4999): loss=0.5422093504096165\n",
      "Log Regression(2124/4999): loss=0.5416632116178756\n",
      "Log Regression(2125/4999): loss=0.5416841878173778\n",
      "Log Regression(2126/4999): loss=0.5421940970832355\n",
      "Log Regression(2127/4999): loss=0.541639368993723\n",
      "Log Regression(2128/4999): loss=0.5415991203689136\n",
      "Log Regression(2129/4999): loss=0.542017429739135\n",
      "Log Regression(2130/4999): loss=0.542357700586407\n",
      "Log Regression(2131/4999): loss=0.5418943615055282\n",
      "Log Regression(2132/4999): loss=0.5416294106940123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(2133/4999): loss=0.5434718006887275\n",
      "Log Regression(2134/4999): loss=0.5425792103874361\n",
      "Log Regression(2135/4999): loss=0.5443474275261514\n",
      "Log Regression(2136/4999): loss=0.5416244111137289\n",
      "Log Regression(2137/4999): loss=0.5417471891353125\n",
      "Log Regression(2138/4999): loss=0.5425783268048706\n",
      "Log Regression(2139/4999): loss=0.5505754079428269\n",
      "Log Regression(2140/4999): loss=0.5429156396355758\n",
      "Log Regression(2141/4999): loss=0.5497513093213198\n",
      "Log Regression(2142/4999): loss=0.5434979142787488\n",
      "Log Regression(2143/4999): loss=0.5418450841612417\n",
      "Log Regression(2144/4999): loss=0.5431424256044162\n",
      "Log Regression(2145/4999): loss=0.5472300957353533\n",
      "Log Regression(2146/4999): loss=0.5443529916567366\n",
      "Log Regression(2147/4999): loss=0.5423197416926007\n",
      "Log Regression(2148/4999): loss=0.5416172633988374\n",
      "Log Regression(2149/4999): loss=0.5419382406206646\n",
      "Log Regression(2150/4999): loss=0.5420812073531389\n",
      "Log Regression(2151/4999): loss=0.5495232031549514\n",
      "Log Regression(2152/4999): loss=0.5463770542446075\n",
      "Log Regression(2153/4999): loss=0.5489411053426735\n",
      "Log Regression(2154/4999): loss=0.5455547717051847\n",
      "Log Regression(2155/4999): loss=0.5439148985454505\n",
      "Log Regression(2156/4999): loss=0.5425968663722498\n",
      "Log Regression(2157/4999): loss=0.5416353175651607\n",
      "Log Regression(2158/4999): loss=0.5415655321984412\n",
      "Log Regression(2159/4999): loss=0.5419362796777419\n",
      "Log Regression(2160/4999): loss=0.5432396670957813\n",
      "Log Regression(2161/4999): loss=0.5416402824458684\n",
      "Log Regression(2162/4999): loss=0.5417569691066335\n",
      "Log Regression(2163/4999): loss=0.5433346921176397\n",
      "Log Regression(2164/4999): loss=0.5416885979179373\n",
      "Log Regression(2165/4999): loss=0.5417234522173583\n",
      "Log Regression(2166/4999): loss=0.5419031791603173\n",
      "Log Regression(2167/4999): loss=0.5456284998164628\n",
      "Log Regression(2168/4999): loss=0.5443744635655312\n",
      "Log Regression(2169/4999): loss=0.5446971461274059\n",
      "Log Regression(2170/4999): loss=0.5425107529860037\n",
      "Log Regression(2171/4999): loss=0.5426991807516081\n",
      "Log Regression(2172/4999): loss=0.5419251148978868\n",
      "Log Regression(2173/4999): loss=0.5418989234320499\n",
      "Log Regression(2174/4999): loss=0.5419181173489795\n",
      "Log Regression(2175/4999): loss=0.5430235605276607\n",
      "Log Regression(2176/4999): loss=0.542980824166055\n",
      "Log Regression(2177/4999): loss=0.5487242553733135\n",
      "Log Regression(2178/4999): loss=0.5430893347201695\n",
      "Log Regression(2179/4999): loss=0.5438939737549273\n",
      "Log Regression(2180/4999): loss=0.5422911408915122\n",
      "Log Regression(2181/4999): loss=0.5417855388501632\n",
      "Log Regression(2182/4999): loss=0.5426932329701937\n",
      "Log Regression(2183/4999): loss=0.5439203876021772\n",
      "Log Regression(2184/4999): loss=0.5417260523119227\n",
      "Log Regression(2185/4999): loss=0.5420440002582211\n",
      "Log Regression(2186/4999): loss=0.5442647785699297\n",
      "Log Regression(2187/4999): loss=0.5440686998054571\n",
      "Log Regression(2188/4999): loss=0.5464956472637873\n",
      "Log Regression(2189/4999): loss=0.5457291773750569\n",
      "Log Regression(2190/4999): loss=0.5511736234399048\n",
      "Log Regression(2191/4999): loss=0.5432242559231051\n",
      "Log Regression(2192/4999): loss=0.5422749127611782\n",
      "Log Regression(2193/4999): loss=0.5419740752441727\n",
      "Log Regression(2194/4999): loss=0.5417942323093231\n",
      "Log Regression(2195/4999): loss=0.5419627911513595\n",
      "Log Regression(2196/4999): loss=0.5464284131217926\n",
      "Log Regression(2197/4999): loss=0.5644879021242781\n",
      "Log Regression(2198/4999): loss=0.5631046489466741\n",
      "Log Regression(2199/4999): loss=0.5612172743632943\n",
      "Log Regression(2200/4999): loss=0.5444179572090836\n",
      "Log Regression(2201/4999): loss=0.5478613782449077\n",
      "Log Regression(2202/4999): loss=0.542481696054615\n",
      "Log Regression(2203/4999): loss=0.5418823329630098\n",
      "Log Regression(2204/4999): loss=0.5436040877384691\n",
      "Log Regression(2205/4999): loss=0.5420891227682183\n",
      "Log Regression(2206/4999): loss=0.542144885301436\n",
      "Log Regression(2207/4999): loss=0.5417833947147532\n",
      "Log Regression(2208/4999): loss=0.5437616011642094\n",
      "Log Regression(2209/4999): loss=0.5416542384690364\n",
      "Log Regression(2210/4999): loss=0.544214039568741\n",
      "Log Regression(2211/4999): loss=0.5418700145254908\n",
      "Log Regression(2212/4999): loss=0.5418682317511259\n",
      "Log Regression(2213/4999): loss=0.5423112049568946\n",
      "Log Regression(2214/4999): loss=0.5419691271437708\n",
      "Log Regression(2215/4999): loss=0.5417040418493987\n",
      "Log Regression(2216/4999): loss=0.5418108878137439\n",
      "Log Regression(2217/4999): loss=0.5456049948955939\n",
      "Log Regression(2218/4999): loss=0.5418129186147\n",
      "Log Regression(2219/4999): loss=0.5417615568667167\n",
      "Log Regression(2220/4999): loss=0.5420170294217306\n",
      "Log Regression(2221/4999): loss=0.5434404169564266\n",
      "Log Regression(2222/4999): loss=0.5416686471570082\n",
      "Log Regression(2223/4999): loss=0.5472251257956967\n",
      "Log Regression(2224/4999): loss=0.5472734882961352\n",
      "Log Regression(2225/4999): loss=0.5427287013589165\n",
      "Log Regression(2226/4999): loss=0.5443813215072815\n",
      "Log Regression(2227/4999): loss=0.543289015383378\n",
      "Log Regression(2228/4999): loss=0.5416554056447982\n",
      "Log Regression(2229/4999): loss=0.541987927712487\n",
      "Log Regression(2230/4999): loss=0.542449715253462\n",
      "Log Regression(2231/4999): loss=0.544787972448514\n",
      "Log Regression(2232/4999): loss=0.5466920319571268\n",
      "Log Regression(2233/4999): loss=0.5470480090544203\n",
      "Log Regression(2234/4999): loss=0.5423267926752258\n",
      "Log Regression(2235/4999): loss=0.5455119345115244\n",
      "Log Regression(2236/4999): loss=0.5415037129905368\n",
      "Log Regression(2237/4999): loss=0.5414566928314314\n",
      "Log Regression(2238/4999): loss=0.5421905683132909\n",
      "Log Regression(2239/4999): loss=0.5416499823388782\n",
      "Log Regression(2240/4999): loss=0.5419210245340129\n",
      "Log Regression(2241/4999): loss=0.5414624166147144\n",
      "Log Regression(2242/4999): loss=0.541651621149156\n",
      "Log Regression(2243/4999): loss=0.5418437575664664\n",
      "Log Regression(2244/4999): loss=0.5414448723866465\n",
      "Log Regression(2245/4999): loss=0.5418636073634806\n",
      "Log Regression(2246/4999): loss=0.5419924787966492\n",
      "Log Regression(2247/4999): loss=0.5423297989143226\n",
      "Log Regression(2248/4999): loss=0.5437082100999951\n",
      "Log Regression(2249/4999): loss=0.5417555763255013\n",
      "Log Regression(2250/4999): loss=0.541423168840921\n",
      "Log Regression(2251/4999): loss=0.5415490628186571\n",
      "Log Regression(2252/4999): loss=0.5441146176277183\n",
      "Log Regression(2253/4999): loss=0.5426250062748397\n",
      "Log Regression(2254/4999): loss=0.541426011087296\n",
      "Log Regression(2255/4999): loss=0.541587992070382\n",
      "Log Regression(2256/4999): loss=0.5414160113802073\n",
      "Log Regression(2257/4999): loss=0.5454073385594813\n",
      "Log Regression(2258/4999): loss=0.5419562647873711\n",
      "Log Regression(2259/4999): loss=0.5435607482211193\n",
      "Log Regression(2260/4999): loss=0.5415277971863601\n",
      "Log Regression(2261/4999): loss=0.5469619619862658\n",
      "Log Regression(2262/4999): loss=0.5511171600475522\n",
      "Log Regression(2263/4999): loss=0.5460647465750844\n",
      "Log Regression(2264/4999): loss=0.5416556970902495\n",
      "Log Regression(2265/4999): loss=0.5418087431753392\n",
      "Log Regression(2266/4999): loss=0.5415687766397361\n",
      "Log Regression(2267/4999): loss=0.5416539273953067\n",
      "Log Regression(2268/4999): loss=0.5422953782461534\n",
      "Log Regression(2269/4999): loss=0.544114130679579\n",
      "Log Regression(2270/4999): loss=0.5415734716765722\n",
      "Log Regression(2271/4999): loss=0.5420283715211561\n",
      "Log Regression(2272/4999): loss=0.5414501692330883\n",
      "Log Regression(2273/4999): loss=0.541469500708035\n",
      "Log Regression(2274/4999): loss=0.5414235766154448\n",
      "Log Regression(2275/4999): loss=0.5413641663114857\n",
      "Log Regression(2276/4999): loss=0.5426049982765605\n",
      "Log Regression(2277/4999): loss=0.5415658249473415\n",
      "Log Regression(2278/4999): loss=0.5414360546813866\n",
      "Log Regression(2279/4999): loss=0.5419627813740301\n",
      "Log Regression(2280/4999): loss=0.5414624221538051\n",
      "Log Regression(2281/4999): loss=0.5415221882814482\n",
      "Log Regression(2282/4999): loss=0.5414990217435354\n",
      "Log Regression(2283/4999): loss=0.5419256215393388\n",
      "Log Regression(2284/4999): loss=0.5414262912436378\n",
      "Log Regression(2285/4999): loss=0.5416808094750076\n",
      "Log Regression(2286/4999): loss=0.5414484721596405\n",
      "Log Regression(2287/4999): loss=0.5415770466284571\n",
      "Log Regression(2288/4999): loss=0.5419497134153016\n",
      "Log Regression(2289/4999): loss=0.5429682752945723\n",
      "Log Regression(2290/4999): loss=0.5431010985753818\n",
      "Log Regression(2291/4999): loss=0.5416412714044571\n",
      "Log Regression(2292/4999): loss=0.5421830037732321\n",
      "Log Regression(2293/4999): loss=0.5414741409206942\n",
      "Log Regression(2294/4999): loss=0.5414200348158583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(2295/4999): loss=0.5414535688194247\n",
      "Log Regression(2296/4999): loss=0.5413402941500168\n",
      "Log Regression(2297/4999): loss=0.5413793813110434\n",
      "Log Regression(2298/4999): loss=0.5417549531670421\n",
      "Log Regression(2299/4999): loss=0.5413371291003092\n",
      "Log Regression(2300/4999): loss=0.5414106768176369\n",
      "Log Regression(2301/4999): loss=0.5418375989908968\n",
      "Log Regression(2302/4999): loss=0.5436281015363047\n",
      "Log Regression(2303/4999): loss=0.5414529983487\n",
      "Log Regression(2304/4999): loss=0.5415209139519356\n",
      "Log Regression(2305/4999): loss=0.5425690613436144\n",
      "Log Regression(2306/4999): loss=0.541707379258598\n",
      "Log Regression(2307/4999): loss=0.5416813557566503\n",
      "Log Regression(2308/4999): loss=0.5413515478043778\n",
      "Log Regression(2309/4999): loss=0.5416452239756457\n",
      "Log Regression(2310/4999): loss=0.5414866629070343\n",
      "Log Regression(2311/4999): loss=0.54213121413172\n",
      "Log Regression(2312/4999): loss=0.5414635865598814\n",
      "Log Regression(2313/4999): loss=0.5413993312254501\n",
      "Log Regression(2314/4999): loss=0.5421142360259068\n",
      "Log Regression(2315/4999): loss=0.5414798783590931\n",
      "Log Regression(2316/4999): loss=0.54338593888012\n",
      "Log Regression(2317/4999): loss=0.5507059418177742\n",
      "Log Regression(2318/4999): loss=0.5515587662556621\n",
      "Log Regression(2319/4999): loss=0.5502894795954898\n",
      "Log Regression(2320/4999): loss=0.5443281737589231\n",
      "Log Regression(2321/4999): loss=0.5475890349641406\n",
      "Log Regression(2322/4999): loss=0.547505625539203\n",
      "Log Regression(2323/4999): loss=0.5428106413198015\n",
      "Log Regression(2324/4999): loss=0.5413589467569625\n",
      "Log Regression(2325/4999): loss=0.5422850934759236\n",
      "Log Regression(2326/4999): loss=0.545740162363293\n",
      "Log Regression(2327/4999): loss=0.5437128390690309\n",
      "Log Regression(2328/4999): loss=0.5417955143648572\n",
      "Log Regression(2329/4999): loss=0.5415932171432635\n",
      "Log Regression(2330/4999): loss=0.541372654135948\n",
      "Log Regression(2331/4999): loss=0.5442403831447045\n",
      "Log Regression(2332/4999): loss=0.5413106804389666\n",
      "Log Regression(2333/4999): loss=0.5424265664912987\n",
      "Log Regression(2334/4999): loss=0.5469068323114354\n",
      "Log Regression(2335/4999): loss=0.5501079910636116\n",
      "Log Regression(2336/4999): loss=0.5460578356339301\n",
      "Log Regression(2337/4999): loss=0.5423803202381615\n",
      "Log Regression(2338/4999): loss=0.5423581609014918\n",
      "Log Regression(2339/4999): loss=0.5419436218898523\n",
      "Log Regression(2340/4999): loss=0.5417760156074383\n",
      "Log Regression(2341/4999): loss=0.5430519749294788\n",
      "Log Regression(2342/4999): loss=0.5432353167247298\n",
      "Log Regression(2343/4999): loss=0.5436060253249193\n",
      "Log Regression(2344/4999): loss=0.5430857709092204\n",
      "Log Regression(2345/4999): loss=0.5477246885234702\n",
      "Log Regression(2346/4999): loss=0.5458609214542086\n",
      "Log Regression(2347/4999): loss=0.5427895805056361\n",
      "Log Regression(2348/4999): loss=0.5447555748618926\n",
      "Log Regression(2349/4999): loss=0.5430351115936874\n",
      "Log Regression(2350/4999): loss=0.5487603962391915\n",
      "Log Regression(2351/4999): loss=0.5450789892033866\n",
      "Log Regression(2352/4999): loss=0.548906912235281\n",
      "Log Regression(2353/4999): loss=0.552213123927277\n",
      "Log Regression(2354/4999): loss=0.5481593311403105\n",
      "Log Regression(2355/4999): loss=0.541556243287067\n",
      "Log Regression(2356/4999): loss=0.5415063788639064\n",
      "Log Regression(2357/4999): loss=0.5426166427298424\n",
      "Log Regression(2358/4999): loss=0.5416387917240849\n",
      "Log Regression(2359/4999): loss=0.5417265359652991\n",
      "Log Regression(2360/4999): loss=0.5446202757202204\n",
      "Log Regression(2361/4999): loss=0.5442488343721158\n",
      "Log Regression(2362/4999): loss=0.5416371673084213\n",
      "Log Regression(2363/4999): loss=0.5413114641478605\n",
      "Log Regression(2364/4999): loss=0.5412632952692208\n",
      "Log Regression(2365/4999): loss=0.5413405310351695\n",
      "Log Regression(2366/4999): loss=0.5414450315506891\n",
      "Log Regression(2367/4999): loss=0.5428190954728385\n",
      "Log Regression(2368/4999): loss=0.5461435944814953\n",
      "Log Regression(2369/4999): loss=0.5472882327364706\n",
      "Log Regression(2370/4999): loss=0.5430287605796094\n",
      "Log Regression(2371/4999): loss=0.5418685630493891\n",
      "Log Regression(2372/4999): loss=0.5414081692260792\n",
      "Log Regression(2373/4999): loss=0.5412697207216078\n",
      "Log Regression(2374/4999): loss=0.5420254551438832\n",
      "Log Regression(2375/4999): loss=0.5419701723553414\n",
      "Log Regression(2376/4999): loss=0.5412284536736924\n",
      "Log Regression(2377/4999): loss=0.5416961215033398\n",
      "Log Regression(2378/4999): loss=0.5412323582589769\n",
      "Log Regression(2379/4999): loss=0.5412381312739909\n",
      "Log Regression(2380/4999): loss=0.5424303366821361\n",
      "Log Regression(2381/4999): loss=0.5423126916118748\n",
      "Log Regression(2382/4999): loss=0.5413059129067577\n",
      "Log Regression(2383/4999): loss=0.541612726230642\n",
      "Log Regression(2384/4999): loss=0.5462692248465533\n",
      "Log Regression(2385/4999): loss=0.5508627375301939\n",
      "Log Regression(2386/4999): loss=0.5558088791467328\n",
      "Log Regression(2387/4999): loss=0.5573447532526541\n",
      "Log Regression(2388/4999): loss=0.5540725152072112\n",
      "Log Regression(2389/4999): loss=0.5461931041044755\n",
      "Log Regression(2390/4999): loss=0.5415576964693577\n",
      "Log Regression(2391/4999): loss=0.541526007899517\n",
      "Log Regression(2392/4999): loss=0.5416946906259041\n",
      "Log Regression(2393/4999): loss=0.5412659554306342\n",
      "Log Regression(2394/4999): loss=0.5415166270865243\n",
      "Log Regression(2395/4999): loss=0.5416693003515138\n",
      "Log Regression(2396/4999): loss=0.5422633379135622\n",
      "Log Regression(2397/4999): loss=0.5412425231219368\n",
      "Log Regression(2398/4999): loss=0.5418420389243188\n",
      "Log Regression(2399/4999): loss=0.5413649700037889\n",
      "Log Regression(2400/4999): loss=0.5451573513337182\n",
      "Log Regression(2401/4999): loss=0.5414146672059386\n",
      "Log Regression(2402/4999): loss=0.5428128190965955\n",
      "Log Regression(2403/4999): loss=0.5449414200461046\n",
      "Log Regression(2404/4999): loss=0.5437451972102209\n",
      "Log Regression(2405/4999): loss=0.541416410841174\n",
      "Log Regression(2406/4999): loss=0.541346632929532\n",
      "Log Regression(2407/4999): loss=0.543841023030692\n",
      "Log Regression(2408/4999): loss=0.5423216349115616\n",
      "Log Regression(2409/4999): loss=0.5417255023025375\n",
      "Log Regression(2410/4999): loss=0.5444268774676043\n",
      "Log Regression(2411/4999): loss=0.5450117677575447\n",
      "Log Regression(2412/4999): loss=0.5418852326375297\n",
      "Log Regression(2413/4999): loss=0.5417552129294914\n",
      "Log Regression(2414/4999): loss=0.5417390331582049\n",
      "Log Regression(2415/4999): loss=0.5428907050406524\n",
      "Log Regression(2416/4999): loss=0.5419480656991469\n",
      "Log Regression(2417/4999): loss=0.5415273631839612\n",
      "Log Regression(2418/4999): loss=0.5414099070663988\n",
      "Log Regression(2419/4999): loss=0.54120436886134\n",
      "Log Regression(2420/4999): loss=0.5421048455160267\n",
      "Log Regression(2421/4999): loss=0.5447735012967452\n",
      "Log Regression(2422/4999): loss=0.5454749401946039\n",
      "Log Regression(2423/4999): loss=0.5443422248768155\n",
      "Log Regression(2424/4999): loss=0.5438874812712456\n",
      "Log Regression(2425/4999): loss=0.5427683178668322\n",
      "Log Regression(2426/4999): loss=0.5412042757841969\n",
      "Log Regression(2427/4999): loss=0.5411890199890728\n",
      "Log Regression(2428/4999): loss=0.5412267504297974\n",
      "Log Regression(2429/4999): loss=0.5417144249252112\n",
      "Log Regression(2430/4999): loss=0.5412071970596465\n",
      "Log Regression(2431/4999): loss=0.5413723847217874\n",
      "Log Regression(2432/4999): loss=0.5427200210006287\n",
      "Log Regression(2433/4999): loss=0.5415315922938057\n",
      "Log Regression(2434/4999): loss=0.5432063630928615\n",
      "Log Regression(2435/4999): loss=0.5440796814198415\n",
      "Log Regression(2436/4999): loss=0.5472219973999928\n",
      "Log Regression(2437/4999): loss=0.5469360519514747\n",
      "Log Regression(2438/4999): loss=0.5414549979061929\n",
      "Log Regression(2439/4999): loss=0.5421630182130031\n",
      "Log Regression(2440/4999): loss=0.5454828937239046\n",
      "Log Regression(2441/4999): loss=0.541248571000297\n",
      "Log Regression(2442/4999): loss=0.5440132546740083\n",
      "Log Regression(2443/4999): loss=0.541350398428716\n",
      "Log Regression(2444/4999): loss=0.5421963040831188\n",
      "Log Regression(2445/4999): loss=0.5413783101719156\n",
      "Log Regression(2446/4999): loss=0.5475014556604209\n",
      "Log Regression(2447/4999): loss=0.5418057240331641\n",
      "Log Regression(2448/4999): loss=0.5419855382485124\n",
      "Log Regression(2449/4999): loss=0.5435120327071921\n",
      "Log Regression(2450/4999): loss=0.5422444727960565\n",
      "Log Regression(2451/4999): loss=0.5413756914784014\n",
      "Log Regression(2452/4999): loss=0.541610163082899\n",
      "Log Regression(2453/4999): loss=0.5475484377958861\n",
      "Log Regression(2454/4999): loss=0.5413602658662702\n",
      "Log Regression(2455/4999): loss=0.5414206396125951\n",
      "Log Regression(2456/4999): loss=0.542640012892341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(2457/4999): loss=0.541604233700527\n",
      "Log Regression(2458/4999): loss=0.5414758918654431\n",
      "Log Regression(2459/4999): loss=0.5435743700487787\n",
      "Log Regression(2460/4999): loss=0.5477102432823401\n",
      "Log Regression(2461/4999): loss=0.5451396896794214\n",
      "Log Regression(2462/4999): loss=0.5436083813731124\n",
      "Log Regression(2463/4999): loss=0.542174420721865\n",
      "Log Regression(2464/4999): loss=0.5412662692810469\n",
      "Log Regression(2465/4999): loss=0.541224801308627\n",
      "Log Regression(2466/4999): loss=0.5412741535692331\n",
      "Log Regression(2467/4999): loss=0.5414689097058929\n",
      "Log Regression(2468/4999): loss=0.5411518540745215\n",
      "Log Regression(2469/4999): loss=0.5422093963627963\n",
      "Log Regression(2470/4999): loss=0.5416064983377309\n",
      "Log Regression(2471/4999): loss=0.545385251301042\n",
      "Log Regression(2472/4999): loss=0.5420742127568524\n",
      "Log Regression(2473/4999): loss=0.5414808824573022\n",
      "Log Regression(2474/4999): loss=0.54178847629655\n",
      "Log Regression(2475/4999): loss=0.5419507042274084\n",
      "Log Regression(2476/4999): loss=0.5415063544056434\n",
      "Log Regression(2477/4999): loss=0.5421427397388688\n",
      "Log Regression(2478/4999): loss=0.5420954708557213\n",
      "Log Regression(2479/4999): loss=0.5412576529209643\n",
      "Log Regression(2480/4999): loss=0.541951218280273\n",
      "Log Regression(2481/4999): loss=0.5411192382229995\n",
      "Log Regression(2482/4999): loss=0.541636301146498\n",
      "Log Regression(2483/4999): loss=0.5415143115123151\n",
      "Log Regression(2484/4999): loss=0.5416670958038965\n",
      "Log Regression(2485/4999): loss=0.5425546366007882\n",
      "Log Regression(2486/4999): loss=0.5411735529127754\n",
      "Log Regression(2487/4999): loss=0.5412611210723464\n",
      "Log Regression(2488/4999): loss=0.5477225094734294\n",
      "Log Regression(2489/4999): loss=0.544961386336906\n",
      "Log Regression(2490/4999): loss=0.5434191624879217\n",
      "Log Regression(2491/4999): loss=0.5434788600756745\n",
      "Log Regression(2492/4999): loss=0.5412530859403566\n",
      "Log Regression(2493/4999): loss=0.5413431618038792\n",
      "Log Regression(2494/4999): loss=0.543247085761177\n",
      "Log Regression(2495/4999): loss=0.5420448303911927\n",
      "Log Regression(2496/4999): loss=0.541218674402056\n",
      "Log Regression(2497/4999): loss=0.5411310590297183\n",
      "Log Regression(2498/4999): loss=0.5416655627680071\n",
      "Log Regression(2499/4999): loss=0.5415500012779338\n",
      "Log Regression(2500/4999): loss=0.5416405409762723\n",
      "Log Regression(2501/4999): loss=0.5414693098596307\n",
      "Log Regression(2502/4999): loss=0.5414964226827951\n",
      "Log Regression(2503/4999): loss=0.541583367938388\n",
      "Log Regression(2504/4999): loss=0.5450099631476091\n",
      "Log Regression(2505/4999): loss=0.5431230970601192\n",
      "Log Regression(2506/4999): loss=0.5448186446908387\n",
      "Log Regression(2507/4999): loss=0.5428259035493218\n",
      "Log Regression(2508/4999): loss=0.5421126587985317\n",
      "Log Regression(2509/4999): loss=0.5437412766908992\n",
      "Log Regression(2510/4999): loss=0.5413607153261477\n",
      "Log Regression(2511/4999): loss=0.5411974681028007\n",
      "Log Regression(2512/4999): loss=0.5414645303546525\n",
      "Log Regression(2513/4999): loss=0.5417117721497795\n",
      "Log Regression(2514/4999): loss=0.5425445312596625\n",
      "Log Regression(2515/4999): loss=0.5410913372241524\n",
      "Log Regression(2516/4999): loss=0.5414921009888004\n",
      "Log Regression(2517/4999): loss=0.541318796990714\n",
      "Log Regression(2518/4999): loss=0.5415619427021973\n",
      "Log Regression(2519/4999): loss=0.5418427706410246\n",
      "Log Regression(2520/4999): loss=0.5438155654439923\n",
      "Log Regression(2521/4999): loss=0.5413112504659752\n",
      "Log Regression(2522/4999): loss=0.5411956504896699\n",
      "Log Regression(2523/4999): loss=0.5412845551508589\n",
      "Log Regression(2524/4999): loss=0.5442777624183934\n",
      "Log Regression(2525/4999): loss=0.5421052952464572\n",
      "Log Regression(2526/4999): loss=0.543713184746479\n",
      "Log Regression(2527/4999): loss=0.5419659438476262\n",
      "Log Regression(2528/4999): loss=0.5432701345338012\n",
      "Log Regression(2529/4999): loss=0.5435867152012847\n",
      "Log Regression(2530/4999): loss=0.5534724950058688\n",
      "Log Regression(2531/4999): loss=0.5466696839639252\n",
      "Log Regression(2532/4999): loss=0.5446624852687315\n",
      "Log Regression(2533/4999): loss=0.5432688584041788\n",
      "Log Regression(2534/4999): loss=0.5429376078544934\n",
      "Log Regression(2535/4999): loss=0.5414644945220741\n",
      "Log Regression(2536/4999): loss=0.541313813153748\n",
      "Log Regression(2537/4999): loss=0.5411483455606133\n",
      "Log Regression(2538/4999): loss=0.5428714197735753\n",
      "Log Regression(2539/4999): loss=0.5431034379243818\n",
      "Log Regression(2540/4999): loss=0.5413587111402046\n",
      "Log Regression(2541/4999): loss=0.542564628467001\n",
      "Log Regression(2542/4999): loss=0.5414304568461877\n",
      "Log Regression(2543/4999): loss=0.5453939360987844\n",
      "Log Regression(2544/4999): loss=0.547662684585584\n",
      "Log Regression(2545/4999): loss=0.5460887241005455\n",
      "Log Regression(2546/4999): loss=0.5416933239669057\n",
      "Log Regression(2547/4999): loss=0.5417701718279387\n",
      "Log Regression(2548/4999): loss=0.5411942495449957\n",
      "Log Regression(2549/4999): loss=0.5412249070082911\n",
      "Log Regression(2550/4999): loss=0.5440699070127343\n",
      "Log Regression(2551/4999): loss=0.5430490628922017\n",
      "Log Regression(2552/4999): loss=0.541553173415165\n",
      "Log Regression(2553/4999): loss=0.5412297524738138\n",
      "Log Regression(2554/4999): loss=0.5410830655792752\n",
      "Log Regression(2555/4999): loss=0.5415063542237711\n",
      "Log Regression(2556/4999): loss=0.5442532581265029\n",
      "Log Regression(2557/4999): loss=0.5437937595137496\n",
      "Log Regression(2558/4999): loss=0.5418369547599776\n",
      "Log Regression(2559/4999): loss=0.5419464058768897\n",
      "Log Regression(2560/4999): loss=0.5433585400899287\n",
      "Log Regression(2561/4999): loss=0.5417127654606131\n",
      "Log Regression(2562/4999): loss=0.5419023894636847\n",
      "Log Regression(2563/4999): loss=0.5410176211898243\n",
      "Log Regression(2564/4999): loss=0.541156826348422\n",
      "Log Regression(2565/4999): loss=0.5413218435812805\n",
      "Log Regression(2566/4999): loss=0.5415238939129603\n",
      "Log Regression(2567/4999): loss=0.5418679378032392\n",
      "Log Regression(2568/4999): loss=0.5437248605560099\n",
      "Log Regression(2569/4999): loss=0.5430133516799523\n",
      "Log Regression(2570/4999): loss=0.5416400318726641\n",
      "Log Regression(2571/4999): loss=0.5420493646888535\n",
      "Log Regression(2572/4999): loss=0.5421237347073682\n",
      "Log Regression(2573/4999): loss=0.5414154374928889\n",
      "Log Regression(2574/4999): loss=0.5426427146567836\n",
      "Log Regression(2575/4999): loss=0.5419936160886069\n",
      "Log Regression(2576/4999): loss=0.5440773472358756\n",
      "Log Regression(2577/4999): loss=0.5418021068776393\n",
      "Log Regression(2578/4999): loss=0.5425848273439813\n",
      "Log Regression(2579/4999): loss=0.5467677534312896\n",
      "Log Regression(2580/4999): loss=0.5450161776751603\n",
      "Log Regression(2581/4999): loss=0.5421043003953643\n",
      "Log Regression(2582/4999): loss=0.5411276272117965\n",
      "Log Regression(2583/4999): loss=0.5411558931854621\n",
      "Log Regression(2584/4999): loss=0.5418564309465957\n",
      "Log Regression(2585/4999): loss=0.5429354906695748\n",
      "Log Regression(2586/4999): loss=0.5458759343320414\n",
      "Log Regression(2587/4999): loss=0.5440208699644808\n",
      "Log Regression(2588/4999): loss=0.5431133451768957\n",
      "Log Regression(2589/4999): loss=0.5411051341351999\n",
      "Log Regression(2590/4999): loss=0.5421644147375376\n",
      "Log Regression(2591/4999): loss=0.542427560412978\n",
      "Log Regression(2592/4999): loss=0.5423824864254111\n",
      "Log Regression(2593/4999): loss=0.5415785599803079\n",
      "Log Regression(2594/4999): loss=0.5423195760171815\n",
      "Log Regression(2595/4999): loss=0.543524809808694\n",
      "Log Regression(2596/4999): loss=0.5425613503655969\n",
      "Log Regression(2597/4999): loss=0.5450280824843102\n",
      "Log Regression(2598/4999): loss=0.5415177524897428\n",
      "Log Regression(2599/4999): loss=0.5410036312481021\n",
      "Log Regression(2600/4999): loss=0.5413419869169986\n",
      "Log Regression(2601/4999): loss=0.5410006793723056\n",
      "Log Regression(2602/4999): loss=0.5411426066396569\n",
      "Log Regression(2603/4999): loss=0.5417299741099756\n",
      "Log Regression(2604/4999): loss=0.5416554579217425\n",
      "Log Regression(2605/4999): loss=0.5411119575764666\n",
      "Log Regression(2606/4999): loss=0.5422326430352425\n",
      "Log Regression(2607/4999): loss=0.5422970951957885\n",
      "Log Regression(2608/4999): loss=0.5420241825020368\n",
      "Log Regression(2609/4999): loss=0.542392872967937\n",
      "Log Regression(2610/4999): loss=0.5466670228160796\n",
      "Log Regression(2611/4999): loss=0.5440112967151244\n",
      "Log Regression(2612/4999): loss=0.5450095446294227\n",
      "Log Regression(2613/4999): loss=0.5413083006632045\n",
      "Log Regression(2614/4999): loss=0.5412799194470618\n",
      "Log Regression(2615/4999): loss=0.5412379351515479\n",
      "Log Regression(2616/4999): loss=0.5412201946744624\n",
      "Log Regression(2617/4999): loss=0.5413394348884183\n",
      "Log Regression(2618/4999): loss=0.5430781883587402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(2619/4999): loss=0.540989666565783\n",
      "Log Regression(2620/4999): loss=0.5422335815876052\n",
      "Log Regression(2621/4999): loss=0.5410180663084186\n",
      "Log Regression(2622/4999): loss=0.5409627975218174\n",
      "Log Regression(2623/4999): loss=0.5412385048122352\n",
      "Log Regression(2624/4999): loss=0.5434920923449017\n",
      "Log Regression(2625/4999): loss=0.5429147934294652\n",
      "Log Regression(2626/4999): loss=0.5427263540408982\n",
      "Log Regression(2627/4999): loss=0.5433853899403601\n",
      "Log Regression(2628/4999): loss=0.5420678022270636\n",
      "Log Regression(2629/4999): loss=0.5414796072795907\n",
      "Log Regression(2630/4999): loss=0.5409583278013731\n",
      "Log Regression(2631/4999): loss=0.5427492085667998\n",
      "Log Regression(2632/4999): loss=0.5432301746406802\n",
      "Log Regression(2633/4999): loss=0.5440205570555385\n",
      "Log Regression(2634/4999): loss=0.5467466411502113\n",
      "Log Regression(2635/4999): loss=0.5458727282102123\n",
      "Log Regression(2636/4999): loss=0.54692783436325\n",
      "Log Regression(2637/4999): loss=0.5409550067398021\n",
      "Log Regression(2638/4999): loss=0.540944013556994\n",
      "Log Regression(2639/4999): loss=0.5422131251896345\n",
      "Log Regression(2640/4999): loss=0.542135536924727\n",
      "Log Regression(2641/4999): loss=0.5490220427458856\n",
      "Log Regression(2642/4999): loss=0.54309104233511\n",
      "Log Regression(2643/4999): loss=0.541321433560945\n",
      "Log Regression(2644/4999): loss=0.5423132672309386\n",
      "Log Regression(2645/4999): loss=0.5416817273920871\n",
      "Log Regression(2646/4999): loss=0.541923807343326\n",
      "Log Regression(2647/4999): loss=0.5419678069588368\n",
      "Log Regression(2648/4999): loss=0.5422992557800866\n",
      "Log Regression(2649/4999): loss=0.5415276454705448\n",
      "Log Regression(2650/4999): loss=0.5427193014291926\n",
      "Log Regression(2651/4999): loss=0.5415193416409538\n",
      "Log Regression(2652/4999): loss=0.5419891885152935\n",
      "Log Regression(2653/4999): loss=0.5414415892483441\n",
      "Log Regression(2654/4999): loss=0.541615434441785\n",
      "Log Regression(2655/4999): loss=0.5415918504437427\n",
      "Log Regression(2656/4999): loss=0.5435766436461597\n",
      "Log Regression(2657/4999): loss=0.541948778795826\n",
      "Log Regression(2658/4999): loss=0.5414550746612545\n",
      "Log Regression(2659/4999): loss=0.5418293237401582\n",
      "Log Regression(2660/4999): loss=0.5422675313268815\n",
      "Log Regression(2661/4999): loss=0.5427331200177117\n",
      "Log Regression(2662/4999): loss=0.5418546273945053\n",
      "Log Regression(2663/4999): loss=0.5465041620514828\n",
      "Log Regression(2664/4999): loss=0.5472894035021871\n",
      "Log Regression(2665/4999): loss=0.5416661662524452\n",
      "Log Regression(2666/4999): loss=0.5416164828966511\n",
      "Log Regression(2667/4999): loss=0.5455019213447861\n",
      "Log Regression(2668/4999): loss=0.5423670148580403\n",
      "Log Regression(2669/4999): loss=0.5410239963912563\n",
      "Log Regression(2670/4999): loss=0.541642239199163\n",
      "Log Regression(2671/4999): loss=0.5437860193709255\n",
      "Log Regression(2672/4999): loss=0.5427897227611082\n",
      "Log Regression(2673/4999): loss=0.5446904264363284\n",
      "Log Regression(2674/4999): loss=0.5416572740787733\n",
      "Log Regression(2675/4999): loss=0.5424377879916459\n",
      "Log Regression(2676/4999): loss=0.5413509533957818\n",
      "Log Regression(2677/4999): loss=0.5420250262826786\n",
      "Log Regression(2678/4999): loss=0.5409583783199744\n",
      "Log Regression(2679/4999): loss=0.541214957573256\n",
      "Log Regression(2680/4999): loss=0.541720553035465\n",
      "Log Regression(2681/4999): loss=0.5427455311209628\n",
      "Log Regression(2682/4999): loss=0.5410856308692693\n",
      "Log Regression(2683/4999): loss=0.5409271930761381\n",
      "Log Regression(2684/4999): loss=0.5409412340155763\n",
      "Log Regression(2685/4999): loss=0.540894203479529\n",
      "Log Regression(2686/4999): loss=0.5456680854616437\n",
      "Log Regression(2687/4999): loss=0.5417111097446901\n",
      "Log Regression(2688/4999): loss=0.5417770465265102\n",
      "Log Regression(2689/4999): loss=0.5410799176853007\n",
      "Log Regression(2690/4999): loss=0.5411066962784854\n",
      "Log Regression(2691/4999): loss=0.5420620261518072\n",
      "Log Regression(2692/4999): loss=0.5417670343365154\n",
      "Log Regression(2693/4999): loss=0.5409689083965897\n",
      "Log Regression(2694/4999): loss=0.5427270522395309\n",
      "Log Regression(2695/4999): loss=0.5410684522519726\n",
      "Log Regression(2696/4999): loss=0.5417510625396533\n",
      "Log Regression(2697/4999): loss=0.5419436861282706\n",
      "Log Regression(2698/4999): loss=0.5410657635782202\n",
      "Log Regression(2699/4999): loss=0.5417574428934759\n",
      "Log Regression(2700/4999): loss=0.5416756651334087\n",
      "Log Regression(2701/4999): loss=0.5505247643139568\n",
      "Log Regression(2702/4999): loss=0.5472083142353771\n",
      "Log Regression(2703/4999): loss=0.5414223830533501\n",
      "Log Regression(2704/4999): loss=0.5411550290080099\n",
      "Log Regression(2705/4999): loss=0.5448821451766724\n",
      "Log Regression(2706/4999): loss=0.5469112429742599\n",
      "Log Regression(2707/4999): loss=0.5419850082388307\n",
      "Log Regression(2708/4999): loss=0.5422479077286981\n",
      "Log Regression(2709/4999): loss=0.5425486298527413\n",
      "Log Regression(2710/4999): loss=0.5421126514874973\n",
      "Log Regression(2711/4999): loss=0.5413592751858888\n",
      "Log Regression(2712/4999): loss=0.5412443237366371\n",
      "Log Regression(2713/4999): loss=0.5427331872260875\n",
      "Log Regression(2714/4999): loss=0.5426524036297213\n",
      "Log Regression(2715/4999): loss=0.5429978354309407\n",
      "Log Regression(2716/4999): loss=0.5408579272917524\n",
      "Log Regression(2717/4999): loss=0.5416714609840823\n",
      "Log Regression(2718/4999): loss=0.5424052437625881\n",
      "Log Regression(2719/4999): loss=0.5433820000568805\n",
      "Log Regression(2720/4999): loss=0.5428884031238379\n",
      "Log Regression(2721/4999): loss=0.540947538818592\n",
      "Log Regression(2722/4999): loss=0.5411113218328163\n",
      "Log Regression(2723/4999): loss=0.5416994140910911\n",
      "Log Regression(2724/4999): loss=0.5408613362620003\n",
      "Log Regression(2725/4999): loss=0.5408376482948349\n",
      "Log Regression(2726/4999): loss=0.5418534266161752\n",
      "Log Regression(2727/4999): loss=0.5413135247891836\n",
      "Log Regression(2728/4999): loss=0.5419171721121758\n",
      "Log Regression(2729/4999): loss=0.5411445367861302\n",
      "Log Regression(2730/4999): loss=0.5409542833458746\n",
      "Log Regression(2731/4999): loss=0.5409419534720665\n",
      "Log Regression(2732/4999): loss=0.5412902008673081\n",
      "Log Regression(2733/4999): loss=0.5420451700512854\n",
      "Log Regression(2734/4999): loss=0.5416806867583946\n",
      "Log Regression(2735/4999): loss=0.5414671886571805\n",
      "Log Regression(2736/4999): loss=0.5451524243003725\n",
      "Log Regression(2737/4999): loss=0.5441575754241026\n",
      "Log Regression(2738/4999): loss=0.5417554849887375\n",
      "Log Regression(2739/4999): loss=0.5411668209265182\n",
      "Log Regression(2740/4999): loss=0.5422408115322581\n",
      "Log Regression(2741/4999): loss=0.5442184783787056\n",
      "Log Regression(2742/4999): loss=0.5446073294191632\n",
      "Log Regression(2743/4999): loss=0.5411955461567188\n",
      "Log Regression(2744/4999): loss=0.5438586030121252\n",
      "Log Regression(2745/4999): loss=0.5486749203253701\n",
      "Log Regression(2746/4999): loss=0.5504336161128147\n",
      "Log Regression(2747/4999): loss=0.5442541339349652\n",
      "Log Regression(2748/4999): loss=0.5438916077262728\n",
      "Log Regression(2749/4999): loss=0.5415221400352374\n",
      "Log Regression(2750/4999): loss=0.542986381418295\n",
      "Log Regression(2751/4999): loss=0.5408572300312637\n",
      "Log Regression(2752/4999): loss=0.5409008411922442\n",
      "Log Regression(2753/4999): loss=0.5420734162118593\n",
      "Log Regression(2754/4999): loss=0.5409715761987616\n",
      "Log Regression(2755/4999): loss=0.5409909624940913\n",
      "Log Regression(2756/4999): loss=0.5409652616038448\n",
      "Log Regression(2757/4999): loss=0.5427885599903121\n",
      "Log Regression(2758/4999): loss=0.5408347726988668\n",
      "Log Regression(2759/4999): loss=0.5418501282486812\n",
      "Log Regression(2760/4999): loss=0.541641222224855\n",
      "Log Regression(2761/4999): loss=0.5453573130694862\n",
      "Log Regression(2762/4999): loss=0.5486308966115573\n",
      "Log Regression(2763/4999): loss=0.5543153340705811\n",
      "Log Regression(2764/4999): loss=0.5464624900495413\n",
      "Log Regression(2765/4999): loss=0.5411748490422394\n",
      "Log Regression(2766/4999): loss=0.541081206094284\n",
      "Log Regression(2767/4999): loss=0.5417443195006929\n",
      "Log Regression(2768/4999): loss=0.5416603235538774\n",
      "Log Regression(2769/4999): loss=0.5408621407342686\n",
      "Log Regression(2770/4999): loss=0.5420138630082537\n",
      "Log Regression(2771/4999): loss=0.5428871339761172\n",
      "Log Regression(2772/4999): loss=0.5410692762689226\n",
      "Log Regression(2773/4999): loss=0.5410082900667454\n",
      "Log Regression(2774/4999): loss=0.5410826513259777\n",
      "Log Regression(2775/4999): loss=0.5413421718990302\n",
      "Log Regression(2776/4999): loss=0.5491670146656834\n",
      "Log Regression(2777/4999): loss=0.5433124310227101\n",
      "Log Regression(2778/4999): loss=0.5448199251556752\n",
      "Log Regression(2779/4999): loss=0.5460304841322876\n",
      "Log Regression(2780/4999): loss=0.5409359607798392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(2781/4999): loss=0.5410354357622843\n",
      "Log Regression(2782/4999): loss=0.5417611685877636\n",
      "Log Regression(2783/4999): loss=0.5421035967840091\n",
      "Log Regression(2784/4999): loss=0.5410741318482946\n",
      "Log Regression(2785/4999): loss=0.5417086786614896\n",
      "Log Regression(2786/4999): loss=0.5409072791248728\n",
      "Log Regression(2787/4999): loss=0.5415140644424142\n",
      "Log Regression(2788/4999): loss=0.5423843122454137\n",
      "Log Regression(2789/4999): loss=0.5420854529240714\n",
      "Log Regression(2790/4999): loss=0.5421066255594812\n",
      "Log Regression(2791/4999): loss=0.5410766925539341\n",
      "Log Regression(2792/4999): loss=0.5476425450149572\n",
      "Log Regression(2793/4999): loss=0.5426453726201322\n",
      "Log Regression(2794/4999): loss=0.5480372266310725\n",
      "Log Regression(2795/4999): loss=0.5416599915340509\n",
      "Log Regression(2796/4999): loss=0.5426506964079605\n",
      "Log Regression(2797/4999): loss=0.5436031803669535\n",
      "Log Regression(2798/4999): loss=0.5473258317966038\n",
      "Log Regression(2799/4999): loss=0.5416814381931363\n",
      "Log Regression(2800/4999): loss=0.5464769801087097\n",
      "Log Regression(2801/4999): loss=0.5427956472129184\n",
      "Log Regression(2802/4999): loss=0.5408983028187441\n",
      "Log Regression(2803/4999): loss=0.5409471781406219\n",
      "Log Regression(2804/4999): loss=0.5438490504251758\n",
      "Log Regression(2805/4999): loss=0.5410038221340766\n",
      "Log Regression(2806/4999): loss=0.5415944019907506\n",
      "Log Regression(2807/4999): loss=0.5424043369594197\n",
      "Log Regression(2808/4999): loss=0.5408903751966205\n",
      "Log Regression(2809/4999): loss=0.5411313216146807\n",
      "Log Regression(2810/4999): loss=0.5411535767175407\n",
      "Log Regression(2811/4999): loss=0.5409615086932221\n",
      "Log Regression(2812/4999): loss=0.5413064473233578\n",
      "Log Regression(2813/4999): loss=0.5420567704976077\n",
      "Log Regression(2814/4999): loss=0.5409806019357992\n",
      "Log Regression(2815/4999): loss=0.5417237960365713\n",
      "Log Regression(2816/4999): loss=0.5411178238886233\n",
      "Log Regression(2817/4999): loss=0.5439021519866828\n",
      "Log Regression(2818/4999): loss=0.5434496338438369\n",
      "Log Regression(2819/4999): loss=0.5414198925243401\n",
      "Log Regression(2820/4999): loss=0.542870313320314\n",
      "Log Regression(2821/4999): loss=0.541247106253939\n",
      "Log Regression(2822/4999): loss=0.5409788822496644\n",
      "Log Regression(2823/4999): loss=0.5414055338056537\n",
      "Log Regression(2824/4999): loss=0.5412565779527682\n",
      "Log Regression(2825/4999): loss=0.5424279709248232\n",
      "Log Regression(2826/4999): loss=0.5425449527512128\n",
      "Log Regression(2827/4999): loss=0.5460512009000977\n",
      "Log Regression(2828/4999): loss=0.5437966449694212\n",
      "Log Regression(2829/4999): loss=0.5425021193528355\n",
      "Log Regression(2830/4999): loss=0.5418372102067767\n",
      "Log Regression(2831/4999): loss=0.5419800982021877\n",
      "Log Regression(2832/4999): loss=0.5426924885943596\n",
      "Log Regression(2833/4999): loss=0.5439312359267208\n",
      "Log Regression(2834/4999): loss=0.5465711660759979\n",
      "Log Regression(2835/4999): loss=0.5414501891337071\n",
      "Log Regression(2836/4999): loss=0.5450012345659672\n",
      "Log Regression(2837/4999): loss=0.5411259942852469\n",
      "Log Regression(2838/4999): loss=0.5425058963458035\n",
      "Log Regression(2839/4999): loss=0.5427815111236503\n",
      "Log Regression(2840/4999): loss=0.5419134504998399\n",
      "Log Regression(2841/4999): loss=0.5420721826017961\n",
      "Log Regression(2842/4999): loss=0.5421202072392224\n",
      "Log Regression(2843/4999): loss=0.5408875814676235\n",
      "Log Regression(2844/4999): loss=0.5414391125508042\n",
      "Log Regression(2845/4999): loss=0.5419063844159229\n",
      "Log Regression(2846/4999): loss=0.5480376095819186\n",
      "Log Regression(2847/4999): loss=0.5443552607929225\n",
      "Log Regression(2848/4999): loss=0.5433805974319513\n",
      "Log Regression(2849/4999): loss=0.5408231526822022\n",
      "Log Regression(2850/4999): loss=0.5417506468394735\n",
      "Log Regression(2851/4999): loss=0.5418930242126759\n",
      "Log Regression(2852/4999): loss=0.5423532918485401\n",
      "Log Regression(2853/4999): loss=0.5418434252791355\n",
      "Log Regression(2854/4999): loss=0.5408362947504921\n",
      "Log Regression(2855/4999): loss=0.5465901913520238\n",
      "Log Regression(2856/4999): loss=0.5420661657451596\n",
      "Log Regression(2857/4999): loss=0.5407323920382456\n",
      "Log Regression(2858/4999): loss=0.5429613099636742\n",
      "Log Regression(2859/4999): loss=0.5421040694325439\n",
      "Log Regression(2860/4999): loss=0.5439483366826641\n",
      "Log Regression(2861/4999): loss=0.5407724730373689\n",
      "Log Regression(2862/4999): loss=0.5407186155960597\n",
      "Log Regression(2863/4999): loss=0.5414321021239025\n",
      "Log Regression(2864/4999): loss=0.5414839730808347\n",
      "Log Regression(2865/4999): loss=0.5411610276582867\n",
      "Log Regression(2866/4999): loss=0.5410710639858123\n",
      "Log Regression(2867/4999): loss=0.5407496061841166\n",
      "Log Regression(2868/4999): loss=0.5407554650463574\n",
      "Log Regression(2869/4999): loss=0.5410505124396666\n",
      "Log Regression(2870/4999): loss=0.5407505401553391\n",
      "Log Regression(2871/4999): loss=0.5444538971510181\n",
      "Log Regression(2872/4999): loss=0.544035497722149\n",
      "Log Regression(2873/4999): loss=0.5438804082148662\n",
      "Log Regression(2874/4999): loss=0.555296914737099\n",
      "Log Regression(2875/4999): loss=0.5528572333531517\n",
      "Log Regression(2876/4999): loss=0.5503429763565632\n",
      "Log Regression(2877/4999): loss=0.5477225017487605\n",
      "Log Regression(2878/4999): loss=0.5522508979728333\n",
      "Log Regression(2879/4999): loss=0.5487054431160734\n",
      "Log Regression(2880/4999): loss=0.5527401818226957\n",
      "Log Regression(2881/4999): loss=0.5561873199736302\n",
      "Log Regression(2882/4999): loss=0.5494299975323583\n",
      "Log Regression(2883/4999): loss=0.5462346093128547\n",
      "Log Regression(2884/4999): loss=0.5470504045132452\n",
      "Log Regression(2885/4999): loss=0.5441730775327869\n",
      "Log Regression(2886/4999): loss=0.5410194786768917\n",
      "Log Regression(2887/4999): loss=0.5411986073508569\n",
      "Log Regression(2888/4999): loss=0.5411875768542895\n",
      "Log Regression(2889/4999): loss=0.5431604236505438\n",
      "Log Regression(2890/4999): loss=0.5414655144120128\n",
      "Log Regression(2891/4999): loss=0.5424125119550753\n",
      "Log Regression(2892/4999): loss=0.5410360976751524\n",
      "Log Regression(2893/4999): loss=0.541149813707164\n",
      "Log Regression(2894/4999): loss=0.5410219968507549\n",
      "Log Regression(2895/4999): loss=0.5423372771557581\n",
      "Log Regression(2896/4999): loss=0.5409549451946019\n",
      "Log Regression(2897/4999): loss=0.5412150087296159\n",
      "Log Regression(2898/4999): loss=0.5408713975187711\n",
      "Log Regression(2899/4999): loss=0.5414914745167441\n",
      "Log Regression(2900/4999): loss=0.5409629183291264\n",
      "Log Regression(2901/4999): loss=0.5411601888505059\n",
      "Log Regression(2902/4999): loss=0.5417315292863586\n",
      "Log Regression(2903/4999): loss=0.5413747719172554\n",
      "Log Regression(2904/4999): loss=0.5409576125996878\n",
      "Log Regression(2905/4999): loss=0.5423855660118183\n",
      "Log Regression(2906/4999): loss=0.5450328016798949\n",
      "Log Regression(2907/4999): loss=0.5420102714803413\n",
      "Log Regression(2908/4999): loss=0.5438840208339067\n",
      "Log Regression(2909/4999): loss=0.541017813804875\n",
      "Log Regression(2910/4999): loss=0.5410420800163137\n",
      "Log Regression(2911/4999): loss=0.541763938769988\n",
      "Log Regression(2912/4999): loss=0.5410669523663496\n",
      "Log Regression(2913/4999): loss=0.541172179012618\n",
      "Log Regression(2914/4999): loss=0.5430011907599841\n",
      "Log Regression(2915/4999): loss=0.542923955304389\n",
      "Log Regression(2916/4999): loss=0.5425295812903536\n",
      "Log Regression(2917/4999): loss=0.5465405999830741\n",
      "Log Regression(2918/4999): loss=0.5462511790426066\n",
      "Log Regression(2919/4999): loss=0.5410813411648024\n",
      "Log Regression(2920/4999): loss=0.5422715711396413\n",
      "Log Regression(2921/4999): loss=0.5407579118270012\n",
      "Log Regression(2922/4999): loss=0.5407263679916886\n",
      "Log Regression(2923/4999): loss=0.5417427842882747\n",
      "Log Regression(2924/4999): loss=0.544629588372944\n",
      "Log Regression(2925/4999): loss=0.5426981696323238\n",
      "Log Regression(2926/4999): loss=0.5407759416076052\n",
      "Log Regression(2927/4999): loss=0.5415748660938161\n",
      "Log Regression(2928/4999): loss=0.540700063482056\n",
      "Log Regression(2929/4999): loss=0.5429598517939803\n",
      "Log Regression(2930/4999): loss=0.548777838822305\n",
      "Log Regression(2931/4999): loss=0.5435818329918684\n",
      "Log Regression(2932/4999): loss=0.541005055990858\n",
      "Log Regression(2933/4999): loss=0.540998023972436\n",
      "Log Regression(2934/4999): loss=0.5407814744084379\n",
      "Log Regression(2935/4999): loss=0.5413839924364542\n",
      "Log Regression(2936/4999): loss=0.5407195611658209\n",
      "Log Regression(2937/4999): loss=0.5407819850291965\n",
      "Log Regression(2938/4999): loss=0.5420489584628214\n",
      "Log Regression(2939/4999): loss=0.5415581067223775\n",
      "Log Regression(2940/4999): loss=0.5426032578624428\n",
      "Log Regression(2941/4999): loss=0.5428353927475468\n",
      "Log Regression(2942/4999): loss=0.5439619846971835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(2943/4999): loss=0.541886191712387\n",
      "Log Regression(2944/4999): loss=0.5415318908847851\n",
      "Log Regression(2945/4999): loss=0.5431041044649894\n",
      "Log Regression(2946/4999): loss=0.5433659391350115\n",
      "Log Regression(2947/4999): loss=0.5469714107289719\n",
      "Log Regression(2948/4999): loss=0.5431334442781163\n",
      "Log Regression(2949/4999): loss=0.5416060389734687\n",
      "Log Regression(2950/4999): loss=0.5461146448039937\n",
      "Log Regression(2951/4999): loss=0.5441426401854158\n",
      "Log Regression(2952/4999): loss=0.5470847882650426\n",
      "Log Regression(2953/4999): loss=0.5453382909239147\n",
      "Log Regression(2954/4999): loss=0.5428687580950444\n",
      "Log Regression(2955/4999): loss=0.5473457923531133\n",
      "Log Regression(2956/4999): loss=0.5420094112915178\n",
      "Log Regression(2957/4999): loss=0.5412282657017718\n",
      "Log Regression(2958/4999): loss=0.5414109579361516\n",
      "Log Regression(2959/4999): loss=0.5416622919184623\n",
      "Log Regression(2960/4999): loss=0.5414052576867299\n",
      "Log Regression(2961/4999): loss=0.5412270119515549\n",
      "Log Regression(2962/4999): loss=0.5415046749150629\n",
      "Log Regression(2963/4999): loss=0.5407406590499926\n",
      "Log Regression(2964/4999): loss=0.5407728231686166\n",
      "Log Regression(2965/4999): loss=0.5408580796403369\n",
      "Log Regression(2966/4999): loss=0.5409443360442255\n",
      "Log Regression(2967/4999): loss=0.5435538153364843\n",
      "Log Regression(2968/4999): loss=0.5449281363830781\n",
      "Log Regression(2969/4999): loss=0.540864871176148\n",
      "Log Regression(2970/4999): loss=0.5420177095052259\n",
      "Log Regression(2971/4999): loss=0.54089529976831\n",
      "Log Regression(2972/4999): loss=0.5409413028302943\n",
      "Log Regression(2973/4999): loss=0.5408149626837709\n",
      "Log Regression(2974/4999): loss=0.541255898810944\n",
      "Log Regression(2975/4999): loss=0.541221707140307\n",
      "Log Regression(2976/4999): loss=0.5408275370314494\n",
      "Log Regression(2977/4999): loss=0.540723249904624\n",
      "Log Regression(2978/4999): loss=0.5413100969246218\n",
      "Log Regression(2979/4999): loss=0.5406919142787303\n",
      "Log Regression(2980/4999): loss=0.540685783717492\n",
      "Log Regression(2981/4999): loss=0.5407501845171788\n",
      "Log Regression(2982/4999): loss=0.5412901278243979\n",
      "Log Regression(2983/4999): loss=0.5410734968869885\n",
      "Log Regression(2984/4999): loss=0.5414681822207433\n",
      "Log Regression(2985/4999): loss=0.5407724630194152\n",
      "Log Regression(2986/4999): loss=0.5447963726879396\n",
      "Log Regression(2987/4999): loss=0.5455940966241857\n",
      "Log Regression(2988/4999): loss=0.549546877109172\n",
      "Log Regression(2989/4999): loss=0.5503763648719604\n",
      "Log Regression(2990/4999): loss=0.5494234246146377\n",
      "Log Regression(2991/4999): loss=0.5423852363499895\n",
      "Log Regression(2992/4999): loss=0.5423284047235706\n",
      "Log Regression(2993/4999): loss=0.5407323351091367\n",
      "Log Regression(2994/4999): loss=0.544316529440309\n",
      "Log Regression(2995/4999): loss=0.5407273943599941\n",
      "Log Regression(2996/4999): loss=0.5414738798920102\n",
      "Log Regression(2997/4999): loss=0.541464876404263\n",
      "Log Regression(2998/4999): loss=0.5432170891848855\n",
      "Log Regression(2999/4999): loss=0.5449263876952312\n",
      "Log Regression(3000/4999): loss=0.5416604995464007\n",
      "Log Regression(3001/4999): loss=0.5411270359929623\n",
      "Log Regression(3002/4999): loss=0.5425730384650552\n",
      "Log Regression(3003/4999): loss=0.5421638461211371\n",
      "Log Regression(3004/4999): loss=0.5417366093701405\n",
      "Log Regression(3005/4999): loss=0.5449819889305423\n",
      "Log Regression(3006/4999): loss=0.5414349387442812\n",
      "Log Regression(3007/4999): loss=0.5417156290488302\n",
      "Log Regression(3008/4999): loss=0.541459723921181\n",
      "Log Regression(3009/4999): loss=0.5424771509956138\n",
      "Log Regression(3010/4999): loss=0.5421689186850721\n",
      "Log Regression(3011/4999): loss=0.544505002902025\n",
      "Log Regression(3012/4999): loss=0.541900874148699\n",
      "Log Regression(3013/4999): loss=0.5418338284913851\n",
      "Log Regression(3014/4999): loss=0.5435442336293054\n",
      "Log Regression(3015/4999): loss=0.547078482091041\n",
      "Log Regression(3016/4999): loss=0.546270104818123\n",
      "Log Regression(3017/4999): loss=0.5428109850931901\n",
      "Log Regression(3018/4999): loss=0.5416796761522545\n",
      "Log Regression(3019/4999): loss=0.5417733343707759\n",
      "Log Regression(3020/4999): loss=0.5430114545393737\n",
      "Log Regression(3021/4999): loss=0.5448488490725942\n",
      "Log Regression(3022/4999): loss=0.5410121797512343\n",
      "Log Regression(3023/4999): loss=0.5409554916307977\n",
      "Log Regression(3024/4999): loss=0.5408596758838039\n",
      "Log Regression(3025/4999): loss=0.5419819043721883\n",
      "Log Regression(3026/4999): loss=0.5413639291539063\n",
      "Log Regression(3027/4999): loss=0.5442421575179276\n",
      "Log Regression(3028/4999): loss=0.5416302401645996\n",
      "Log Regression(3029/4999): loss=0.540647061281343\n",
      "Log Regression(3030/4999): loss=0.5409954050064134\n",
      "Log Regression(3031/4999): loss=0.5429673137824675\n",
      "Log Regression(3032/4999): loss=0.5453845433133722\n",
      "Log Regression(3033/4999): loss=0.5461717651904145\n",
      "Log Regression(3034/4999): loss=0.5478789595210792\n",
      "Log Regression(3035/4999): loss=0.540657885581331\n",
      "Log Regression(3036/4999): loss=0.5410465345293389\n",
      "Log Regression(3037/4999): loss=0.5407184877100547\n",
      "Log Regression(3038/4999): loss=0.5417851484310329\n",
      "Log Regression(3039/4999): loss=0.5408041532754857\n",
      "Log Regression(3040/4999): loss=0.5415132931140335\n",
      "Log Regression(3041/4999): loss=0.5417321264668583\n",
      "Log Regression(3042/4999): loss=0.5408077263100198\n",
      "Log Regression(3043/4999): loss=0.5409637586083335\n",
      "Log Regression(3044/4999): loss=0.540824386453583\n",
      "Log Regression(3045/4999): loss=0.5461990887444516\n",
      "Log Regression(3046/4999): loss=0.5476220983729942\n",
      "Log Regression(3047/4999): loss=0.5423568374952193\n",
      "Log Regression(3048/4999): loss=0.543667517573644\n",
      "Log Regression(3049/4999): loss=0.5418726452644784\n",
      "Log Regression(3050/4999): loss=0.5442705841771238\n",
      "Log Regression(3051/4999): loss=0.5497774070262782\n",
      "Log Regression(3052/4999): loss=0.5448083326163643\n",
      "Log Regression(3053/4999): loss=0.5462340084746763\n",
      "Log Regression(3054/4999): loss=0.543760729608601\n",
      "Log Regression(3055/4999): loss=0.5415173917157348\n",
      "Log Regression(3056/4999): loss=0.5407271654361911\n",
      "Log Regression(3057/4999): loss=0.5406780793741478\n",
      "Log Regression(3058/4999): loss=0.5413195612369849\n",
      "Log Regression(3059/4999): loss=0.5413106368535301\n",
      "Log Regression(3060/4999): loss=0.5419382267451462\n",
      "Log Regression(3061/4999): loss=0.5448097289904237\n",
      "Log Regression(3062/4999): loss=0.5422907353346835\n",
      "Log Regression(3063/4999): loss=0.5419341836901916\n",
      "Log Regression(3064/4999): loss=0.5450958033062016\n",
      "Log Regression(3065/4999): loss=0.5426005307893877\n",
      "Log Regression(3066/4999): loss=0.5411106152915487\n",
      "Log Regression(3067/4999): loss=0.5411115784723166\n",
      "Log Regression(3068/4999): loss=0.5408026868495656\n",
      "Log Regression(3069/4999): loss=0.5429478422752698\n",
      "Log Regression(3070/4999): loss=0.5427841608455698\n",
      "Log Regression(3071/4999): loss=0.5414545337532056\n",
      "Log Regression(3072/4999): loss=0.5408690334002191\n",
      "Log Regression(3073/4999): loss=0.5406321903219213\n",
      "Log Regression(3074/4999): loss=0.540791449611321\n",
      "Log Regression(3075/4999): loss=0.5408938852244549\n",
      "Log Regression(3076/4999): loss=0.5407871099710787\n",
      "Log Regression(3077/4999): loss=0.5409654741573209\n",
      "Log Regression(3078/4999): loss=0.5418750196911817\n",
      "Log Regression(3079/4999): loss=0.5419515530608378\n",
      "Log Regression(3080/4999): loss=0.5406271773772031\n",
      "Log Regression(3081/4999): loss=0.5416155469195686\n",
      "Log Regression(3082/4999): loss=0.5413843937797862\n",
      "Log Regression(3083/4999): loss=0.5427988119190067\n",
      "Log Regression(3084/4999): loss=0.5425710667853607\n",
      "Log Regression(3085/4999): loss=0.5434047493096392\n",
      "Log Regression(3086/4999): loss=0.5438223279486974\n",
      "Log Regression(3087/4999): loss=0.5429923113280769\n",
      "Log Regression(3088/4999): loss=0.5422734840526816\n",
      "Log Regression(3089/4999): loss=0.5410971658070625\n",
      "Log Regression(3090/4999): loss=0.5491977322044552\n",
      "Log Regression(3091/4999): loss=0.5421216139891296\n",
      "Log Regression(3092/4999): loss=0.5444654679765346\n",
      "Log Regression(3093/4999): loss=0.5411954153276446\n",
      "Log Regression(3094/4999): loss=0.5407714880776453\n",
      "Log Regression(3095/4999): loss=0.5419969624842081\n",
      "Log Regression(3096/4999): loss=0.5405680977881676\n",
      "Log Regression(3097/4999): loss=0.5406947980530368\n",
      "Log Regression(3098/4999): loss=0.5415384912219509\n",
      "Log Regression(3099/4999): loss=0.5413769354520469\n",
      "Log Regression(3100/4999): loss=0.5408189187805874\n",
      "Log Regression(3101/4999): loss=0.55079366708627\n",
      "Log Regression(3102/4999): loss=0.55014763427086\n",
      "Log Regression(3103/4999): loss=0.5426443513667905\n",
      "Log Regression(3104/4999): loss=0.5405945933425642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(3105/4999): loss=0.5424055367455085\n",
      "Log Regression(3106/4999): loss=0.5416145039455004\n",
      "Log Regression(3107/4999): loss=0.5406041417678296\n",
      "Log Regression(3108/4999): loss=0.5443557698343895\n",
      "Log Regression(3109/4999): loss=0.5422078586726077\n",
      "Log Regression(3110/4999): loss=0.5451514818488947\n",
      "Log Regression(3111/4999): loss=0.5420929041976432\n",
      "Log Regression(3112/4999): loss=0.5437934289490344\n",
      "Log Regression(3113/4999): loss=0.5417375396666924\n",
      "Log Regression(3114/4999): loss=0.5417636002521812\n",
      "Log Regression(3115/4999): loss=0.542653891609096\n",
      "Log Regression(3116/4999): loss=0.5443322383201395\n",
      "Log Regression(3117/4999): loss=0.5432128592603941\n",
      "Log Regression(3118/4999): loss=0.5448979872134242\n",
      "Log Regression(3119/4999): loss=0.5460585842291205\n",
      "Log Regression(3120/4999): loss=0.5504546286018792\n",
      "Log Regression(3121/4999): loss=0.5458852213093288\n",
      "Log Regression(3122/4999): loss=0.5444740681488272\n",
      "Log Regression(3123/4999): loss=0.5409474707978015\n",
      "Log Regression(3124/4999): loss=0.5414771544226159\n",
      "Log Regression(3125/4999): loss=0.5456241477536505\n",
      "Log Regression(3126/4999): loss=0.5442680759294647\n",
      "Log Regression(3127/4999): loss=0.5462808506358533\n",
      "Log Regression(3128/4999): loss=0.5446023862905998\n",
      "Log Regression(3129/4999): loss=0.5454322197942203\n",
      "Log Regression(3130/4999): loss=0.543638773001115\n",
      "Log Regression(3131/4999): loss=0.5409507918954207\n",
      "Log Regression(3132/4999): loss=0.5430516416342629\n",
      "Log Regression(3133/4999): loss=0.5408326831585426\n",
      "Log Regression(3134/4999): loss=0.5425433954298784\n",
      "Log Regression(3135/4999): loss=0.5418412463198696\n",
      "Log Regression(3136/4999): loss=0.5414010583618083\n",
      "Log Regression(3137/4999): loss=0.540665476000752\n",
      "Log Regression(3138/4999): loss=0.5412487118318325\n",
      "Log Regression(3139/4999): loss=0.5420729729969406\n",
      "Log Regression(3140/4999): loss=0.5416965767298796\n",
      "Log Regression(3141/4999): loss=0.5405758242365213\n",
      "Log Regression(3142/4999): loss=0.5442333526009222\n",
      "Log Regression(3143/4999): loss=0.5467254627743967\n",
      "Log Regression(3144/4999): loss=0.5432552670574887\n",
      "Log Regression(3145/4999): loss=0.540918175977867\n",
      "Log Regression(3146/4999): loss=0.5415485005440784\n",
      "Log Regression(3147/4999): loss=0.5457908065534267\n",
      "Log Regression(3148/4999): loss=0.5433364585822856\n",
      "Log Regression(3149/4999): loss=0.5409038444735476\n",
      "Log Regression(3150/4999): loss=0.5408053966117581\n",
      "Log Regression(3151/4999): loss=0.5425019753054974\n",
      "Log Regression(3152/4999): loss=0.5438098073948917\n",
      "Log Regression(3153/4999): loss=0.5412312845760671\n",
      "Log Regression(3154/4999): loss=0.540768322778948\n",
      "Log Regression(3155/4999): loss=0.5407678905530272\n",
      "Log Regression(3156/4999): loss=0.5418047360538861\n",
      "Log Regression(3157/4999): loss=0.5496069423489118\n",
      "Log Regression(3158/4999): loss=0.555311925581587\n",
      "Log Regression(3159/4999): loss=0.5512949537399402\n",
      "Log Regression(3160/4999): loss=0.5444972125465966\n",
      "Log Regression(3161/4999): loss=0.544179922247201\n",
      "Log Regression(3162/4999): loss=0.5436218163201987\n",
      "Log Regression(3163/4999): loss=0.5405622645529874\n",
      "Log Regression(3164/4999): loss=0.5409513564592188\n",
      "Log Regression(3165/4999): loss=0.5406816916231985\n",
      "Log Regression(3166/4999): loss=0.5406492304577842\n",
      "Log Regression(3167/4999): loss=0.5461878534299072\n",
      "Log Regression(3168/4999): loss=0.540522058852138\n",
      "Log Regression(3169/4999): loss=0.5405954092865801\n",
      "Log Regression(3170/4999): loss=0.5468297766176293\n",
      "Log Regression(3171/4999): loss=0.5499627974167652\n",
      "Log Regression(3172/4999): loss=0.5407731795629903\n",
      "Log Regression(3173/4999): loss=0.5458549533746776\n",
      "Log Regression(3174/4999): loss=0.5475630471717247\n",
      "Log Regression(3175/4999): loss=0.5418604705589773\n",
      "Log Regression(3176/4999): loss=0.5426456744392391\n",
      "Log Regression(3177/4999): loss=0.547466216624827\n",
      "Log Regression(3178/4999): loss=0.5429692504031761\n",
      "Log Regression(3179/4999): loss=0.5405232450696974\n",
      "Log Regression(3180/4999): loss=0.541430941799637\n",
      "Log Regression(3181/4999): loss=0.540512176853561\n",
      "Log Regression(3182/4999): loss=0.5405107430013276\n",
      "Log Regression(3183/4999): loss=0.5417072759617316\n",
      "Log Regression(3184/4999): loss=0.5408011031706151\n",
      "Log Regression(3185/4999): loss=0.5405228099982866\n",
      "Log Regression(3186/4999): loss=0.543729552114208\n",
      "Log Regression(3187/4999): loss=0.5413690063327794\n",
      "Log Regression(3188/4999): loss=0.540617225390979\n",
      "Log Regression(3189/4999): loss=0.5405210084265921\n",
      "Log Regression(3190/4999): loss=0.5413300812621935\n",
      "Log Regression(3191/4999): loss=0.5431139058923511\n",
      "Log Regression(3192/4999): loss=0.5429872942104957\n",
      "Log Regression(3193/4999): loss=0.5405241828652033\n",
      "Log Regression(3194/4999): loss=0.5409417736683865\n",
      "Log Regression(3195/4999): loss=0.5404951877754142\n",
      "Log Regression(3196/4999): loss=0.5427920444055618\n",
      "Log Regression(3197/4999): loss=0.5425550786178495\n",
      "Log Regression(3198/4999): loss=0.5427098917476533\n",
      "Log Regression(3199/4999): loss=0.5412063148601992\n",
      "Log Regression(3200/4999): loss=0.5406282306513113\n",
      "Log Regression(3201/4999): loss=0.5410774310223151\n",
      "Log Regression(3202/4999): loss=0.5429296046740849\n",
      "Log Regression(3203/4999): loss=0.5417817479924958\n",
      "Log Regression(3204/4999): loss=0.5416227279867009\n",
      "Log Regression(3205/4999): loss=0.5411746599177121\n",
      "Log Regression(3206/4999): loss=0.5408980744603241\n",
      "Log Regression(3207/4999): loss=0.5424319707144717\n",
      "Log Regression(3208/4999): loss=0.5425788552456966\n",
      "Log Regression(3209/4999): loss=0.5407506669161375\n",
      "Log Regression(3210/4999): loss=0.5416394683948478\n",
      "Log Regression(3211/4999): loss=0.5432773899350906\n",
      "Log Regression(3212/4999): loss=0.5422194345679113\n",
      "Log Regression(3213/4999): loss=0.5410318975200115\n",
      "Log Regression(3214/4999): loss=0.5406282559253427\n",
      "Log Regression(3215/4999): loss=0.5406289646077863\n",
      "Log Regression(3216/4999): loss=0.5442214471378111\n",
      "Log Regression(3217/4999): loss=0.5409124971978004\n",
      "Log Regression(3218/4999): loss=0.540779734962962\n",
      "Log Regression(3219/4999): loss=0.5404779177625988\n",
      "Log Regression(3220/4999): loss=0.5405714197294561\n",
      "Log Regression(3221/4999): loss=0.5444248874274699\n",
      "Log Regression(3222/4999): loss=0.5418906935761039\n",
      "Log Regression(3223/4999): loss=0.5404611216944746\n",
      "Log Regression(3224/4999): loss=0.5405477416812702\n",
      "Log Regression(3225/4999): loss=0.5406232834673834\n",
      "Log Regression(3226/4999): loss=0.5406919931943392\n",
      "Log Regression(3227/4999): loss=0.5405411821382236\n",
      "Log Regression(3228/4999): loss=0.54343005557589\n",
      "Log Regression(3229/4999): loss=0.5469631740644334\n",
      "Log Regression(3230/4999): loss=0.5410928795159157\n",
      "Log Regression(3231/4999): loss=0.5472610570662295\n",
      "Log Regression(3232/4999): loss=0.5454540849657499\n",
      "Log Regression(3233/4999): loss=0.5448748320400061\n",
      "Log Regression(3234/4999): loss=0.5410198550974344\n",
      "Log Regression(3235/4999): loss=0.544006397122749\n",
      "Log Regression(3236/4999): loss=0.5425182252155499\n",
      "Log Regression(3237/4999): loss=0.5412871133914201\n",
      "Log Regression(3238/4999): loss=0.5406282222859006\n",
      "Log Regression(3239/4999): loss=0.5406337326009797\n",
      "Log Regression(3240/4999): loss=0.5407345554701322\n",
      "Log Regression(3241/4999): loss=0.5413154344368742\n",
      "Log Regression(3242/4999): loss=0.5410397323853666\n",
      "Log Regression(3243/4999): loss=0.5406592130645774\n",
      "Log Regression(3244/4999): loss=0.5422363907577746\n",
      "Log Regression(3245/4999): loss=0.5429516825760208\n",
      "Log Regression(3246/4999): loss=0.5405415631761286\n",
      "Log Regression(3247/4999): loss=0.5412737603300964\n",
      "Log Regression(3248/4999): loss=0.5429714169682032\n",
      "Log Regression(3249/4999): loss=0.5423499536592452\n",
      "Log Regression(3250/4999): loss=0.5431685009270448\n",
      "Log Regression(3251/4999): loss=0.5411313905161699\n",
      "Log Regression(3252/4999): loss=0.542220683153124\n",
      "Log Regression(3253/4999): loss=0.5412144883392629\n",
      "Log Regression(3254/4999): loss=0.5409605864966969\n",
      "Log Regression(3255/4999): loss=0.5431772097197956\n",
      "Log Regression(3256/4999): loss=0.5449617204691799\n",
      "Log Regression(3257/4999): loss=0.5440905343573104\n",
      "Log Regression(3258/4999): loss=0.5421066000096423\n",
      "Log Regression(3259/4999): loss=0.5408413845346244\n",
      "Log Regression(3260/4999): loss=0.5415724026667185\n",
      "Log Regression(3261/4999): loss=0.5408892278283638\n",
      "Log Regression(3262/4999): loss=0.5409054722051878\n",
      "Log Regression(3263/4999): loss=0.5423175699703691\n",
      "Log Regression(3264/4999): loss=0.5454141321140077\n",
      "Log Regression(3265/4999): loss=0.543556175027373\n",
      "Log Regression(3266/4999): loss=0.5412529063456967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(3267/4999): loss=0.541404599522358\n",
      "Log Regression(3268/4999): loss=0.5413373507959988\n",
      "Log Regression(3269/4999): loss=0.5406678882455588\n",
      "Log Regression(3270/4999): loss=0.5422666170654095\n",
      "Log Regression(3271/4999): loss=0.5407185573043405\n",
      "Log Regression(3272/4999): loss=0.541888151810015\n",
      "Log Regression(3273/4999): loss=0.5449795846609946\n",
      "Log Regression(3274/4999): loss=0.5417712608626252\n",
      "Log Regression(3275/4999): loss=0.5481343693870495\n",
      "Log Regression(3276/4999): loss=0.555394000283974\n",
      "Log Regression(3277/4999): loss=0.5512230137686146\n",
      "Log Regression(3278/4999): loss=0.5410702422382322\n",
      "Log Regression(3279/4999): loss=0.5409589391510383\n",
      "Log Regression(3280/4999): loss=0.5426247195813537\n",
      "Log Regression(3281/4999): loss=0.5428692960563483\n",
      "Log Regression(3282/4999): loss=0.5504095492108734\n",
      "Log Regression(3283/4999): loss=0.541041973675185\n",
      "Log Regression(3284/4999): loss=0.5411809994304259\n",
      "Log Regression(3285/4999): loss=0.5406423920773125\n",
      "Log Regression(3286/4999): loss=0.5459225103323471\n",
      "Log Regression(3287/4999): loss=0.5417327881584383\n",
      "Log Regression(3288/4999): loss=0.5404325733335813\n",
      "Log Regression(3289/4999): loss=0.543396888749492\n",
      "Log Regression(3290/4999): loss=0.5433858811128061\n",
      "Log Regression(3291/4999): loss=0.5411533626086734\n",
      "Log Regression(3292/4999): loss=0.5406981664127465\n",
      "Log Regression(3293/4999): loss=0.5406148140638809\n",
      "Log Regression(3294/4999): loss=0.5414578137893155\n",
      "Log Regression(3295/4999): loss=0.5464664101686726\n",
      "Log Regression(3296/4999): loss=0.5422938377041839\n",
      "Log Regression(3297/4999): loss=0.5406430289493375\n",
      "Log Regression(3298/4999): loss=0.544508234635829\n",
      "Log Regression(3299/4999): loss=0.5409068985712611\n",
      "Log Regression(3300/4999): loss=0.5435161791607601\n",
      "Log Regression(3301/4999): loss=0.5524389989265427\n",
      "Log Regression(3302/4999): loss=0.5412203534253103\n",
      "Log Regression(3303/4999): loss=0.5410323244638386\n",
      "Log Regression(3304/4999): loss=0.5445915277384226\n",
      "Log Regression(3305/4999): loss=0.5433802383969007\n",
      "Log Regression(3306/4999): loss=0.5415830836901836\n",
      "Log Regression(3307/4999): loss=0.5428292369737437\n",
      "Log Regression(3308/4999): loss=0.5404467179894715\n",
      "Log Regression(3309/4999): loss=0.5409188272846486\n",
      "Log Regression(3310/4999): loss=0.5431722545540015\n",
      "Log Regression(3311/4999): loss=0.5405671477485342\n",
      "Log Regression(3312/4999): loss=0.5404435558861722\n",
      "Log Regression(3313/4999): loss=0.5415412310157312\n",
      "Log Regression(3314/4999): loss=0.5417286981967087\n",
      "Log Regression(3315/4999): loss=0.5411550595252368\n",
      "Log Regression(3316/4999): loss=0.540458975978807\n",
      "Log Regression(3317/4999): loss=0.5403977811028776\n",
      "Log Regression(3318/4999): loss=0.5429685891526499\n",
      "Log Regression(3319/4999): loss=0.5448658143023818\n",
      "Log Regression(3320/4999): loss=0.546802581303774\n",
      "Log Regression(3321/4999): loss=0.5447716554294054\n",
      "Log Regression(3322/4999): loss=0.5487490935363873\n",
      "Log Regression(3323/4999): loss=0.554523642770477\n",
      "Log Regression(3324/4999): loss=0.5474144979603061\n",
      "Log Regression(3325/4999): loss=0.5496615437923651\n",
      "Log Regression(3326/4999): loss=0.5452718463538851\n",
      "Log Regression(3327/4999): loss=0.5406715412324621\n",
      "Log Regression(3328/4999): loss=0.5413316713509323\n",
      "Log Regression(3329/4999): loss=0.5410971257480675\n",
      "Log Regression(3330/4999): loss=0.5410345742629951\n",
      "Log Regression(3331/4999): loss=0.541717441080634\n",
      "Log Regression(3332/4999): loss=0.5436645787658203\n",
      "Log Regression(3333/4999): loss=0.5440037168637932\n",
      "Log Regression(3334/4999): loss=0.5484613807530565\n",
      "Log Regression(3335/4999): loss=0.5497860687786447\n",
      "Log Regression(3336/4999): loss=0.5422950952058871\n",
      "Log Regression(3337/4999): loss=0.5405306862359912\n",
      "Log Regression(3338/4999): loss=0.5408111764189423\n",
      "Log Regression(3339/4999): loss=0.5403573999142038\n",
      "Log Regression(3340/4999): loss=0.5403714770381264\n",
      "Log Regression(3341/4999): loss=0.5404062623742047\n",
      "Log Regression(3342/4999): loss=0.5404340527432471\n",
      "Log Regression(3343/4999): loss=0.5403630375007297\n",
      "Log Regression(3344/4999): loss=0.5404370541577489\n",
      "Log Regression(3345/4999): loss=0.5403899487407413\n",
      "Log Regression(3346/4999): loss=0.5404884606054859\n",
      "Log Regression(3347/4999): loss=0.5411195894522629\n",
      "Log Regression(3348/4999): loss=0.5467583738945146\n",
      "Log Regression(3349/4999): loss=0.5420761028504517\n",
      "Log Regression(3350/4999): loss=0.5421039670983231\n",
      "Log Regression(3351/4999): loss=0.542445880613891\n",
      "Log Regression(3352/4999): loss=0.5419494109121556\n",
      "Log Regression(3353/4999): loss=0.5417629999784602\n",
      "Log Regression(3354/4999): loss=0.544148284304823\n",
      "Log Regression(3355/4999): loss=0.5422105554342243\n",
      "Log Regression(3356/4999): loss=0.5406017965333686\n",
      "Log Regression(3357/4999): loss=0.5404843016676617\n",
      "Log Regression(3358/4999): loss=0.5423051068803881\n",
      "Log Regression(3359/4999): loss=0.5423135021866345\n",
      "Log Regression(3360/4999): loss=0.5404731862672261\n",
      "Log Regression(3361/4999): loss=0.5413976726272004\n",
      "Log Regression(3362/4999): loss=0.542962201666233\n",
      "Log Regression(3363/4999): loss=0.5420349780101947\n",
      "Log Regression(3364/4999): loss=0.5422519127850287\n",
      "Log Regression(3365/4999): loss=0.5416647155488619\n",
      "Log Regression(3366/4999): loss=0.5480861534948235\n",
      "Log Regression(3367/4999): loss=0.5460177749176142\n",
      "Log Regression(3368/4999): loss=0.5473990994357035\n",
      "Log Regression(3369/4999): loss=0.5479785837901178\n",
      "Log Regression(3370/4999): loss=0.5416839440036586\n",
      "Log Regression(3371/4999): loss=0.543635955063684\n",
      "Log Regression(3372/4999): loss=0.5406825244954347\n",
      "Log Regression(3373/4999): loss=0.5421905302697294\n",
      "Log Regression(3374/4999): loss=0.5443371043021925\n",
      "Log Regression(3375/4999): loss=0.5447228597132041\n",
      "Log Regression(3376/4999): loss=0.5425358225152566\n",
      "Log Regression(3377/4999): loss=0.5410848794144417\n",
      "Log Regression(3378/4999): loss=0.5419924496070527\n",
      "Log Regression(3379/4999): loss=0.5416763000037561\n",
      "Log Regression(3380/4999): loss=0.5411285992700832\n",
      "Log Regression(3381/4999): loss=0.5417948091796115\n",
      "Log Regression(3382/4999): loss=0.5417020130017773\n",
      "Log Regression(3383/4999): loss=0.5417707603533551\n",
      "Log Regression(3384/4999): loss=0.5410050822069072\n",
      "Log Regression(3385/4999): loss=0.5412012678153173\n",
      "Log Regression(3386/4999): loss=0.5415982536303653\n",
      "Log Regression(3387/4999): loss=0.5413336213513337\n",
      "Log Regression(3388/4999): loss=0.5412496260626042\n",
      "Log Regression(3389/4999): loss=0.5410102991643484\n",
      "Log Regression(3390/4999): loss=0.5408601236894166\n",
      "Log Regression(3391/4999): loss=0.5435020171003709\n",
      "Log Regression(3392/4999): loss=0.5410303215173664\n",
      "Log Regression(3393/4999): loss=0.5415145780367304\n",
      "Log Regression(3394/4999): loss=0.5413951516481083\n",
      "Log Regression(3395/4999): loss=0.5411207105543576\n",
      "Log Regression(3396/4999): loss=0.5415140419115416\n",
      "Log Regression(3397/4999): loss=0.5408340132605641\n",
      "Log Regression(3398/4999): loss=0.5407409196309187\n",
      "Log Regression(3399/4999): loss=0.5404673596774078\n",
      "Log Regression(3400/4999): loss=0.5404182477333624\n",
      "Log Regression(3401/4999): loss=0.5404853373403228\n",
      "Log Regression(3402/4999): loss=0.5406047679633255\n",
      "Log Regression(3403/4999): loss=0.5420461380879072\n",
      "Log Regression(3404/4999): loss=0.5406869016383057\n",
      "Log Regression(3405/4999): loss=0.5406878608637617\n",
      "Log Regression(3406/4999): loss=0.540645667182237\n",
      "Log Regression(3407/4999): loss=0.5415662220072675\n",
      "Log Regression(3408/4999): loss=0.5417349277889619\n",
      "Log Regression(3409/4999): loss=0.541114772280934\n",
      "Log Regression(3410/4999): loss=0.540952282509007\n",
      "Log Regression(3411/4999): loss=0.5406950130974353\n",
      "Log Regression(3412/4999): loss=0.5406006080073095\n",
      "Log Regression(3413/4999): loss=0.5411758175934966\n",
      "Log Regression(3414/4999): loss=0.5407313202574809\n",
      "Log Regression(3415/4999): loss=0.5450962573787647\n",
      "Log Regression(3416/4999): loss=0.5406841969576226\n",
      "Log Regression(3417/4999): loss=0.5404222998393335\n",
      "Log Regression(3418/4999): loss=0.542723139215451\n",
      "Log Regression(3419/4999): loss=0.5408236975038542\n",
      "Log Regression(3420/4999): loss=0.54028635003427\n",
      "Log Regression(3421/4999): loss=0.5423985953182265\n",
      "Log Regression(3422/4999): loss=0.5419972637065262\n",
      "Log Regression(3423/4999): loss=0.5406928244112708\n",
      "Log Regression(3424/4999): loss=0.5441310246973119\n",
      "Log Regression(3425/4999): loss=0.5403405329789867\n",
      "Log Regression(3426/4999): loss=0.540287598744077\n",
      "Log Regression(3427/4999): loss=0.5404243512891925\n",
      "Log Regression(3428/4999): loss=0.5408626516992022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(3429/4999): loss=0.5417656189176131\n",
      "Log Regression(3430/4999): loss=0.5411259934437762\n",
      "Log Regression(3431/4999): loss=0.5404751822498576\n",
      "Log Regression(3432/4999): loss=0.5406163254043193\n",
      "Log Regression(3433/4999): loss=0.5417860282355076\n",
      "Log Regression(3434/4999): loss=0.5465506927454773\n",
      "Log Regression(3435/4999): loss=0.542245729958542\n",
      "Log Regression(3436/4999): loss=0.5418787335941746\n",
      "Log Regression(3437/4999): loss=0.5432486472960247\n",
      "Log Regression(3438/4999): loss=0.5404918881452714\n",
      "Log Regression(3439/4999): loss=0.5405174921391179\n",
      "Log Regression(3440/4999): loss=0.5405334079390696\n",
      "Log Regression(3441/4999): loss=0.5422904204614665\n",
      "Log Regression(3442/4999): loss=0.5422045057161786\n",
      "Log Regression(3443/4999): loss=0.541839931696302\n",
      "Log Regression(3444/4999): loss=0.5418115846102481\n",
      "Log Regression(3445/4999): loss=0.5459730150976578\n",
      "Log Regression(3446/4999): loss=0.5407761589473179\n",
      "Log Regression(3447/4999): loss=0.5414771214422657\n",
      "Log Regression(3448/4999): loss=0.5417622531330086\n",
      "Log Regression(3449/4999): loss=0.5426649775570258\n",
      "Log Regression(3450/4999): loss=0.5435665115687189\n",
      "Log Regression(3451/4999): loss=0.5411982002067398\n",
      "Log Regression(3452/4999): loss=0.5416556716510649\n",
      "Log Regression(3453/4999): loss=0.5425815197906425\n",
      "Log Regression(3454/4999): loss=0.5454363753226399\n",
      "Log Regression(3455/4999): loss=0.5486067176910403\n",
      "Log Regression(3456/4999): loss=0.5421409718139603\n",
      "Log Regression(3457/4999): loss=0.5405104157154819\n",
      "Log Regression(3458/4999): loss=0.5406850087695537\n",
      "Log Regression(3459/4999): loss=0.5437134741853219\n",
      "Log Regression(3460/4999): loss=0.5410510952889237\n",
      "Log Regression(3461/4999): loss=0.5404751817201201\n",
      "Log Regression(3462/4999): loss=0.5428079386585958\n",
      "Log Regression(3463/4999): loss=0.5454911918486284\n",
      "Log Regression(3464/4999): loss=0.5471557991921485\n",
      "Log Regression(3465/4999): loss=0.5415638707004277\n",
      "Log Regression(3466/4999): loss=0.5410043143633928\n",
      "Log Regression(3467/4999): loss=0.5447973461233357\n",
      "Log Regression(3468/4999): loss=0.5451765609213297\n",
      "Log Regression(3469/4999): loss=0.5423146053030801\n",
      "Log Regression(3470/4999): loss=0.5436900377154258\n",
      "Log Regression(3471/4999): loss=0.5427848709288362\n",
      "Log Regression(3472/4999): loss=0.5418508175917625\n",
      "Log Regression(3473/4999): loss=0.5414592926672123\n",
      "Log Regression(3474/4999): loss=0.543043986706537\n",
      "Log Regression(3475/4999): loss=0.5427135286849185\n",
      "Log Regression(3476/4999): loss=0.5409855402645231\n",
      "Log Regression(3477/4999): loss=0.5411818018545729\n",
      "Log Regression(3478/4999): loss=0.5427869920424561\n",
      "Log Regression(3479/4999): loss=0.5442834927391259\n",
      "Log Regression(3480/4999): loss=0.5455227814290164\n",
      "Log Regression(3481/4999): loss=0.5438369713488206\n",
      "Log Regression(3482/4999): loss=0.546338669906328\n",
      "Log Regression(3483/4999): loss=0.5440405437834396\n",
      "Log Regression(3484/4999): loss=0.5409779980361679\n",
      "Log Regression(3485/4999): loss=0.5411969062676892\n",
      "Log Regression(3486/4999): loss=0.5436413365329062\n",
      "Log Regression(3487/4999): loss=0.5428838867284417\n",
      "Log Regression(3488/4999): loss=0.5428950026621435\n",
      "Log Regression(3489/4999): loss=0.5407416630766722\n",
      "Log Regression(3490/4999): loss=0.5406969978542882\n",
      "Log Regression(3491/4999): loss=0.5429515656563864\n",
      "Log Regression(3492/4999): loss=0.5440335714783822\n",
      "Log Regression(3493/4999): loss=0.5431127022987721\n",
      "Log Regression(3494/4999): loss=0.5421189962312795\n",
      "Log Regression(3495/4999): loss=0.5426797957026711\n",
      "Log Regression(3496/4999): loss=0.5430119662903824\n",
      "Log Regression(3497/4999): loss=0.5406409429992106\n",
      "Log Regression(3498/4999): loss=0.5415089662998838\n",
      "Log Regression(3499/4999): loss=0.5407726989406988\n",
      "Log Regression(3500/4999): loss=0.540326151079496\n",
      "Log Regression(3501/4999): loss=0.5434673147936897\n",
      "Log Regression(3502/4999): loss=0.5409794004338434\n",
      "Log Regression(3503/4999): loss=0.5466729607313107\n",
      "Log Regression(3504/4999): loss=0.5472029109866635\n",
      "Log Regression(3505/4999): loss=0.5427861829387249\n",
      "Log Regression(3506/4999): loss=0.5405702137363396\n",
      "Log Regression(3507/4999): loss=0.5407058856865129\n",
      "Log Regression(3508/4999): loss=0.54104704149502\n",
      "Log Regression(3509/4999): loss=0.5409832373257352\n",
      "Log Regression(3510/4999): loss=0.5406110888214282\n",
      "Log Regression(3511/4999): loss=0.5428442089108528\n",
      "Log Regression(3512/4999): loss=0.5405952226245974\n",
      "Log Regression(3513/4999): loss=0.5403302472965854\n",
      "Log Regression(3514/4999): loss=0.5409433686821981\n",
      "Log Regression(3515/4999): loss=0.5403500518480094\n",
      "Log Regression(3516/4999): loss=0.5410660741935385\n",
      "Log Regression(3517/4999): loss=0.543508404884899\n",
      "Log Regression(3518/4999): loss=0.551322192818162\n",
      "Log Regression(3519/4999): loss=0.5563197836733197\n",
      "Log Regression(3520/4999): loss=0.5436900568904024\n",
      "Log Regression(3521/4999): loss=0.5407510463075532\n",
      "Log Regression(3522/4999): loss=0.5409880337201229\n",
      "Log Regression(3523/4999): loss=0.5413723561088223\n",
      "Log Regression(3524/4999): loss=0.5411703696713056\n",
      "Log Regression(3525/4999): loss=0.5464968047970492\n",
      "Log Regression(3526/4999): loss=0.5418272499012634\n",
      "Log Regression(3527/4999): loss=0.540667998516224\n",
      "Log Regression(3528/4999): loss=0.54053687670759\n",
      "Log Regression(3529/4999): loss=0.5430048880846119\n",
      "Log Regression(3530/4999): loss=0.5450400118612977\n",
      "Log Regression(3531/4999): loss=0.541191078459381\n",
      "Log Regression(3532/4999): loss=0.5409230047347131\n",
      "Log Regression(3533/4999): loss=0.5404906340867617\n",
      "Log Regression(3534/4999): loss=0.5409711370046517\n",
      "Log Regression(3535/4999): loss=0.5411688720522704\n",
      "Log Regression(3536/4999): loss=0.5403342376556244\n",
      "Log Regression(3537/4999): loss=0.5421339956165375\n",
      "Log Regression(3538/4999): loss=0.5430804497000963\n",
      "Log Regression(3539/4999): loss=0.5430373413522204\n",
      "Log Regression(3540/4999): loss=0.5424629509707031\n",
      "Log Regression(3541/4999): loss=0.5420490778122398\n",
      "Log Regression(3542/4999): loss=0.5407477839203226\n",
      "Log Regression(3543/4999): loss=0.5412811523591078\n",
      "Log Regression(3544/4999): loss=0.542019848793023\n",
      "Log Regression(3545/4999): loss=0.5410543963002397\n",
      "Log Regression(3546/4999): loss=0.5483457935490483\n",
      "Log Regression(3547/4999): loss=0.5419487676210129\n",
      "Log Regression(3548/4999): loss=0.5417291976852092\n",
      "Log Regression(3549/4999): loss=0.540578209435073\n",
      "Log Regression(3550/4999): loss=0.5420990294970874\n",
      "Log Regression(3551/4999): loss=0.5430148519526854\n",
      "Log Regression(3552/4999): loss=0.5416719323936307\n",
      "Log Regression(3553/4999): loss=0.5428205343028147\n",
      "Log Regression(3554/4999): loss=0.5483667858743297\n",
      "Log Regression(3555/4999): loss=0.5450727604080685\n",
      "Log Regression(3556/4999): loss=0.5435753028957457\n",
      "Log Regression(3557/4999): loss=0.5419946539147859\n",
      "Log Regression(3558/4999): loss=0.5411204865972246\n",
      "Log Regression(3559/4999): loss=0.5405239595414622\n",
      "Log Regression(3560/4999): loss=0.5413147083873363\n",
      "Log Regression(3561/4999): loss=0.5416019108236462\n",
      "Log Regression(3562/4999): loss=0.5408384359544407\n",
      "Log Regression(3563/4999): loss=0.5403183812161985\n",
      "Log Regression(3564/4999): loss=0.5411677590906067\n",
      "Log Regression(3565/4999): loss=0.5403567511603835\n",
      "Log Regression(3566/4999): loss=0.5403664775023612\n",
      "Log Regression(3567/4999): loss=0.5405467302721574\n",
      "Log Regression(3568/4999): loss=0.5408894493341538\n",
      "Log Regression(3569/4999): loss=0.542864257795878\n",
      "Log Regression(3570/4999): loss=0.5456054479017958\n",
      "Log Regression(3571/4999): loss=0.5499685800646359\n",
      "Log Regression(3572/4999): loss=0.5465149184794718\n",
      "Log Regression(3573/4999): loss=0.5428491121501178\n",
      "Log Regression(3574/4999): loss=0.5405772841983468\n",
      "Log Regression(3575/4999): loss=0.5413458781529138\n",
      "Log Regression(3576/4999): loss=0.5416010750219566\n",
      "Log Regression(3577/4999): loss=0.5443377970830112\n",
      "Log Regression(3578/4999): loss=0.5457235559329496\n",
      "Log Regression(3579/4999): loss=0.5409950104937578\n",
      "Log Regression(3580/4999): loss=0.5433530549407508\n",
      "Log Regression(3581/4999): loss=0.5415400571766892\n",
      "Log Regression(3582/4999): loss=0.5412730959161877\n",
      "Log Regression(3583/4999): loss=0.5411011162477806\n",
      "Log Regression(3584/4999): loss=0.541433518464753\n",
      "Log Regression(3585/4999): loss=0.541732582775202\n",
      "Log Regression(3586/4999): loss=0.5452946359551207\n",
      "Log Regression(3587/4999): loss=0.5423559019963454\n",
      "Log Regression(3588/4999): loss=0.5490231599577521\n",
      "Log Regression(3589/4999): loss=0.5447585222599434\n",
      "Log Regression(3590/4999): loss=0.5409182142231277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(3591/4999): loss=0.5422597336641652\n",
      "Log Regression(3592/4999): loss=0.5407245256309421\n",
      "Log Regression(3593/4999): loss=0.5433254382286402\n",
      "Log Regression(3594/4999): loss=0.541470157249919\n",
      "Log Regression(3595/4999): loss=0.540844117553087\n",
      "Log Regression(3596/4999): loss=0.5430197217192483\n",
      "Log Regression(3597/4999): loss=0.5420228159276735\n",
      "Log Regression(3598/4999): loss=0.5403885045440635\n",
      "Log Regression(3599/4999): loss=0.5420565860392477\n",
      "Log Regression(3600/4999): loss=0.5419850962982979\n",
      "Log Regression(3601/4999): loss=0.5406936157047024\n",
      "Log Regression(3602/4999): loss=0.5409637546274362\n",
      "Log Regression(3603/4999): loss=0.5504972571988689\n",
      "Log Regression(3604/4999): loss=0.5514226766305623\n",
      "Log Regression(3605/4999): loss=0.5442794184358717\n",
      "Log Regression(3606/4999): loss=0.5418054199227614\n",
      "Log Regression(3607/4999): loss=0.5402857698113596\n",
      "Log Regression(3608/4999): loss=0.5409042463215341\n",
      "Log Regression(3609/4999): loss=0.5522258032911214\n",
      "Log Regression(3610/4999): loss=0.554661619512317\n",
      "Log Regression(3611/4999): loss=0.5439435558940435\n",
      "Log Regression(3612/4999): loss=0.5412345896767873\n",
      "Log Regression(3613/4999): loss=0.5419752339080756\n",
      "Log Regression(3614/4999): loss=0.5406891598756572\n",
      "Log Regression(3615/4999): loss=0.5406011213784133\n",
      "Log Regression(3616/4999): loss=0.542007988586121\n",
      "Log Regression(3617/4999): loss=0.540904871299065\n",
      "Log Regression(3618/4999): loss=0.5410435241049415\n",
      "Log Regression(3619/4999): loss=0.541166914314409\n",
      "Log Regression(3620/4999): loss=0.546331107685973\n",
      "Log Regression(3621/4999): loss=0.5439248794111182\n",
      "Log Regression(3622/4999): loss=0.5436649489563435\n",
      "Log Regression(3623/4999): loss=0.5452478620691017\n",
      "Log Regression(3624/4999): loss=0.54045124827238\n",
      "Log Regression(3625/4999): loss=0.5403946920979085\n",
      "Log Regression(3626/4999): loss=0.5404999500295868\n",
      "Log Regression(3627/4999): loss=0.5415110916092843\n",
      "Log Regression(3628/4999): loss=0.5417563551143701\n",
      "Log Regression(3629/4999): loss=0.5409902755682144\n",
      "Log Regression(3630/4999): loss=0.5402514003997486\n",
      "Log Regression(3631/4999): loss=0.5418705990261891\n",
      "Log Regression(3632/4999): loss=0.5420070660561311\n",
      "Log Regression(3633/4999): loss=0.5431727699135582\n",
      "Log Regression(3634/4999): loss=0.5410473603249517\n",
      "Log Regression(3635/4999): loss=0.5427378123768568\n",
      "Log Regression(3636/4999): loss=0.5413829519513504\n",
      "Log Regression(3637/4999): loss=0.5407945151259566\n",
      "Log Regression(3638/4999): loss=0.5419807235186601\n",
      "Log Regression(3639/4999): loss=0.5404627693269078\n",
      "Log Regression(3640/4999): loss=0.5411168920106338\n",
      "Log Regression(3641/4999): loss=0.5427355157913937\n",
      "Log Regression(3642/4999): loss=0.5416426361142943\n",
      "Log Regression(3643/4999): loss=0.5466241806760626\n",
      "Log Regression(3644/4999): loss=0.5433941642407278\n",
      "Log Regression(3645/4999): loss=0.5411196437224258\n",
      "Log Regression(3646/4999): loss=0.5409045226745622\n",
      "Log Regression(3647/4999): loss=0.5404678806988881\n",
      "Log Regression(3648/4999): loss=0.5408499288419598\n",
      "Log Regression(3649/4999): loss=0.5410523174817597\n",
      "Log Regression(3650/4999): loss=0.5406713691415191\n",
      "Log Regression(3651/4999): loss=0.5472751074727878\n",
      "Log Regression(3652/4999): loss=0.5424714581121344\n",
      "Log Regression(3653/4999): loss=0.5411796064038453\n",
      "Log Regression(3654/4999): loss=0.541199161944562\n",
      "Log Regression(3655/4999): loss=0.5406410812741371\n",
      "Log Regression(3656/4999): loss=0.5404959641667821\n",
      "Log Regression(3657/4999): loss=0.5422211646820623\n",
      "Log Regression(3658/4999): loss=0.5419652250997149\n",
      "Log Regression(3659/4999): loss=0.540633361613845\n",
      "Log Regression(3660/4999): loss=0.5446356779821949\n",
      "Log Regression(3661/4999): loss=0.540846967954663\n",
      "Log Regression(3662/4999): loss=0.5414078910088316\n",
      "Log Regression(3663/4999): loss=0.543570735840622\n",
      "Log Regression(3664/4999): loss=0.5410355823581187\n",
      "Log Regression(3665/4999): loss=0.5404537359790811\n",
      "Log Regression(3666/4999): loss=0.5417512094317203\n",
      "Log Regression(3667/4999): loss=0.5404264536277724\n",
      "Log Regression(3668/4999): loss=0.5407596061585089\n",
      "Log Regression(3669/4999): loss=0.5420521768585328\n",
      "Log Regression(3670/4999): loss=0.5421214082307806\n",
      "Log Regression(3671/4999): loss=0.5402945247738232\n",
      "Log Regression(3672/4999): loss=0.5414235650674213\n",
      "Log Regression(3673/4999): loss=0.5409445472750285\n",
      "Log Regression(3674/4999): loss=0.5424080788454221\n",
      "Log Regression(3675/4999): loss=0.5429165532252744\n",
      "Log Regression(3676/4999): loss=0.540552069447172\n",
      "Log Regression(3677/4999): loss=0.5413979933819659\n",
      "Log Regression(3678/4999): loss=0.5411658401857738\n",
      "Log Regression(3679/4999): loss=0.5406590898885719\n",
      "Log Regression(3680/4999): loss=0.5421222242614979\n",
      "Log Regression(3681/4999): loss=0.5490547515842027\n",
      "Log Regression(3682/4999): loss=0.5413052620055911\n",
      "Log Regression(3683/4999): loss=0.5408427825217288\n",
      "Log Regression(3684/4999): loss=0.5402745746492302\n",
      "Log Regression(3685/4999): loss=0.5403967814089253\n",
      "Log Regression(3686/4999): loss=0.5403155540628212\n",
      "Log Regression(3687/4999): loss=0.5406712327694208\n",
      "Log Regression(3688/4999): loss=0.5404506590868131\n",
      "Log Regression(3689/4999): loss=0.5434937586914563\n",
      "Log Regression(3690/4999): loss=0.5427173100855133\n",
      "Log Regression(3691/4999): loss=0.5404800682277905\n",
      "Log Regression(3692/4999): loss=0.540773157244473\n",
      "Log Regression(3693/4999): loss=0.5419432584096553\n",
      "Log Regression(3694/4999): loss=0.5405713062515073\n",
      "Log Regression(3695/4999): loss=0.5405631485714656\n",
      "Log Regression(3696/4999): loss=0.5413125684960701\n",
      "Log Regression(3697/4999): loss=0.5406574464518119\n",
      "Log Regression(3698/4999): loss=0.5408794781570907\n",
      "Log Regression(3699/4999): loss=0.5406662913355793\n",
      "Log Regression(3700/4999): loss=0.5429302504074359\n",
      "Log Regression(3701/4999): loss=0.5406775971101412\n",
      "Log Regression(3702/4999): loss=0.5402819402570264\n",
      "Log Regression(3703/4999): loss=0.5402885596947228\n",
      "Log Regression(3704/4999): loss=0.5407324933625912\n",
      "Log Regression(3705/4999): loss=0.542092650794673\n",
      "Log Regression(3706/4999): loss=0.5489051215041917\n",
      "Log Regression(3707/4999): loss=0.5441899656539237\n",
      "Log Regression(3708/4999): loss=0.5500748164887012\n",
      "Log Regression(3709/4999): loss=0.543836794050153\n",
      "Log Regression(3710/4999): loss=0.5404491633280825\n",
      "Log Regression(3711/4999): loss=0.5402645496758424\n",
      "Log Regression(3712/4999): loss=0.5405976810928298\n",
      "Log Regression(3713/4999): loss=0.541715251622197\n",
      "Log Regression(3714/4999): loss=0.5474957064912197\n",
      "Log Regression(3715/4999): loss=0.552096349701499\n",
      "Log Regression(3716/4999): loss=0.5433686242676854\n",
      "Log Regression(3717/4999): loss=0.540226154262025\n",
      "Log Regression(3718/4999): loss=0.5409697607309103\n",
      "Log Regression(3719/4999): loss=0.540594058187939\n",
      "Log Regression(3720/4999): loss=0.5405049704473585\n",
      "Log Regression(3721/4999): loss=0.5403036657700199\n",
      "Log Regression(3722/4999): loss=0.5418868684238294\n",
      "Log Regression(3723/4999): loss=0.5414465714754082\n",
      "Log Regression(3724/4999): loss=0.5404184395781458\n",
      "Log Regression(3725/4999): loss=0.5410503575858789\n",
      "Log Regression(3726/4999): loss=0.5403121142333043\n",
      "Log Regression(3727/4999): loss=0.5410646353069303\n",
      "Log Regression(3728/4999): loss=0.5411906619847757\n",
      "Log Regression(3729/4999): loss=0.5402668421058665\n",
      "Log Regression(3730/4999): loss=0.5405889389563057\n",
      "Log Regression(3731/4999): loss=0.5403362280477526\n",
      "Log Regression(3732/4999): loss=0.5403572048598879\n",
      "Log Regression(3733/4999): loss=0.5402510551334423\n",
      "Log Regression(3734/4999): loss=0.5404643663905021\n",
      "Log Regression(3735/4999): loss=0.5403117005950806\n",
      "Log Regression(3736/4999): loss=0.5403633503980025\n",
      "Log Regression(3737/4999): loss=0.5414403585376617\n",
      "Log Regression(3738/4999): loss=0.5415400345044155\n",
      "Log Regression(3739/4999): loss=0.5424053583670801\n",
      "Log Regression(3740/4999): loss=0.5403410548512259\n",
      "Log Regression(3741/4999): loss=0.5406700318294599\n",
      "Log Regression(3742/4999): loss=0.5404264044712475\n",
      "Log Regression(3743/4999): loss=0.541906250998777\n",
      "Log Regression(3744/4999): loss=0.5421292507463548\n",
      "Log Regression(3745/4999): loss=0.5409706367084558\n",
      "Log Regression(3746/4999): loss=0.5403779396647659\n",
      "Log Regression(3747/4999): loss=0.5403358425640494\n",
      "Log Regression(3748/4999): loss=0.542069802408024\n",
      "Log Regression(3749/4999): loss=0.546076978377141\n",
      "Log Regression(3750/4999): loss=0.5420336257795563\n",
      "Log Regression(3751/4999): loss=0.545176208111087\n",
      "Log Regression(3752/4999): loss=0.546699256657306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(3753/4999): loss=0.5408147033793915\n",
      "Log Regression(3754/4999): loss=0.5403873001271778\n",
      "Log Regression(3755/4999): loss=0.5403673388483335\n",
      "Log Regression(3756/4999): loss=0.5405031318286986\n",
      "Log Regression(3757/4999): loss=0.5414460007493471\n",
      "Log Regression(3758/4999): loss=0.541424119311992\n",
      "Log Regression(3759/4999): loss=0.5431876037306046\n",
      "Log Regression(3760/4999): loss=0.5420842325005896\n",
      "Log Regression(3761/4999): loss=0.5405866293415367\n",
      "Log Regression(3762/4999): loss=0.5413259618894486\n",
      "Log Regression(3763/4999): loss=0.5488725519486036\n",
      "Log Regression(3764/4999): loss=0.5459520196949205\n",
      "Log Regression(3765/4999): loss=0.5496023471955069\n",
      "Log Regression(3766/4999): loss=0.5432380022536774\n",
      "Log Regression(3767/4999): loss=0.5414576686083515\n",
      "Log Regression(3768/4999): loss=0.5423323279083615\n",
      "Log Regression(3769/4999): loss=0.5403578023982488\n",
      "Log Regression(3770/4999): loss=0.5417621334222997\n",
      "Log Regression(3771/4999): loss=0.5407856325984257\n",
      "Log Regression(3772/4999): loss=0.5405123712400762\n",
      "Log Regression(3773/4999): loss=0.5403815228465063\n",
      "Log Regression(3774/4999): loss=0.5403268634323012\n",
      "Log Regression(3775/4999): loss=0.5402971269047058\n",
      "Log Regression(3776/4999): loss=0.5406780916302993\n",
      "Log Regression(3777/4999): loss=0.5412174843385449\n",
      "Log Regression(3778/4999): loss=0.5408687135240537\n",
      "Log Regression(3779/4999): loss=0.5431419651388586\n",
      "Log Regression(3780/4999): loss=0.5438651751674874\n",
      "Log Regression(3781/4999): loss=0.54201498478585\n",
      "Log Regression(3782/4999): loss=0.5402256046362836\n",
      "Log Regression(3783/4999): loss=0.543200723662811\n",
      "Log Regression(3784/4999): loss=0.5440737901285728\n",
      "Log Regression(3785/4999): loss=0.5405752633185823\n",
      "Log Regression(3786/4999): loss=0.5402874708373939\n",
      "Log Regression(3787/4999): loss=0.5405987672157504\n",
      "Log Regression(3788/4999): loss=0.5410764293834276\n",
      "Log Regression(3789/4999): loss=0.5416722383880153\n",
      "Log Regression(3790/4999): loss=0.5420632248019662\n",
      "Log Regression(3791/4999): loss=0.5402286482317586\n",
      "Log Regression(3792/4999): loss=0.540457945730458\n",
      "Log Regression(3793/4999): loss=0.5406270736056137\n",
      "Log Regression(3794/4999): loss=0.5404330315069538\n",
      "Log Regression(3795/4999): loss=0.540469827730122\n",
      "Log Regression(3796/4999): loss=0.5404567038330776\n",
      "Log Regression(3797/4999): loss=0.5402500150176746\n",
      "Log Regression(3798/4999): loss=0.5405358674083379\n",
      "Log Regression(3799/4999): loss=0.5427585701898563\n",
      "Log Regression(3800/4999): loss=0.5428771459203843\n",
      "Log Regression(3801/4999): loss=0.5402160422578414\n",
      "Log Regression(3802/4999): loss=0.5403407080991424\n",
      "Log Regression(3803/4999): loss=0.5403977168969304\n",
      "Log Regression(3804/4999): loss=0.5402401295939763\n",
      "Log Regression(3805/4999): loss=0.5403897582655414\n",
      "Log Regression(3806/4999): loss=0.5414909674626858\n",
      "Log Regression(3807/4999): loss=0.5419402135836835\n",
      "Log Regression(3808/4999): loss=0.5412103532854636\n",
      "Log Regression(3809/4999): loss=0.5441218301196784\n",
      "Log Regression(3810/4999): loss=0.5404690268791644\n",
      "Log Regression(3811/4999): loss=0.5421215076808438\n",
      "Log Regression(3812/4999): loss=0.5404323575692577\n",
      "Log Regression(3813/4999): loss=0.5415198045543012\n",
      "Log Regression(3814/4999): loss=0.5413899349747111\n",
      "Log Regression(3815/4999): loss=0.5403558759125852\n",
      "Log Regression(3816/4999): loss=0.5411703336623495\n",
      "Log Regression(3817/4999): loss=0.5404049311088432\n",
      "Log Regression(3818/4999): loss=0.5402769187221763\n",
      "Log Regression(3819/4999): loss=0.5410096237072513\n",
      "Log Regression(3820/4999): loss=0.5402964762546247\n",
      "Log Regression(3821/4999): loss=0.5465335973831225\n",
      "Log Regression(3822/4999): loss=0.5427157358299883\n",
      "Log Regression(3823/4999): loss=0.5405995728087583\n",
      "Log Regression(3824/4999): loss=0.5402349473411823\n",
      "Log Regression(3825/4999): loss=0.5410740481205212\n",
      "Log Regression(3826/4999): loss=0.5403921715918791\n",
      "Log Regression(3827/4999): loss=0.540314566153676\n",
      "Log Regression(3828/4999): loss=0.5437269080072179\n",
      "Log Regression(3829/4999): loss=0.5406520347001937\n",
      "Log Regression(3830/4999): loss=0.5420531121950181\n",
      "Log Regression(3831/4999): loss=0.5406259960436351\n",
      "Log Regression(3832/4999): loss=0.5412329424743807\n",
      "Log Regression(3833/4999): loss=0.540551611167839\n",
      "Log Regression(3834/4999): loss=0.5484492601682017\n",
      "Log Regression(3835/4999): loss=0.5407933868008001\n",
      "Log Regression(3836/4999): loss=0.5403143680910879\n",
      "Log Regression(3837/4999): loss=0.5407581597009299\n",
      "Log Regression(3838/4999): loss=0.5424450152592029\n",
      "Log Regression(3839/4999): loss=0.5402122878969469\n",
      "Log Regression(3840/4999): loss=0.5403088993165321\n",
      "Log Regression(3841/4999): loss=0.5406735270369691\n",
      "Log Regression(3842/4999): loss=0.5452665667951702\n",
      "Log Regression(3843/4999): loss=0.5458977596884269\n",
      "Log Regression(3844/4999): loss=0.5409576095413107\n",
      "Log Regression(3845/4999): loss=0.5431259340151362\n",
      "Log Regression(3846/4999): loss=0.5408367730340177\n",
      "Log Regression(3847/4999): loss=0.540938340179732\n",
      "Log Regression(3848/4999): loss=0.5406117735979068\n",
      "Log Regression(3849/4999): loss=0.5410893564372524\n",
      "Log Regression(3850/4999): loss=0.541013011296243\n",
      "Log Regression(3851/4999): loss=0.5419515119805406\n",
      "Log Regression(3852/4999): loss=0.5419059984202236\n",
      "Log Regression(3853/4999): loss=0.5426890917124153\n",
      "Log Regression(3854/4999): loss=0.5422883569581631\n",
      "Log Regression(3855/4999): loss=0.5405794602352082\n",
      "Log Regression(3856/4999): loss=0.5407409400368379\n",
      "Log Regression(3857/4999): loss=0.5403594496974438\n",
      "Log Regression(3858/4999): loss=0.5404426008564999\n",
      "Log Regression(3859/4999): loss=0.5416156742832956\n",
      "Log Regression(3860/4999): loss=0.543920794800067\n",
      "Log Regression(3861/4999): loss=0.5411907682135658\n",
      "Log Regression(3862/4999): loss=0.5402111731902287\n",
      "Log Regression(3863/4999): loss=0.540210974644892\n",
      "Log Regression(3864/4999): loss=0.5417792895141406\n",
      "Log Regression(3865/4999): loss=0.5417779968326252\n",
      "Log Regression(3866/4999): loss=0.541766238825993\n",
      "Log Regression(3867/4999): loss=0.5402251935123813\n",
      "Log Regression(3868/4999): loss=0.5411750499789452\n",
      "Log Regression(3869/4999): loss=0.5406279284925324\n",
      "Log Regression(3870/4999): loss=0.5419130812644904\n",
      "Log Regression(3871/4999): loss=0.5405147005157871\n",
      "Log Regression(3872/4999): loss=0.5404775992413108\n",
      "Log Regression(3873/4999): loss=0.5406490109312869\n",
      "Log Regression(3874/4999): loss=0.5432288216844561\n",
      "Log Regression(3875/4999): loss=0.5418993569328461\n",
      "Log Regression(3876/4999): loss=0.5403415762441162\n",
      "Log Regression(3877/4999): loss=0.5405098599526708\n",
      "Log Regression(3878/4999): loss=0.541454748208717\n",
      "Log Regression(3879/4999): loss=0.5413843227330335\n",
      "Log Regression(3880/4999): loss=0.5483114118865249\n",
      "Log Regression(3881/4999): loss=0.5518212937023097\n",
      "Log Regression(3882/4999): loss=0.5545139335301491\n",
      "Log Regression(3883/4999): loss=0.5411378711553303\n",
      "Log Regression(3884/4999): loss=0.5405712649500599\n",
      "Log Regression(3885/4999): loss=0.5404135490854298\n",
      "Log Regression(3886/4999): loss=0.540654731198401\n",
      "Log Regression(3887/4999): loss=0.5405504438129047\n",
      "Log Regression(3888/4999): loss=0.5444348497354993\n",
      "Log Regression(3889/4999): loss=0.5442189148709579\n",
      "Log Regression(3890/4999): loss=0.5465269235827439\n",
      "Log Regression(3891/4999): loss=0.5445515498591351\n",
      "Log Regression(3892/4999): loss=0.5407636831572749\n",
      "Log Regression(3893/4999): loss=0.5409918062838812\n",
      "Log Regression(3894/4999): loss=0.5432875019922985\n",
      "Log Regression(3895/4999): loss=0.5424931671839127\n",
      "Log Regression(3896/4999): loss=0.5412068030273927\n",
      "Log Regression(3897/4999): loss=0.5413937537377904\n",
      "Log Regression(3898/4999): loss=0.540461181687036\n",
      "Log Regression(3899/4999): loss=0.5407564230122028\n",
      "Log Regression(3900/4999): loss=0.5411834742355544\n",
      "Log Regression(3901/4999): loss=0.5417393074357373\n",
      "Log Regression(3902/4999): loss=0.540507757387027\n",
      "Log Regression(3903/4999): loss=0.5404386730557222\n",
      "Log Regression(3904/4999): loss=0.5507726088470402\n",
      "Log Regression(3905/4999): loss=0.5435839902137156\n",
      "Log Regression(3906/4999): loss=0.5431678553882173\n",
      "Log Regression(3907/4999): loss=0.5404938902553207\n",
      "Log Regression(3908/4999): loss=0.5418125671125721\n",
      "Log Regression(3909/4999): loss=0.5420485651182301\n",
      "Log Regression(3910/4999): loss=0.5406541874139996\n",
      "Log Regression(3911/4999): loss=0.5405111643616453\n",
      "Log Regression(3912/4999): loss=0.540507099969302\n",
      "Log Regression(3913/4999): loss=0.5432282336961952\n",
      "Log Regression(3914/4999): loss=0.5423375685937035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(3915/4999): loss=0.5403676637859233\n",
      "Log Regression(3916/4999): loss=0.5414845103467015\n",
      "Log Regression(3917/4999): loss=0.5413026841730532\n",
      "Log Regression(3918/4999): loss=0.5402321696286508\n",
      "Log Regression(3919/4999): loss=0.5405971714519896\n",
      "Log Regression(3920/4999): loss=0.5402588902293377\n",
      "Log Regression(3921/4999): loss=0.5403110117586455\n",
      "Log Regression(3922/4999): loss=0.542361800743062\n",
      "Log Regression(3923/4999): loss=0.5431652028452253\n",
      "Log Regression(3924/4999): loss=0.5412260342724589\n",
      "Log Regression(3925/4999): loss=0.5406525627377795\n",
      "Log Regression(3926/4999): loss=0.5403976759254189\n",
      "Log Regression(3927/4999): loss=0.5402824446775532\n",
      "Log Regression(3928/4999): loss=0.5406741185464622\n",
      "Log Regression(3929/4999): loss=0.5404156131162352\n",
      "Log Regression(3930/4999): loss=0.5407166034596728\n",
      "Log Regression(3931/4999): loss=0.5406912182302103\n",
      "Log Regression(3932/4999): loss=0.540240537377083\n",
      "Log Regression(3933/4999): loss=0.5436029884970461\n",
      "Log Regression(3934/4999): loss=0.5450810229799824\n",
      "Log Regression(3935/4999): loss=0.5423134205838199\n",
      "Log Regression(3936/4999): loss=0.5404560880147987\n",
      "Log Regression(3937/4999): loss=0.5412339865872912\n",
      "Log Regression(3938/4999): loss=0.5433037446201966\n",
      "Log Regression(3939/4999): loss=0.5405282637101048\n",
      "Log Regression(3940/4999): loss=0.5403279701453512\n",
      "Log Regression(3941/4999): loss=0.5410303472115912\n",
      "Log Regression(3942/4999): loss=0.5401923068986562\n",
      "Log Regression(3943/4999): loss=0.5403373335852392\n",
      "Log Regression(3944/4999): loss=0.5414322092801547\n",
      "Log Regression(3945/4999): loss=0.541349626603545\n",
      "Log Regression(3946/4999): loss=0.5403105450478755\n",
      "Log Regression(3947/4999): loss=0.5403371134770727\n",
      "Log Regression(3948/4999): loss=0.540311081714701\n",
      "Log Regression(3949/4999): loss=0.5414848048092291\n",
      "Log Regression(3950/4999): loss=0.5402537788562579\n",
      "Log Regression(3951/4999): loss=0.5414647184525098\n",
      "Log Regression(3952/4999): loss=0.5405833411318034\n",
      "Log Regression(3953/4999): loss=0.540892093894594\n",
      "Log Regression(3954/4999): loss=0.5414336747740874\n",
      "Log Regression(3955/4999): loss=0.5403754306262453\n",
      "Log Regression(3956/4999): loss=0.5402068393680947\n",
      "Log Regression(3957/4999): loss=0.5401880821347198\n",
      "Log Regression(3958/4999): loss=0.5410982108819206\n",
      "Log Regression(3959/4999): loss=0.5425643520125181\n",
      "Log Regression(3960/4999): loss=0.5426208907229239\n",
      "Log Regression(3961/4999): loss=0.540277624735328\n",
      "Log Regression(3962/4999): loss=0.5409398114764224\n",
      "Log Regression(3963/4999): loss=0.5431182685849779\n",
      "Log Regression(3964/4999): loss=0.5402632149759852\n",
      "Log Regression(3965/4999): loss=0.5406910863383271\n",
      "Log Regression(3966/4999): loss=0.5416908757254438\n",
      "Log Regression(3967/4999): loss=0.5402716121318744\n",
      "Log Regression(3968/4999): loss=0.5404691039226228\n",
      "Log Regression(3969/4999): loss=0.5402749616789585\n",
      "Log Regression(3970/4999): loss=0.5402949337588074\n",
      "Log Regression(3971/4999): loss=0.5402774143024338\n",
      "Log Regression(3972/4999): loss=0.5448335385943016\n",
      "Log Regression(3973/4999): loss=0.5450944758816726\n",
      "Log Regression(3974/4999): loss=0.5421978200654354\n",
      "Log Regression(3975/4999): loss=0.5447241417764478\n",
      "Log Regression(3976/4999): loss=0.5471767084219957\n",
      "Log Regression(3977/4999): loss=0.5438021613345225\n",
      "Log Regression(3978/4999): loss=0.5413569519366578\n",
      "Log Regression(3979/4999): loss=0.5414775378138289\n",
      "Log Regression(3980/4999): loss=0.5432784072809849\n",
      "Log Regression(3981/4999): loss=0.542659971596506\n",
      "Log Regression(3982/4999): loss=0.540940257335544\n",
      "Log Regression(3983/4999): loss=0.5410202979278091\n",
      "Log Regression(3984/4999): loss=0.542473801699297\n",
      "Log Regression(3985/4999): loss=0.5453342890405212\n",
      "Log Regression(3986/4999): loss=0.5426270028030283\n",
      "Log Regression(3987/4999): loss=0.5401734608992819\n",
      "Log Regression(3988/4999): loss=0.5408889852788079\n",
      "Log Regression(3989/4999): loss=0.5422222952561295\n",
      "Log Regression(3990/4999): loss=0.5403968581861418\n",
      "Log Regression(3991/4999): loss=0.5423843778299435\n",
      "Log Regression(3992/4999): loss=0.5414141398803178\n",
      "Log Regression(3993/4999): loss=0.5439575432814703\n",
      "Log Regression(3994/4999): loss=0.5431240837988918\n",
      "Log Regression(3995/4999): loss=0.5455164227600844\n",
      "Log Regression(3996/4999): loss=0.5419011495404888\n",
      "Log Regression(3997/4999): loss=0.541929785802673\n",
      "Log Regression(3998/4999): loss=0.5438708311950387\n",
      "Log Regression(3999/4999): loss=0.5410099646677474\n",
      "Log Regression(4000/4999): loss=0.541531625359462\n",
      "Log Regression(4001/4999): loss=0.5403458536224224\n",
      "Log Regression(4002/4999): loss=0.5406649278421247\n",
      "Log Regression(4003/4999): loss=0.5403842901725255\n",
      "Log Regression(4004/4999): loss=0.5418710065819404\n",
      "Log Regression(4005/4999): loss=0.5401893964015859\n",
      "Log Regression(4006/4999): loss=0.5410453709031182\n",
      "Log Regression(4007/4999): loss=0.5417157726641343\n",
      "Log Regression(4008/4999): loss=0.5413975675520596\n",
      "Log Regression(4009/4999): loss=0.5402729145300782\n",
      "Log Regression(4010/4999): loss=0.5414384917940478\n",
      "Log Regression(4011/4999): loss=0.5412254731893893\n",
      "Log Regression(4012/4999): loss=0.5406977981097651\n",
      "Log Regression(4013/4999): loss=0.5401628270423541\n",
      "Log Regression(4014/4999): loss=0.5402390964357549\n",
      "Log Regression(4015/4999): loss=0.5418432372735849\n",
      "Log Regression(4016/4999): loss=0.5403143073512315\n",
      "Log Regression(4017/4999): loss=0.5407748597323563\n",
      "Log Regression(4018/4999): loss=0.5409202068679276\n",
      "Log Regression(4019/4999): loss=0.5403359383635753\n",
      "Log Regression(4020/4999): loss=0.540254928063493\n",
      "Log Regression(4021/4999): loss=0.5402233355785372\n",
      "Log Regression(4022/4999): loss=0.5403647410515748\n",
      "Log Regression(4023/4999): loss=0.5412871367619605\n",
      "Log Regression(4024/4999): loss=0.5430956830822941\n",
      "Log Regression(4025/4999): loss=0.5410751339155025\n",
      "Log Regression(4026/4999): loss=0.5409217910418064\n",
      "Log Regression(4027/4999): loss=0.5411073061682025\n",
      "Log Regression(4028/4999): loss=0.5421817322244689\n",
      "Log Regression(4029/4999): loss=0.5421576064107619\n",
      "Log Regression(4030/4999): loss=0.5401551619611074\n",
      "Log Regression(4031/4999): loss=0.5404007679448726\n",
      "Log Regression(4032/4999): loss=0.5403617181879802\n",
      "Log Regression(4033/4999): loss=0.5407509301327879\n",
      "Log Regression(4034/4999): loss=0.5402554867290713\n",
      "Log Regression(4035/4999): loss=0.5409782104547662\n",
      "Log Regression(4036/4999): loss=0.5407279840941662\n",
      "Log Regression(4037/4999): loss=0.540938244684189\n",
      "Log Regression(4038/4999): loss=0.5418661218247842\n",
      "Log Regression(4039/4999): loss=0.5418916030090707\n",
      "Log Regression(4040/4999): loss=0.5401371391065802\n",
      "Log Regression(4041/4999): loss=0.5421176676791775\n",
      "Log Regression(4042/4999): loss=0.540204289597519\n",
      "Log Regression(4043/4999): loss=0.5401617925733826\n",
      "Log Regression(4044/4999): loss=0.5406210116326792\n",
      "Log Regression(4045/4999): loss=0.5402117296113887\n",
      "Log Regression(4046/4999): loss=0.5402300526921328\n",
      "Log Regression(4047/4999): loss=0.5405299603034414\n",
      "Log Regression(4048/4999): loss=0.5420288490103908\n",
      "Log Regression(4049/4999): loss=0.5404084123079682\n",
      "Log Regression(4050/4999): loss=0.5444620997657479\n",
      "Log Regression(4051/4999): loss=0.5406980363139134\n",
      "Log Regression(4052/4999): loss=0.5406287190814217\n",
      "Log Regression(4053/4999): loss=0.5409614841244845\n",
      "Log Regression(4054/4999): loss=0.5408143326515865\n",
      "Log Regression(4055/4999): loss=0.5429685089132867\n",
      "Log Regression(4056/4999): loss=0.5410023089429475\n",
      "Log Regression(4057/4999): loss=0.5411462482773465\n",
      "Log Regression(4058/4999): loss=0.5417800155565681\n",
      "Log Regression(4059/4999): loss=0.5423075250452053\n",
      "Log Regression(4060/4999): loss=0.5420988116320569\n",
      "Log Regression(4061/4999): loss=0.5412500591582983\n",
      "Log Regression(4062/4999): loss=0.5410534040786965\n",
      "Log Regression(4063/4999): loss=0.5408432080460632\n",
      "Log Regression(4064/4999): loss=0.5407782079718876\n",
      "Log Regression(4065/4999): loss=0.5407135084019946\n",
      "Log Regression(4066/4999): loss=0.5411819662903553\n",
      "Log Regression(4067/4999): loss=0.5416503147613982\n",
      "Log Regression(4068/4999): loss=0.5410293802551424\n",
      "Log Regression(4069/4999): loss=0.5414514130437127\n",
      "Log Regression(4070/4999): loss=0.5406621379601778\n",
      "Log Regression(4071/4999): loss=0.5405749513664351\n",
      "Log Regression(4072/4999): loss=0.5403980355000405\n",
      "Log Regression(4073/4999): loss=0.5436264927647824\n",
      "Log Regression(4074/4999): loss=0.5405767785008863\n",
      "Log Regression(4075/4999): loss=0.5412184615395589\n",
      "Log Regression(4076/4999): loss=0.5425256112659929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(4077/4999): loss=0.5436047397649634\n",
      "Log Regression(4078/4999): loss=0.5440063052743882\n",
      "Log Regression(4079/4999): loss=0.5417719183955774\n",
      "Log Regression(4080/4999): loss=0.5422943208237584\n",
      "Log Regression(4081/4999): loss=0.54038954200668\n",
      "Log Regression(4082/4999): loss=0.5402263980934636\n",
      "Log Regression(4083/4999): loss=0.5404925097746157\n",
      "Log Regression(4084/4999): loss=0.540381135857016\n",
      "Log Regression(4085/4999): loss=0.5401466540256984\n",
      "Log Regression(4086/4999): loss=0.5408755421637306\n",
      "Log Regression(4087/4999): loss=0.543691845133258\n",
      "Log Regression(4088/4999): loss=0.5431286895231132\n",
      "Log Regression(4089/4999): loss=0.5403083751068783\n",
      "Log Regression(4090/4999): loss=0.5412275571718198\n",
      "Log Regression(4091/4999): loss=0.5405839627812927\n",
      "Log Regression(4092/4999): loss=0.5438672819172329\n",
      "Log Regression(4093/4999): loss=0.5401466014968441\n",
      "Log Regression(4094/4999): loss=0.5418458521488698\n",
      "Log Regression(4095/4999): loss=0.5404051057938799\n",
      "Log Regression(4096/4999): loss=0.5416888013397052\n",
      "Log Regression(4097/4999): loss=0.5401409567528042\n",
      "Log Regression(4098/4999): loss=0.5402067008714396\n",
      "Log Regression(4099/4999): loss=0.5441906463542908\n",
      "Log Regression(4100/4999): loss=0.5422848901684634\n",
      "Log Regression(4101/4999): loss=0.5419715697113796\n",
      "Log Regression(4102/4999): loss=0.5401811404549292\n",
      "Log Regression(4103/4999): loss=0.5410622148732676\n",
      "Log Regression(4104/4999): loss=0.5404862415115221\n",
      "Log Regression(4105/4999): loss=0.5413888406932387\n",
      "Log Regression(4106/4999): loss=0.5410663897882664\n",
      "Log Regression(4107/4999): loss=0.540236893459424\n",
      "Log Regression(4108/4999): loss=0.5411766766304102\n",
      "Log Regression(4109/4999): loss=0.5403169601333548\n",
      "Log Regression(4110/4999): loss=0.5405444327691032\n",
      "Log Regression(4111/4999): loss=0.5424327613897222\n",
      "Log Regression(4112/4999): loss=0.5406532670279263\n",
      "Log Regression(4113/4999): loss=0.5478579105010007\n",
      "Log Regression(4114/4999): loss=0.54299960155342\n",
      "Log Regression(4115/4999): loss=0.5402672217529582\n",
      "Log Regression(4116/4999): loss=0.5419084842587779\n",
      "Log Regression(4117/4999): loss=0.5433938749928042\n",
      "Log Regression(4118/4999): loss=0.5439293167863651\n",
      "Log Regression(4119/4999): loss=0.5407587799263336\n",
      "Log Regression(4120/4999): loss=0.5444277683149004\n",
      "Log Regression(4121/4999): loss=0.540190632576638\n",
      "Log Regression(4122/4999): loss=0.5403007399957714\n",
      "Log Regression(4123/4999): loss=0.5405005853599685\n",
      "Log Regression(4124/4999): loss=0.5404467989890369\n",
      "Log Regression(4125/4999): loss=0.5404910154027884\n",
      "Log Regression(4126/4999): loss=0.5427020412752834\n",
      "Log Regression(4127/4999): loss=0.5402102357457806\n",
      "Log Regression(4128/4999): loss=0.5401735395235565\n",
      "Log Regression(4129/4999): loss=0.5451896822389433\n",
      "Log Regression(4130/4999): loss=0.5477201511334798\n",
      "Log Regression(4131/4999): loss=0.5452033792304839\n",
      "Log Regression(4132/4999): loss=0.5489824667660556\n",
      "Log Regression(4133/4999): loss=0.5488743326760096\n",
      "Log Regression(4134/4999): loss=0.5452161653412855\n",
      "Log Regression(4135/4999): loss=0.5404833731917635\n",
      "Log Regression(4136/4999): loss=0.5401007053588762\n",
      "Log Regression(4137/4999): loss=0.5410448815360824\n",
      "Log Regression(4138/4999): loss=0.5444464347795418\n",
      "Log Regression(4139/4999): loss=0.5480629210500315\n",
      "Log Regression(4140/4999): loss=0.5436318712387742\n",
      "Log Regression(4141/4999): loss=0.5406497132027424\n",
      "Log Regression(4142/4999): loss=0.5407317585929828\n",
      "Log Regression(4143/4999): loss=0.5405158296056609\n",
      "Log Regression(4144/4999): loss=0.5426762319198063\n",
      "Log Regression(4145/4999): loss=0.5409485450269415\n",
      "Log Regression(4146/4999): loss=0.5412122043652166\n",
      "Log Regression(4147/4999): loss=0.5405674514999786\n",
      "Log Regression(4148/4999): loss=0.5408298849032871\n",
      "Log Regression(4149/4999): loss=0.5412379897971228\n",
      "Log Regression(4150/4999): loss=0.5406854136945245\n",
      "Log Regression(4151/4999): loss=0.5411780161495511\n",
      "Log Regression(4152/4999): loss=0.540780242562666\n",
      "Log Regression(4153/4999): loss=0.5405310330020403\n",
      "Log Regression(4154/4999): loss=0.5407401347373256\n",
      "Log Regression(4155/4999): loss=0.5420360198979732\n",
      "Log Regression(4156/4999): loss=0.5404549518621604\n",
      "Log Regression(4157/4999): loss=0.5404070422484082\n",
      "Log Regression(4158/4999): loss=0.5411423925361337\n",
      "Log Regression(4159/4999): loss=0.5406626005756755\n",
      "Log Regression(4160/4999): loss=0.5408046343716983\n",
      "Log Regression(4161/4999): loss=0.5405648885269696\n",
      "Log Regression(4162/4999): loss=0.5404303021152197\n",
      "Log Regression(4163/4999): loss=0.5412379959069917\n",
      "Log Regression(4164/4999): loss=0.5455067517602251\n",
      "Log Regression(4165/4999): loss=0.5459202594850957\n",
      "Log Regression(4166/4999): loss=0.5407971195862683\n",
      "Log Regression(4167/4999): loss=0.5403494070055422\n",
      "Log Regression(4168/4999): loss=0.5464975727349108\n",
      "Log Regression(4169/4999): loss=0.5453093618163352\n",
      "Log Regression(4170/4999): loss=0.5430115438301564\n",
      "Log Regression(4171/4999): loss=0.5432287645977354\n",
      "Log Regression(4172/4999): loss=0.5415694918206095\n",
      "Log Regression(4173/4999): loss=0.5402201708170211\n",
      "Log Regression(4174/4999): loss=0.5407316611451315\n",
      "Log Regression(4175/4999): loss=0.540433868094658\n",
      "Log Regression(4176/4999): loss=0.5401256125753612\n",
      "Log Regression(4177/4999): loss=0.5404907854601307\n",
      "Log Regression(4178/4999): loss=0.5405013797695623\n",
      "Log Regression(4179/4999): loss=0.5406485452721878\n",
      "Log Regression(4180/4999): loss=0.5400525087741255\n",
      "Log Regression(4181/4999): loss=0.5400443044377086\n",
      "Log Regression(4182/4999): loss=0.5402228916102508\n",
      "Log Regression(4183/4999): loss=0.5437893191493444\n",
      "Log Regression(4184/4999): loss=0.5428037962473514\n",
      "Log Regression(4185/4999): loss=0.5402460675298806\n",
      "Log Regression(4186/4999): loss=0.542258833886109\n",
      "Log Regression(4187/4999): loss=0.5403667716833666\n",
      "Log Regression(4188/4999): loss=0.5432033281748968\n",
      "Log Regression(4189/4999): loss=0.5414306423014176\n",
      "Log Regression(4190/4999): loss=0.5440618607673974\n",
      "Log Regression(4191/4999): loss=0.5407926714035078\n",
      "Log Regression(4192/4999): loss=0.5408010858252664\n",
      "Log Regression(4193/4999): loss=0.5403019515016058\n",
      "Log Regression(4194/4999): loss=0.5407180987528114\n",
      "Log Regression(4195/4999): loss=0.5408271844435663\n",
      "Log Regression(4196/4999): loss=0.5408730365399718\n",
      "Log Regression(4197/4999): loss=0.5451400270297703\n",
      "Log Regression(4198/4999): loss=0.5429936387035573\n",
      "Log Regression(4199/4999): loss=0.5411072947793142\n",
      "Log Regression(4200/4999): loss=0.5412275072894849\n",
      "Log Regression(4201/4999): loss=0.5416312960880386\n",
      "Log Regression(4202/4999): loss=0.54135804826364\n",
      "Log Regression(4203/4999): loss=0.5416660131313105\n",
      "Log Regression(4204/4999): loss=0.5413962331194095\n",
      "Log Regression(4205/4999): loss=0.5411179103070688\n",
      "Log Regression(4206/4999): loss=0.5415796592945737\n",
      "Log Regression(4207/4999): loss=0.5414542468694312\n",
      "Log Regression(4208/4999): loss=0.5418369092207822\n",
      "Log Regression(4209/4999): loss=0.540845387240878\n",
      "Log Regression(4210/4999): loss=0.5411073000419695\n",
      "Log Regression(4211/4999): loss=0.5422759680520365\n",
      "Log Regression(4212/4999): loss=0.542399384131259\n",
      "Log Regression(4213/4999): loss=0.540682214063587\n",
      "Log Regression(4214/4999): loss=0.5411949446671611\n",
      "Log Regression(4215/4999): loss=0.5421171334210719\n",
      "Log Regression(4216/4999): loss=0.5404317090041927\n",
      "Log Regression(4217/4999): loss=0.5404275912629346\n",
      "Log Regression(4218/4999): loss=0.5408014060685608\n",
      "Log Regression(4219/4999): loss=0.5418192621228624\n",
      "Log Regression(4220/4999): loss=0.5411186365769444\n",
      "Log Regression(4221/4999): loss=0.5407116055690984\n",
      "Log Regression(4222/4999): loss=0.541766391861212\n",
      "Log Regression(4223/4999): loss=0.5424286484507365\n",
      "Log Regression(4224/4999): loss=0.5405426691178742\n",
      "Log Regression(4225/4999): loss=0.5425300584853155\n",
      "Log Regression(4226/4999): loss=0.5416457042073768\n",
      "Log Regression(4227/4999): loss=0.5445028304803129\n",
      "Log Regression(4228/4999): loss=0.5471842924375612\n",
      "Log Regression(4229/4999): loss=0.5425807702638336\n",
      "Log Regression(4230/4999): loss=0.5446215428505565\n",
      "Log Regression(4231/4999): loss=0.5420045455335383\n",
      "Log Regression(4232/4999): loss=0.5422859959958448\n",
      "Log Regression(4233/4999): loss=0.5423807198745776\n",
      "Log Regression(4234/4999): loss=0.5414527763851408\n",
      "Log Regression(4235/4999): loss=0.5444773117310748\n",
      "Log Regression(4236/4999): loss=0.5481021389232217\n",
      "Log Regression(4237/4999): loss=0.5451384388065315\n",
      "Log Regression(4238/4999): loss=0.5437887284167837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(4239/4999): loss=0.5432633105496097\n",
      "Log Regression(4240/4999): loss=0.5411663317290543\n",
      "Log Regression(4241/4999): loss=0.5402656710162611\n",
      "Log Regression(4242/4999): loss=0.5403694469669886\n",
      "Log Regression(4243/4999): loss=0.5401963860286627\n",
      "Log Regression(4244/4999): loss=0.5403161765040443\n",
      "Log Regression(4245/4999): loss=0.540564806361201\n",
      "Log Regression(4246/4999): loss=0.5472805585940311\n",
      "Log Regression(4247/4999): loss=0.5409449880624291\n",
      "Log Regression(4248/4999): loss=0.5404476172705778\n",
      "Log Regression(4249/4999): loss=0.5424110628620782\n",
      "Log Regression(4250/4999): loss=0.5432343789380197\n",
      "Log Regression(4251/4999): loss=0.541915735282912\n",
      "Log Regression(4252/4999): loss=0.5466250845128692\n",
      "Log Regression(4253/4999): loss=0.546240706787495\n",
      "Log Regression(4254/4999): loss=0.5437338861117359\n",
      "Log Regression(4255/4999): loss=0.5405178915093877\n",
      "Log Regression(4256/4999): loss=0.5402395585316417\n",
      "Log Regression(4257/4999): loss=0.5408505774592526\n",
      "Log Regression(4258/4999): loss=0.5407181515718361\n",
      "Log Regression(4259/4999): loss=0.5402364574053132\n",
      "Log Regression(4260/4999): loss=0.5428595350204483\n",
      "Log Regression(4261/4999): loss=0.5412162781837443\n",
      "Log Regression(4262/4999): loss=0.540092456458971\n",
      "Log Regression(4263/4999): loss=0.5403645303560508\n",
      "Log Regression(4264/4999): loss=0.5408779416856029\n",
      "Log Regression(4265/4999): loss=0.5406735641776945\n",
      "Log Regression(4266/4999): loss=0.5415098134233183\n",
      "Log Regression(4267/4999): loss=0.5407564114856449\n",
      "Log Regression(4268/4999): loss=0.5401594362671832\n",
      "Log Regression(4269/4999): loss=0.5404888106093411\n",
      "Log Regression(4270/4999): loss=0.5411445287706224\n",
      "Log Regression(4271/4999): loss=0.5414778464283235\n",
      "Log Regression(4272/4999): loss=0.5406870668471522\n",
      "Log Regression(4273/4999): loss=0.5408869896150467\n",
      "Log Regression(4274/4999): loss=0.5440739866421194\n",
      "Log Regression(4275/4999): loss=0.5409723820440653\n",
      "Log Regression(4276/4999): loss=0.5411442269091501\n",
      "Log Regression(4277/4999): loss=0.5407338744926712\n",
      "Log Regression(4278/4999): loss=0.5417273629265524\n",
      "Log Regression(4279/4999): loss=0.5421335579253641\n",
      "Log Regression(4280/4999): loss=0.543043024364245\n",
      "Log Regression(4281/4999): loss=0.540738312297688\n",
      "Log Regression(4282/4999): loss=0.5406365970418018\n",
      "Log Regression(4283/4999): loss=0.5417205868618421\n",
      "Log Regression(4284/4999): loss=0.5432208386039828\n",
      "Log Regression(4285/4999): loss=0.5419441663031533\n",
      "Log Regression(4286/4999): loss=0.5438743587193215\n",
      "Log Regression(4287/4999): loss=0.5516220786034819\n",
      "Log Regression(4288/4999): loss=0.5438096427078961\n",
      "Log Regression(4289/4999): loss=0.543715351952375\n",
      "Log Regression(4290/4999): loss=0.5439963070342456\n",
      "Log Regression(4291/4999): loss=0.5407302804577796\n",
      "Log Regression(4292/4999): loss=0.5474142721307799\n",
      "Log Regression(4293/4999): loss=0.5419548907543904\n",
      "Log Regression(4294/4999): loss=0.5408292609845662\n",
      "Log Regression(4295/4999): loss=0.5408160095460616\n",
      "Log Regression(4296/4999): loss=0.5405941981904624\n",
      "Log Regression(4297/4999): loss=0.5409381860656627\n",
      "Log Regression(4298/4999): loss=0.5408872431107777\n",
      "Log Regression(4299/4999): loss=0.5412149793078508\n",
      "Log Regression(4300/4999): loss=0.5413135292229273\n",
      "Log Regression(4301/4999): loss=0.5421468546511151\n",
      "Log Regression(4302/4999): loss=0.5417568271002315\n",
      "Log Regression(4303/4999): loss=0.5408749702264618\n",
      "Log Regression(4304/4999): loss=0.5408822978137166\n",
      "Log Regression(4305/4999): loss=0.5451765077265258\n",
      "Log Regression(4306/4999): loss=0.5443746252374997\n",
      "Log Regression(4307/4999): loss=0.5448488449180772\n",
      "Log Regression(4308/4999): loss=0.5467482347521438\n",
      "Log Regression(4309/4999): loss=0.541530166723777\n",
      "Log Regression(4310/4999): loss=0.5415424477623908\n",
      "Log Regression(4311/4999): loss=0.5417912405929894\n",
      "Log Regression(4312/4999): loss=0.5422862817504783\n",
      "Log Regression(4313/4999): loss=0.5483280022384325\n",
      "Log Regression(4314/4999): loss=0.5422364850104797\n",
      "Log Regression(4315/4999): loss=0.5412587107926448\n",
      "Log Regression(4316/4999): loss=0.5410701048004277\n",
      "Log Regression(4317/4999): loss=0.5414277711150257\n",
      "Log Regression(4318/4999): loss=0.5425036516177365\n",
      "Log Regression(4319/4999): loss=0.5423371078001377\n",
      "Log Regression(4320/4999): loss=0.540846135132784\n",
      "Log Regression(4321/4999): loss=0.5406520103884493\n",
      "Log Regression(4322/4999): loss=0.5406294949096176\n",
      "Log Regression(4323/4999): loss=0.5408117430182076\n",
      "Log Regression(4324/4999): loss=0.5415716938112827\n",
      "Log Regression(4325/4999): loss=0.5412297156973711\n",
      "Log Regression(4326/4999): loss=0.5407699893380727\n",
      "Log Regression(4327/4999): loss=0.5410463495794673\n",
      "Log Regression(4328/4999): loss=0.5405084376656717\n",
      "Log Regression(4329/4999): loss=0.5404626687038439\n",
      "Log Regression(4330/4999): loss=0.5401309118896074\n",
      "Log Regression(4331/4999): loss=0.540080552685974\n",
      "Log Regression(4332/4999): loss=0.5405070483955987\n",
      "Log Regression(4333/4999): loss=0.540575125629489\n",
      "Log Regression(4334/4999): loss=0.5403810467873819\n",
      "Log Regression(4335/4999): loss=0.5400915291820848\n",
      "Log Regression(4336/4999): loss=0.5412802548045793\n",
      "Log Regression(4337/4999): loss=0.5417427815342315\n",
      "Log Regression(4338/4999): loss=0.5400731082971837\n",
      "Log Regression(4339/4999): loss=0.541579026393278\n",
      "Log Regression(4340/4999): loss=0.5401074805587285\n",
      "Log Regression(4341/4999): loss=0.5400354118958487\n",
      "Log Regression(4342/4999): loss=0.5460559316203953\n",
      "Log Regression(4343/4999): loss=0.5444074451303074\n",
      "Log Regression(4344/4999): loss=0.5405633955863611\n",
      "Log Regression(4345/4999): loss=0.5402168337685906\n",
      "Log Regression(4346/4999): loss=0.540907995358655\n",
      "Log Regression(4347/4999): loss=0.5445444578142224\n",
      "Log Regression(4348/4999): loss=0.5433973813680473\n",
      "Log Regression(4349/4999): loss=0.5517596785720824\n",
      "Log Regression(4350/4999): loss=0.5454723851017097\n",
      "Log Regression(4351/4999): loss=0.5401965981596053\n",
      "Log Regression(4352/4999): loss=0.5410836368863333\n",
      "Log Regression(4353/4999): loss=0.5450472844655331\n",
      "Log Regression(4354/4999): loss=0.5439119119675291\n",
      "Log Regression(4355/4999): loss=0.543160018101626\n",
      "Log Regression(4356/4999): loss=0.5420221341066159\n",
      "Log Regression(4357/4999): loss=0.5403161534471118\n",
      "Log Regression(4358/4999): loss=0.5404601516003292\n",
      "Log Regression(4359/4999): loss=0.5403700577825792\n",
      "Log Regression(4360/4999): loss=0.5422865619225158\n",
      "Log Regression(4361/4999): loss=0.5442002513844763\n",
      "Log Regression(4362/4999): loss=0.5461303313854698\n",
      "Log Regression(4363/4999): loss=0.540149320695173\n",
      "Log Regression(4364/4999): loss=0.5402657764556272\n",
      "Log Regression(4365/4999): loss=0.5401508540452895\n",
      "Log Regression(4366/4999): loss=0.5406194510095363\n",
      "Log Regression(4367/4999): loss=0.5409478174135176\n",
      "Log Regression(4368/4999): loss=0.5405402793345072\n",
      "Log Regression(4369/4999): loss=0.5400222885513773\n",
      "Log Regression(4370/4999): loss=0.5418378698831696\n",
      "Log Regression(4371/4999): loss=0.540128450987439\n",
      "Log Regression(4372/4999): loss=0.5406502977727632\n",
      "Log Regression(4373/4999): loss=0.5404301290314045\n",
      "Log Regression(4374/4999): loss=0.5403605954191344\n",
      "Log Regression(4375/4999): loss=0.541163915741966\n",
      "Log Regression(4376/4999): loss=0.5416859177412673\n",
      "Log Regression(4377/4999): loss=0.5412338364898812\n",
      "Log Regression(4378/4999): loss=0.5419737628102629\n",
      "Log Regression(4379/4999): loss=0.5413783777191853\n",
      "Log Regression(4380/4999): loss=0.5422432869165918\n",
      "Log Regression(4381/4999): loss=0.5421495796594503\n",
      "Log Regression(4382/4999): loss=0.541782679299067\n",
      "Log Regression(4383/4999): loss=0.543542094663863\n",
      "Log Regression(4384/4999): loss=0.5425781268247141\n",
      "Log Regression(4385/4999): loss=0.5444952070214407\n",
      "Log Regression(4386/4999): loss=0.5415547309590567\n",
      "Log Regression(4387/4999): loss=0.5421100323445494\n",
      "Log Regression(4388/4999): loss=0.5404154710991236\n",
      "Log Regression(4389/4999): loss=0.5404937047470275\n",
      "Log Regression(4390/4999): loss=0.5414867254146639\n",
      "Log Regression(4391/4999): loss=0.5420345505122248\n",
      "Log Regression(4392/4999): loss=0.5445221813620247\n",
      "Log Regression(4393/4999): loss=0.5419888847082016\n",
      "Log Regression(4394/4999): loss=0.5454949244005018\n",
      "Log Regression(4395/4999): loss=0.5461194779311515\n",
      "Log Regression(4396/4999): loss=0.5418667473113584\n",
      "Log Regression(4397/4999): loss=0.5400885246179487\n",
      "Log Regression(4398/4999): loss=0.542089616544686\n",
      "Log Regression(4399/4999): loss=0.5403327000268053\n",
      "Log Regression(4400/4999): loss=0.5424469261495299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(4401/4999): loss=0.5435149366527687\n",
      "Log Regression(4402/4999): loss=0.5411132035897978\n",
      "Log Regression(4403/4999): loss=0.5400187352534896\n",
      "Log Regression(4404/4999): loss=0.5402848061882752\n",
      "Log Regression(4405/4999): loss=0.5408038509782724\n",
      "Log Regression(4406/4999): loss=0.5404226879251399\n",
      "Log Regression(4407/4999): loss=0.5470928898165939\n",
      "Log Regression(4408/4999): loss=0.552989055908269\n",
      "Log Regression(4409/4999): loss=0.5411113899209191\n",
      "Log Regression(4410/4999): loss=0.5440556739145249\n",
      "Log Regression(4411/4999): loss=0.5466483054176965\n",
      "Log Regression(4412/4999): loss=0.5446476615443084\n",
      "Log Regression(4413/4999): loss=0.5530484530185766\n",
      "Log Regression(4414/4999): loss=0.5511393964978037\n",
      "Log Regression(4415/4999): loss=0.5422491924422568\n",
      "Log Regression(4416/4999): loss=0.5400574622932551\n",
      "Log Regression(4417/4999): loss=0.5406830544589298\n",
      "Log Regression(4418/4999): loss=0.5426852232374301\n",
      "Log Regression(4419/4999): loss=0.5417247444340305\n",
      "Log Regression(4420/4999): loss=0.5403496524719927\n",
      "Log Regression(4421/4999): loss=0.5411709310717929\n",
      "Log Regression(4422/4999): loss=0.5405800787953206\n",
      "Log Regression(4423/4999): loss=0.5406239145425712\n",
      "Log Regression(4424/4999): loss=0.5419393168384649\n",
      "Log Regression(4425/4999): loss=0.5436001837684773\n",
      "Log Regression(4426/4999): loss=0.5420004684855751\n",
      "Log Regression(4427/4999): loss=0.541713060694194\n",
      "Log Regression(4428/4999): loss=0.543534821829603\n",
      "Log Regression(4429/4999): loss=0.5411419482517277\n",
      "Log Regression(4430/4999): loss=0.5402919432548474\n",
      "Log Regression(4431/4999): loss=0.5401948086349055\n",
      "Log Regression(4432/4999): loss=0.540356275069066\n",
      "Log Regression(4433/4999): loss=0.5402782305305963\n",
      "Log Regression(4434/4999): loss=0.5402164609543179\n",
      "Log Regression(4435/4999): loss=0.5400592294646779\n",
      "Log Regression(4436/4999): loss=0.5408006815599553\n",
      "Log Regression(4437/4999): loss=0.5402696027099192\n",
      "Log Regression(4438/4999): loss=0.5401723633507527\n",
      "Log Regression(4439/4999): loss=0.5411813565914815\n",
      "Log Regression(4440/4999): loss=0.5401156259554708\n",
      "Log Regression(4441/4999): loss=0.5406551584677932\n",
      "Log Regression(4442/4999): loss=0.5413011204841981\n",
      "Log Regression(4443/4999): loss=0.5455459635218805\n",
      "Log Regression(4444/4999): loss=0.5455859793554855\n",
      "Log Regression(4445/4999): loss=0.5464410206680472\n",
      "Log Regression(4446/4999): loss=0.5444943508585421\n",
      "Log Regression(4447/4999): loss=0.5425063370118138\n",
      "Log Regression(4448/4999): loss=0.5428635407971815\n",
      "Log Regression(4449/4999): loss=0.5410384978379261\n",
      "Log Regression(4450/4999): loss=0.5421236532898338\n",
      "Log Regression(4451/4999): loss=0.5414026793927664\n",
      "Log Regression(4452/4999): loss=0.5415659788445317\n",
      "Log Regression(4453/4999): loss=0.5452477953263077\n",
      "Log Regression(4454/4999): loss=0.5486768899441093\n",
      "Log Regression(4455/4999): loss=0.5419746317559194\n",
      "Log Regression(4456/4999): loss=0.5408960949737237\n",
      "Log Regression(4457/4999): loss=0.5406570324962618\n",
      "Log Regression(4458/4999): loss=0.5423054396091221\n",
      "Log Regression(4459/4999): loss=0.5439564877378573\n",
      "Log Regression(4460/4999): loss=0.5440905660412823\n",
      "Log Regression(4461/4999): loss=0.5438878821763016\n",
      "Log Regression(4462/4999): loss=0.5404945972582693\n",
      "Log Regression(4463/4999): loss=0.5408383841220229\n",
      "Log Regression(4464/4999): loss=0.5406791457600375\n",
      "Log Regression(4465/4999): loss=0.5407623546264503\n",
      "Log Regression(4466/4999): loss=0.5414011195935935\n",
      "Log Regression(4467/4999): loss=0.5412691361963357\n",
      "Log Regression(4468/4999): loss=0.5419207464555518\n",
      "Log Regression(4469/4999): loss=0.5445743804026477\n",
      "Log Regression(4470/4999): loss=0.5421944020173468\n",
      "Log Regression(4471/4999): loss=0.5411976084369561\n",
      "Log Regression(4472/4999): loss=0.5439235542795445\n",
      "Log Regression(4473/4999): loss=0.5404305441470035\n",
      "Log Regression(4474/4999): loss=0.5406173447728921\n",
      "Log Regression(4475/4999): loss=0.5407242028870101\n",
      "Log Regression(4476/4999): loss=0.5401101595990819\n",
      "Log Regression(4477/4999): loss=0.5408496448399361\n",
      "Log Regression(4478/4999): loss=0.5401966985598797\n",
      "Log Regression(4479/4999): loss=0.5406446538221531\n",
      "Log Regression(4480/4999): loss=0.5423362965042826\n",
      "Log Regression(4481/4999): loss=0.5401411379222372\n",
      "Log Regression(4482/4999): loss=0.5401977982233863\n",
      "Log Regression(4483/4999): loss=0.5403119095071675\n",
      "Log Regression(4484/4999): loss=0.5412806730546343\n",
      "Log Regression(4485/4999): loss=0.54006169307271\n",
      "Log Regression(4486/4999): loss=0.5401493152832365\n",
      "Log Regression(4487/4999): loss=0.5401848540794179\n",
      "Log Regression(4488/4999): loss=0.5400411584545854\n",
      "Log Regression(4489/4999): loss=0.5400292773569856\n",
      "Log Regression(4490/4999): loss=0.5408011639307595\n",
      "Log Regression(4491/4999): loss=0.540153330716858\n",
      "Log Regression(4492/4999): loss=0.5400334551541185\n",
      "Log Regression(4493/4999): loss=0.5405817249966145\n",
      "Log Regression(4494/4999): loss=0.5407747709639871\n",
      "Log Regression(4495/4999): loss=0.5403665223441332\n",
      "Log Regression(4496/4999): loss=0.5400699468254122\n",
      "Log Regression(4497/4999): loss=0.5403788373259895\n",
      "Log Regression(4498/4999): loss=0.5401951679292611\n",
      "Log Regression(4499/4999): loss=0.5405084007298148\n",
      "Log Regression(4500/4999): loss=0.5412323494231196\n",
      "Log Regression(4501/4999): loss=0.5400499658919423\n",
      "Log Regression(4502/4999): loss=0.5401344133155762\n",
      "Log Regression(4503/4999): loss=0.5407390514098248\n",
      "Log Regression(4504/4999): loss=0.5419494773273771\n",
      "Log Regression(4505/4999): loss=0.5432231362787443\n",
      "Log Regression(4506/4999): loss=0.5413801432150291\n",
      "Log Regression(4507/4999): loss=0.5423511023961191\n",
      "Log Regression(4508/4999): loss=0.5454302068203442\n",
      "Log Regression(4509/4999): loss=0.5495156715112154\n",
      "Log Regression(4510/4999): loss=0.5506277522849429\n",
      "Log Regression(4511/4999): loss=0.5484210312126775\n",
      "Log Regression(4512/4999): loss=0.5406358227815053\n",
      "Log Regression(4513/4999): loss=0.5406203983392195\n",
      "Log Regression(4514/4999): loss=0.5403678697297932\n",
      "Log Regression(4515/4999): loss=0.5406085994714966\n",
      "Log Regression(4516/4999): loss=0.5403962586363271\n",
      "Log Regression(4517/4999): loss=0.5409320924487669\n",
      "Log Regression(4518/4999): loss=0.5412186719348304\n",
      "Log Regression(4519/4999): loss=0.5420206643743295\n",
      "Log Regression(4520/4999): loss=0.5400361211127389\n",
      "Log Regression(4521/4999): loss=0.5403569539285532\n",
      "Log Regression(4522/4999): loss=0.5401782562312483\n",
      "Log Regression(4523/4999): loss=0.5406232316871709\n",
      "Log Regression(4524/4999): loss=0.5400835946765732\n",
      "Log Regression(4525/4999): loss=0.541515836230833\n",
      "Log Regression(4526/4999): loss=0.5435845402572099\n",
      "Log Regression(4527/4999): loss=0.5405590218062999\n",
      "Log Regression(4528/4999): loss=0.5401191451214515\n",
      "Log Regression(4529/4999): loss=0.5413509755355447\n",
      "Log Regression(4530/4999): loss=0.5412307069678604\n",
      "Log Regression(4531/4999): loss=0.544236029644735\n",
      "Log Regression(4532/4999): loss=0.5472701596927726\n",
      "Log Regression(4533/4999): loss=0.5448784266389682\n",
      "Log Regression(4534/4999): loss=0.5449834742820834\n",
      "Log Regression(4535/4999): loss=0.5402734861026908\n",
      "Log Regression(4536/4999): loss=0.5418765541479106\n",
      "Log Regression(4537/4999): loss=0.5417542218065192\n",
      "Log Regression(4538/4999): loss=0.541071679428647\n",
      "Log Regression(4539/4999): loss=0.5426908668518288\n",
      "Log Regression(4540/4999): loss=0.5404313994514134\n",
      "Log Regression(4541/4999): loss=0.5407693829894255\n",
      "Log Regression(4542/4999): loss=0.5434798428062849\n",
      "Log Regression(4543/4999): loss=0.5404618341105619\n",
      "Log Regression(4544/4999): loss=0.5401409710277384\n",
      "Log Regression(4545/4999): loss=0.5406418290174355\n",
      "Log Regression(4546/4999): loss=0.5420730224408634\n",
      "Log Regression(4547/4999): loss=0.5430298822044375\n",
      "Log Regression(4548/4999): loss=0.5413557865288777\n",
      "Log Regression(4549/4999): loss=0.5415637264717735\n",
      "Log Regression(4550/4999): loss=0.5401898106661959\n",
      "Log Regression(4551/4999): loss=0.5400405953986699\n",
      "Log Regression(4552/4999): loss=0.5413289502130781\n",
      "Log Regression(4553/4999): loss=0.5428982189842178\n",
      "Log Regression(4554/4999): loss=0.5411795970792913\n",
      "Log Regression(4555/4999): loss=0.5408204050137472\n",
      "Log Regression(4556/4999): loss=0.5404002919029295\n",
      "Log Regression(4557/4999): loss=0.5401330754910764\n",
      "Log Regression(4558/4999): loss=0.5402792698036482\n",
      "Log Regression(4559/4999): loss=0.5409623182046092\n",
      "Log Regression(4560/4999): loss=0.5433490345477805\n",
      "Log Regression(4561/4999): loss=0.5422863580961534\n",
      "Log Regression(4562/4999): loss=0.5403181453379469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(4563/4999): loss=0.5401155364593095\n",
      "Log Regression(4564/4999): loss=0.5404158436511216\n",
      "Log Regression(4565/4999): loss=0.5401370992786341\n",
      "Log Regression(4566/4999): loss=0.5418051084245501\n",
      "Log Regression(4567/4999): loss=0.5434381889464661\n",
      "Log Regression(4568/4999): loss=0.5415953044796102\n",
      "Log Regression(4569/4999): loss=0.5419540331895129\n",
      "Log Regression(4570/4999): loss=0.5400009747208119\n",
      "Log Regression(4571/4999): loss=0.5403880484313079\n",
      "Log Regression(4572/4999): loss=0.5400517322997024\n",
      "Log Regression(4573/4999): loss=0.5422385505781123\n",
      "Log Regression(4574/4999): loss=0.5420370811747895\n",
      "Log Regression(4575/4999): loss=0.5447992782402019\n",
      "Log Regression(4576/4999): loss=0.5406134526867618\n",
      "Log Regression(4577/4999): loss=0.5401029497843226\n",
      "Log Regression(4578/4999): loss=0.5410229556809029\n",
      "Log Regression(4579/4999): loss=0.5403581372215877\n",
      "Log Regression(4580/4999): loss=0.5409605559812692\n",
      "Log Regression(4581/4999): loss=0.5403084776154552\n",
      "Log Regression(4582/4999): loss=0.5403585353957859\n",
      "Log Regression(4583/4999): loss=0.541184112602735\n",
      "Log Regression(4584/4999): loss=0.5416619609600238\n",
      "Log Regression(4585/4999): loss=0.5411759544881838\n",
      "Log Regression(4586/4999): loss=0.5409476872148606\n",
      "Log Regression(4587/4999): loss=0.5405438201412173\n",
      "Log Regression(4588/4999): loss=0.5414776847926271\n",
      "Log Regression(4589/4999): loss=0.5403806363109815\n",
      "Log Regression(4590/4999): loss=0.5405360639830922\n",
      "Log Regression(4591/4999): loss=0.5410441326639011\n",
      "Log Regression(4592/4999): loss=0.5439194043244692\n",
      "Log Regression(4593/4999): loss=0.54089601068488\n",
      "Log Regression(4594/4999): loss=0.5431674865416755\n",
      "Log Regression(4595/4999): loss=0.5421116551500479\n",
      "Log Regression(4596/4999): loss=0.5407211392771722\n",
      "Log Regression(4597/4999): loss=0.5416436350553067\n",
      "Log Regression(4598/4999): loss=0.5407490269110041\n",
      "Log Regression(4599/4999): loss=0.5485309639346102\n",
      "Log Regression(4600/4999): loss=0.5477700432830964\n",
      "Log Regression(4601/4999): loss=0.5436339574561659\n",
      "Log Regression(4602/4999): loss=0.5461397522806736\n",
      "Log Regression(4603/4999): loss=0.5415795374595448\n",
      "Log Regression(4604/4999): loss=0.5402954607492487\n",
      "Log Regression(4605/4999): loss=0.5402447380280875\n",
      "Log Regression(4606/4999): loss=0.5402452382490563\n",
      "Log Regression(4607/4999): loss=0.543151752442341\n",
      "Log Regression(4608/4999): loss=0.5403768496209874\n",
      "Log Regression(4609/4999): loss=0.5401557617849093\n",
      "Log Regression(4610/4999): loss=0.540224547304816\n",
      "Log Regression(4611/4999): loss=0.5402146397708428\n",
      "Log Regression(4612/4999): loss=0.5401642502010613\n",
      "Log Regression(4613/4999): loss=0.5407694897120926\n",
      "Log Regression(4614/4999): loss=0.5401001849298874\n",
      "Log Regression(4615/4999): loss=0.5402636695267695\n",
      "Log Regression(4616/4999): loss=0.5400694703049468\n",
      "Log Regression(4617/4999): loss=0.5401286979881387\n",
      "Log Regression(4618/4999): loss=0.5412839748998962\n",
      "Log Regression(4619/4999): loss=0.5401797868831208\n",
      "Log Regression(4620/4999): loss=0.5399569316864437\n",
      "Log Regression(4621/4999): loss=0.5400333117515737\n",
      "Log Regression(4622/4999): loss=0.54071402815373\n",
      "Log Regression(4623/4999): loss=0.5403878107481892\n",
      "Log Regression(4624/4999): loss=0.5402510490355744\n",
      "Log Regression(4625/4999): loss=0.5434960087306968\n",
      "Log Regression(4626/4999): loss=0.5402642989071138\n",
      "Log Regression(4627/4999): loss=0.5415580838849516\n",
      "Log Regression(4628/4999): loss=0.5441867626507654\n",
      "Log Regression(4629/4999): loss=0.5400551666317641\n",
      "Log Regression(4630/4999): loss=0.5402074504631276\n",
      "Log Regression(4631/4999): loss=0.5405521551286858\n",
      "Log Regression(4632/4999): loss=0.5401219351563155\n",
      "Log Regression(4633/4999): loss=0.5399859274728469\n",
      "Log Regression(4634/4999): loss=0.5400035736686946\n",
      "Log Regression(4635/4999): loss=0.5400249070111147\n",
      "Log Regression(4636/4999): loss=0.5405893557273352\n",
      "Log Regression(4637/4999): loss=0.5401038946919068\n",
      "Log Regression(4638/4999): loss=0.5417091230054641\n",
      "Log Regression(4639/4999): loss=0.5400486835556821\n",
      "Log Regression(4640/4999): loss=0.5403989244917726\n",
      "Log Regression(4641/4999): loss=0.5426695144098398\n",
      "Log Regression(4642/4999): loss=0.5445652895434406\n",
      "Log Regression(4643/4999): loss=0.5423023862091554\n",
      "Log Regression(4644/4999): loss=0.5415629123492236\n",
      "Log Regression(4645/4999): loss=0.5433393653737203\n",
      "Log Regression(4646/4999): loss=0.5408359904386988\n",
      "Log Regression(4647/4999): loss=0.5406612462395518\n",
      "Log Regression(4648/4999): loss=0.5413238288244823\n",
      "Log Regression(4649/4999): loss=0.54212922802528\n",
      "Log Regression(4650/4999): loss=0.5432571805278065\n",
      "Log Regression(4651/4999): loss=0.5429336415897501\n",
      "Log Regression(4652/4999): loss=0.544171017342475\n",
      "Log Regression(4653/4999): loss=0.5430338042887747\n",
      "Log Regression(4654/4999): loss=0.5412057967459799\n",
      "Log Regression(4655/4999): loss=0.5414536987957469\n",
      "Log Regression(4656/4999): loss=0.5424924867569311\n",
      "Log Regression(4657/4999): loss=0.5471807374950028\n",
      "Log Regression(4658/4999): loss=0.5493526767224084\n",
      "Log Regression(4659/4999): loss=0.5452779292201049\n",
      "Log Regression(4660/4999): loss=0.550801200610961\n",
      "Log Regression(4661/4999): loss=0.5485263385003211\n",
      "Log Regression(4662/4999): loss=0.5433466662927627\n",
      "Log Regression(4663/4999): loss=0.5450898812897974\n",
      "Log Regression(4664/4999): loss=0.5406714550175887\n",
      "Log Regression(4665/4999): loss=0.5407686496062201\n",
      "Log Regression(4666/4999): loss=0.5407063542991659\n",
      "Log Regression(4667/4999): loss=0.5410420812552006\n",
      "Log Regression(4668/4999): loss=0.5413043116494872\n",
      "Log Regression(4669/4999): loss=0.5423868707703766\n",
      "Log Regression(4670/4999): loss=0.5409350752781873\n",
      "Log Regression(4671/4999): loss=0.5410695509371939\n",
      "Log Regression(4672/4999): loss=0.54348357658398\n",
      "Log Regression(4673/4999): loss=0.5427184440864466\n",
      "Log Regression(4674/4999): loss=0.54640468597827\n",
      "Log Regression(4675/4999): loss=0.5422775978499256\n",
      "Log Regression(4676/4999): loss=0.5425339308787726\n",
      "Log Regression(4677/4999): loss=0.540441028028375\n",
      "Log Regression(4678/4999): loss=0.5405796642841249\n",
      "Log Regression(4679/4999): loss=0.5421089838102121\n",
      "Log Regression(4680/4999): loss=0.5455182984663052\n",
      "Log Regression(4681/4999): loss=0.5402099745336187\n",
      "Log Regression(4682/4999): loss=0.5404570899902009\n",
      "Log Regression(4683/4999): loss=0.5409576066663261\n",
      "Log Regression(4684/4999): loss=0.5415631753931447\n",
      "Log Regression(4685/4999): loss=0.5426755013063206\n",
      "Log Regression(4686/4999): loss=0.5422035785691012\n",
      "Log Regression(4687/4999): loss=0.5413653890548856\n",
      "Log Regression(4688/4999): loss=0.5408436641852757\n",
      "Log Regression(4689/4999): loss=0.5436821784368496\n",
      "Log Regression(4690/4999): loss=0.543555668429305\n",
      "Log Regression(4691/4999): loss=0.5420008606545254\n",
      "Log Regression(4692/4999): loss=0.5409573976336067\n",
      "Log Regression(4693/4999): loss=0.5405092026081881\n",
      "Log Regression(4694/4999): loss=0.5426958716990666\n",
      "Log Regression(4695/4999): loss=0.5404802561927758\n",
      "Log Regression(4696/4999): loss=0.5410442941547072\n",
      "Log Regression(4697/4999): loss=0.5448669614250742\n",
      "Log Regression(4698/4999): loss=0.5427640852226895\n",
      "Log Regression(4699/4999): loss=0.5403862971927675\n",
      "Log Regression(4700/4999): loss=0.5409147431149609\n",
      "Log Regression(4701/4999): loss=0.5402516728659262\n",
      "Log Regression(4702/4999): loss=0.5413468164166906\n",
      "Log Regression(4703/4999): loss=0.5408801223954146\n",
      "Log Regression(4704/4999): loss=0.5412304735091664\n",
      "Log Regression(4705/4999): loss=0.5416600826109098\n",
      "Log Regression(4706/4999): loss=0.5427523472115802\n",
      "Log Regression(4707/4999): loss=0.5408473196590485\n",
      "Log Regression(4708/4999): loss=0.5401252953565456\n",
      "Log Regression(4709/4999): loss=0.5400521306839191\n",
      "Log Regression(4710/4999): loss=0.5402061954923834\n",
      "Log Regression(4711/4999): loss=0.5441222233573956\n",
      "Log Regression(4712/4999): loss=0.5419355350335877\n",
      "Log Regression(4713/4999): loss=0.5417167184476914\n",
      "Log Regression(4714/4999): loss=0.5423861062759009\n",
      "Log Regression(4715/4999): loss=0.5407296666313616\n",
      "Log Regression(4716/4999): loss=0.5406342913496027\n",
      "Log Regression(4717/4999): loss=0.5407821141791803\n",
      "Log Regression(4718/4999): loss=0.5410607140048906\n",
      "Log Regression(4719/4999): loss=0.5404747502426616\n",
      "Log Regression(4720/4999): loss=0.5402517257247763\n",
      "Log Regression(4721/4999): loss=0.5417727351999313\n",
      "Log Regression(4722/4999): loss=0.540187638091618\n",
      "Log Regression(4723/4999): loss=0.5422487230570907\n",
      "Log Regression(4724/4999): loss=0.5407833228306795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(4725/4999): loss=0.5429823624515151\n",
      "Log Regression(4726/4999): loss=0.54393710426243\n",
      "Log Regression(4727/4999): loss=0.5449477680614165\n",
      "Log Regression(4728/4999): loss=0.5402682012594097\n",
      "Log Regression(4729/4999): loss=0.5400283070980063\n",
      "Log Regression(4730/4999): loss=0.5410051074733527\n",
      "Log Regression(4731/4999): loss=0.5403186667105088\n",
      "Log Regression(4732/4999): loss=0.5400251878623503\n",
      "Log Regression(4733/4999): loss=0.5401649323342126\n",
      "Log Regression(4734/4999): loss=0.5407325125193421\n",
      "Log Regression(4735/4999): loss=0.5406477136683345\n",
      "Log Regression(4736/4999): loss=0.5405385672747391\n",
      "Log Regression(4737/4999): loss=0.5402045807536722\n",
      "Log Regression(4738/4999): loss=0.5400974036805627\n",
      "Log Regression(4739/4999): loss=0.5420112698713885\n",
      "Log Regression(4740/4999): loss=0.5408615153305306\n",
      "Log Regression(4741/4999): loss=0.540137208122748\n",
      "Log Regression(4742/4999): loss=0.5403393880841589\n",
      "Log Regression(4743/4999): loss=0.5404710318919508\n",
      "Log Regression(4744/4999): loss=0.5404438277593177\n",
      "Log Regression(4745/4999): loss=0.5404952778711343\n",
      "Log Regression(4746/4999): loss=0.5404632972963618\n",
      "Log Regression(4747/4999): loss=0.5430952689109428\n",
      "Log Regression(4748/4999): loss=0.5431073074033231\n",
      "Log Regression(4749/4999): loss=0.5405641310511798\n",
      "Log Regression(4750/4999): loss=0.5434609242419298\n",
      "Log Regression(4751/4999): loss=0.5439288608648707\n",
      "Log Regression(4752/4999): loss=0.545955589737945\n",
      "Log Regression(4753/4999): loss=0.543705991264202\n",
      "Log Regression(4754/4999): loss=0.5435622285072619\n",
      "Log Regression(4755/4999): loss=0.5411637546122136\n",
      "Log Regression(4756/4999): loss=0.5404267849832928\n",
      "Log Regression(4757/4999): loss=0.5404898676689782\n",
      "Log Regression(4758/4999): loss=0.5414411487146056\n",
      "Log Regression(4759/4999): loss=0.5439167923442094\n",
      "Log Regression(4760/4999): loss=0.5411723671360765\n",
      "Log Regression(4761/4999): loss=0.5407816821355897\n",
      "Log Regression(4762/4999): loss=0.5403562349429148\n",
      "Log Regression(4763/4999): loss=0.5424300796007498\n",
      "Log Regression(4764/4999): loss=0.5401254505876255\n",
      "Log Regression(4765/4999): loss=0.5402317771143845\n",
      "Log Regression(4766/4999): loss=0.5402876377817009\n",
      "Log Regression(4767/4999): loss=0.5409979554177392\n",
      "Log Regression(4768/4999): loss=0.5406134425803257\n",
      "Log Regression(4769/4999): loss=0.5402400459519983\n",
      "Log Regression(4770/4999): loss=0.5401528289210631\n",
      "Log Regression(4771/4999): loss=0.5416507533090458\n",
      "Log Regression(4772/4999): loss=0.5405610848391476\n",
      "Log Regression(4773/4999): loss=0.5404234635767553\n",
      "Log Regression(4774/4999): loss=0.5408952268534085\n",
      "Log Regression(4775/4999): loss=0.5400224661367284\n",
      "Log Regression(4776/4999): loss=0.5434160227891884\n",
      "Log Regression(4777/4999): loss=0.5416190422967758\n",
      "Log Regression(4778/4999): loss=0.541276980847108\n",
      "Log Regression(4779/4999): loss=0.5449185396483729\n",
      "Log Regression(4780/4999): loss=0.5444327997698118\n",
      "Log Regression(4781/4999): loss=0.5425696883135465\n",
      "Log Regression(4782/4999): loss=0.5421652601473798\n",
      "Log Regression(4783/4999): loss=0.5401088613699402\n",
      "Log Regression(4784/4999): loss=0.5403162270485045\n",
      "Log Regression(4785/4999): loss=0.5403965528701958\n",
      "Log Regression(4786/4999): loss=0.5409020868039441\n",
      "Log Regression(4787/4999): loss=0.5416819332043183\n",
      "Log Regression(4788/4999): loss=0.5442597185933487\n",
      "Log Regression(4789/4999): loss=0.5441477310656424\n",
      "Log Regression(4790/4999): loss=0.541199286780557\n",
      "Log Regression(4791/4999): loss=0.5407598789170269\n",
      "Log Regression(4792/4999): loss=0.540702391997083\n",
      "Log Regression(4793/4999): loss=0.5409014071462074\n",
      "Log Regression(4794/4999): loss=0.5402433386038026\n",
      "Log Regression(4795/4999): loss=0.5415588589203263\n",
      "Log Regression(4796/4999): loss=0.5424442484429617\n",
      "Log Regression(4797/4999): loss=0.5444303035909857\n",
      "Log Regression(4798/4999): loss=0.5403967943252929\n",
      "Log Regression(4799/4999): loss=0.5423992536565232\n",
      "Log Regression(4800/4999): loss=0.5438925231824836\n",
      "Log Regression(4801/4999): loss=0.542408097756559\n",
      "Log Regression(4802/4999): loss=0.540720733996301\n",
      "Log Regression(4803/4999): loss=0.5404589460720941\n",
      "Log Regression(4804/4999): loss=0.5427049436054375\n",
      "Log Regression(4805/4999): loss=0.5409225029444322\n",
      "Log Regression(4806/4999): loss=0.5406885383283814\n",
      "Log Regression(4807/4999): loss=0.5427372484968103\n",
      "Log Regression(4808/4999): loss=0.5414407170899768\n",
      "Log Regression(4809/4999): loss=0.5482270545357663\n",
      "Log Regression(4810/4999): loss=0.5419888375827318\n",
      "Log Regression(4811/4999): loss=0.5412195693765683\n",
      "Log Regression(4812/4999): loss=0.5456402808862293\n",
      "Log Regression(4813/4999): loss=0.5432688053017645\n",
      "Log Regression(4814/4999): loss=0.5413190398813404\n",
      "Log Regression(4815/4999): loss=0.5411309537335339\n",
      "Log Regression(4816/4999): loss=0.5413584993066201\n",
      "Log Regression(4817/4999): loss=0.5399934791969216\n",
      "Log Regression(4818/4999): loss=0.5400020373940331\n",
      "Log Regression(4819/4999): loss=0.5445430088951603\n",
      "Log Regression(4820/4999): loss=0.5418276223468734\n",
      "Log Regression(4821/4999): loss=0.5406715915744466\n",
      "Log Regression(4822/4999): loss=0.5399599442984705\n",
      "Log Regression(4823/4999): loss=0.5400969579307979\n",
      "Log Regression(4824/4999): loss=0.5404737577823359\n",
      "Log Regression(4825/4999): loss=0.5442415347882017\n",
      "Log Regression(4826/4999): loss=0.543283085062251\n",
      "Log Regression(4827/4999): loss=0.5408234268578509\n",
      "Log Regression(4828/4999): loss=0.5441849861403717\n",
      "Log Regression(4829/4999): loss=0.5440619813754882\n",
      "Log Regression(4830/4999): loss=0.5403498790730297\n",
      "Log Regression(4831/4999): loss=0.5413282823296008\n",
      "Log Regression(4832/4999): loss=0.5409326137867562\n",
      "Log Regression(4833/4999): loss=0.5405184456642843\n",
      "Log Regression(4834/4999): loss=0.5402847866338923\n",
      "Log Regression(4835/4999): loss=0.5402255926276079\n",
      "Log Regression(4836/4999): loss=0.5419873533310683\n",
      "Log Regression(4837/4999): loss=0.5441460037553861\n",
      "Log Regression(4838/4999): loss=0.5417671080073696\n",
      "Log Regression(4839/4999): loss=0.5405771730240306\n",
      "Log Regression(4840/4999): loss=0.5410191439948279\n",
      "Log Regression(4841/4999): loss=0.541599671730849\n",
      "Log Regression(4842/4999): loss=0.5404489199205331\n",
      "Log Regression(4843/4999): loss=0.540639284843395\n",
      "Log Regression(4844/4999): loss=0.5418398692375304\n",
      "Log Regression(4845/4999): loss=0.5421488020317605\n",
      "Log Regression(4846/4999): loss=0.5406530364119537\n",
      "Log Regression(4847/4999): loss=0.5403021192978886\n",
      "Log Regression(4848/4999): loss=0.5428215966773184\n",
      "Log Regression(4849/4999): loss=0.5417793203572354\n",
      "Log Regression(4850/4999): loss=0.5414362925921885\n",
      "Log Regression(4851/4999): loss=0.545774982806664\n",
      "Log Regression(4852/4999): loss=0.5406632838633774\n",
      "Log Regression(4853/4999): loss=0.5424738949120023\n",
      "Log Regression(4854/4999): loss=0.5426961611410972\n",
      "Log Regression(4855/4999): loss=0.5453378043150281\n",
      "Log Regression(4856/4999): loss=0.5440110017968928\n",
      "Log Regression(4857/4999): loss=0.5434637491500571\n",
      "Log Regression(4858/4999): loss=0.5417622972992515\n",
      "Log Regression(4859/4999): loss=0.5404947399094794\n",
      "Log Regression(4860/4999): loss=0.5402660484149955\n",
      "Log Regression(4861/4999): loss=0.5400900840575534\n",
      "Log Regression(4862/4999): loss=0.5401815047366387\n",
      "Log Regression(4863/4999): loss=0.5401430325800323\n",
      "Log Regression(4864/4999): loss=0.5419468436928898\n",
      "Log Regression(4865/4999): loss=0.5405129998059578\n",
      "Log Regression(4866/4999): loss=0.5407141931480545\n",
      "Log Regression(4867/4999): loss=0.540110325726168\n",
      "Log Regression(4868/4999): loss=0.5405549979928859\n",
      "Log Regression(4869/4999): loss=0.5429745876943957\n",
      "Log Regression(4870/4999): loss=0.5467129853061629\n",
      "Log Regression(4871/4999): loss=0.54118863831432\n",
      "Log Regression(4872/4999): loss=0.541745112576368\n",
      "Log Regression(4873/4999): loss=0.5429156960603772\n",
      "Log Regression(4874/4999): loss=0.540345682622722\n",
      "Log Regression(4875/4999): loss=0.5401775261365529\n",
      "Log Regression(4876/4999): loss=0.5411613244293934\n",
      "Log Regression(4877/4999): loss=0.5404641864498829\n",
      "Log Regression(4878/4999): loss=0.5403273043704299\n",
      "Log Regression(4879/4999): loss=0.5414400404741692\n",
      "Log Regression(4880/4999): loss=0.5400957290754038\n",
      "Log Regression(4881/4999): loss=0.5412465968061382\n",
      "Log Regression(4882/4999): loss=0.540778544017362\n",
      "Log Regression(4883/4999): loss=0.540673111346674\n",
      "Log Regression(4884/4999): loss=0.5431105712128635\n",
      "Log Regression(4885/4999): loss=0.5426013509450424\n",
      "Log Regression(4886/4999): loss=0.5407107281794502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(4887/4999): loss=0.5402863618921366\n",
      "Log Regression(4888/4999): loss=0.543441649674543\n",
      "Log Regression(4889/4999): loss=0.5414767785044818\n",
      "Log Regression(4890/4999): loss=0.5411180907993698\n",
      "Log Regression(4891/4999): loss=0.5402551408374878\n",
      "Log Regression(4892/4999): loss=0.5407124541725742\n",
      "Log Regression(4893/4999): loss=0.5414900652126782\n",
      "Log Regression(4894/4999): loss=0.5436570324355459\n",
      "Log Regression(4895/4999): loss=0.5424281085296577\n",
      "Log Regression(4896/4999): loss=0.5428855704832396\n",
      "Log Regression(4897/4999): loss=0.5414727448341016\n",
      "Log Regression(4898/4999): loss=0.5408970558928979\n",
      "Log Regression(4899/4999): loss=0.5404657294434442\n",
      "Log Regression(4900/4999): loss=0.5404423602719173\n",
      "Log Regression(4901/4999): loss=0.5430837714792239\n",
      "Log Regression(4902/4999): loss=0.5424359772281476\n",
      "Log Regression(4903/4999): loss=0.5438356371110523\n",
      "Log Regression(4904/4999): loss=0.5415968136364168\n",
      "Log Regression(4905/4999): loss=0.5402320373328016\n",
      "Log Regression(4906/4999): loss=0.5404091468048426\n",
      "Log Regression(4907/4999): loss=0.5409691275430509\n",
      "Log Regression(4908/4999): loss=0.5410637112745519\n",
      "Log Regression(4909/4999): loss=0.5435082180719014\n",
      "Log Regression(4910/4999): loss=0.5429895100629821\n",
      "Log Regression(4911/4999): loss=0.5440683207511426\n",
      "Log Regression(4912/4999): loss=0.5421989152382173\n",
      "Log Regression(4913/4999): loss=0.5402453928728912\n",
      "Log Regression(4914/4999): loss=0.5408267252048228\n",
      "Log Regression(4915/4999): loss=0.5402927070162967\n",
      "Log Regression(4916/4999): loss=0.5400085544596365\n",
      "Log Regression(4917/4999): loss=0.540289980866881\n",
      "Log Regression(4918/4999): loss=0.5404934459395194\n",
      "Log Regression(4919/4999): loss=0.5422604693923759\n",
      "Log Regression(4920/4999): loss=0.5411161817222299\n",
      "Log Regression(4921/4999): loss=0.5416800757728498\n",
      "Log Regression(4922/4999): loss=0.5406672572517894\n",
      "Log Regression(4923/4999): loss=0.542741216396301\n",
      "Log Regression(4924/4999): loss=0.545373935092092\n",
      "Log Regression(4925/4999): loss=0.5432992687974239\n",
      "Log Regression(4926/4999): loss=0.5400875617511959\n",
      "Log Regression(4927/4999): loss=0.5412256094337584\n",
      "Log Regression(4928/4999): loss=0.5451119275349635\n",
      "Log Regression(4929/4999): loss=0.5439967668390037\n",
      "Log Regression(4930/4999): loss=0.5426711276530642\n",
      "Log Regression(4931/4999): loss=0.5404632108230935\n",
      "Log Regression(4932/4999): loss=0.5402828880829879\n",
      "Log Regression(4933/4999): loss=0.5408158750997331\n",
      "Log Regression(4934/4999): loss=0.5408104916706498\n",
      "Log Regression(4935/4999): loss=0.5408620813064506\n",
      "Log Regression(4936/4999): loss=0.5402024475507117\n",
      "Log Regression(4937/4999): loss=0.5411158470745452\n",
      "Log Regression(4938/4999): loss=0.5414111451416611\n",
      "Log Regression(4939/4999): loss=0.5429552766626379\n",
      "Log Regression(4940/4999): loss=0.5405973819229423\n",
      "Log Regression(4941/4999): loss=0.5402245288004177\n",
      "Log Regression(4942/4999): loss=0.5403793058987236\n",
      "Log Regression(4943/4999): loss=0.543353111641774\n",
      "Log Regression(4944/4999): loss=0.5461234033234513\n",
      "Log Regression(4945/4999): loss=0.5426523194420861\n",
      "Log Regression(4946/4999): loss=0.5441212465585866\n",
      "Log Regression(4947/4999): loss=0.5425196251542964\n",
      "Log Regression(4948/4999): loss=0.5417521258323315\n",
      "Log Regression(4949/4999): loss=0.5424186931133587\n",
      "Log Regression(4950/4999): loss=0.5399170386473462\n",
      "Log Regression(4951/4999): loss=0.5399148815285325\n",
      "Log Regression(4952/4999): loss=0.5401432360960244\n",
      "Log Regression(4953/4999): loss=0.5399228254674358\n",
      "Log Regression(4954/4999): loss=0.5404890178194925\n",
      "Log Regression(4955/4999): loss=0.5404229924115093\n",
      "Log Regression(4956/4999): loss=0.5402153910509857\n",
      "Log Regression(4957/4999): loss=0.5421634189312675\n",
      "Log Regression(4958/4999): loss=0.5415500070657138\n",
      "Log Regression(4959/4999): loss=0.5413861829105765\n",
      "Log Regression(4960/4999): loss=0.5399161854587803\n",
      "Log Regression(4961/4999): loss=0.5399129215256733\n",
      "Log Regression(4962/4999): loss=0.5399052368027527\n",
      "Log Regression(4963/4999): loss=0.5401678994256283\n",
      "Log Regression(4964/4999): loss=0.5409504050530188\n",
      "Log Regression(4965/4999): loss=0.5409151596204597\n",
      "Log Regression(4966/4999): loss=0.5431625552082547\n",
      "Log Regression(4967/4999): loss=0.5424303132322194\n",
      "Log Regression(4968/4999): loss=0.5402608814169003\n",
      "Log Regression(4969/4999): loss=0.5409005077847477\n",
      "Log Regression(4970/4999): loss=0.5402008227624532\n",
      "Log Regression(4971/4999): loss=0.5421683871395194\n",
      "Log Regression(4972/4999): loss=0.5401886100697906\n",
      "Log Regression(4973/4999): loss=0.5468022966348309\n",
      "Log Regression(4974/4999): loss=0.5407482691230759\n",
      "Log Regression(4975/4999): loss=0.5410897090492214\n",
      "Log Regression(4976/4999): loss=0.5410293534489158\n",
      "Log Regression(4977/4999): loss=0.5408025545148399\n",
      "Log Regression(4978/4999): loss=0.5402090121068576\n",
      "Log Regression(4979/4999): loss=0.5400050371794891\n",
      "Log Regression(4980/4999): loss=0.5401267615546662\n",
      "Log Regression(4981/4999): loss=0.5407435679064396\n",
      "Log Regression(4982/4999): loss=0.5401009897570733\n",
      "Log Regression(4983/4999): loss=0.5399560971448677\n",
      "Log Regression(4984/4999): loss=0.5407795470512716\n",
      "Log Regression(4985/4999): loss=0.5444959599388619\n",
      "Log Regression(4986/4999): loss=0.5402306858149869\n",
      "Log Regression(4987/4999): loss=0.540516200869372\n",
      "Log Regression(4988/4999): loss=0.5405234825928713\n",
      "Log Regression(4989/4999): loss=0.5410451046082938\n",
      "Log Regression(4990/4999): loss=0.540497125973792\n",
      "Log Regression(4991/4999): loss=0.5409155291538336\n",
      "Log Regression(4992/4999): loss=0.5431860435859652\n",
      "Log Regression(4993/4999): loss=0.5453484249895293\n",
      "Log Regression(4994/4999): loss=0.5411504938163632\n",
      "Log Regression(4995/4999): loss=0.5430447844897225\n",
      "Log Regression(4996/4999): loss=0.541633286286059\n",
      "Log Regression(4997/4999): loss=0.5442955737918008\n",
      "Log Regression(4998/4999): loss=0.5459493751622291\n",
      "Log Regression(4999/4999): loss=0.5411215709682157\n"
     ]
    }
   ],
   "source": [
    "w, mse = reg_logistic_regression(y_train_lr, tX_train, lambda_, initial_w, max_iters, gamma, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7297333333333333"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict_labels(w, tX_train)\n",
    "np.mean(y_train.reshape(-1,1)==y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73176"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict_labels(w, tX_val)\n",
    "np.mean(y_val.reshape(-1,1)==y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lambda = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros(tX.shape[1])#np.random.normal(loc=0, scale=1, size=(tX.shape[1],1))\n",
    "max_iters = 5000\n",
    "gamma = 1e-7\n",
    "lambda_ = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training process here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grubdragon/.pyenv/versions/3.7.4/envs/pyML/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict_labels(w, tX_train)\n",
    "np.mean(y_train.reshape(-1,1)==y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5781918208"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict_labels(w, tX_val)\n",
    "np.mean(y_val.reshape(-1,1)==y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lambda = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros(tX.shape[1])#np.random.normal(loc=0, scale=1, size=(tX.shape[1],1))\n",
    "max_iters = 5000\n",
    "gamma = 1e-7\n",
    "lambda_ = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Run the training step here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training process here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.728385"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict_labels(w, tX_train)\n",
    "np.mean(y_train.reshape(-1,1)==y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7314"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict_labels(w, tX_val)\n",
    "np.mean(y_val.reshape(-1,1)==y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After the minimal training, we try to do some extra data processing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can begin with removing the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_processor as dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_train_new, outlier_thresh = dp.remove_outliers(tX_train, conf_int=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the same outlier threshold\n",
    "tX_val_new, _ = dp.remove_outliers(tX_val, outlier_thresh=outlier_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((225000, 30), (25000, 30))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX_train_new.shape, tX_val_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We add polynomial features to help with training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_train_aug, tX_val_aug = dp.poly_features(tX_train_new,3), dp.poly_features(tX_val_new,3)\n",
    "tX_train_aug, tX_val_aug = dp.add_ones(tX_train_aug), dp.add_ones(tX_val_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((225000, 91), (25000, 91))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX_train_aug.shape, tX_val_aug.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge Regression with polynomials, without outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, mse = ridge_regression(y_train, tX_train_aug, lambda_=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1. -1.  1. ...  1. -1. -1.]\n",
      "(225000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6872355555555556"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict_labels(w, tX_train_aug)\n",
    "print(y_pred)\n",
    "print(y_pred.shape)\n",
    "np.mean(y_train==y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1. -1. -1. ... -1.  1. -1.]\n",
      "(25000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.68652"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict_labels(w, tX_val_aug)\n",
    "print(y_pred)\n",
    "print(y_pred.shape)\n",
    "np.mean(y_val==y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly removing outliers isn't doing a better job on the data. So we don't do that yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We try to normalize the data, hoping that effect of certain features doesn't dull others'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *\n",
    "import data_processor as dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_train_new, norm_stats = dp.standardize(tX_train)\n",
    "# We use the same outlier threshold\n",
    "tX_val_new, _ = dp.standardize(tX_val, norm_stats=norm_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((225000, 30), (25000, 30))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX_train_new.shape, tX_val_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We add polynomial features to help with training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_train_aug_poly, tX_val_aug_poly = dp.poly_features(tX_train_new,7), dp.poly_features(tX_val_new,7)\n",
    "tX_train_poly_std, tX_val_poly_std = dp.add_ones(tX_train_aug_poly), dp.add_ones(tX_val_aug_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((225000, 211), (25000, 211))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX_train_poly_std.shape, tX_val_poly_std.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge Regression with polynomials, standardizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda=0\n",
      "0.6854044444444445\n",
      "0.68588\n",
      "Lambda=0.0001\n",
      "0.8037466666666667\n",
      "0.80312\n",
      "Lambda=0.001\n",
      "0.80356\n",
      "0.80348\n",
      "Lambda=0.01\n",
      "0.8036533333333333\n",
      "0.8026\n",
      "Lambda=0.1\n",
      "0.8037288888888889\n",
      "0.80316\n",
      "Lambda=1\n",
      "0.8036888888888889\n",
      "0.80276\n",
      "Lambda=10.0\n",
      "0.8037422222222222\n",
      "0.8036\n",
      "Lambda=100.0\n",
      "0.8035822222222222\n",
      "0.80328\n",
      "Lambda=1000.0\n",
      "0.8033955555555555\n",
      "0.803\n",
      "Lambda=10000.0\n",
      "0.7996488888888889\n",
      "0.80124\n",
      "Lambda=100000.0\n",
      "0.7827822222222223\n",
      "0.78632\n",
      "Lambda=1000000.0\n",
      "0.7507066666666666\n",
      "0.75164\n",
      "Lambda=10000000.0\n",
      "0.7207377777777778\n",
      "0.72232\n"
     ]
    }
   ],
   "source": [
    "ws_ridge_poly_std = []\n",
    "for l in [0, 1e-4, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4, 1e5, 1e6, 1e7]:\n",
    "    w, mse = ridge_regression(y_train, tX_train_poly_std, lambda_=l)\n",
    "    ws_ridge_poly_std.append(w)\n",
    "    #print(w)\n",
    "    y_pred = predict_labels(w, tX_train_poly_std)\n",
    "    #print(y_pred)\n",
    "    #print(y_pred.shape)\n",
    "    print(\"Lambda=\"+str(l))\n",
    "    print(np.mean(y_train==y_pred))\n",
    "    y_pred = predict_labels(w, tX_val_poly_std)\n",
    "    print(np.mean(y_val==y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So some good lambda values are 1e-4 to 1e4, each giving good range of normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can try using logistic regression here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_train_aug_poly, tX_val_aug_poly = dp.poly_features(tX_train_new,6), dp.poly_features(tX_val_new,6)\n",
    "tX_train_aug_poly, stats2 = dp.standardize(tX_train_aug_poly)\n",
    "tX_val_aug_poly, _ = dp.standardize(tX_val_aug_poly, norm_stats=stats2)\n",
    "tX_train_poly_std, tX_val_poly_std = dp.add_ones(tX_train_aug_poly), dp.add_ones(tX_val_aug_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((225000, 181), (25000, 181))"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX_train_poly_std.shape, tX_val_poly_std.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regulaized Logistic Regression with polynomials, standardizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros((tX_train_poly_std.shape[1],1))\n",
    "#initial_w = np.random.normal(loc=0, scale=0.0001, size=(tX_train_poly_std.shape[1],1))\n",
    "max_iters = 100\n",
    "gamma = 1e-4\n",
    "lambda_ = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(225000,)\n"
     ]
    }
   ],
   "source": [
    "y_train_lr = 0.5*(y_train+1)\n",
    "print(y_train_lr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle\n",
    "#with open('weights_naned.pkl', 'rb') as f:\n",
    "#    w = pickle.load(f)\n",
    "w = initial_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(0/99): loss=0.6929989286114777\n",
      "Log Regression(1/99): loss=0.6929064421858071\n",
      "Log Regression(2/99): loss=0.6927519309362435\n",
      "Log Regression(3/99): loss=0.6926013642338953\n",
      "Log Regression(4/99): loss=0.6924914902563086\n",
      "Log Regression(5/99): loss=0.6923338041077298\n",
      "Log Regression(6/99): loss=0.6921853465846246\n",
      "Log Regression(7/99): loss=0.6920446009384293\n",
      "Log Regression(8/99): loss=0.6918608991338409\n",
      "Log Regression(9/99): loss=0.6917635360994608\n",
      "Log Regression(10/99): loss=0.6916487098461154\n",
      "Log Regression(11/99): loss=0.6914984847747875\n",
      "Log Regression(12/99): loss=0.6913960219539407\n",
      "Log Regression(13/99): loss=0.6912357720113087\n",
      "Log Regression(14/99): loss=0.6910737244648242\n",
      "Log Regression(15/99): loss=0.6909221121028009\n",
      "Log Regression(16/99): loss=0.6907712932975185\n",
      "Log Regression(17/99): loss=0.6904722477299763\n",
      "Log Regression(18/99): loss=0.6902870504666768\n",
      "Log Regression(19/99): loss=0.690239766370031\n",
      "Log Regression(20/99): loss=0.690018590722159\n",
      "Log Regression(21/99): loss=0.6898966785518336\n",
      "Log Regression(22/99): loss=0.689748617381022\n",
      "Log Regression(23/99): loss=0.6895213172858704\n",
      "Log Regression(24/99): loss=0.6893956683809872\n",
      "Log Regression(25/99): loss=0.6892818798083876\n",
      "Log Regression(26/99): loss=0.6892331381241248\n",
      "Log Regression(27/99): loss=0.6890599219928136\n",
      "Log Regression(28/99): loss=0.688907215544728\n",
      "Log Regression(29/99): loss=0.6888473430826103\n",
      "Log Regression(30/99): loss=0.6887143808530505\n",
      "Log Regression(31/99): loss=0.6884875209684507\n",
      "Log Regression(32/99): loss=0.6884829528648754\n",
      "Log Regression(33/99): loss=0.6883502764882493\n",
      "Log Regression(34/99): loss=0.6882610733301108\n",
      "Log Regression(35/99): loss=0.6882432944280205\n",
      "Log Regression(36/99): loss=0.688259352841872\n",
      "Log Regression(37/99): loss=0.6881575792470012\n",
      "Log Regression(38/99): loss=0.6880232868698855\n",
      "Log Regression(39/99): loss=0.6881060602979507\n",
      "Log Regression(40/99): loss=0.6879342332814108\n",
      "Log Regression(41/99): loss=0.6875526014798289\n",
      "Log Regression(42/99): loss=0.6874332307634436\n",
      "Log Regression(43/99): loss=0.6873260013654897\n",
      "Log Regression(44/99): loss=0.6870844945288838\n",
      "Log Regression(45/99): loss=0.6866751498856908\n",
      "Log Regression(46/99): loss=0.6865229099568542\n",
      "Log Regression(47/99): loss=0.6864109425574267\n",
      "Log Regression(48/99): loss=0.686284436168118\n",
      "Log Regression(49/99): loss=0.6861410637344912\n",
      "Log Regression(50/99): loss=0.6860695172457912\n",
      "Log Regression(51/99): loss=0.6859755225120628\n",
      "Log Regression(52/99): loss=0.685818076292674\n",
      "Log Regression(53/99): loss=0.68567689889901\n",
      "Log Regression(54/99): loss=0.6855595253298937\n",
      "Log Regression(55/99): loss=0.6854570950164345\n",
      "Log Regression(56/99): loss=0.6852767083864962\n",
      "Log Regression(57/99): loss=0.6850499217557898\n",
      "Log Regression(58/99): loss=0.6849607328234228\n",
      "Log Regression(59/99): loss=0.6847729239106801\n",
      "Log Regression(60/99): loss=0.6847137413693938\n",
      "Log Regression(61/99): loss=0.6845401383441215\n",
      "Log Regression(62/99): loss=0.6844960286256401\n",
      "Log Regression(63/99): loss=0.6844210219696221\n",
      "Log Regression(64/99): loss=0.6841680078512782\n",
      "Log Regression(65/99): loss=0.6840584459664516\n",
      "Log Regression(66/99): loss=0.6839352447666158\n",
      "Log Regression(67/99): loss=0.6837329378188698\n",
      "Log Regression(68/99): loss=0.6834752439071511\n",
      "Log Regression(69/99): loss=0.6832975622422721\n",
      "Log Regression(70/99): loss=0.6832393599665428\n",
      "Log Regression(71/99): loss=0.6831917775651927\n",
      "Log Regression(72/99): loss=0.68312179101591\n",
      "Log Regression(73/99): loss=0.6830787298566522\n",
      "Log Regression(74/99): loss=0.6829389195866435\n",
      "Log Regression(75/99): loss=0.6829390500625487\n",
      "Log Regression(76/99): loss=0.682766491511167\n",
      "Log Regression(77/99): loss=0.6825783131997508\n",
      "Log Regression(78/99): loss=0.682359703862104\n",
      "Log Regression(79/99): loss=0.6822775397369834\n",
      "Log Regression(80/99): loss=0.6821288366169043\n",
      "Log Regression(81/99): loss=0.6819488293592181\n",
      "Log Regression(82/99): loss=0.6818515972702912\n",
      "Log Regression(83/99): loss=0.6816683528304981\n",
      "Log Regression(84/99): loss=0.6815602116192876\n",
      "Log Regression(85/99): loss=0.6815470525741766\n",
      "Log Regression(86/99): loss=0.6814375203714744\n",
      "Log Regression(87/99): loss=0.6813583118601515\n",
      "Log Regression(88/99): loss=0.6812079086988128\n",
      "Log Regression(89/99): loss=0.6809502926791476\n",
      "Log Regression(90/99): loss=0.6808023482250988\n",
      "Log Regression(91/99): loss=0.6807390485191694\n",
      "Log Regression(92/99): loss=0.6806688224596973\n",
      "Log Regression(93/99): loss=0.6806136781544069\n",
      "Log Regression(94/99): loss=0.6804339258434857\n",
      "Log Regression(95/99): loss=0.6803232606728763\n",
      "Log Regression(96/99): loss=0.6801980662438238\n",
      "Log Regression(97/99): loss=0.6801187699321748\n",
      "Log Regression(98/99): loss=0.6800330207129164\n",
      "Log Regression(99/99): loss=0.6798766991955733\n"
     ]
    }
   ],
   "source": [
    "w, mse, loss_array,ws = reg_logistic_regression(y_train_lr, tX_train_poly_std, lambda_, w, max_iters, gamma, batch_size=64, lr_decay=False, lr_decay_rate=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot(losses,ws,tX_val,y_val):\n",
    "    # Plot two plots:\n",
    "    # loss as a function of step\n",
    "    # accuracy as a function of step  (on validation set)\n",
    "\n",
    "    steps = [i for i in range(1,len(losses)+1)]\n",
    "\n",
    "    ax1 = plt.plot(steps,losses)\n",
    "    plt.title('Loss as a function of step')\n",
    "    plt.xlabel('number of steps')\n",
    "    plt.ylabel('loss (approximate)')\n",
    "    plt.show()\n",
    "    #compute accuracy\n",
    "    accuracy = []\n",
    "    for w in ws:\n",
    "        y_pred = predict_labels(w,tX_val)\n",
    "        accuracy.append(np.mean(y_val.reshape(-1,1)==y_pred))\n",
    "\n",
    "        \n",
    "    ax2 = plt.plot(steps,accuracy)\n",
    "    plt.title('Accuracy as a function of step')\n",
    "    plt.xlabel('number of steps')\n",
    "    plt.ylabel('accuracy on validation set')\n",
    "    plt.show()\n",
    "plot(loss_array,ws,tX_val_poly_std,y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Description of steps\n",
    "(Same initial w)\n",
    "- 50k times with batch size 128, with step 5e-5\n",
    "- 1k time with batch size 256, step 5e-5\n",
    "- 3k time with batch size 512, step 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(0/2999): loss=0.4179640016178712\n",
      "Log Regression(1/2999): loss=0.4179636779736126\n",
      "Log Regression(2/2999): loss=0.4179634243136017\n",
      "Log Regression(3/2999): loss=0.41796305116355104\n",
      "Log Regression(4/2999): loss=0.41796301521793294\n",
      "Log Regression(5/2999): loss=0.417962902434708\n",
      "Log Regression(6/2999): loss=0.417962651388805\n",
      "Log Regression(7/2999): loss=0.4179624925939186\n",
      "Log Regression(8/2999): loss=0.41796215984702295\n",
      "Log Regression(9/2999): loss=0.4179621081790955\n",
      "Log Regression(10/2999): loss=0.41796192514713304\n",
      "Log Regression(11/2999): loss=0.41796120335412423\n",
      "Log Regression(12/2999): loss=0.4179609309410697\n",
      "Log Regression(13/2999): loss=0.4179610745764581\n",
      "Log Regression(14/2999): loss=0.4179608833311082\n",
      "Log Regression(15/2999): loss=0.4179607857746245\n",
      "Log Regression(16/2999): loss=0.41796071460220763\n",
      "Log Regression(17/2999): loss=0.4179605150526163\n",
      "Log Regression(18/2999): loss=0.41796049922358147\n",
      "Log Regression(19/2999): loss=0.41796091706883853\n",
      "Log Regression(20/2999): loss=0.4179611862801161\n",
      "Log Regression(21/2999): loss=0.41796068958265825\n",
      "Log Regression(22/2999): loss=0.4179610369959147\n",
      "Log Regression(23/2999): loss=0.4179607820676361\n",
      "Log Regression(24/2999): loss=0.41796064604602245\n",
      "Log Regression(25/2999): loss=0.4179607841226699\n",
      "Log Regression(26/2999): loss=0.4179603930800103\n",
      "Log Regression(27/2999): loss=0.4179605040477148\n",
      "Log Regression(28/2999): loss=0.4179605084168576\n",
      "Log Regression(29/2999): loss=0.4179602127740904\n",
      "Log Regression(30/2999): loss=0.41796021146480644\n",
      "Log Regression(31/2999): loss=0.4179598498948786\n",
      "Log Regression(32/2999): loss=0.41796000945187783\n",
      "Log Regression(33/2999): loss=0.4179596460181446\n",
      "Log Regression(34/2999): loss=0.4179595892546595\n",
      "Log Regression(35/2999): loss=0.4179594987653281\n",
      "Log Regression(36/2999): loss=0.41795934266412854\n",
      "Log Regression(37/2999): loss=0.4179582520416346\n",
      "Log Regression(38/2999): loss=0.417958408973059\n",
      "Log Regression(39/2999): loss=0.41795849684019093\n",
      "Log Regression(40/2999): loss=0.41795850575817045\n",
      "Log Regression(41/2999): loss=0.4179587504400979\n",
      "Log Regression(42/2999): loss=0.41795857823486066\n",
      "Log Regression(43/2999): loss=0.4179584819907108\n",
      "Log Regression(44/2999): loss=0.4179582572502794\n",
      "Log Regression(45/2999): loss=0.41795808614410385\n",
      "Log Regression(46/2999): loss=0.41795794394234426\n",
      "Log Regression(47/2999): loss=0.4179576256890047\n",
      "Log Regression(48/2999): loss=0.417957759645782\n",
      "Log Regression(49/2999): loss=0.41795714612827256\n",
      "Log Regression(50/2999): loss=0.4179572451772199\n",
      "Log Regression(51/2999): loss=0.4179571252951121\n",
      "Log Regression(52/2999): loss=0.41795721349803505\n",
      "Log Regression(53/2999): loss=0.41795707788615755\n",
      "Log Regression(54/2999): loss=0.41795728229757223\n",
      "Log Regression(55/2999): loss=0.4179573392780802\n",
      "Log Regression(56/2999): loss=0.41795722900924803\n",
      "Log Regression(57/2999): loss=0.41795754987873907\n",
      "Log Regression(58/2999): loss=0.41795742235655986\n",
      "Log Regression(59/2999): loss=0.4179575337124859\n",
      "Log Regression(60/2999): loss=0.41795744463729584\n",
      "Log Regression(61/2999): loss=0.4179568267261837\n",
      "Log Regression(62/2999): loss=0.4179567543489275\n",
      "Log Regression(63/2999): loss=0.41795679632184407\n",
      "Log Regression(64/2999): loss=0.4179565477994851\n",
      "Log Regression(65/2999): loss=0.41795627450538825\n",
      "Log Regression(66/2999): loss=0.41795621944987443\n",
      "Log Regression(67/2999): loss=0.4179562823945284\n",
      "Log Regression(68/2999): loss=0.4179560739529877\n",
      "Log Regression(69/2999): loss=0.41795637336448266\n",
      "Log Regression(70/2999): loss=0.4179561200255783\n",
      "Log Regression(71/2999): loss=0.41795594957496235\n",
      "Log Regression(72/2999): loss=0.41795597297379894\n",
      "Log Regression(73/2999): loss=0.417956078189357\n",
      "Log Regression(74/2999): loss=0.41795602191194603\n",
      "Log Regression(75/2999): loss=0.4179563544926831\n",
      "Log Regression(76/2999): loss=0.41795599728054744\n",
      "Log Regression(77/2999): loss=0.4179555864027219\n",
      "Log Regression(78/2999): loss=0.41795517257707276\n",
      "Log Regression(79/2999): loss=0.41795476948832333\n",
      "Log Regression(80/2999): loss=0.4179546469145615\n",
      "Log Regression(81/2999): loss=0.41795448838776833\n",
      "Log Regression(82/2999): loss=0.4179547425034662\n",
      "Log Regression(83/2999): loss=0.41795473387208787\n",
      "Log Regression(84/2999): loss=0.4179545940385631\n",
      "Log Regression(85/2999): loss=0.4179546128768984\n",
      "Log Regression(86/2999): loss=0.41795472763930624\n",
      "Log Regression(87/2999): loss=0.4179549034085158\n",
      "Log Regression(88/2999): loss=0.4179552917269118\n",
      "Log Regression(89/2999): loss=0.41795556273459994\n",
      "Log Regression(90/2999): loss=0.4179557274096271\n",
      "Log Regression(91/2999): loss=0.4179556874180632\n",
      "Log Regression(92/2999): loss=0.41795568724635873\n",
      "Log Regression(93/2999): loss=0.41795543155739573\n",
      "Log Regression(94/2999): loss=0.41795528587947994\n",
      "Log Regression(95/2999): loss=0.4179552610972911\n",
      "Log Regression(96/2999): loss=0.4179551580869933\n",
      "Log Regression(97/2999): loss=0.4179554141430362\n",
      "Log Regression(98/2999): loss=0.4179553493565868\n",
      "Log Regression(99/2999): loss=0.41795540511225404\n",
      "Log Regression(100/2999): loss=0.4179555437682126\n",
      "Log Regression(101/2999): loss=0.4179558275506299\n",
      "Log Regression(102/2999): loss=0.41795614852523394\n",
      "Log Regression(103/2999): loss=0.41795604741717646\n",
      "Log Regression(104/2999): loss=0.41795607129008894\n",
      "Log Regression(105/2999): loss=0.41795604066970954\n",
      "Log Regression(106/2999): loss=0.4179561955363467\n",
      "Log Regression(107/2999): loss=0.41795619745572865\n",
      "Log Regression(108/2999): loss=0.41795617146335023\n",
      "Log Regression(109/2999): loss=0.41795602327180315\n",
      "Log Regression(110/2999): loss=0.4179560930943265\n",
      "Log Regression(111/2999): loss=0.4179558527560294\n",
      "Log Regression(112/2999): loss=0.4179558866987794\n",
      "Log Regression(113/2999): loss=0.41795622787390746\n",
      "Log Regression(114/2999): loss=0.4179560163234395\n",
      "Log Regression(115/2999): loss=0.41795599216178975\n",
      "Log Regression(116/2999): loss=0.41795565257337997\n",
      "Log Regression(117/2999): loss=0.4179559112781001\n",
      "Log Regression(118/2999): loss=0.4179561947440107\n",
      "Log Regression(119/2999): loss=0.4179562498701132\n",
      "Log Regression(120/2999): loss=0.41795592484185395\n",
      "Log Regression(121/2999): loss=0.417956060229966\n",
      "Log Regression(122/2999): loss=0.4179556495903022\n",
      "Log Regression(123/2999): loss=0.4179549869671501\n",
      "Log Regression(124/2999): loss=0.4179549673085219\n",
      "Log Regression(125/2999): loss=0.4179549328145389\n",
      "Log Regression(126/2999): loss=0.4179545595413632\n",
      "Log Regression(127/2999): loss=0.41795429538001655\n",
      "Log Regression(128/2999): loss=0.4179545280108182\n",
      "Log Regression(129/2999): loss=0.4179544054900213\n",
      "Log Regression(130/2999): loss=0.41795446768400213\n",
      "Log Regression(131/2999): loss=0.41795425620319915\n",
      "Log Regression(132/2999): loss=0.4179536702655376\n",
      "Log Regression(133/2999): loss=0.4179535469136623\n",
      "Log Regression(134/2999): loss=0.41795341337208164\n",
      "Log Regression(135/2999): loss=0.4179538926443795\n",
      "Log Regression(136/2999): loss=0.41795380713942254\n",
      "Log Regression(137/2999): loss=0.4179536855607898\n",
      "Log Regression(138/2999): loss=0.41795341221107263\n",
      "Log Regression(139/2999): loss=0.41795273122378895\n",
      "Log Regression(140/2999): loss=0.417952747651441\n",
      "Log Regression(141/2999): loss=0.41795315045095294\n",
      "Log Regression(142/2999): loss=0.41795316943078153\n",
      "Log Regression(143/2999): loss=0.4179529782538986\n",
      "Log Regression(144/2999): loss=0.41795304005166056\n",
      "Log Regression(145/2999): loss=0.4179527681431413\n",
      "Log Regression(146/2999): loss=0.4179528562765604\n",
      "Log Regression(147/2999): loss=0.41795279896315246\n",
      "Log Regression(148/2999): loss=0.41795286269659637\n",
      "Log Regression(149/2999): loss=0.417952850748016\n",
      "Log Regression(150/2999): loss=0.4179527743099988\n",
      "Log Regression(151/2999): loss=0.4179523432494369\n",
      "Log Regression(152/2999): loss=0.4179519390484644\n",
      "Log Regression(153/2999): loss=0.4179522146921276\n",
      "Log Regression(154/2999): loss=0.41795254394447\n",
      "Log Regression(155/2999): loss=0.4179526321909931\n",
      "Log Regression(156/2999): loss=0.41795234005911724\n",
      "Log Regression(157/2999): loss=0.4179522672296539\n",
      "Log Regression(158/2999): loss=0.41795242739186395\n",
      "Log Regression(159/2999): loss=0.4179520248064985\n",
      "Log Regression(160/2999): loss=0.4179522324178237\n",
      "Log Regression(161/2999): loss=0.41795197794086225\n",
      "Log Regression(162/2999): loss=0.4179520281208494\n",
      "Log Regression(163/2999): loss=0.4179519962902953\n",
      "Log Regression(164/2999): loss=0.4179517129471646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(165/2999): loss=0.4179514297410506\n",
      "Log Regression(166/2999): loss=0.41795130262873614\n",
      "Log Regression(167/2999): loss=0.4179512789093546\n",
      "Log Regression(168/2999): loss=0.4179509730608959\n",
      "Log Regression(169/2999): loss=0.41795093842051545\n",
      "Log Regression(170/2999): loss=0.4179504029853469\n",
      "Log Regression(171/2999): loss=0.41795058368823024\n",
      "Log Regression(172/2999): loss=0.4179502389891019\n",
      "Log Regression(173/2999): loss=0.4179504871149396\n",
      "Log Regression(174/2999): loss=0.41795066896125277\n",
      "Log Regression(175/2999): loss=0.4179509777904732\n",
      "Log Regression(176/2999): loss=0.41795105716576253\n",
      "Log Regression(177/2999): loss=0.4179513424744823\n",
      "Log Regression(178/2999): loss=0.41795109776361106\n",
      "Log Regression(179/2999): loss=0.41795073123142673\n",
      "Log Regression(180/2999): loss=0.4179503458355485\n",
      "Log Regression(181/2999): loss=0.4179502634234686\n",
      "Log Regression(182/2999): loss=0.41795031720643816\n",
      "Log Regression(183/2999): loss=0.4179498796037406\n",
      "Log Regression(184/2999): loss=0.4179498018726537\n",
      "Log Regression(185/2999): loss=0.4179502157867099\n",
      "Log Regression(186/2999): loss=0.4179497495963192\n",
      "Log Regression(187/2999): loss=0.41794934113458737\n",
      "Log Regression(188/2999): loss=0.41794927269022625\n",
      "Log Regression(189/2999): loss=0.4179494220207252\n",
      "Log Regression(190/2999): loss=0.41794981028568623\n",
      "Log Regression(191/2999): loss=0.4179494157122143\n",
      "Log Regression(192/2999): loss=0.4179488975965667\n",
      "Log Regression(193/2999): loss=0.41794897941078074\n",
      "Log Regression(194/2999): loss=0.4179490536895225\n",
      "Log Regression(195/2999): loss=0.41794940518748\n",
      "Log Regression(196/2999): loss=0.417949182710759\n",
      "Log Regression(197/2999): loss=0.4179492707573184\n",
      "Log Regression(198/2999): loss=0.4179494569858152\n",
      "Log Regression(199/2999): loss=0.4179493754396058\n",
      "Log Regression(200/2999): loss=0.41794949286989413\n",
      "Log Regression(201/2999): loss=0.41794938813877025\n",
      "Log Regression(202/2999): loss=0.417949034570079\n",
      "Log Regression(203/2999): loss=0.41794901634489323\n",
      "Log Regression(204/2999): loss=0.4179492712213242\n",
      "Log Regression(205/2999): loss=0.4179495008402626\n",
      "Log Regression(206/2999): loss=0.4179495998866265\n",
      "Log Regression(207/2999): loss=0.4179494435757555\n",
      "Log Regression(208/2999): loss=0.4179493646697016\n",
      "Log Regression(209/2999): loss=0.41794930130086155\n",
      "Log Regression(210/2999): loss=0.4179490372655148\n",
      "Log Regression(211/2999): loss=0.41794860033284453\n",
      "Log Regression(212/2999): loss=0.41794873868652715\n",
      "Log Regression(213/2999): loss=0.41794886444966434\n",
      "Log Regression(214/2999): loss=0.4179484127729645\n",
      "Log Regression(215/2999): loss=0.4179482916728265\n",
      "Log Regression(216/2999): loss=0.41794785392761014\n",
      "Log Regression(217/2999): loss=0.4179479142943357\n",
      "Log Regression(218/2999): loss=0.4179479033520468\n",
      "Log Regression(219/2999): loss=0.4179477563248842\n",
      "Log Regression(220/2999): loss=0.4179476929765576\n",
      "Log Regression(221/2999): loss=0.41794762707960553\n",
      "Log Regression(222/2999): loss=0.4179476175394168\n",
      "Log Regression(223/2999): loss=0.41794751653017886\n",
      "Log Regression(224/2999): loss=0.41794738293559264\n",
      "Log Regression(225/2999): loss=0.41794717366040623\n",
      "Log Regression(226/2999): loss=0.4179471775721463\n",
      "Log Regression(227/2999): loss=0.41794767303164754\n",
      "Log Regression(228/2999): loss=0.4179475328337374\n",
      "Log Regression(229/2999): loss=0.4179473268668468\n",
      "Log Regression(230/2999): loss=0.4179472340557153\n",
      "Log Regression(231/2999): loss=0.4179468991875658\n",
      "Log Regression(232/2999): loss=0.4179467964555276\n",
      "Log Regression(233/2999): loss=0.4179472115369979\n",
      "Log Regression(234/2999): loss=0.41794717442366425\n",
      "Log Regression(235/2999): loss=0.4179474440358325\n",
      "Log Regression(236/2999): loss=0.4179468416198143\n",
      "Log Regression(237/2999): loss=0.41794661060242305\n",
      "Log Regression(238/2999): loss=0.4179465111340575\n",
      "Log Regression(239/2999): loss=0.4179465992463352\n",
      "Log Regression(240/2999): loss=0.41794683361279034\n",
      "Log Regression(241/2999): loss=0.41794665987804447\n",
      "Log Regression(242/2999): loss=0.4179467537013084\n",
      "Log Regression(243/2999): loss=0.41794674798370934\n",
      "Log Regression(244/2999): loss=0.41794612808093173\n",
      "Log Regression(245/2999): loss=0.41794589223439554\n",
      "Log Regression(246/2999): loss=0.41794622948438814\n",
      "Log Regression(247/2999): loss=0.4179458550560744\n",
      "Log Regression(248/2999): loss=0.417945728036539\n",
      "Log Regression(249/2999): loss=0.4179455980720346\n",
      "Log Regression(250/2999): loss=0.41794573616218156\n",
      "Log Regression(251/2999): loss=0.41794541130511964\n",
      "Log Regression(252/2999): loss=0.41794530428999876\n",
      "Log Regression(253/2999): loss=0.4179456850565356\n",
      "Log Regression(254/2999): loss=0.41794567671932054\n",
      "Log Regression(255/2999): loss=0.41794550784253603\n",
      "Log Regression(256/2999): loss=0.41794518231562583\n",
      "Log Regression(257/2999): loss=0.41794490952794733\n",
      "Log Regression(258/2999): loss=0.4179448590086759\n",
      "Log Regression(259/2999): loss=0.4179450658101966\n",
      "Log Regression(260/2999): loss=0.41794478101916727\n",
      "Log Regression(261/2999): loss=0.41794455975675276\n",
      "Log Regression(262/2999): loss=0.41794482864793636\n",
      "Log Regression(263/2999): loss=0.41794422614073834\n",
      "Log Regression(264/2999): loss=0.4179439651643298\n",
      "Log Regression(265/2999): loss=0.4179442978089149\n",
      "Log Regression(266/2999): loss=0.4179438011016446\n",
      "Log Regression(267/2999): loss=0.41794364690309754\n",
      "Log Regression(268/2999): loss=0.4179434440671842\n",
      "Log Regression(269/2999): loss=0.41794343065173123\n",
      "Log Regression(270/2999): loss=0.41794347540308086\n",
      "Log Regression(271/2999): loss=0.417943393150973\n",
      "Log Regression(272/2999): loss=0.4179434429057057\n",
      "Log Regression(273/2999): loss=0.41794314446829667\n",
      "Log Regression(274/2999): loss=0.41794304660704307\n",
      "Log Regression(275/2999): loss=0.4179429856770199\n",
      "Log Regression(276/2999): loss=0.41794261548234224\n",
      "Log Regression(277/2999): loss=0.41794265192723984\n",
      "Log Regression(278/2999): loss=0.41794283819532524\n",
      "Log Regression(279/2999): loss=0.41794238591519767\n",
      "Log Regression(280/2999): loss=0.4179424108118977\n",
      "Log Regression(281/2999): loss=0.4179424732771724\n",
      "Log Regression(282/2999): loss=0.417942734555598\n",
      "Log Regression(283/2999): loss=0.4179429290375844\n",
      "Log Regression(284/2999): loss=0.4179433260412749\n",
      "Log Regression(285/2999): loss=0.41794310392145234\n",
      "Log Regression(286/2999): loss=0.41794296769005607\n",
      "Log Regression(287/2999): loss=0.41794316033383333\n",
      "Log Regression(288/2999): loss=0.4179429667764824\n",
      "Log Regression(289/2999): loss=0.41794209393791315\n",
      "Log Regression(290/2999): loss=0.4179418280733925\n",
      "Log Regression(291/2999): loss=0.4179420021998554\n",
      "Log Regression(292/2999): loss=0.4179424937174865\n",
      "Log Regression(293/2999): loss=0.4179422788909183\n",
      "Log Regression(294/2999): loss=0.41794272059229753\n",
      "Log Regression(295/2999): loss=0.4179428115638686\n",
      "Log Regression(296/2999): loss=0.41794254146932924\n",
      "Log Regression(297/2999): loss=0.4179422534289187\n",
      "Log Regression(298/2999): loss=0.41794217441424963\n",
      "Log Regression(299/2999): loss=0.417942058648955\n",
      "Log Regression(300/2999): loss=0.4179417793139932\n",
      "Log Regression(301/2999): loss=0.41794177462171966\n",
      "Log Regression(302/2999): loss=0.4179418724355701\n",
      "Log Regression(303/2999): loss=0.4179417957737432\n",
      "Log Regression(304/2999): loss=0.41794195498221226\n",
      "Log Regression(305/2999): loss=0.41794230365628193\n",
      "Log Regression(306/2999): loss=0.4179424036064183\n",
      "Log Regression(307/2999): loss=0.41794214420768466\n",
      "Log Regression(308/2999): loss=0.41794265233123434\n",
      "Log Regression(309/2999): loss=0.41794273209565586\n",
      "Log Regression(310/2999): loss=0.4179427279540738\n",
      "Log Regression(311/2999): loss=0.41794278046183536\n",
      "Log Regression(312/2999): loss=0.4179426867605479\n",
      "Log Regression(313/2999): loss=0.4179426839544145\n",
      "Log Regression(314/2999): loss=0.41794272385118886\n",
      "Log Regression(315/2999): loss=0.41794308209242076\n",
      "Log Regression(316/2999): loss=0.41794273548422783\n",
      "Log Regression(317/2999): loss=0.417942664853135\n",
      "Log Regression(318/2999): loss=0.4179423024012474\n",
      "Log Regression(319/2999): loss=0.41794246492011156\n",
      "Log Regression(320/2999): loss=0.41794229920905096\n",
      "Log Regression(321/2999): loss=0.4179419559802898\n",
      "Log Regression(322/2999): loss=0.41794161728310353\n",
      "Log Regression(323/2999): loss=0.41794128558873384\n",
      "Log Regression(324/2999): loss=0.4179411990276892\n",
      "Log Regression(325/2999): loss=0.41794120845018845\n",
      "Log Regression(326/2999): loss=0.4179410183262488\n",
      "Log Regression(327/2999): loss=0.4179412005770556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(328/2999): loss=0.4179412376292785\n",
      "Log Regression(329/2999): loss=0.41794155962668283\n",
      "Log Regression(330/2999): loss=0.4179416614053952\n",
      "Log Regression(331/2999): loss=0.4179422109610619\n",
      "Log Regression(332/2999): loss=0.4179421891108183\n",
      "Log Regression(333/2999): loss=0.4179423484920692\n",
      "Log Regression(334/2999): loss=0.41794240741098465\n",
      "Log Regression(335/2999): loss=0.417942347401045\n",
      "Log Regression(336/2999): loss=0.41794234907141065\n",
      "Log Regression(337/2999): loss=0.4179426965469843\n",
      "Log Regression(338/2999): loss=0.41794194704133286\n",
      "Log Regression(339/2999): loss=0.41794184253384775\n",
      "Log Regression(340/2999): loss=0.4179407537538517\n",
      "Log Regression(341/2999): loss=0.41794091276626977\n",
      "Log Regression(342/2999): loss=0.4179409167718244\n",
      "Log Regression(343/2999): loss=0.41794103619804385\n",
      "Log Regression(344/2999): loss=0.41794098713685685\n",
      "Log Regression(345/2999): loss=0.4179409090227243\n",
      "Log Regression(346/2999): loss=0.4179411721572223\n",
      "Log Regression(347/2999): loss=0.4179408301649568\n",
      "Log Regression(348/2999): loss=0.4179408418983447\n",
      "Log Regression(349/2999): loss=0.4179406634374813\n",
      "Log Regression(350/2999): loss=0.41794049805786027\n",
      "Log Regression(351/2999): loss=0.41794055935942687\n",
      "Log Regression(352/2999): loss=0.41794048405325973\n",
      "Log Regression(353/2999): loss=0.4179405862838632\n",
      "Log Regression(354/2999): loss=0.41794063209654525\n",
      "Log Regression(355/2999): loss=0.41794070106582526\n",
      "Log Regression(356/2999): loss=0.4179408297817247\n",
      "Log Regression(357/2999): loss=0.4179404549760082\n",
      "Log Regression(358/2999): loss=0.41794063170316786\n",
      "Log Regression(359/2999): loss=0.4179406525044654\n",
      "Log Regression(360/2999): loss=0.4179404644106221\n",
      "Log Regression(361/2999): loss=0.41794059760205904\n",
      "Log Regression(362/2999): loss=0.417940761730717\n",
      "Log Regression(363/2999): loss=0.41794052393577186\n",
      "Log Regression(364/2999): loss=0.41794084258838715\n",
      "Log Regression(365/2999): loss=0.4179403246959457\n",
      "Log Regression(366/2999): loss=0.41794044943371\n",
      "Log Regression(367/2999): loss=0.4179401397510095\n",
      "Log Regression(368/2999): loss=0.41794014649278555\n",
      "Log Regression(369/2999): loss=0.41794014049394923\n",
      "Log Regression(370/2999): loss=0.41794003924746614\n",
      "Log Regression(371/2999): loss=0.41793982328134194\n",
      "Log Regression(372/2999): loss=0.41793931978411547\n",
      "Log Regression(373/2999): loss=0.4179389085323146\n",
      "Log Regression(374/2999): loss=0.4179386752010592\n",
      "Log Regression(375/2999): loss=0.41793886741068265\n",
      "Log Regression(376/2999): loss=0.4179385790305031\n",
      "Log Regression(377/2999): loss=0.41793822532970454\n",
      "Log Regression(378/2999): loss=0.41793824063382135\n",
      "Log Regression(379/2999): loss=0.4179383839350917\n",
      "Log Regression(380/2999): loss=0.41793816637290765\n",
      "Log Regression(381/2999): loss=0.4179380663572921\n",
      "Log Regression(382/2999): loss=0.41793779366944095\n",
      "Log Regression(383/2999): loss=0.4179376743172224\n",
      "Log Regression(384/2999): loss=0.41793741564908\n",
      "Log Regression(385/2999): loss=0.4179374001446246\n",
      "Log Regression(386/2999): loss=0.4179375572080614\n",
      "Log Regression(387/2999): loss=0.41793760796818435\n",
      "Log Regression(388/2999): loss=0.41793767885184846\n",
      "Log Regression(389/2999): loss=0.4179380408268421\n",
      "Log Regression(390/2999): loss=0.4179380822752427\n",
      "Log Regression(391/2999): loss=0.41793771683612985\n",
      "Log Regression(392/2999): loss=0.4179377831588971\n",
      "Log Regression(393/2999): loss=0.4179377542282916\n",
      "Log Regression(394/2999): loss=0.41793855465734325\n",
      "Log Regression(395/2999): loss=0.41793752139082985\n",
      "Log Regression(396/2999): loss=0.41793731175725046\n",
      "Log Regression(397/2999): loss=0.4179373452642053\n",
      "Log Regression(398/2999): loss=0.4179374222593463\n",
      "Log Regression(399/2999): loss=0.4179371802206422\n",
      "Log Regression(400/2999): loss=0.41793733314622583\n",
      "Log Regression(401/2999): loss=0.41793687694045434\n",
      "Log Regression(402/2999): loss=0.41793688724764444\n",
      "Log Regression(403/2999): loss=0.41793685249546597\n",
      "Log Regression(404/2999): loss=0.41793673749069177\n",
      "Log Regression(405/2999): loss=0.4179368922058722\n",
      "Log Regression(406/2999): loss=0.41793730014297514\n",
      "Log Regression(407/2999): loss=0.41793703451441133\n",
      "Log Regression(408/2999): loss=0.4179373265185478\n",
      "Log Regression(409/2999): loss=0.4179371133432706\n",
      "Log Regression(410/2999): loss=0.4179371454125817\n",
      "Log Regression(411/2999): loss=0.41793710206761797\n",
      "Log Regression(412/2999): loss=0.4179371526487776\n",
      "Log Regression(413/2999): loss=0.4179372545529995\n",
      "Log Regression(414/2999): loss=0.41793714319740904\n",
      "Log Regression(415/2999): loss=0.41793693415831973\n",
      "Log Regression(416/2999): loss=0.4179362293143338\n",
      "Log Regression(417/2999): loss=0.4179361621137718\n",
      "Log Regression(418/2999): loss=0.4179359659982242\n",
      "Log Regression(419/2999): loss=0.4179362064492101\n",
      "Log Regression(420/2999): loss=0.4179359437276531\n",
      "Log Regression(421/2999): loss=0.4179363112560633\n",
      "Log Regression(422/2999): loss=0.41793597091492873\n",
      "Log Regression(423/2999): loss=0.417935826085256\n",
      "Log Regression(424/2999): loss=0.417936111287189\n",
      "Log Regression(425/2999): loss=0.4179359638076265\n",
      "Log Regression(426/2999): loss=0.4179357752657232\n",
      "Log Regression(427/2999): loss=0.41793565711061925\n",
      "Log Regression(428/2999): loss=0.41793533668250776\n",
      "Log Regression(429/2999): loss=0.4179353135021887\n",
      "Log Regression(430/2999): loss=0.4179356251738951\n",
      "Log Regression(431/2999): loss=0.41793541938642387\n",
      "Log Regression(432/2999): loss=0.41793518331744456\n",
      "Log Regression(433/2999): loss=0.41793485545417336\n",
      "Log Regression(434/2999): loss=0.417934733492086\n",
      "Log Regression(435/2999): loss=0.41793318703917987\n",
      "Log Regression(436/2999): loss=0.41793356452936625\n",
      "Log Regression(437/2999): loss=0.4179335439137513\n",
      "Log Regression(438/2999): loss=0.4179326677314381\n",
      "Log Regression(439/2999): loss=0.4179324171674399\n",
      "Log Regression(440/2999): loss=0.4179327162523866\n",
      "Log Regression(441/2999): loss=0.41793272234485046\n",
      "Log Regression(442/2999): loss=0.4179327133702059\n",
      "Log Regression(443/2999): loss=0.41793272861904507\n",
      "Log Regression(444/2999): loss=0.41793250310753927\n",
      "Log Regression(445/2999): loss=0.417932449063898\n",
      "Log Regression(446/2999): loss=0.4179326565357513\n",
      "Log Regression(447/2999): loss=0.41793213452349015\n",
      "Log Regression(448/2999): loss=0.41793224495904846\n",
      "Log Regression(449/2999): loss=0.41793239297023665\n",
      "Log Regression(450/2999): loss=0.4179327165944896\n",
      "Log Regression(451/2999): loss=0.41793311416754764\n",
      "Log Regression(452/2999): loss=0.4179331446163279\n",
      "Log Regression(453/2999): loss=0.417933305927978\n",
      "Log Regression(454/2999): loss=0.4179330124337693\n",
      "Log Regression(455/2999): loss=0.41793295184655094\n",
      "Log Regression(456/2999): loss=0.4179326149219738\n",
      "Log Regression(457/2999): loss=0.41793215830600855\n",
      "Log Regression(458/2999): loss=0.41793257843182957\n",
      "Log Regression(459/2999): loss=0.4179324221591888\n",
      "Log Regression(460/2999): loss=0.4179322685844167\n",
      "Log Regression(461/2999): loss=0.4179324281156322\n",
      "Log Regression(462/2999): loss=0.41793217372418795\n",
      "Log Regression(463/2999): loss=0.4179318466805343\n",
      "Log Regression(464/2999): loss=0.4179316742154445\n",
      "Log Regression(465/2999): loss=0.4179316785879529\n",
      "Log Regression(466/2999): loss=0.4179319107080101\n",
      "Log Regression(467/2999): loss=0.41793220844549006\n",
      "Log Regression(468/2999): loss=0.417932548514256\n",
      "Log Regression(469/2999): loss=0.41793287883905367\n",
      "Log Regression(470/2999): loss=0.41793287550891256\n",
      "Log Regression(471/2999): loss=0.4179328671740648\n",
      "Log Regression(472/2999): loss=0.4179326750194994\n",
      "Log Regression(473/2999): loss=0.417932670426062\n",
      "Log Regression(474/2999): loss=0.41793230207612375\n",
      "Log Regression(475/2999): loss=0.4179329289077548\n",
      "Log Regression(476/2999): loss=0.4179329058693946\n",
      "Log Regression(477/2999): loss=0.4179326328223015\n",
      "Log Regression(478/2999): loss=0.417932431812495\n",
      "Log Regression(479/2999): loss=0.4179321604265812\n",
      "Log Regression(480/2999): loss=0.4179321431372764\n",
      "Log Regression(481/2999): loss=0.41793228290505496\n",
      "Log Regression(482/2999): loss=0.4179327121721726\n",
      "Log Regression(483/2999): loss=0.4179324117269381\n",
      "Log Regression(484/2999): loss=0.41793232517566953\n",
      "Log Regression(485/2999): loss=0.41793287543140734\n",
      "Log Regression(486/2999): loss=0.417932860595399\n",
      "Log Regression(487/2999): loss=0.41793258119974663\n",
      "Log Regression(488/2999): loss=0.41793239056995973\n",
      "Log Regression(489/2999): loss=0.4179322023183154\n",
      "Log Regression(490/2999): loss=0.4179319085464275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(491/2999): loss=0.41793192237644156\n",
      "Log Regression(492/2999): loss=0.4179319618584334\n",
      "Log Regression(493/2999): loss=0.41793265005978625\n",
      "Log Regression(494/2999): loss=0.4179325567188684\n",
      "Log Regression(495/2999): loss=0.41793239653955505\n",
      "Log Regression(496/2999): loss=0.41793220414237586\n",
      "Log Regression(497/2999): loss=0.41793265687677855\n",
      "Log Regression(498/2999): loss=0.41793266898093195\n",
      "Log Regression(499/2999): loss=0.417932611899774\n",
      "Log Regression(500/2999): loss=0.41793244364205884\n",
      "Log Regression(501/2999): loss=0.4179323235549193\n",
      "Log Regression(502/2999): loss=0.41793234776830684\n",
      "Log Regression(503/2999): loss=0.41793239203311044\n",
      "Log Regression(504/2999): loss=0.41793235837385584\n",
      "Log Regression(505/2999): loss=0.4179324041758687\n",
      "Log Regression(506/2999): loss=0.41793212810185165\n",
      "Log Regression(507/2999): loss=0.417932160298181\n",
      "Log Regression(508/2999): loss=0.417931997565641\n",
      "Log Regression(509/2999): loss=0.41793205194281563\n",
      "Log Regression(510/2999): loss=0.4179317255129845\n",
      "Log Regression(511/2999): loss=0.41793148921088524\n",
      "Log Regression(512/2999): loss=0.4179315100570047\n",
      "Log Regression(513/2999): loss=0.41793124888445654\n",
      "Log Regression(514/2999): loss=0.41793113822896993\n",
      "Log Regression(515/2999): loss=0.4179310303421243\n",
      "Log Regression(516/2999): loss=0.41793098867081363\n",
      "Log Regression(517/2999): loss=0.41793130150451124\n",
      "Log Regression(518/2999): loss=0.41793113070933574\n",
      "Log Regression(519/2999): loss=0.41793111218633994\n",
      "Log Regression(520/2999): loss=0.41793066296637027\n",
      "Log Regression(521/2999): loss=0.41793047915559445\n",
      "Log Regression(522/2999): loss=0.4179303282655137\n",
      "Log Regression(523/2999): loss=0.41793092542327337\n",
      "Log Regression(524/2999): loss=0.41793085650911754\n",
      "Log Regression(525/2999): loss=0.4179313143529694\n",
      "Log Regression(526/2999): loss=0.4179310845701814\n",
      "Log Regression(527/2999): loss=0.41793087986178007\n",
      "Log Regression(528/2999): loss=0.4179307416167986\n",
      "Log Regression(529/2999): loss=0.41793099482435137\n",
      "Log Regression(530/2999): loss=0.4179308811969653\n",
      "Log Regression(531/2999): loss=0.41793061921146735\n",
      "Log Regression(532/2999): loss=0.41793041970165307\n",
      "Log Regression(533/2999): loss=0.4179304208770124\n",
      "Log Regression(534/2999): loss=0.4179311397549163\n",
      "Log Regression(535/2999): loss=0.417931043775337\n",
      "Log Regression(536/2999): loss=0.4179310546900697\n",
      "Log Regression(537/2999): loss=0.4179308767421475\n",
      "Log Regression(538/2999): loss=0.4179307690355742\n",
      "Log Regression(539/2999): loss=0.41793079922605864\n",
      "Log Regression(540/2999): loss=0.417930440424748\n",
      "Log Regression(541/2999): loss=0.41793061665898695\n",
      "Log Regression(542/2999): loss=0.41793051092508665\n",
      "Log Regression(543/2999): loss=0.4179308572938836\n",
      "Log Regression(544/2999): loss=0.417930567711501\n",
      "Log Regression(545/2999): loss=0.4179306098669388\n",
      "Log Regression(546/2999): loss=0.4179307882358611\n",
      "Log Regression(547/2999): loss=0.4179309824381127\n",
      "Log Regression(548/2999): loss=0.4179314913973813\n",
      "Log Regression(549/2999): loss=0.4179312976228754\n",
      "Log Regression(550/2999): loss=0.417930809547571\n",
      "Log Regression(551/2999): loss=0.417930854992312\n",
      "Log Regression(552/2999): loss=0.4179307187691926\n",
      "Log Regression(553/2999): loss=0.4179304835996692\n",
      "Log Regression(554/2999): loss=0.4179304238759028\n",
      "Log Regression(555/2999): loss=0.4179300495083788\n",
      "Log Regression(556/2999): loss=0.4179297759800688\n",
      "Log Regression(557/2999): loss=0.41792979828550725\n",
      "Log Regression(558/2999): loss=0.41792969471481134\n",
      "Log Regression(559/2999): loss=0.4179292644362063\n",
      "Log Regression(560/2999): loss=0.41792893560068967\n",
      "Log Regression(561/2999): loss=0.4179290459546335\n",
      "Log Regression(562/2999): loss=0.41792907377926786\n",
      "Log Regression(563/2999): loss=0.417928823460104\n",
      "Log Regression(564/2999): loss=0.4179289433703525\n",
      "Log Regression(565/2999): loss=0.41792875218725983\n",
      "Log Regression(566/2999): loss=0.41792838402610316\n",
      "Log Regression(567/2999): loss=0.4179282863230526\n",
      "Log Regression(568/2999): loss=0.4179279573847222\n",
      "Log Regression(569/2999): loss=0.41792780735508833\n",
      "Log Regression(570/2999): loss=0.4179280915376924\n",
      "Log Regression(571/2999): loss=0.41792819925733754\n",
      "Log Regression(572/2999): loss=0.41792862198400954\n",
      "Log Regression(573/2999): loss=0.4179287670354337\n",
      "Log Regression(574/2999): loss=0.417929024127596\n",
      "Log Regression(575/2999): loss=0.4179288316109494\n",
      "Log Regression(576/2999): loss=0.41792846130522954\n",
      "Log Regression(577/2999): loss=0.4179282902925033\n",
      "Log Regression(578/2999): loss=0.41792840634106415\n",
      "Log Regression(579/2999): loss=0.4179284518811023\n",
      "Log Regression(580/2999): loss=0.4179286048632413\n",
      "Log Regression(581/2999): loss=0.41792834883809693\n",
      "Log Regression(582/2999): loss=0.41792819569579404\n",
      "Log Regression(583/2999): loss=0.4179279170574775\n",
      "Log Regression(584/2999): loss=0.4179275553383583\n",
      "Log Regression(585/2999): loss=0.41792764547826045\n",
      "Log Regression(586/2999): loss=0.4179280726941334\n",
      "Log Regression(587/2999): loss=0.4179281283831946\n",
      "Log Regression(588/2999): loss=0.41792785841433633\n",
      "Log Regression(589/2999): loss=0.4179280313943416\n",
      "Log Regression(590/2999): loss=0.41792780035253696\n",
      "Log Regression(591/2999): loss=0.41792762794197325\n",
      "Log Regression(592/2999): loss=0.41792770995486034\n",
      "Log Regression(593/2999): loss=0.4179276540594125\n",
      "Log Regression(594/2999): loss=0.41792718120535893\n",
      "Log Regression(595/2999): loss=0.417927355193579\n",
      "Log Regression(596/2999): loss=0.41792725284944265\n",
      "Log Regression(597/2999): loss=0.41792781763316467\n",
      "Log Regression(598/2999): loss=0.41792736474302394\n",
      "Log Regression(599/2999): loss=0.4179274597920793\n",
      "Log Regression(600/2999): loss=0.4179273020752305\n",
      "Log Regression(601/2999): loss=0.41792744723608427\n",
      "Log Regression(602/2999): loss=0.4179271339327045\n",
      "Log Regression(603/2999): loss=0.4179269078247686\n",
      "Log Regression(604/2999): loss=0.41792674065402985\n",
      "Log Regression(605/2999): loss=0.4179265565092866\n",
      "Log Regression(606/2999): loss=0.41792655076595586\n",
      "Log Regression(607/2999): loss=0.4179266093078735\n",
      "Log Regression(608/2999): loss=0.4179265345497405\n",
      "Log Regression(609/2999): loss=0.4179263028497931\n",
      "Log Regression(610/2999): loss=0.4179264461302703\n",
      "Log Regression(611/2999): loss=0.41792677982919785\n",
      "Log Regression(612/2999): loss=0.4179271000915023\n",
      "Log Regression(613/2999): loss=0.4179271503336968\n",
      "Log Regression(614/2999): loss=0.4179272699808897\n",
      "Log Regression(615/2999): loss=0.4179272548779108\n",
      "Log Regression(616/2999): loss=0.417927688539035\n",
      "Log Regression(617/2999): loss=0.4179279622877693\n",
      "Log Regression(618/2999): loss=0.4179277587968251\n",
      "Log Regression(619/2999): loss=0.41792759095930376\n",
      "Log Regression(620/2999): loss=0.4179273812954772\n",
      "Log Regression(621/2999): loss=0.41792720642060355\n",
      "Log Regression(622/2999): loss=0.4179270177170902\n",
      "Log Regression(623/2999): loss=0.41792720220705337\n",
      "Log Regression(624/2999): loss=0.417927117941373\n",
      "Log Regression(625/2999): loss=0.4179270627676754\n",
      "Log Regression(626/2999): loss=0.41792681705890766\n",
      "Log Regression(627/2999): loss=0.4179270702356878\n",
      "Log Regression(628/2999): loss=0.41792721942378885\n",
      "Log Regression(629/2999): loss=0.41792746078875254\n",
      "Log Regression(630/2999): loss=0.41792742872143657\n",
      "Log Regression(631/2999): loss=0.4179276395487458\n",
      "Log Regression(632/2999): loss=0.41792737707256267\n",
      "Log Regression(633/2999): loss=0.41792725275701137\n",
      "Log Regression(634/2999): loss=0.41792761816301977\n",
      "Log Regression(635/2999): loss=0.4179276700312743\n",
      "Log Regression(636/2999): loss=0.41792736011554243\n",
      "Log Regression(637/2999): loss=0.41792736260229507\n",
      "Log Regression(638/2999): loss=0.4179273210055866\n",
      "Log Regression(639/2999): loss=0.4179269463537241\n",
      "Log Regression(640/2999): loss=0.4179270397066659\n",
      "Log Regression(641/2999): loss=0.4179265454677605\n",
      "Log Regression(642/2999): loss=0.4179261481030184\n",
      "Log Regression(643/2999): loss=0.4179262299460214\n",
      "Log Regression(644/2999): loss=0.4179262711264536\n",
      "Log Regression(645/2999): loss=0.41792601657638623\n",
      "Log Regression(646/2999): loss=0.41792594725023796\n",
      "Log Regression(647/2999): loss=0.4179271544312205\n",
      "Log Regression(648/2999): loss=0.41792690470528326\n",
      "Log Regression(649/2999): loss=0.41792669014503486\n",
      "Log Regression(650/2999): loss=0.4179266349929678\n",
      "Log Regression(651/2999): loss=0.41792669200246074\n",
      "Log Regression(652/2999): loss=0.4179267322384727\n",
      "Log Regression(653/2999): loss=0.417927164220309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(654/2999): loss=0.4179271930147136\n",
      "Log Regression(655/2999): loss=0.4179271222963046\n",
      "Log Regression(656/2999): loss=0.4179271886980102\n",
      "Log Regression(657/2999): loss=0.4179271965953687\n",
      "Log Regression(658/2999): loss=0.4179268457995033\n",
      "Log Regression(659/2999): loss=0.41792695995874035\n",
      "Log Regression(660/2999): loss=0.4179266041479807\n",
      "Log Regression(661/2999): loss=0.4179264793480271\n",
      "Log Regression(662/2999): loss=0.4179264733748478\n",
      "Log Regression(663/2999): loss=0.41792678357917035\n",
      "Log Regression(664/2999): loss=0.41792654144664704\n",
      "Log Regression(665/2999): loss=0.41792713281936855\n",
      "Log Regression(666/2999): loss=0.417927348913923\n",
      "Log Regression(667/2999): loss=0.41792760795098155\n",
      "Log Regression(668/2999): loss=0.4179278034814725\n",
      "Log Regression(669/2999): loss=0.4179276332943924\n",
      "Log Regression(670/2999): loss=0.4179280477751779\n",
      "Log Regression(671/2999): loss=0.4179280865252285\n",
      "Log Regression(672/2999): loss=0.4179280244727389\n",
      "Log Regression(673/2999): loss=0.417927735286699\n",
      "Log Regression(674/2999): loss=0.4179275226069122\n",
      "Log Regression(675/2999): loss=0.41792710241217784\n",
      "Log Regression(676/2999): loss=0.41792737892774023\n",
      "Log Regression(677/2999): loss=0.4179272509559523\n",
      "Log Regression(678/2999): loss=0.41792718168750415\n",
      "Log Regression(679/2999): loss=0.4179272168185056\n",
      "Log Regression(680/2999): loss=0.41792717607764124\n",
      "Log Regression(681/2999): loss=0.4179275340204345\n",
      "Log Regression(682/2999): loss=0.4179272705960713\n",
      "Log Regression(683/2999): loss=0.4179275871041838\n",
      "Log Regression(684/2999): loss=0.41792740080976754\n",
      "Log Regression(685/2999): loss=0.41792738994255846\n",
      "Log Regression(686/2999): loss=0.4179274364722569\n",
      "Log Regression(687/2999): loss=0.41792740250198773\n",
      "Log Regression(688/2999): loss=0.41792749866452306\n",
      "Log Regression(689/2999): loss=0.4179275714435396\n",
      "Log Regression(690/2999): loss=0.4179274349219554\n",
      "Log Regression(691/2999): loss=0.41792792778435606\n",
      "Log Regression(692/2999): loss=0.41792775349604555\n",
      "Log Regression(693/2999): loss=0.41792747246849793\n",
      "Log Regression(694/2999): loss=0.4179272282203489\n",
      "Log Regression(695/2999): loss=0.41792734954606253\n",
      "Log Regression(696/2999): loss=0.4179273970778822\n",
      "Log Regression(697/2999): loss=0.41792724376045476\n",
      "Log Regression(698/2999): loss=0.4179272911048568\n",
      "Log Regression(699/2999): loss=0.41792710067951244\n",
      "Log Regression(700/2999): loss=0.4179271321001489\n",
      "Log Regression(701/2999): loss=0.41792739146202174\n",
      "Log Regression(702/2999): loss=0.4179271180057538\n",
      "Log Regression(703/2999): loss=0.4179279290403475\n",
      "Log Regression(704/2999): loss=0.4179276984323167\n",
      "Log Regression(705/2999): loss=0.4179273396710056\n",
      "Log Regression(706/2999): loss=0.4179271792111446\n",
      "Log Regression(707/2999): loss=0.41792739279785185\n",
      "Log Regression(708/2999): loss=0.41792755067023624\n",
      "Log Regression(709/2999): loss=0.4179275376478086\n",
      "Log Regression(710/2999): loss=0.41792768639755234\n",
      "Log Regression(711/2999): loss=0.41792783253590804\n",
      "Log Regression(712/2999): loss=0.41792754997903714\n",
      "Log Regression(713/2999): loss=0.4179275436109553\n",
      "Log Regression(714/2999): loss=0.4179274803536284\n",
      "Log Regression(715/2999): loss=0.41792732505840197\n",
      "Log Regression(716/2999): loss=0.4179271304666247\n",
      "Log Regression(717/2999): loss=0.4179273349736347\n",
      "Log Regression(718/2999): loss=0.4179275091094816\n",
      "Log Regression(719/2999): loss=0.41792755110224333\n",
      "Log Regression(720/2999): loss=0.41792633023350956\n",
      "Log Regression(721/2999): loss=0.41792698102882536\n",
      "Log Regression(722/2999): loss=0.41792711358115564\n",
      "Log Regression(723/2999): loss=0.41792682577118434\n",
      "Log Regression(724/2999): loss=0.4179267091400972\n",
      "Log Regression(725/2999): loss=0.4179267732723444\n",
      "Log Regression(726/2999): loss=0.4179264366665004\n",
      "Log Regression(727/2999): loss=0.4179261784705259\n",
      "Log Regression(728/2999): loss=0.4179271305764919\n",
      "Log Regression(729/2999): loss=0.4179268507493996\n",
      "Log Regression(730/2999): loss=0.417927401249853\n",
      "Log Regression(731/2999): loss=0.4179271283374073\n",
      "Log Regression(732/2999): loss=0.4179269152443453\n",
      "Log Regression(733/2999): loss=0.41792702642274804\n",
      "Log Regression(734/2999): loss=0.41792702407299376\n",
      "Log Regression(735/2999): loss=0.41792659416711286\n",
      "Log Regression(736/2999): loss=0.41792652964074145\n",
      "Log Regression(737/2999): loss=0.41792677349010143\n",
      "Log Regression(738/2999): loss=0.4179265410875573\n",
      "Log Regression(739/2999): loss=0.4179265387278974\n",
      "Log Regression(740/2999): loss=0.41792662553926824\n",
      "Log Regression(741/2999): loss=0.4179264752026476\n",
      "Log Regression(742/2999): loss=0.4179263025852155\n",
      "Log Regression(743/2999): loss=0.4179261870302662\n",
      "Log Regression(744/2999): loss=0.4179259834577996\n",
      "Log Regression(745/2999): loss=0.41792600190061\n",
      "Log Regression(746/2999): loss=0.41792609508491774\n",
      "Log Regression(747/2999): loss=0.4179259091572382\n",
      "Log Regression(748/2999): loss=0.417925144501299\n",
      "Log Regression(749/2999): loss=0.4179249194589372\n",
      "Log Regression(750/2999): loss=0.4179245055265335\n",
      "Log Regression(751/2999): loss=0.4179246930447988\n",
      "Log Regression(752/2999): loss=0.4179246486359917\n",
      "Log Regression(753/2999): loss=0.41792453667004115\n",
      "Log Regression(754/2999): loss=0.41792460715580876\n",
      "Log Regression(755/2999): loss=0.41792467690591856\n",
      "Log Regression(756/2999): loss=0.4179247642761769\n",
      "Log Regression(757/2999): loss=0.41792458232933544\n",
      "Log Regression(758/2999): loss=0.417924485859087\n",
      "Log Regression(759/2999): loss=0.41792453950439307\n",
      "Log Regression(760/2999): loss=0.4179239763228402\n",
      "Log Regression(761/2999): loss=0.41792365436573414\n",
      "Log Regression(762/2999): loss=0.4179236836766019\n",
      "Log Regression(763/2999): loss=0.41792310418437856\n",
      "Log Regression(764/2999): loss=0.41792308204709194\n",
      "Log Regression(765/2999): loss=0.4179235682757522\n",
      "Log Regression(766/2999): loss=0.4179234373176373\n",
      "Log Regression(767/2999): loss=0.4179234358302891\n",
      "Log Regression(768/2999): loss=0.41792341278891465\n",
      "Log Regression(769/2999): loss=0.417923830839952\n",
      "Log Regression(770/2999): loss=0.4179236581877042\n",
      "Log Regression(771/2999): loss=0.4179236340200486\n",
      "Log Regression(772/2999): loss=0.4179244013835797\n",
      "Log Regression(773/2999): loss=0.4179240291552976\n",
      "Log Regression(774/2999): loss=0.41792422592728834\n",
      "Log Regression(775/2999): loss=0.4179244597013543\n",
      "Log Regression(776/2999): loss=0.4179246922302759\n",
      "Log Regression(777/2999): loss=0.41792456552553287\n",
      "Log Regression(778/2999): loss=0.41792473416563153\n",
      "Log Regression(779/2999): loss=0.4179247298559746\n",
      "Log Regression(780/2999): loss=0.41792442670117474\n",
      "Log Regression(781/2999): loss=0.41792448021650913\n",
      "Log Regression(782/2999): loss=0.4179248587707876\n",
      "Log Regression(783/2999): loss=0.417924689920334\n",
      "Log Regression(784/2999): loss=0.4179244862699822\n",
      "Log Regression(785/2999): loss=0.4179246439706981\n",
      "Log Regression(786/2999): loss=0.4179243969948976\n",
      "Log Regression(787/2999): loss=0.41792439866649067\n",
      "Log Regression(788/2999): loss=0.41792451312653234\n",
      "Log Regression(789/2999): loss=0.41792443002275753\n",
      "Log Regression(790/2999): loss=0.4179241208736628\n",
      "Log Regression(791/2999): loss=0.41792371971878484\n",
      "Log Regression(792/2999): loss=0.4179236140782897\n",
      "Log Regression(793/2999): loss=0.4179237497349188\n",
      "Log Regression(794/2999): loss=0.4179236486984314\n",
      "Log Regression(795/2999): loss=0.41792346883154313\n",
      "Log Regression(796/2999): loss=0.4179230597151333\n",
      "Log Regression(797/2999): loss=0.4179229111281109\n",
      "Log Regression(798/2999): loss=0.4179226989670301\n",
      "Log Regression(799/2999): loss=0.41792269548625666\n",
      "Log Regression(800/2999): loss=0.4179231494847094\n",
      "Log Regression(801/2999): loss=0.41792344587751795\n",
      "Log Regression(802/2999): loss=0.4179222610597154\n",
      "Log Regression(803/2999): loss=0.4179221688705435\n",
      "Log Regression(804/2999): loss=0.41792199680558956\n",
      "Log Regression(805/2999): loss=0.41792198541484776\n",
      "Log Regression(806/2999): loss=0.41792196531467535\n",
      "Log Regression(807/2999): loss=0.4179218431852992\n",
      "Log Regression(808/2999): loss=0.41792158937223967\n",
      "Log Regression(809/2999): loss=0.41792128360661035\n",
      "Log Regression(810/2999): loss=0.41792118048365606\n",
      "Log Regression(811/2999): loss=0.4179209614877151\n",
      "Log Regression(812/2999): loss=0.4179208804820283\n",
      "Log Regression(813/2999): loss=0.4179206917876236\n",
      "Log Regression(814/2999): loss=0.41792096008267815\n",
      "Log Regression(815/2999): loss=0.41792074459084166\n",
      "Log Regression(816/2999): loss=0.41792048553247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(817/2999): loss=0.41792038421989547\n",
      "Log Regression(818/2999): loss=0.4179202614949436\n",
      "Log Regression(819/2999): loss=0.4179204876098948\n",
      "Log Regression(820/2999): loss=0.41792016909666735\n",
      "Log Regression(821/2999): loss=0.4179200810870212\n",
      "Log Regression(822/2999): loss=0.4179200786286508\n",
      "Log Regression(823/2999): loss=0.41792015303019237\n",
      "Log Regression(824/2999): loss=0.41791999334340385\n",
      "Log Regression(825/2999): loss=0.4179197892841806\n",
      "Log Regression(826/2999): loss=0.41792000790693945\n",
      "Log Regression(827/2999): loss=0.4179200516198905\n",
      "Log Regression(828/2999): loss=0.41792011347193225\n",
      "Log Regression(829/2999): loss=0.4179199927739741\n",
      "Log Regression(830/2999): loss=0.4179200142402187\n",
      "Log Regression(831/2999): loss=0.4179194644396501\n",
      "Log Regression(832/2999): loss=0.4179196674552657\n",
      "Log Regression(833/2999): loss=0.4179195493995288\n",
      "Log Regression(834/2999): loss=0.41791952458750886\n",
      "Log Regression(835/2999): loss=0.4179194952817757\n",
      "Log Regression(836/2999): loss=0.417919444066586\n",
      "Log Regression(837/2999): loss=0.41791946386719064\n",
      "Log Regression(838/2999): loss=0.4179196238038229\n",
      "Log Regression(839/2999): loss=0.417919247805763\n",
      "Log Regression(840/2999): loss=0.4179189740843939\n",
      "Log Regression(841/2999): loss=0.41791922324691344\n",
      "Log Regression(842/2999): loss=0.4179190358480678\n",
      "Log Regression(843/2999): loss=0.4179189287974293\n",
      "Log Regression(844/2999): loss=0.41791859794481295\n",
      "Log Regression(845/2999): loss=0.41791867862419113\n",
      "Log Regression(846/2999): loss=0.41791847117748915\n",
      "Log Regression(847/2999): loss=0.4179181838154811\n",
      "Log Regression(848/2999): loss=0.41791803312292986\n",
      "Log Regression(849/2999): loss=0.4179183344375991\n",
      "Log Regression(850/2999): loss=0.4179181819467632\n",
      "Log Regression(851/2999): loss=0.4179182442646057\n",
      "Log Regression(852/2999): loss=0.4179182729636947\n",
      "Log Regression(853/2999): loss=0.4179183949217627\n",
      "Log Regression(854/2999): loss=0.41791824375143055\n",
      "Log Regression(855/2999): loss=0.417918085776817\n",
      "Log Regression(856/2999): loss=0.41791788991773215\n",
      "Log Regression(857/2999): loss=0.41791821704982823\n",
      "Log Regression(858/2999): loss=0.4179184515984334\n",
      "Log Regression(859/2999): loss=0.41791843169155707\n",
      "Log Regression(860/2999): loss=0.4179183959085019\n",
      "Log Regression(861/2999): loss=0.4179188015468529\n",
      "Log Regression(862/2999): loss=0.41791850311371376\n",
      "Log Regression(863/2999): loss=0.417918301871477\n",
      "Log Regression(864/2999): loss=0.417918346821043\n",
      "Log Regression(865/2999): loss=0.41791880625621985\n",
      "Log Regression(866/2999): loss=0.41791937188857436\n",
      "Log Regression(867/2999): loss=0.4179194670770495\n",
      "Log Regression(868/2999): loss=0.41791941415248085\n",
      "Log Regression(869/2999): loss=0.4179190295012705\n",
      "Log Regression(870/2999): loss=0.4179189821877946\n",
      "Log Regression(871/2999): loss=0.4179188841095832\n",
      "Log Regression(872/2999): loss=0.41791847695214945\n",
      "Log Regression(873/2999): loss=0.41791830885756803\n",
      "Log Regression(874/2999): loss=0.4179181700193979\n",
      "Log Regression(875/2999): loss=0.41791773638631924\n",
      "Log Regression(876/2999): loss=0.4179176348237786\n",
      "Log Regression(877/2999): loss=0.41791772156847956\n",
      "Log Regression(878/2999): loss=0.41791737998995016\n",
      "Log Regression(879/2999): loss=0.4179174296512354\n",
      "Log Regression(880/2999): loss=0.4179172554972398\n",
      "Log Regression(881/2999): loss=0.4179177261651257\n",
      "Log Regression(882/2999): loss=0.4179177117740882\n",
      "Log Regression(883/2999): loss=0.417917594466636\n",
      "Log Regression(884/2999): loss=0.41791719187028065\n",
      "Log Regression(885/2999): loss=0.4179175114027134\n",
      "Log Regression(886/2999): loss=0.41791741316864695\n",
      "Log Regression(887/2999): loss=0.4179171055607916\n",
      "Log Regression(888/2999): loss=0.41791678980760927\n",
      "Log Regression(889/2999): loss=0.4179168197989579\n",
      "Log Regression(890/2999): loss=0.41791692843710426\n",
      "Log Regression(891/2999): loss=0.4179171830743847\n",
      "Log Regression(892/2999): loss=0.41791693065363944\n",
      "Log Regression(893/2999): loss=0.4179168062263471\n",
      "Log Regression(894/2999): loss=0.4179170225214594\n",
      "Log Regression(895/2999): loss=0.4179170774240405\n",
      "Log Regression(896/2999): loss=0.4179170415818122\n",
      "Log Regression(897/2999): loss=0.4179166691411572\n",
      "Log Regression(898/2999): loss=0.417916769625308\n",
      "Log Regression(899/2999): loss=0.41791700407451504\n",
      "Log Regression(900/2999): loss=0.41791737564031317\n",
      "Log Regression(901/2999): loss=0.4179171337406254\n",
      "Log Regression(902/2999): loss=0.41791656144375405\n",
      "Log Regression(903/2999): loss=0.4179165839101995\n",
      "Log Regression(904/2999): loss=0.4179165891395467\n",
      "Log Regression(905/2999): loss=0.4179169947083459\n",
      "Log Regression(906/2999): loss=0.4179170313360614\n",
      "Log Regression(907/2999): loss=0.4179169370871965\n",
      "Log Regression(908/2999): loss=0.417916877740886\n",
      "Log Regression(909/2999): loss=0.4179169453421301\n",
      "Log Regression(910/2999): loss=0.41791695796007644\n",
      "Log Regression(911/2999): loss=0.4179173673886524\n",
      "Log Regression(912/2999): loss=0.4179172267269699\n",
      "Log Regression(913/2999): loss=0.41791706635437087\n",
      "Log Regression(914/2999): loss=0.41791729164816793\n",
      "Log Regression(915/2999): loss=0.41791737454588285\n",
      "Log Regression(916/2999): loss=0.4179176037923891\n",
      "Log Regression(917/2999): loss=0.41791764908833684\n",
      "Log Regression(918/2999): loss=0.4179173683264892\n",
      "Log Regression(919/2999): loss=0.41791736582567185\n",
      "Log Regression(920/2999): loss=0.41791719008104267\n",
      "Log Regression(921/2999): loss=0.41791700774318674\n",
      "Log Regression(922/2999): loss=0.41791690567840206\n",
      "Log Regression(923/2999): loss=0.41791689662925446\n",
      "Log Regression(924/2999): loss=0.4179170252190665\n",
      "Log Regression(925/2999): loss=0.4179167524361767\n",
      "Log Regression(926/2999): loss=0.4179165668213443\n",
      "Log Regression(927/2999): loss=0.41791620701721943\n",
      "Log Regression(928/2999): loss=0.4179162521380848\n",
      "Log Regression(929/2999): loss=0.41791643844619014\n",
      "Log Regression(930/2999): loss=0.41791639697119687\n",
      "Log Regression(931/2999): loss=0.4179166223241957\n",
      "Log Regression(932/2999): loss=0.4179166271220248\n",
      "Log Regression(933/2999): loss=0.4179164058406557\n",
      "Log Regression(934/2999): loss=0.417916846218916\n",
      "Log Regression(935/2999): loss=0.4179170656868442\n",
      "Log Regression(936/2999): loss=0.4179170426714724\n",
      "Log Regression(937/2999): loss=0.41791695199900114\n",
      "Log Regression(938/2999): loss=0.4179166479762605\n",
      "Log Regression(939/2999): loss=0.4179164571618885\n",
      "Log Regression(940/2999): loss=0.41791633073470835\n",
      "Log Regression(941/2999): loss=0.4179163398489274\n",
      "Log Regression(942/2999): loss=0.4179164677015063\n",
      "Log Regression(943/2999): loss=0.41791613673567096\n",
      "Log Regression(944/2999): loss=0.41791610297669696\n",
      "Log Regression(945/2999): loss=0.41791622213601337\n",
      "Log Regression(946/2999): loss=0.41791626824510403\n",
      "Log Regression(947/2999): loss=0.41791626140459215\n",
      "Log Regression(948/2999): loss=0.4179163596440246\n",
      "Log Regression(949/2999): loss=0.41791628522913515\n",
      "Log Regression(950/2999): loss=0.4179164417336736\n",
      "Log Regression(951/2999): loss=0.4179162162480346\n",
      "Log Regression(952/2999): loss=0.4179158877852933\n",
      "Log Regression(953/2999): loss=0.41791608920075\n",
      "Log Regression(954/2999): loss=0.4179161801017943\n",
      "Log Regression(955/2999): loss=0.41791602863087246\n",
      "Log Regression(956/2999): loss=0.41791586710520395\n",
      "Log Regression(957/2999): loss=0.4179160925873351\n",
      "Log Regression(958/2999): loss=0.41791594082870154\n",
      "Log Regression(959/2999): loss=0.41791561192254456\n",
      "Log Regression(960/2999): loss=0.4179158537434792\n",
      "Log Regression(961/2999): loss=0.41791572164328167\n",
      "Log Regression(962/2999): loss=0.417915590894727\n",
      "Log Regression(963/2999): loss=0.4179157088666806\n",
      "Log Regression(964/2999): loss=0.4179155281507522\n",
      "Log Regression(965/2999): loss=0.4179155387838534\n",
      "Log Regression(966/2999): loss=0.4179155305628343\n",
      "Log Regression(967/2999): loss=0.4179156330329685\n",
      "Log Regression(968/2999): loss=0.4179156379482186\n",
      "Log Regression(969/2999): loss=0.41791594379848185\n",
      "Log Regression(970/2999): loss=0.41791552516644603\n",
      "Log Regression(971/2999): loss=0.4179155185392791\n",
      "Log Regression(972/2999): loss=0.4179154750210174\n",
      "Log Regression(973/2999): loss=0.41791557133859525\n",
      "Log Regression(974/2999): loss=0.4179154849543602\n",
      "Log Regression(975/2999): loss=0.4179153180578112\n",
      "Log Regression(976/2999): loss=0.41791536157636244\n",
      "Log Regression(977/2999): loss=0.4179154498913765\n",
      "Log Regression(978/2999): loss=0.41791522599318165\n",
      "Log Regression(979/2999): loss=0.4179148720059757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(980/2999): loss=0.417914557693887\n",
      "Log Regression(981/2999): loss=0.4179143876854833\n",
      "Log Regression(982/2999): loss=0.41791417940878073\n",
      "Log Regression(983/2999): loss=0.41791414905671936\n",
      "Log Regression(984/2999): loss=0.4179139220428515\n",
      "Log Regression(985/2999): loss=0.41791360902774605\n",
      "Log Regression(986/2999): loss=0.41791395878643073\n",
      "Log Regression(987/2999): loss=0.4179143843904501\n",
      "Log Regression(988/2999): loss=0.41791437431012857\n",
      "Log Regression(989/2999): loss=0.41791442133863826\n",
      "Log Regression(990/2999): loss=0.41791414467614474\n",
      "Log Regression(991/2999): loss=0.4179142058996378\n",
      "Log Regression(992/2999): loss=0.417914295876186\n",
      "Log Regression(993/2999): loss=0.417914772165597\n",
      "Log Regression(994/2999): loss=0.41791473678011426\n",
      "Log Regression(995/2999): loss=0.4179139611127433\n",
      "Log Regression(996/2999): loss=0.4179138858122281\n",
      "Log Regression(997/2999): loss=0.41791418681744413\n",
      "Log Regression(998/2999): loss=0.4179141372084443\n",
      "Log Regression(999/2999): loss=0.4179139312458855\n",
      "Log Regression(1000/2999): loss=0.41791393296543117\n",
      "Log Regression(1001/2999): loss=0.41791390113680654\n",
      "Log Regression(1002/2999): loss=0.41791378743523255\n",
      "Log Regression(1003/2999): loss=0.41791340366199053\n",
      "Log Regression(1004/2999): loss=0.41791355927995794\n",
      "Log Regression(1005/2999): loss=0.4179139854554557\n",
      "Log Regression(1006/2999): loss=0.41791395839664447\n",
      "Log Regression(1007/2999): loss=0.41791380367549563\n",
      "Log Regression(1008/2999): loss=0.41791371626146734\n",
      "Log Regression(1009/2999): loss=0.4179136318319296\n",
      "Log Regression(1010/2999): loss=0.41791345251177736\n",
      "Log Regression(1011/2999): loss=0.41791332372828227\n",
      "Log Regression(1012/2999): loss=0.41791337947101226\n",
      "Log Regression(1013/2999): loss=0.4179131445050708\n",
      "Log Regression(1014/2999): loss=0.417913214018531\n",
      "Log Regression(1015/2999): loss=0.4179131336305786\n",
      "Log Regression(1016/2999): loss=0.4179130237319883\n",
      "Log Regression(1017/2999): loss=0.4179126483638629\n",
      "Log Regression(1018/2999): loss=0.4179125801841321\n",
      "Log Regression(1019/2999): loss=0.4179125827997669\n",
      "Log Regression(1020/2999): loss=0.4179126326791096\n",
      "Log Regression(1021/2999): loss=0.4179128758612362\n",
      "Log Regression(1022/2999): loss=0.41791297271558\n",
      "Log Regression(1023/2999): loss=0.4179128677647348\n",
      "Log Regression(1024/2999): loss=0.4179126055463208\n",
      "Log Regression(1025/2999): loss=0.41791258672034837\n",
      "Log Regression(1026/2999): loss=0.4179127481755732\n",
      "Log Regression(1027/2999): loss=0.41791298514749103\n",
      "Log Regression(1028/2999): loss=0.4179128276398698\n",
      "Log Regression(1029/2999): loss=0.41791303829718807\n",
      "Log Regression(1030/2999): loss=0.41791257921627833\n",
      "Log Regression(1031/2999): loss=0.41791294662125883\n",
      "Log Regression(1032/2999): loss=0.41791276055507653\n",
      "Log Regression(1033/2999): loss=0.41791303997273777\n",
      "Log Regression(1034/2999): loss=0.4179125939952743\n",
      "Log Regression(1035/2999): loss=0.4179124691618817\n",
      "Log Regression(1036/2999): loss=0.4179123924515308\n",
      "Log Regression(1037/2999): loss=0.4179121563916175\n",
      "Log Regression(1038/2999): loss=0.41791189716197213\n",
      "Log Regression(1039/2999): loss=0.4179115733013658\n",
      "Log Regression(1040/2999): loss=0.417911614349921\n",
      "Log Regression(1041/2999): loss=0.4179116074450479\n",
      "Log Regression(1042/2999): loss=0.4179114894769089\n",
      "Log Regression(1043/2999): loss=0.4179115373108312\n",
      "Log Regression(1044/2999): loss=0.4179123396192153\n",
      "Log Regression(1045/2999): loss=0.41791250188227463\n",
      "Log Regression(1046/2999): loss=0.4179124109212969\n",
      "Log Regression(1047/2999): loss=0.4179122272572887\n",
      "Log Regression(1048/2999): loss=0.41791198600312\n",
      "Log Regression(1049/2999): loss=0.41791185569672445\n",
      "Log Regression(1050/2999): loss=0.41791217529710684\n",
      "Log Regression(1051/2999): loss=0.41791217067817654\n",
      "Log Regression(1052/2999): loss=0.4179122673925963\n",
      "Log Regression(1053/2999): loss=0.4179122523747965\n",
      "Log Regression(1054/2999): loss=0.41791220939969026\n",
      "Log Regression(1055/2999): loss=0.41791190686109975\n",
      "Log Regression(1056/2999): loss=0.41791194834558804\n",
      "Log Regression(1057/2999): loss=0.4179116377031737\n",
      "Log Regression(1058/2999): loss=0.41791128130047955\n",
      "Log Regression(1059/2999): loss=0.4179113043156746\n",
      "Log Regression(1060/2999): loss=0.41791127930325656\n",
      "Log Regression(1061/2999): loss=0.4179109567897432\n",
      "Log Regression(1062/2999): loss=0.4179107946084445\n",
      "Log Regression(1063/2999): loss=0.41791067343139465\n",
      "Log Regression(1064/2999): loss=0.417910907235644\n",
      "Log Regression(1065/2999): loss=0.417911031999798\n",
      "Log Regression(1066/2999): loss=0.41791084415726015\n",
      "Log Regression(1067/2999): loss=0.41791090623933386\n",
      "Log Regression(1068/2999): loss=0.4179110759518487\n",
      "Log Regression(1069/2999): loss=0.4179114309306997\n",
      "Log Regression(1070/2999): loss=0.41791144645917083\n",
      "Log Regression(1071/2999): loss=0.41791139591671367\n",
      "Log Regression(1072/2999): loss=0.41791138551648616\n",
      "Log Regression(1073/2999): loss=0.417911378787218\n",
      "Log Regression(1074/2999): loss=0.4179116867908675\n",
      "Log Regression(1075/2999): loss=0.4179119371434234\n",
      "Log Regression(1076/2999): loss=0.4179114469355453\n",
      "Log Regression(1077/2999): loss=0.41791154006447684\n",
      "Log Regression(1078/2999): loss=0.4179117618510113\n",
      "Log Regression(1079/2999): loss=0.4179116077492182\n",
      "Log Regression(1080/2999): loss=0.41791153675139014\n",
      "Log Regression(1081/2999): loss=0.41791138553454205\n",
      "Log Regression(1082/2999): loss=0.41791130042763147\n",
      "Log Regression(1083/2999): loss=0.41791175593226426\n",
      "Log Regression(1084/2999): loss=0.41791152154610756\n",
      "Log Regression(1085/2999): loss=0.4179112493112735\n",
      "Log Regression(1086/2999): loss=0.41791119137711785\n",
      "Log Regression(1087/2999): loss=0.4179113708886104\n",
      "Log Regression(1088/2999): loss=0.4179111424719334\n",
      "Log Regression(1089/2999): loss=0.4179111242317847\n",
      "Log Regression(1090/2999): loss=0.41791127285975893\n",
      "Log Regression(1091/2999): loss=0.4179111062296621\n",
      "Log Regression(1092/2999): loss=0.4179114883320047\n",
      "Log Regression(1093/2999): loss=0.4179117185829629\n",
      "Log Regression(1094/2999): loss=0.4179119203149916\n",
      "Log Regression(1095/2999): loss=0.41791214539320054\n",
      "Log Regression(1096/2999): loss=0.4179123267839418\n",
      "Log Regression(1097/2999): loss=0.41791213213153244\n",
      "Log Regression(1098/2999): loss=0.4179118553584739\n",
      "Log Regression(1099/2999): loss=0.41791188749007363\n",
      "Log Regression(1100/2999): loss=0.41791204506808965\n",
      "Log Regression(1101/2999): loss=0.41791228933428676\n",
      "Log Regression(1102/2999): loss=0.41791261551134523\n",
      "Log Regression(1103/2999): loss=0.417912439258341\n",
      "Log Regression(1104/2999): loss=0.41791231713025406\n",
      "Log Regression(1105/2999): loss=0.4179122828921912\n",
      "Log Regression(1106/2999): loss=0.4179120574664984\n",
      "Log Regression(1107/2999): loss=0.41791185726742397\n",
      "Log Regression(1108/2999): loss=0.4179116846798426\n",
      "Log Regression(1109/2999): loss=0.4179119854352029\n",
      "Log Regression(1110/2999): loss=0.4179117537944744\n",
      "Log Regression(1111/2999): loss=0.4179114149437934\n",
      "Log Regression(1112/2999): loss=0.4179116607683124\n",
      "Log Regression(1113/2999): loss=0.4179116598607955\n",
      "Log Regression(1114/2999): loss=0.4179117520641325\n",
      "Log Regression(1115/2999): loss=0.4179112394320088\n",
      "Log Regression(1116/2999): loss=0.4179112239184344\n",
      "Log Regression(1117/2999): loss=0.41791111169491035\n",
      "Log Regression(1118/2999): loss=0.41791014876865\n",
      "Log Regression(1119/2999): loss=0.41791017589872387\n",
      "Log Regression(1120/2999): loss=0.4179100859384539\n",
      "Log Regression(1121/2999): loss=0.417909578744992\n",
      "Log Regression(1122/2999): loss=0.41790936394436995\n",
      "Log Regression(1123/2999): loss=0.41790890001366815\n",
      "Log Regression(1124/2999): loss=0.41790883504724136\n",
      "Log Regression(1125/2999): loss=0.4179086969336399\n",
      "Log Regression(1126/2999): loss=0.41790844968736673\n",
      "Log Regression(1127/2999): loss=0.41790829977386484\n",
      "Log Regression(1128/2999): loss=0.41790834475511995\n",
      "Log Regression(1129/2999): loss=0.4179081218306345\n",
      "Log Regression(1130/2999): loss=0.4179080119855629\n",
      "Log Regression(1131/2999): loss=0.4179079307945955\n",
      "Log Regression(1132/2999): loss=0.4179077632004959\n",
      "Log Regression(1133/2999): loss=0.4179078340565361\n",
      "Log Regression(1134/2999): loss=0.417907740140204\n",
      "Log Regression(1135/2999): loss=0.4179075892904019\n",
      "Log Regression(1136/2999): loss=0.4179078612787585\n",
      "Log Regression(1137/2999): loss=0.4179083649927339\n",
      "Log Regression(1138/2999): loss=0.41790850433577675\n",
      "Log Regression(1139/2999): loss=0.41790860336442165\n",
      "Log Regression(1140/2999): loss=0.4179082187115609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(1141/2999): loss=0.41790766583428385\n",
      "Log Regression(1142/2999): loss=0.4179076080580054\n",
      "Log Regression(1143/2999): loss=0.4179075108751143\n",
      "Log Regression(1144/2999): loss=0.41790732887837495\n",
      "Log Regression(1145/2999): loss=0.4179077228236583\n",
      "Log Regression(1146/2999): loss=0.41790786711028366\n",
      "Log Regression(1147/2999): loss=0.4179083086852413\n",
      "Log Regression(1148/2999): loss=0.4179081183582518\n",
      "Log Regression(1149/2999): loss=0.41790801511448455\n",
      "Log Regression(1150/2999): loss=0.4179078090799284\n",
      "Log Regression(1151/2999): loss=0.4179075140805627\n",
      "Log Regression(1152/2999): loss=0.4179076252311314\n",
      "Log Regression(1153/2999): loss=0.41790772166071716\n",
      "Log Regression(1154/2999): loss=0.4179075579700457\n",
      "Log Regression(1155/2999): loss=0.4179080697185391\n",
      "Log Regression(1156/2999): loss=0.417908223114534\n",
      "Log Regression(1157/2999): loss=0.4179087748080137\n",
      "Log Regression(1158/2999): loss=0.4179091537889562\n",
      "Log Regression(1159/2999): loss=0.4179089897986428\n",
      "Log Regression(1160/2999): loss=0.4179088309849208\n",
      "Log Regression(1161/2999): loss=0.41790850665301277\n",
      "Log Regression(1162/2999): loss=0.4179087901457772\n",
      "Log Regression(1163/2999): loss=0.41790890873483194\n",
      "Log Regression(1164/2999): loss=0.417909047915104\n",
      "Log Regression(1165/2999): loss=0.41790876272691946\n",
      "Log Regression(1166/2999): loss=0.41790863966508623\n",
      "Log Regression(1167/2999): loss=0.41790837547567083\n",
      "Log Regression(1168/2999): loss=0.4179082838847185\n",
      "Log Regression(1169/2999): loss=0.4179081343280463\n",
      "Log Regression(1170/2999): loss=0.41790806390696106\n",
      "Log Regression(1171/2999): loss=0.41790829311759886\n",
      "Log Regression(1172/2999): loss=0.41790868287044897\n",
      "Log Regression(1173/2999): loss=0.4179084116153942\n",
      "Log Regression(1174/2999): loss=0.4179083393822543\n",
      "Log Regression(1175/2999): loss=0.4179084838544475\n",
      "Log Regression(1176/2999): loss=0.41790862284285984\n",
      "Log Regression(1177/2999): loss=0.4179084662184654\n",
      "Log Regression(1178/2999): loss=0.41790834582233605\n",
      "Log Regression(1179/2999): loss=0.4179083648546529\n",
      "Log Regression(1180/2999): loss=0.4179084240035406\n",
      "Log Regression(1181/2999): loss=0.41790886504822844\n",
      "Log Regression(1182/2999): loss=0.4179092023452309\n",
      "Log Regression(1183/2999): loss=0.417909038522615\n",
      "Log Regression(1184/2999): loss=0.41790930542991805\n",
      "Log Regression(1185/2999): loss=0.41790935199675705\n",
      "Log Regression(1186/2999): loss=0.41790934231865223\n",
      "Log Regression(1187/2999): loss=0.4179093714796862\n",
      "Log Regression(1188/2999): loss=0.4179094599245884\n",
      "Log Regression(1189/2999): loss=0.4179100319769053\n",
      "Log Regression(1190/2999): loss=0.4179101032045504\n",
      "Log Regression(1191/2999): loss=0.4179100289203734\n",
      "Log Regression(1192/2999): loss=0.41790966946702257\n",
      "Log Regression(1193/2999): loss=0.41790966318901285\n",
      "Log Regression(1194/2999): loss=0.4179098248011397\n",
      "Log Regression(1195/2999): loss=0.41790965781583495\n",
      "Log Regression(1196/2999): loss=0.4179095659456546\n",
      "Log Regression(1197/2999): loss=0.4179096333238922\n",
      "Log Regression(1198/2999): loss=0.41790991550220913\n",
      "Log Regression(1199/2999): loss=0.41790969982148135\n",
      "Log Regression(1200/2999): loss=0.4179094087544519\n",
      "Log Regression(1201/2999): loss=0.41790940633321016\n",
      "Log Regression(1202/2999): loss=0.4179090965627643\n",
      "Log Regression(1203/2999): loss=0.4179090986994059\n",
      "Log Regression(1204/2999): loss=0.4179091247433956\n",
      "Log Regression(1205/2999): loss=0.41790921454533914\n",
      "Log Regression(1206/2999): loss=0.41790908106910013\n",
      "Log Regression(1207/2999): loss=0.41790882389856604\n",
      "Log Regression(1208/2999): loss=0.4179089423596086\n",
      "Log Regression(1209/2999): loss=0.417908772955375\n",
      "Log Regression(1210/2999): loss=0.4179088183003325\n",
      "Log Regression(1211/2999): loss=0.41790894950770846\n",
      "Log Regression(1212/2999): loss=0.4179092444036776\n",
      "Log Regression(1213/2999): loss=0.41790950971267793\n",
      "Log Regression(1214/2999): loss=0.41790983908200285\n",
      "Log Regression(1215/2999): loss=0.41790990110068926\n",
      "Log Regression(1216/2999): loss=0.41790991894510005\n",
      "Log Regression(1217/2999): loss=0.41790940986232167\n",
      "Log Regression(1218/2999): loss=0.4179094388165271\n",
      "Log Regression(1219/2999): loss=0.4179094225884472\n",
      "Log Regression(1220/2999): loss=0.4179094103169455\n",
      "Log Regression(1221/2999): loss=0.41790930745575\n",
      "Log Regression(1222/2999): loss=0.41790973685609484\n",
      "Log Regression(1223/2999): loss=0.4179097390876417\n",
      "Log Regression(1224/2999): loss=0.41790972653234704\n",
      "Log Regression(1225/2999): loss=0.4179097834923793\n",
      "Log Regression(1226/2999): loss=0.4179097748517766\n",
      "Log Regression(1227/2999): loss=0.4179097217107443\n",
      "Log Regression(1228/2999): loss=0.4179100609348253\n",
      "Log Regression(1229/2999): loss=0.41790993508634244\n",
      "Log Regression(1230/2999): loss=0.4179097125456274\n",
      "Log Regression(1231/2999): loss=0.41790981581514214\n",
      "Log Regression(1232/2999): loss=0.4179098235631475\n",
      "Log Regression(1233/2999): loss=0.4179100564621196\n",
      "Log Regression(1234/2999): loss=0.4179102679635466\n",
      "Log Regression(1235/2999): loss=0.41791030834294257\n",
      "Log Regression(1236/2999): loss=0.4179102049036309\n",
      "Log Regression(1237/2999): loss=0.4179099374951982\n",
      "Log Regression(1238/2999): loss=0.4179101767121779\n",
      "Log Regression(1239/2999): loss=0.4179102797968483\n",
      "Log Regression(1240/2999): loss=0.41791024284485095\n",
      "Log Regression(1241/2999): loss=0.4179103660562547\n",
      "Log Regression(1242/2999): loss=0.41791038705956285\n",
      "Log Regression(1243/2999): loss=0.41790995565238753\n",
      "Log Regression(1244/2999): loss=0.4179101098048403\n",
      "Log Regression(1245/2999): loss=0.417909766433964\n",
      "Log Regression(1246/2999): loss=0.4179096811962248\n",
      "Log Regression(1247/2999): loss=0.41790962078823274\n",
      "Log Regression(1248/2999): loss=0.41790967871072465\n",
      "Log Regression(1249/2999): loss=0.4179093695689914\n",
      "Log Regression(1250/2999): loss=0.4179095785804584\n",
      "Log Regression(1251/2999): loss=0.4179095482023331\n",
      "Log Regression(1252/2999): loss=0.41790970608544614\n",
      "Log Regression(1253/2999): loss=0.41790961787420866\n",
      "Log Regression(1254/2999): loss=0.4179094157333619\n",
      "Log Regression(1255/2999): loss=0.4179097866152064\n",
      "Log Regression(1256/2999): loss=0.4179097674100484\n",
      "Log Regression(1257/2999): loss=0.41790934554589276\n",
      "Log Regression(1258/2999): loss=0.4179094803504908\n",
      "Log Regression(1259/2999): loss=0.41790951138549437\n",
      "Log Regression(1260/2999): loss=0.4179095180489363\n",
      "Log Regression(1261/2999): loss=0.4179094648752652\n",
      "Log Regression(1262/2999): loss=0.4179097215831249\n",
      "Log Regression(1263/2999): loss=0.41790970542992045\n",
      "Log Regression(1264/2999): loss=0.4179091795900806\n",
      "Log Regression(1265/2999): loss=0.4179092391983234\n",
      "Log Regression(1266/2999): loss=0.41790910221065425\n",
      "Log Regression(1267/2999): loss=0.4179095462434415\n",
      "Log Regression(1268/2999): loss=0.4179093781725692\n",
      "Log Regression(1269/2999): loss=0.4179093231843891\n",
      "Log Regression(1270/2999): loss=0.4179096106761593\n",
      "Log Regression(1271/2999): loss=0.4179093592770775\n",
      "Log Regression(1272/2999): loss=0.4179096006042464\n",
      "Log Regression(1273/2999): loss=0.4179095654652923\n",
      "Log Regression(1274/2999): loss=0.41790981918821685\n",
      "Log Regression(1275/2999): loss=0.4179100697724637\n",
      "Log Regression(1276/2999): loss=0.41790984528039726\n",
      "Log Regression(1277/2999): loss=0.417909623129799\n",
      "Log Regression(1278/2999): loss=0.4179096261505173\n",
      "Log Regression(1279/2999): loss=0.41790977631895454\n",
      "Log Regression(1280/2999): loss=0.4179094166567321\n",
      "Log Regression(1281/2999): loss=0.4179092748887971\n",
      "Log Regression(1282/2999): loss=0.4179093613475582\n",
      "Log Regression(1283/2999): loss=0.41790924900149845\n",
      "Log Regression(1284/2999): loss=0.41790922830529936\n",
      "Log Regression(1285/2999): loss=0.41790912687097637\n",
      "Log Regression(1286/2999): loss=0.41790930634021545\n",
      "Log Regression(1287/2999): loss=0.41790930454024405\n",
      "Log Regression(1288/2999): loss=0.41790875250078746\n",
      "Log Regression(1289/2999): loss=0.4179088871027093\n",
      "Log Regression(1290/2999): loss=0.41790883303850757\n",
      "Log Regression(1291/2999): loss=0.4179087641284242\n",
      "Log Regression(1292/2999): loss=0.4179088370978976\n",
      "Log Regression(1293/2999): loss=0.4179088967947019\n",
      "Log Regression(1294/2999): loss=0.417909200350514\n",
      "Log Regression(1295/2999): loss=0.4179088497950304\n",
      "Log Regression(1296/2999): loss=0.4179088645087995\n",
      "Log Regression(1297/2999): loss=0.4179088207065304\n",
      "Log Regression(1298/2999): loss=0.4179087175549209\n",
      "Log Regression(1299/2999): loss=0.41790861821742287\n",
      "Log Regression(1300/2999): loss=0.4179085454974086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(1301/2999): loss=0.4179082087000233\n",
      "Log Regression(1302/2999): loss=0.41790841624736214\n",
      "Log Regression(1303/2999): loss=0.4179085469158649\n",
      "Log Regression(1304/2999): loss=0.41790887550450756\n",
      "Log Regression(1305/2999): loss=0.41790838581482004\n",
      "Log Regression(1306/2999): loss=0.41790833023739765\n",
      "Log Regression(1307/2999): loss=0.41790831047123345\n",
      "Log Regression(1308/2999): loss=0.4179076293556855\n",
      "Log Regression(1309/2999): loss=0.4179073408009814\n",
      "Log Regression(1310/2999): loss=0.4179074852701325\n",
      "Log Regression(1311/2999): loss=0.41790736794502154\n",
      "Log Regression(1312/2999): loss=0.41790717042515935\n",
      "Log Regression(1313/2999): loss=0.41790717474471395\n",
      "Log Regression(1314/2999): loss=0.4179072957060881\n",
      "Log Regression(1315/2999): loss=0.4179075808373378\n",
      "Log Regression(1316/2999): loss=0.4179074514406576\n",
      "Log Regression(1317/2999): loss=0.4179073392271713\n",
      "Log Regression(1318/2999): loss=0.41790722119920776\n",
      "Log Regression(1319/2999): loss=0.4179067143725492\n",
      "Log Regression(1320/2999): loss=0.41790651524503974\n",
      "Log Regression(1321/2999): loss=0.4179071651264344\n",
      "Log Regression(1322/2999): loss=0.41790688302439916\n",
      "Log Regression(1323/2999): loss=0.41790700708355055\n",
      "Log Regression(1324/2999): loss=0.41790697257376763\n",
      "Log Regression(1325/2999): loss=0.41790714576899446\n",
      "Log Regression(1326/2999): loss=0.4179071735648385\n",
      "Log Regression(1327/2999): loss=0.417907029996792\n",
      "Log Regression(1328/2999): loss=0.41790709676970783\n",
      "Log Regression(1329/2999): loss=0.41790693257311234\n",
      "Log Regression(1330/2999): loss=0.41790669602774166\n",
      "Log Regression(1331/2999): loss=0.41790625102915624\n",
      "Log Regression(1332/2999): loss=0.4179064208365047\n",
      "Log Regression(1333/2999): loss=0.4179062195800078\n",
      "Log Regression(1334/2999): loss=0.417906171027872\n",
      "Log Regression(1335/2999): loss=0.41790599293144587\n",
      "Log Regression(1336/2999): loss=0.41790605068723746\n",
      "Log Regression(1337/2999): loss=0.4179061175898443\n",
      "Log Regression(1338/2999): loss=0.417906309362653\n",
      "Log Regression(1339/2999): loss=0.4179065161257893\n",
      "Log Regression(1340/2999): loss=0.41790630925515265\n",
      "Log Regression(1341/2999): loss=0.4179066223313014\n",
      "Log Regression(1342/2999): loss=0.41790646537116605\n",
      "Log Regression(1343/2999): loss=0.4179062347219256\n",
      "Log Regression(1344/2999): loss=0.41790660444179023\n",
      "Log Regression(1345/2999): loss=0.41790639766101023\n",
      "Log Regression(1346/2999): loss=0.41790625345539484\n",
      "Log Regression(1347/2999): loss=0.41790573844952916\n",
      "Log Regression(1348/2999): loss=0.41790589844363674\n",
      "Log Regression(1349/2999): loss=0.4179062213172366\n",
      "Log Regression(1350/2999): loss=0.417905949443526\n",
      "Log Regression(1351/2999): loss=0.41790573894700594\n",
      "Log Regression(1352/2999): loss=0.41790604048677105\n",
      "Log Regression(1353/2999): loss=0.41790587821407715\n",
      "Log Regression(1354/2999): loss=0.41790573475054765\n",
      "Log Regression(1355/2999): loss=0.41790551105173135\n",
      "Log Regression(1356/2999): loss=0.417905229709242\n",
      "Log Regression(1357/2999): loss=0.41790578790989563\n",
      "Log Regression(1358/2999): loss=0.4179056979873446\n",
      "Log Regression(1359/2999): loss=0.417905629354698\n",
      "Log Regression(1360/2999): loss=0.41790541467831266\n",
      "Log Regression(1361/2999): loss=0.4179053234706198\n",
      "Log Regression(1362/2999): loss=0.41790574202094805\n",
      "Log Regression(1363/2999): loss=0.4179060926780108\n",
      "Log Regression(1364/2999): loss=0.417906159928022\n",
      "Log Regression(1365/2999): loss=0.4179064692180457\n",
      "Log Regression(1366/2999): loss=0.41790621772705283\n",
      "Log Regression(1367/2999): loss=0.41790687578726343\n",
      "Log Regression(1368/2999): loss=0.41790698071366367\n",
      "Log Regression(1369/2999): loss=0.4179072238928413\n",
      "Log Regression(1370/2999): loss=0.41790704942868556\n",
      "Log Regression(1371/2999): loss=0.41790704059507716\n",
      "Log Regression(1372/2999): loss=0.41790708830182977\n",
      "Log Regression(1373/2999): loss=0.41790718838743984\n",
      "Log Regression(1374/2999): loss=0.41790726712553394\n",
      "Log Regression(1375/2999): loss=0.4179070075956841\n",
      "Log Regression(1376/2999): loss=0.4179069664404888\n",
      "Log Regression(1377/2999): loss=0.41790677781702534\n",
      "Log Regression(1378/2999): loss=0.4179065840392835\n",
      "Log Regression(1379/2999): loss=0.41790631159959635\n",
      "Log Regression(1380/2999): loss=0.4179063241549231\n",
      "Log Regression(1381/2999): loss=0.417906430438152\n",
      "Log Regression(1382/2999): loss=0.41790629567846854\n",
      "Log Regression(1383/2999): loss=0.41790626746961246\n",
      "Log Regression(1384/2999): loss=0.41790657549741433\n",
      "Log Regression(1385/2999): loss=0.4179067780432856\n",
      "Log Regression(1386/2999): loss=0.4179065798939442\n",
      "Log Regression(1387/2999): loss=0.4179065805007924\n",
      "Log Regression(1388/2999): loss=0.41790698144280614\n",
      "Log Regression(1389/2999): loss=0.4179068533642223\n",
      "Log Regression(1390/2999): loss=0.4179065194947053\n",
      "Log Regression(1391/2999): loss=0.4179067622761229\n",
      "Log Regression(1392/2999): loss=0.41790716334561745\n",
      "Log Regression(1393/2999): loss=0.4179062229109698\n",
      "Log Regression(1394/2999): loss=0.4179062444995472\n",
      "Log Regression(1395/2999): loss=0.41790592952442035\n",
      "Log Regression(1396/2999): loss=0.4179057435271687\n",
      "Log Regression(1397/2999): loss=0.4179055540400997\n",
      "Log Regression(1398/2999): loss=0.4179058093242308\n",
      "Log Regression(1399/2999): loss=0.4179054715686881\n",
      "Log Regression(1400/2999): loss=0.41790544490985904\n",
      "Log Regression(1401/2999): loss=0.41790537257612276\n",
      "Log Regression(1402/2999): loss=0.41790532237436473\n",
      "Log Regression(1403/2999): loss=0.41790549173811103\n",
      "Log Regression(1404/2999): loss=0.417905641853086\n",
      "Log Regression(1405/2999): loss=0.41790580140846684\n",
      "Log Regression(1406/2999): loss=0.4179059749688825\n",
      "Log Regression(1407/2999): loss=0.4179057972205809\n",
      "Log Regression(1408/2999): loss=0.4179055130986995\n",
      "Log Regression(1409/2999): loss=0.41790536175575776\n",
      "Log Regression(1410/2999): loss=0.4179053911135171\n",
      "Log Regression(1411/2999): loss=0.417905364522613\n",
      "Log Regression(1412/2999): loss=0.4179056508450341\n",
      "Log Regression(1413/2999): loss=0.41790598848091765\n",
      "Log Regression(1414/2999): loss=0.41790539332391846\n",
      "Log Regression(1415/2999): loss=0.41790513868370877\n",
      "Log Regression(1416/2999): loss=0.4179051006327443\n",
      "Log Regression(1417/2999): loss=0.4179052234047402\n",
      "Log Regression(1418/2999): loss=0.4179046875211902\n",
      "Log Regression(1419/2999): loss=0.4179045158994537\n",
      "Log Regression(1420/2999): loss=0.4179045936943692\n",
      "Log Regression(1421/2999): loss=0.4179044843030577\n",
      "Log Regression(1422/2999): loss=0.4179045135039824\n",
      "Log Regression(1423/2999): loss=0.4179042852703449\n",
      "Log Regression(1424/2999): loss=0.4179040174705375\n",
      "Log Regression(1425/2999): loss=0.4179042805277108\n",
      "Log Regression(1426/2999): loss=0.4179040694727055\n",
      "Log Regression(1427/2999): loss=0.41790383964340794\n",
      "Log Regression(1428/2999): loss=0.41790409737709977\n",
      "Log Regression(1429/2999): loss=0.4179043442967374\n",
      "Log Regression(1430/2999): loss=0.41790352454179036\n",
      "Log Regression(1431/2999): loss=0.4179038754439993\n",
      "Log Regression(1432/2999): loss=0.4179042745422843\n",
      "Log Regression(1433/2999): loss=0.4179042263835735\n",
      "Log Regression(1434/2999): loss=0.4179040327877165\n",
      "Log Regression(1435/2999): loss=0.4179043296292929\n",
      "Log Regression(1436/2999): loss=0.41790446016338384\n",
      "Log Regression(1437/2999): loss=0.4179042269431423\n",
      "Log Regression(1438/2999): loss=0.4179051818950061\n",
      "Log Regression(1439/2999): loss=0.41790593873919507\n",
      "Log Regression(1440/2999): loss=0.41790601203678474\n",
      "Log Regression(1441/2999): loss=0.4179057564210903\n",
      "Log Regression(1442/2999): loss=0.4179058695583913\n",
      "Log Regression(1443/2999): loss=0.4179055664736445\n",
      "Log Regression(1444/2999): loss=0.41790557950711227\n",
      "Log Regression(1445/2999): loss=0.4179053072783218\n",
      "Log Regression(1446/2999): loss=0.41790521625500193\n",
      "Log Regression(1447/2999): loss=0.4179053254806331\n",
      "Log Regression(1448/2999): loss=0.4179055321903239\n",
      "Log Regression(1449/2999): loss=0.4179056346902376\n",
      "Log Regression(1450/2999): loss=0.4179056287086111\n",
      "Log Regression(1451/2999): loss=0.4179055977004103\n",
      "Log Regression(1452/2999): loss=0.4179056954408929\n",
      "Log Regression(1453/2999): loss=0.41790576728694007\n",
      "Log Regression(1454/2999): loss=0.4179054052451168\n",
      "Log Regression(1455/2999): loss=0.4179051391323803\n",
      "Log Regression(1456/2999): loss=0.41790554717907624\n",
      "Log Regression(1457/2999): loss=0.41790568298264374\n",
      "Log Regression(1458/2999): loss=0.41790560928933695\n",
      "Log Regression(1459/2999): loss=0.41790496182563897\n",
      "Log Regression(1460/2999): loss=0.417904871380912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(1461/2999): loss=0.41790465359530804\n",
      "Log Regression(1462/2999): loss=0.41790505357021995\n",
      "Log Regression(1463/2999): loss=0.4179047342258163\n",
      "Log Regression(1464/2999): loss=0.4179049305975269\n",
      "Log Regression(1465/2999): loss=0.4179050445838737\n",
      "Log Regression(1466/2999): loss=0.4179043451528623\n",
      "Log Regression(1467/2999): loss=0.4179040499388858\n",
      "Log Regression(1468/2999): loss=0.41790408859966294\n",
      "Log Regression(1469/2999): loss=0.41790402338592875\n",
      "Log Regression(1470/2999): loss=0.417904142976627\n",
      "Log Regression(1471/2999): loss=0.4179042238443917\n",
      "Log Regression(1472/2999): loss=0.4179041156975683\n",
      "Log Regression(1473/2999): loss=0.4179043263131825\n",
      "Log Regression(1474/2999): loss=0.41790392734209136\n",
      "Log Regression(1475/2999): loss=0.417904184056848\n",
      "Log Regression(1476/2999): loss=0.41790429014854524\n",
      "Log Regression(1477/2999): loss=0.41790446127402187\n",
      "Log Regression(1478/2999): loss=0.4179047013076237\n",
      "Log Regression(1479/2999): loss=0.4179048218790376\n",
      "Log Regression(1480/2999): loss=0.4179050022945178\n",
      "Log Regression(1481/2999): loss=0.4179048628468674\n",
      "Log Regression(1482/2999): loss=0.41790446655142444\n",
      "Log Regression(1483/2999): loss=0.4179044600225472\n",
      "Log Regression(1484/2999): loss=0.4179043356399215\n",
      "Log Regression(1485/2999): loss=0.4179045345322424\n",
      "Log Regression(1486/2999): loss=0.4179042600225671\n",
      "Log Regression(1487/2999): loss=0.4179041899522276\n",
      "Log Regression(1488/2999): loss=0.4179042184126962\n",
      "Log Regression(1489/2999): loss=0.41790403815678356\n",
      "Log Regression(1490/2999): loss=0.41790451282363217\n",
      "Log Regression(1491/2999): loss=0.41790427937383773\n",
      "Log Regression(1492/2999): loss=0.4179040005495396\n",
      "Log Regression(1493/2999): loss=0.41790440795660133\n",
      "Log Regression(1494/2999): loss=0.4179049112640849\n",
      "Log Regression(1495/2999): loss=0.4179044669660757\n",
      "Log Regression(1496/2999): loss=0.41790461653019895\n",
      "Log Regression(1497/2999): loss=0.41790456574203777\n",
      "Log Regression(1498/2999): loss=0.4179047427936746\n",
      "Log Regression(1499/2999): loss=0.41790437729010604\n",
      "Log Regression(1500/2999): loss=0.41790412612720723\n",
      "Log Regression(1501/2999): loss=0.4179039600613945\n",
      "Log Regression(1502/2999): loss=0.41790374648736484\n",
      "Log Regression(1503/2999): loss=0.4179037609239767\n",
      "Log Regression(1504/2999): loss=0.4179033585967374\n",
      "Log Regression(1505/2999): loss=0.4179034212978627\n",
      "Log Regression(1506/2999): loss=0.41790308416796884\n",
      "Log Regression(1507/2999): loss=0.4179030647079087\n",
      "Log Regression(1508/2999): loss=0.4179028650295991\n",
      "Log Regression(1509/2999): loss=0.41790329114412644\n",
      "Log Regression(1510/2999): loss=0.417903376798179\n",
      "Log Regression(1511/2999): loss=0.41790320284136917\n",
      "Log Regression(1512/2999): loss=0.41790305708407377\n",
      "Log Regression(1513/2999): loss=0.41790285775842223\n",
      "Log Regression(1514/2999): loss=0.4179028379977879\n",
      "Log Regression(1515/2999): loss=0.41790264098907004\n",
      "Log Regression(1516/2999): loss=0.41790254258686826\n",
      "Log Regression(1517/2999): loss=0.41790262059207917\n",
      "Log Regression(1518/2999): loss=0.41790271175720256\n",
      "Log Regression(1519/2999): loss=0.41790271602138607\n",
      "Log Regression(1520/2999): loss=0.41790259985060785\n",
      "Log Regression(1521/2999): loss=0.41790247431495475\n",
      "Log Regression(1522/2999): loss=0.4179024476967009\n",
      "Log Regression(1523/2999): loss=0.4179019460657339\n",
      "Log Regression(1524/2999): loss=0.41790180778339686\n",
      "Log Regression(1525/2999): loss=0.41790256755710126\n",
      "Log Regression(1526/2999): loss=0.41790299130267944\n",
      "Log Regression(1527/2999): loss=0.41790303739551693\n",
      "Log Regression(1528/2999): loss=0.41790297201525567\n",
      "Log Regression(1529/2999): loss=0.41790285311973785\n",
      "Log Regression(1530/2999): loss=0.41790281990636124\n",
      "Log Regression(1531/2999): loss=0.4179025940207788\n",
      "Log Regression(1532/2999): loss=0.41790243371178326\n",
      "Log Regression(1533/2999): loss=0.4179024694645487\n",
      "Log Regression(1534/2999): loss=0.4179025472445883\n",
      "Log Regression(1535/2999): loss=0.4179024610977457\n",
      "Log Regression(1536/2999): loss=0.41790239395861506\n",
      "Log Regression(1537/2999): loss=0.41790199833967406\n",
      "Log Regression(1538/2999): loss=0.4179019013264615\n",
      "Log Regression(1539/2999): loss=0.41790189024774527\n",
      "Log Regression(1540/2999): loss=0.4179014993651584\n",
      "Log Regression(1541/2999): loss=0.41790175009001934\n",
      "Log Regression(1542/2999): loss=0.4179015970624272\n",
      "Log Regression(1543/2999): loss=0.4179014580956205\n",
      "Log Regression(1544/2999): loss=0.41790094102019515\n",
      "Log Regression(1545/2999): loss=0.41790122514453504\n",
      "Log Regression(1546/2999): loss=0.4179013859156647\n",
      "Log Regression(1547/2999): loss=0.4179016396728743\n",
      "Log Regression(1548/2999): loss=0.4179015749998497\n",
      "Log Regression(1549/2999): loss=0.4179016627108544\n",
      "Log Regression(1550/2999): loss=0.4179021380550323\n",
      "Log Regression(1551/2999): loss=0.417902053262794\n",
      "Log Regression(1552/2999): loss=0.41790236760948746\n",
      "Log Regression(1553/2999): loss=0.41790228291042\n",
      "Log Regression(1554/2999): loss=0.4179024755072979\n",
      "Log Regression(1555/2999): loss=0.4179027481355466\n",
      "Log Regression(1556/2999): loss=0.41790283035034687\n",
      "Log Regression(1557/2999): loss=0.41790314394696265\n",
      "Log Regression(1558/2999): loss=0.41790295271257644\n",
      "Log Regression(1559/2999): loss=0.417903157615442\n",
      "Log Regression(1560/2999): loss=0.41790296737974175\n",
      "Log Regression(1561/2999): loss=0.4179030371305693\n",
      "Log Regression(1562/2999): loss=0.41790338141850436\n",
      "Log Regression(1563/2999): loss=0.41790369685754475\n",
      "Log Regression(1564/2999): loss=0.4179036456127188\n",
      "Log Regression(1565/2999): loss=0.4179041173054732\n",
      "Log Regression(1566/2999): loss=0.41790406825818854\n",
      "Log Regression(1567/2999): loss=0.4179042671635388\n",
      "Log Regression(1568/2999): loss=0.41790411822597573\n",
      "Log Regression(1569/2999): loss=0.41790440430154335\n",
      "Log Regression(1570/2999): loss=0.4179035340145309\n",
      "Log Regression(1571/2999): loss=0.41790344620667325\n",
      "Log Regression(1572/2999): loss=0.4179035136883122\n",
      "Log Regression(1573/2999): loss=0.41790348642150404\n",
      "Log Regression(1574/2999): loss=0.41790316783168147\n",
      "Log Regression(1575/2999): loss=0.4179031390970814\n",
      "Log Regression(1576/2999): loss=0.41790312519177875\n",
      "Log Regression(1577/2999): loss=0.41790289202071784\n",
      "Log Regression(1578/2999): loss=0.417902691438611\n",
      "Log Regression(1579/2999): loss=0.41790272935960804\n",
      "Log Regression(1580/2999): loss=0.4179025379789115\n",
      "Log Regression(1581/2999): loss=0.41790248282986536\n",
      "Log Regression(1582/2999): loss=0.41790235014454385\n",
      "Log Regression(1583/2999): loss=0.41790203465990095\n",
      "Log Regression(1584/2999): loss=0.4179022526557226\n",
      "Log Regression(1585/2999): loss=0.4179020976583633\n",
      "Log Regression(1586/2999): loss=0.41790229551145097\n",
      "Log Regression(1587/2999): loss=0.4179022298196238\n",
      "Log Regression(1588/2999): loss=0.41790184034935396\n",
      "Log Regression(1589/2999): loss=0.4179016943618488\n",
      "Log Regression(1590/2999): loss=0.4179015982475129\n",
      "Log Regression(1591/2999): loss=0.41790174257449975\n",
      "Log Regression(1592/2999): loss=0.4179018561241532\n",
      "Log Regression(1593/2999): loss=0.41790144307129246\n",
      "Log Regression(1594/2999): loss=0.41790163608890074\n",
      "Log Regression(1595/2999): loss=0.4179017847726535\n",
      "Log Regression(1596/2999): loss=0.417901721854322\n",
      "Log Regression(1597/2999): loss=0.41790140657728225\n",
      "Log Regression(1598/2999): loss=0.4179013414655273\n",
      "Log Regression(1599/2999): loss=0.41790121818499365\n",
      "Log Regression(1600/2999): loss=0.41790138369931473\n",
      "Log Regression(1601/2999): loss=0.41790122663409784\n",
      "Log Regression(1602/2999): loss=0.41790079302606437\n",
      "Log Regression(1603/2999): loss=0.41790080681004155\n",
      "Log Regression(1604/2999): loss=0.417900878390284\n",
      "Log Regression(1605/2999): loss=0.4179009514524253\n",
      "Log Regression(1606/2999): loss=0.41790096550782785\n",
      "Log Regression(1607/2999): loss=0.4179009834901504\n",
      "Log Regression(1608/2999): loss=0.4179008802180489\n",
      "Log Regression(1609/2999): loss=0.417900942611164\n",
      "Log Regression(1610/2999): loss=0.4179008156343453\n",
      "Log Regression(1611/2999): loss=0.41790084733366717\n",
      "Log Regression(1612/2999): loss=0.41790209706835446\n",
      "Log Regression(1613/2999): loss=0.41790224221054273\n",
      "Log Regression(1614/2999): loss=0.4179019731759172\n",
      "Log Regression(1615/2999): loss=0.41790229221827435\n",
      "Log Regression(1616/2999): loss=0.417902487401898\n",
      "Log Regression(1617/2999): loss=0.4179023079859289\n",
      "Log Regression(1618/2999): loss=0.41790299350332255\n",
      "Log Regression(1619/2999): loss=0.4179030796721916\n",
      "Log Regression(1620/2999): loss=0.4179028311975979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(1621/2999): loss=0.41790228383738015\n",
      "Log Regression(1622/2999): loss=0.417902110765719\n",
      "Log Regression(1623/2999): loss=0.4179024571568483\n",
      "Log Regression(1624/2999): loss=0.41790272223937475\n",
      "Log Regression(1625/2999): loss=0.4179026038529847\n",
      "Log Regression(1626/2999): loss=0.4179037043876164\n",
      "Log Regression(1627/2999): loss=0.4179035086309199\n",
      "Log Regression(1628/2999): loss=0.41790305984648196\n",
      "Log Regression(1629/2999): loss=0.4179033967498426\n",
      "Log Regression(1630/2999): loss=0.4179033992785667\n",
      "Log Regression(1631/2999): loss=0.41790313122339784\n",
      "Log Regression(1632/2999): loss=0.4179030754652971\n",
      "Log Regression(1633/2999): loss=0.417902916764119\n",
      "Log Regression(1634/2999): loss=0.4179033413380858\n",
      "Log Regression(1635/2999): loss=0.41790346613478024\n",
      "Log Regression(1636/2999): loss=0.41790328302828694\n",
      "Log Regression(1637/2999): loss=0.41790328074261296\n",
      "Log Regression(1638/2999): loss=0.4179032900069106\n",
      "Log Regression(1639/2999): loss=0.4179029819920924\n",
      "Log Regression(1640/2999): loss=0.4179031729906497\n",
      "Log Regression(1641/2999): loss=0.4179032888992383\n",
      "Log Regression(1642/2999): loss=0.4179030163945719\n",
      "Log Regression(1643/2999): loss=0.41790290072325176\n",
      "Log Regression(1644/2999): loss=0.41790310528129926\n",
      "Log Regression(1645/2999): loss=0.4179030955468838\n",
      "Log Regression(1646/2999): loss=0.4179031050813094\n",
      "Log Regression(1647/2999): loss=0.4179031519848202\n",
      "Log Regression(1648/2999): loss=0.4179031716178644\n",
      "Log Regression(1649/2999): loss=0.41790329184197794\n",
      "Log Regression(1650/2999): loss=0.4179033169981243\n",
      "Log Regression(1651/2999): loss=0.41790371329332265\n",
      "Log Regression(1652/2999): loss=0.4179035361012179\n",
      "Log Regression(1653/2999): loss=0.41790344075888586\n",
      "Log Regression(1654/2999): loss=0.4179032774299584\n",
      "Log Regression(1655/2999): loss=0.41790292679734214\n",
      "Log Regression(1656/2999): loss=0.4179029140568382\n",
      "Log Regression(1657/2999): loss=0.4179029700730423\n",
      "Log Regression(1658/2999): loss=0.4179028788548992\n",
      "Log Regression(1659/2999): loss=0.41790269244894596\n",
      "Log Regression(1660/2999): loss=0.41790333585490136\n",
      "Log Regression(1661/2999): loss=0.41790361058583\n",
      "Log Regression(1662/2999): loss=0.41790375657522383\n",
      "Log Regression(1663/2999): loss=0.4179037811636092\n",
      "Log Regression(1664/2999): loss=0.41790382179481966\n",
      "Log Regression(1665/2999): loss=0.41790378652685534\n",
      "Log Regression(1666/2999): loss=0.4179038462960509\n",
      "Log Regression(1667/2999): loss=0.4179035997987784\n",
      "Log Regression(1668/2999): loss=0.41790369475946065\n",
      "Log Regression(1669/2999): loss=0.41790337626042195\n",
      "Log Regression(1670/2999): loss=0.417903188120574\n",
      "Log Regression(1671/2999): loss=0.4179031038314092\n",
      "Log Regression(1672/2999): loss=0.41790294894329394\n",
      "Log Regression(1673/2999): loss=0.41790320247328466\n",
      "Log Regression(1674/2999): loss=0.41790332555113013\n",
      "Log Regression(1675/2999): loss=0.4179033713796981\n",
      "Log Regression(1676/2999): loss=0.41790340645069746\n",
      "Log Regression(1677/2999): loss=0.41790354810720876\n",
      "Log Regression(1678/2999): loss=0.417903256857349\n",
      "Log Regression(1679/2999): loss=0.41790417403067887\n",
      "Log Regression(1680/2999): loss=0.4179042959590245\n",
      "Log Regression(1681/2999): loss=0.4179041282993094\n",
      "Log Regression(1682/2999): loss=0.4179040849205681\n",
      "Log Regression(1683/2999): loss=0.41790370714942177\n",
      "Log Regression(1684/2999): loss=0.4179039680934712\n",
      "Log Regression(1685/2999): loss=0.4179041531208652\n",
      "Log Regression(1686/2999): loss=0.4179038157458536\n",
      "Log Regression(1687/2999): loss=0.4179039230667677\n",
      "Log Regression(1688/2999): loss=0.4179042925333987\n",
      "Log Regression(1689/2999): loss=0.4179046852160195\n",
      "Log Regression(1690/2999): loss=0.41790491638498756\n",
      "Log Regression(1691/2999): loss=0.4179048205335291\n",
      "Log Regression(1692/2999): loss=0.4179045660407851\n",
      "Log Regression(1693/2999): loss=0.4179046315394039\n",
      "Log Regression(1694/2999): loss=0.41790450371222226\n",
      "Log Regression(1695/2999): loss=0.41790421793472443\n",
      "Log Regression(1696/2999): loss=0.4179040657430994\n",
      "Log Regression(1697/2999): loss=0.41790381437552654\n",
      "Log Regression(1698/2999): loss=0.4179041069676991\n",
      "Log Regression(1699/2999): loss=0.41790457177483953\n",
      "Log Regression(1700/2999): loss=0.41790422448674885\n",
      "Log Regression(1701/2999): loss=0.41790428447122757\n",
      "Log Regression(1702/2999): loss=0.4179042019792437\n",
      "Log Regression(1703/2999): loss=0.41790438038463684\n",
      "Log Regression(1704/2999): loss=0.41790429140642077\n",
      "Log Regression(1705/2999): loss=0.4179043872591723\n",
      "Log Regression(1706/2999): loss=0.4179046202784634\n",
      "Log Regression(1707/2999): loss=0.41790459401156244\n",
      "Log Regression(1708/2999): loss=0.4179046322753251\n",
      "Log Regression(1709/2999): loss=0.4179046486798959\n",
      "Log Regression(1710/2999): loss=0.4179052151936309\n",
      "Log Regression(1711/2999): loss=0.41790496708146563\n",
      "Log Regression(1712/2999): loss=0.4179046977669554\n",
      "Log Regression(1713/2999): loss=0.4179046339437231\n",
      "Log Regression(1714/2999): loss=0.41790448679343084\n",
      "Log Regression(1715/2999): loss=0.41790470686945963\n",
      "Log Regression(1716/2999): loss=0.4179050124730393\n",
      "Log Regression(1717/2999): loss=0.41790518222073275\n",
      "Log Regression(1718/2999): loss=0.41790492111056043\n",
      "Log Regression(1719/2999): loss=0.41790481465355456\n",
      "Log Regression(1720/2999): loss=0.4179051575551104\n",
      "Log Regression(1721/2999): loss=0.4179052973968897\n",
      "Log Regression(1722/2999): loss=0.4179048916425264\n",
      "Log Regression(1723/2999): loss=0.41790506906607855\n",
      "Log Regression(1724/2999): loss=0.4179049948180067\n",
      "Log Regression(1725/2999): loss=0.41790488142036436\n",
      "Log Regression(1726/2999): loss=0.41790483259216826\n",
      "Log Regression(1727/2999): loss=0.4179045908661611\n",
      "Log Regression(1728/2999): loss=0.41790474858308707\n",
      "Log Regression(1729/2999): loss=0.4179043879691244\n",
      "Log Regression(1730/2999): loss=0.4179046039186612\n",
      "Log Regression(1731/2999): loss=0.4179049082264237\n",
      "Log Regression(1732/2999): loss=0.41790518274919597\n",
      "Log Regression(1733/2999): loss=0.41790541223773964\n",
      "Log Regression(1734/2999): loss=0.41790528429927043\n",
      "Log Regression(1735/2999): loss=0.41790517614265266\n",
      "Log Regression(1736/2999): loss=0.4179047416779383\n",
      "Log Regression(1737/2999): loss=0.41790470753484377\n",
      "Log Regression(1738/2999): loss=0.4179047337732065\n",
      "Log Regression(1739/2999): loss=0.4179048870476253\n",
      "Log Regression(1740/2999): loss=0.41790470396387985\n",
      "Log Regression(1741/2999): loss=0.4179046484468885\n",
      "Log Regression(1742/2999): loss=0.4179045482369229\n",
      "Log Regression(1743/2999): loss=0.41790454952582573\n",
      "Log Regression(1744/2999): loss=0.41790450624086195\n",
      "Log Regression(1745/2999): loss=0.41790473751249674\n",
      "Log Regression(1746/2999): loss=0.41790489902453415\n",
      "Log Regression(1747/2999): loss=0.41790468131831077\n",
      "Log Regression(1748/2999): loss=0.4179045054281966\n",
      "Log Regression(1749/2999): loss=0.41790456310266677\n",
      "Log Regression(1750/2999): loss=0.41790476320570596\n",
      "Log Regression(1751/2999): loss=0.417904976905675\n",
      "Log Regression(1752/2999): loss=0.41790439574351085\n",
      "Log Regression(1753/2999): loss=0.4179042918791139\n",
      "Log Regression(1754/2999): loss=0.41790450228364195\n",
      "Log Regression(1755/2999): loss=0.4179047610117379\n",
      "Log Regression(1756/2999): loss=0.4179046499389955\n",
      "Log Regression(1757/2999): loss=0.4179043408531709\n",
      "Log Regression(1758/2999): loss=0.41790456953287447\n",
      "Log Regression(1759/2999): loss=0.417904795439594\n",
      "Log Regression(1760/2999): loss=0.4179046742641122\n",
      "Log Regression(1761/2999): loss=0.41790467598850645\n",
      "Log Regression(1762/2999): loss=0.4179052643985201\n",
      "Log Regression(1763/2999): loss=0.41790491940135893\n",
      "Log Regression(1764/2999): loss=0.4179049425845791\n",
      "Log Regression(1765/2999): loss=0.41790488495045713\n",
      "Log Regression(1766/2999): loss=0.41790494426313035\n",
      "Log Regression(1767/2999): loss=0.417904937586103\n",
      "Log Regression(1768/2999): loss=0.41790494144284235\n",
      "Log Regression(1769/2999): loss=0.4179045954055829\n",
      "Log Regression(1770/2999): loss=0.417904885803925\n",
      "Log Regression(1771/2999): loss=0.4179043791878471\n",
      "Log Regression(1772/2999): loss=0.417904439428095\n",
      "Log Regression(1773/2999): loss=0.4179041062291742\n",
      "Log Regression(1774/2999): loss=0.41790413557870815\n",
      "Log Regression(1775/2999): loss=0.4179043576649394\n",
      "Log Regression(1776/2999): loss=0.4179045306434567\n",
      "Log Regression(1777/2999): loss=0.4179044892973464\n",
      "Log Regression(1778/2999): loss=0.41790436326869945\n",
      "Log Regression(1779/2999): loss=0.41790426598249447\n",
      "Log Regression(1780/2999): loss=0.41790384671225184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(1781/2999): loss=0.4179036321608913\n",
      "Log Regression(1782/2999): loss=0.4179033270148453\n",
      "Log Regression(1783/2999): loss=0.41790314274802703\n",
      "Log Regression(1784/2999): loss=0.41790324080665003\n",
      "Log Regression(1785/2999): loss=0.41790322019790177\n",
      "Log Regression(1786/2999): loss=0.41790298974098977\n",
      "Log Regression(1787/2999): loss=0.41790311611021275\n",
      "Log Regression(1788/2999): loss=0.4179026265857252\n",
      "Log Regression(1789/2999): loss=0.4179026085585465\n",
      "Log Regression(1790/2999): loss=0.4179025102513227\n",
      "Log Regression(1791/2999): loss=0.4179026915494915\n",
      "Log Regression(1792/2999): loss=0.4179026250755536\n",
      "Log Regression(1793/2999): loss=0.4179034149950891\n",
      "Log Regression(1794/2999): loss=0.41790338927878073\n",
      "Log Regression(1795/2999): loss=0.4179034172836402\n",
      "Log Regression(1796/2999): loss=0.41790259742461894\n",
      "Log Regression(1797/2999): loss=0.41790264104738795\n",
      "Log Regression(1798/2999): loss=0.4179020479152209\n",
      "Log Regression(1799/2999): loss=0.4179019363462357\n",
      "Log Regression(1800/2999): loss=0.41790196064885554\n",
      "Log Regression(1801/2999): loss=0.417901867202255\n",
      "Log Regression(1802/2999): loss=0.417901583465411\n",
      "Log Regression(1803/2999): loss=0.41790181298436485\n",
      "Log Regression(1804/2999): loss=0.4179017843468583\n",
      "Log Regression(1805/2999): loss=0.4179020950667307\n",
      "Log Regression(1806/2999): loss=0.417902234714555\n",
      "Log Regression(1807/2999): loss=0.41790233564303536\n",
      "Log Regression(1808/2999): loss=0.41790204259276037\n",
      "Log Regression(1809/2999): loss=0.4179019625922786\n",
      "Log Regression(1810/2999): loss=0.4179017284582173\n",
      "Log Regression(1811/2999): loss=0.4179017845352283\n",
      "Log Regression(1812/2999): loss=0.417901456999643\n",
      "Log Regression(1813/2999): loss=0.41790120548202403\n",
      "Log Regression(1814/2999): loss=0.4179010858779177\n",
      "Log Regression(1815/2999): loss=0.4179010088926219\n",
      "Log Regression(1816/2999): loss=0.4179015377944983\n",
      "Log Regression(1817/2999): loss=0.41790135417089047\n",
      "Log Regression(1818/2999): loss=0.41790111906252553\n",
      "Log Regression(1819/2999): loss=0.41790094358435675\n",
      "Log Regression(1820/2999): loss=0.4179006768015247\n",
      "Log Regression(1821/2999): loss=0.41790039907666193\n",
      "Log Regression(1822/2999): loss=0.41790076396367715\n",
      "Log Regression(1823/2999): loss=0.4179010197890509\n",
      "Log Regression(1824/2999): loss=0.4179006930551464\n",
      "Log Regression(1825/2999): loss=0.4179011125226049\n",
      "Log Regression(1826/2999): loss=0.4179011587067473\n",
      "Log Regression(1827/2999): loss=0.4179011999762273\n",
      "Log Regression(1828/2999): loss=0.41790134521007766\n",
      "Log Regression(1829/2999): loss=0.4179021536308396\n",
      "Log Regression(1830/2999): loss=0.4179020136616068\n",
      "Log Regression(1831/2999): loss=0.4179019603120424\n",
      "Log Regression(1832/2999): loss=0.4179021581846853\n",
      "Log Regression(1833/2999): loss=0.4179020728413876\n",
      "Log Regression(1834/2999): loss=0.41790195005154607\n",
      "Log Regression(1835/2999): loss=0.41790189971555713\n",
      "Log Regression(1836/2999): loss=0.41790167001699274\n",
      "Log Regression(1837/2999): loss=0.41790200460084526\n",
      "Log Regression(1838/2999): loss=0.4179017939125914\n",
      "Log Regression(1839/2999): loss=0.4179017099465047\n",
      "Log Regression(1840/2999): loss=0.4179013385338806\n",
      "Log Regression(1841/2999): loss=0.4179011905125095\n",
      "Log Regression(1842/2999): loss=0.4179007480977775\n",
      "Log Regression(1843/2999): loss=0.41790070646961947\n",
      "Log Regression(1844/2999): loss=0.4179005473184262\n",
      "Log Regression(1845/2999): loss=0.41790062472718387\n",
      "Log Regression(1846/2999): loss=0.41790048695017923\n",
      "Log Regression(1847/2999): loss=0.417900335758909\n",
      "Log Regression(1848/2999): loss=0.4179008017335842\n",
      "Log Regression(1849/2999): loss=0.41790077589892616\n",
      "Log Regression(1850/2999): loss=0.41790065237485347\n",
      "Log Regression(1851/2999): loss=0.4179007671730983\n",
      "Log Regression(1852/2999): loss=0.41790091791993295\n",
      "Log Regression(1853/2999): loss=0.4179006151690572\n",
      "Log Regression(1854/2999): loss=0.41790079894383186\n",
      "Log Regression(1855/2999): loss=0.41790095922964215\n",
      "Log Regression(1856/2999): loss=0.4179007892930186\n",
      "Log Regression(1857/2999): loss=0.4179009956051288\n",
      "Log Regression(1858/2999): loss=0.4179014350817209\n",
      "Log Regression(1859/2999): loss=0.4179015961455955\n",
      "Log Regression(1860/2999): loss=0.41790147660735993\n",
      "Log Regression(1861/2999): loss=0.4179014163740879\n",
      "Log Regression(1862/2999): loss=0.4179010452195297\n",
      "Log Regression(1863/2999): loss=0.4179011256583047\n",
      "Log Regression(1864/2999): loss=0.41790110869246916\n",
      "Log Regression(1865/2999): loss=0.4179013898757041\n",
      "Log Regression(1866/2999): loss=0.41790149551917527\n",
      "Log Regression(1867/2999): loss=0.4179012160577146\n",
      "Log Regression(1868/2999): loss=0.4179013434987714\n",
      "Log Regression(1869/2999): loss=0.41790133187330236\n",
      "Log Regression(1870/2999): loss=0.41790139384672903\n",
      "Log Regression(1871/2999): loss=0.41790161788275654\n",
      "Log Regression(1872/2999): loss=0.417901599978299\n",
      "Log Regression(1873/2999): loss=0.41790158565212837\n",
      "Log Regression(1874/2999): loss=0.4179015626307848\n",
      "Log Regression(1875/2999): loss=0.41790163943350433\n",
      "Log Regression(1876/2999): loss=0.4179015707740418\n",
      "Log Regression(1877/2999): loss=0.41790149612273336\n",
      "Log Regression(1878/2999): loss=0.41790131276329423\n",
      "Log Regression(1879/2999): loss=0.41790141824045635\n",
      "Log Regression(1880/2999): loss=0.41790149718928116\n",
      "Log Regression(1881/2999): loss=0.4179017115547137\n",
      "Log Regression(1882/2999): loss=0.4179015491912482\n",
      "Log Regression(1883/2999): loss=0.41790157498420744\n",
      "Log Regression(1884/2999): loss=0.417901702084712\n",
      "Log Regression(1885/2999): loss=0.4179016535003899\n",
      "Log Regression(1886/2999): loss=0.41790136484476287\n",
      "Log Regression(1887/2999): loss=0.4179012016538763\n",
      "Log Regression(1888/2999): loss=0.4179011959076328\n",
      "Log Regression(1889/2999): loss=0.4179012859831401\n",
      "Log Regression(1890/2999): loss=0.4179012946042581\n",
      "Log Regression(1891/2999): loss=0.4179014424042014\n",
      "Log Regression(1892/2999): loss=0.4179010435538211\n",
      "Log Regression(1893/2999): loss=0.41790098347277577\n",
      "Log Regression(1894/2999): loss=0.4179013243130882\n",
      "Log Regression(1895/2999): loss=0.41790113336597506\n",
      "Log Regression(1896/2999): loss=0.4179004556367359\n",
      "Log Regression(1897/2999): loss=0.4179003736093824\n",
      "Log Regression(1898/2999): loss=0.4179007191926605\n",
      "Log Regression(1899/2999): loss=0.41790042162942737\n",
      "Log Regression(1900/2999): loss=0.41790037492000387\n",
      "Log Regression(1901/2999): loss=0.4179003431918696\n",
      "Log Regression(1902/2999): loss=0.4179002614430549\n",
      "Log Regression(1903/2999): loss=0.41790025017448196\n",
      "Log Regression(1904/2999): loss=0.41790014261687913\n",
      "Log Regression(1905/2999): loss=0.41789995456344\n",
      "Log Regression(1906/2999): loss=0.41789973021759586\n",
      "Log Regression(1907/2999): loss=0.4178996716412016\n",
      "Log Regression(1908/2999): loss=0.4178995979070118\n",
      "Log Regression(1909/2999): loss=0.4178997100388019\n",
      "Log Regression(1910/2999): loss=0.41789951541546705\n",
      "Log Regression(1911/2999): loss=0.4178997371641227\n",
      "Log Regression(1912/2999): loss=0.41789966613232044\n",
      "Log Regression(1913/2999): loss=0.41789918862590636\n",
      "Log Regression(1914/2999): loss=0.4178992144014967\n",
      "Log Regression(1915/2999): loss=0.4178992476182997\n",
      "Log Regression(1916/2999): loss=0.4178992244266492\n",
      "Log Regression(1917/2999): loss=0.4178989033295688\n",
      "Log Regression(1918/2999): loss=0.41789920374485273\n",
      "Log Regression(1919/2999): loss=0.41789933978476496\n",
      "Log Regression(1920/2999): loss=0.4178994671841124\n",
      "Log Regression(1921/2999): loss=0.4178997234475455\n",
      "Log Regression(1922/2999): loss=0.41789977457167965\n",
      "Log Regression(1923/2999): loss=0.41790005846942363\n",
      "Log Regression(1924/2999): loss=0.41789977848131044\n",
      "Log Regression(1925/2999): loss=0.4178997801704118\n",
      "Log Regression(1926/2999): loss=0.41789994175493234\n",
      "Log Regression(1927/2999): loss=0.4178999150162429\n",
      "Log Regression(1928/2999): loss=0.4178997916667654\n",
      "Log Regression(1929/2999): loss=0.41790045199431775\n",
      "Log Regression(1930/2999): loss=0.41790047964511623\n",
      "Log Regression(1931/2999): loss=0.4179005176987071\n",
      "Log Regression(1932/2999): loss=0.4179005489336381\n",
      "Log Regression(1933/2999): loss=0.4178999428944019\n",
      "Log Regression(1934/2999): loss=0.41790013488133926\n",
      "Log Regression(1935/2999): loss=0.4179001555236052\n",
      "Log Regression(1936/2999): loss=0.41789995565810145\n",
      "Log Regression(1937/2999): loss=0.4179000380304647\n",
      "Log Regression(1938/2999): loss=0.4179001470016043\n",
      "Log Regression(1939/2999): loss=0.41790026622687354\n",
      "Log Regression(1940/2999): loss=0.4179002471298367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(1941/2999): loss=0.417900356910723\n",
      "Log Regression(1942/2999): loss=0.4179001394912232\n",
      "Log Regression(1943/2999): loss=0.4179000973923104\n",
      "Log Regression(1944/2999): loss=0.4178996636538733\n",
      "Log Regression(1945/2999): loss=0.4178992424646751\n",
      "Log Regression(1946/2999): loss=0.41789940595285724\n",
      "Log Regression(1947/2999): loss=0.4178994189720502\n",
      "Log Regression(1948/2999): loss=0.41789970362587253\n",
      "Log Regression(1949/2999): loss=0.417900565824072\n",
      "Log Regression(1950/2999): loss=0.41790047152587717\n",
      "Log Regression(1951/2999): loss=0.41790055887329997\n",
      "Log Regression(1952/2999): loss=0.41790024409263254\n",
      "Log Regression(1953/2999): loss=0.41790100683546166\n",
      "Log Regression(1954/2999): loss=0.41790111564725496\n",
      "Log Regression(1955/2999): loss=0.41790106360596774\n",
      "Log Regression(1956/2999): loss=0.4179009574231226\n",
      "Log Regression(1957/2999): loss=0.4179010321128319\n",
      "Log Regression(1958/2999): loss=0.4179009290065315\n",
      "Log Regression(1959/2999): loss=0.4179010671610542\n",
      "Log Regression(1960/2999): loss=0.4179010265352403\n",
      "Log Regression(1961/2999): loss=0.4179008478867807\n",
      "Log Regression(1962/2999): loss=0.41790060136925467\n",
      "Log Regression(1963/2999): loss=0.4179006507811975\n",
      "Log Regression(1964/2999): loss=0.41790063393457155\n",
      "Log Regression(1965/2999): loss=0.41790066942013443\n",
      "Log Regression(1966/2999): loss=0.4179007314950795\n",
      "Log Regression(1967/2999): loss=0.4179004962330148\n",
      "Log Regression(1968/2999): loss=0.41790112931050793\n",
      "Log Regression(1969/2999): loss=0.41790034139027593\n",
      "Log Regression(1970/2999): loss=0.41790041295907393\n",
      "Log Regression(1971/2999): loss=0.4179004679987606\n",
      "Log Regression(1972/2999): loss=0.417900473691528\n",
      "Log Regression(1973/2999): loss=0.4179003243566251\n",
      "Log Regression(1974/2999): loss=0.41790010505320124\n",
      "Log Regression(1975/2999): loss=0.4179002667441239\n",
      "Log Regression(1976/2999): loss=0.4179003719579023\n",
      "Log Regression(1977/2999): loss=0.41790040823351743\n",
      "Log Regression(1978/2999): loss=0.4179000779087004\n",
      "Log Regression(1979/2999): loss=0.4178997155382266\n",
      "Log Regression(1980/2999): loss=0.41789982328627906\n",
      "Log Regression(1981/2999): loss=0.4179002695769826\n",
      "Log Regression(1982/2999): loss=0.41790023710334206\n",
      "Log Regression(1983/2999): loss=0.41789998220500324\n",
      "Log Regression(1984/2999): loss=0.41790005934745733\n",
      "Log Regression(1985/2999): loss=0.4178999215045371\n",
      "Log Regression(1986/2999): loss=0.4179005720311826\n",
      "Log Regression(1987/2999): loss=0.4179001178438557\n",
      "Log Regression(1988/2999): loss=0.41790037687967035\n",
      "Log Regression(1989/2999): loss=0.41790031790727705\n",
      "Log Regression(1990/2999): loss=0.41790026490911675\n",
      "Log Regression(1991/2999): loss=0.4179000462675162\n",
      "Log Regression(1992/2999): loss=0.4178999998859765\n",
      "Log Regression(1993/2999): loss=0.4179000222587607\n",
      "Log Regression(1994/2999): loss=0.41790005149267034\n",
      "Log Regression(1995/2999): loss=0.41789990197521565\n",
      "Log Regression(1996/2999): loss=0.4178997986393759\n",
      "Log Regression(1997/2999): loss=0.417899560365837\n",
      "Log Regression(1998/2999): loss=0.4178991189575453\n",
      "Log Regression(1999/2999): loss=0.4178988645783572\n",
      "Log Regression(2000/2999): loss=0.41789886354189804\n",
      "Log Regression(2001/2999): loss=0.4178986702478464\n",
      "Log Regression(2002/2999): loss=0.4178985535495351\n",
      "Log Regression(2003/2999): loss=0.4178984484612088\n",
      "Log Regression(2004/2999): loss=0.4178984914167629\n",
      "Log Regression(2005/2999): loss=0.4178986787324866\n",
      "Log Regression(2006/2999): loss=0.4178985676768185\n",
      "Log Regression(2007/2999): loss=0.41789867450800244\n",
      "Log Regression(2008/2999): loss=0.417898344275321\n",
      "Log Regression(2009/2999): loss=0.41789822773209584\n",
      "Log Regression(2010/2999): loss=0.41789803622161514\n",
      "Log Regression(2011/2999): loss=0.4178983364751723\n",
      "Log Regression(2012/2999): loss=0.41789836620152887\n",
      "Log Regression(2013/2999): loss=0.4178986398164924\n",
      "Log Regression(2014/2999): loss=0.4178982925643842\n",
      "Log Regression(2015/2999): loss=0.4178982646786022\n",
      "Log Regression(2016/2999): loss=0.4178979501543453\n",
      "Log Regression(2017/2999): loss=0.4178976225567597\n",
      "Log Regression(2018/2999): loss=0.4178977515507742\n",
      "Log Regression(2019/2999): loss=0.4178981237553902\n",
      "Log Regression(2020/2999): loss=0.4178979641700827\n",
      "Log Regression(2021/2999): loss=0.41789780412415234\n",
      "Log Regression(2022/2999): loss=0.4178976567446097\n",
      "Log Regression(2023/2999): loss=0.4178977887864379\n",
      "Log Regression(2024/2999): loss=0.417897268007163\n",
      "Log Regression(2025/2999): loss=0.41789716598469867\n",
      "Log Regression(2026/2999): loss=0.41789698782432977\n",
      "Log Regression(2027/2999): loss=0.41789729218517624\n",
      "Log Regression(2028/2999): loss=0.41789777357813573\n",
      "Log Regression(2029/2999): loss=0.41789738679115856\n",
      "Log Regression(2030/2999): loss=0.417897352968922\n",
      "Log Regression(2031/2999): loss=0.41789750114964824\n",
      "Log Regression(2032/2999): loss=0.4178973342629375\n",
      "Log Regression(2033/2999): loss=0.4178972233479109\n",
      "Log Regression(2034/2999): loss=0.41789726340307065\n",
      "Log Regression(2035/2999): loss=0.4178972063178661\n",
      "Log Regression(2036/2999): loss=0.41789726366992597\n",
      "Log Regression(2037/2999): loss=0.41789711688864384\n",
      "Log Regression(2038/2999): loss=0.41789676919737373\n",
      "Log Regression(2039/2999): loss=0.4178972132395669\n",
      "Log Regression(2040/2999): loss=0.4178973758382988\n",
      "Log Regression(2041/2999): loss=0.41789742377419237\n",
      "Log Regression(2042/2999): loss=0.41789739352143485\n",
      "Log Regression(2043/2999): loss=0.4178969503660756\n",
      "Log Regression(2044/2999): loss=0.41789727724820547\n",
      "Log Regression(2045/2999): loss=0.4178976763463322\n",
      "Log Regression(2046/2999): loss=0.41789764568431337\n",
      "Log Regression(2047/2999): loss=0.4178975015413969\n",
      "Log Regression(2048/2999): loss=0.4178976720243955\n",
      "Log Regression(2049/2999): loss=0.4178974712569767\n",
      "Log Regression(2050/2999): loss=0.4178974233970975\n",
      "Log Regression(2051/2999): loss=0.4178975511380253\n",
      "Log Regression(2052/2999): loss=0.4178975051937612\n",
      "Log Regression(2053/2999): loss=0.41789745488003505\n",
      "Log Regression(2054/2999): loss=0.41789756288325813\n",
      "Log Regression(2055/2999): loss=0.41789735421728214\n",
      "Log Regression(2056/2999): loss=0.41789737995227105\n",
      "Log Regression(2057/2999): loss=0.4178976494231455\n",
      "Log Regression(2058/2999): loss=0.4178975789330619\n",
      "Log Regression(2059/2999): loss=0.41789769861061765\n",
      "Log Regression(2060/2999): loss=0.41789773836066724\n",
      "Log Regression(2061/2999): loss=0.41789777809681233\n",
      "Log Regression(2062/2999): loss=0.41789789392573434\n",
      "Log Regression(2063/2999): loss=0.4178978634838761\n",
      "Log Regression(2064/2999): loss=0.41789777233833275\n",
      "Log Regression(2065/2999): loss=0.4178978695810609\n",
      "Log Regression(2066/2999): loss=0.4178979303378613\n",
      "Log Regression(2067/2999): loss=0.4178978613374167\n",
      "Log Regression(2068/2999): loss=0.41789789813814043\n",
      "Log Regression(2069/2999): loss=0.4178976617096297\n",
      "Log Regression(2070/2999): loss=0.41789773387065665\n",
      "Log Regression(2071/2999): loss=0.4178968737906947\n",
      "Log Regression(2072/2999): loss=0.41789690474798225\n",
      "Log Regression(2073/2999): loss=0.4178968667939888\n",
      "Log Regression(2074/2999): loss=0.4178968001177744\n",
      "Log Regression(2075/2999): loss=0.4178969963393236\n",
      "Log Regression(2076/2999): loss=0.4178972093242856\n",
      "Log Regression(2077/2999): loss=0.4178971391356855\n",
      "Log Regression(2078/2999): loss=0.4178971181187206\n",
      "Log Regression(2079/2999): loss=0.41789726677795036\n",
      "Log Regression(2080/2999): loss=0.4178973752989259\n",
      "Log Regression(2081/2999): loss=0.4178972063277331\n",
      "Log Regression(2082/2999): loss=0.4178973519329171\n",
      "Log Regression(2083/2999): loss=0.4178972186153105\n",
      "Log Regression(2084/2999): loss=0.4178971194924872\n",
      "Log Regression(2085/2999): loss=0.4178969467430253\n",
      "Log Regression(2086/2999): loss=0.41789695919442404\n",
      "Log Regression(2087/2999): loss=0.4178968164530829\n",
      "Log Regression(2088/2999): loss=0.41789685203441684\n",
      "Log Regression(2089/2999): loss=0.4178968781163644\n",
      "Log Regression(2090/2999): loss=0.4178967993108361\n",
      "Log Regression(2091/2999): loss=0.4178965339277054\n",
      "Log Regression(2092/2999): loss=0.41789673212323136\n",
      "Log Regression(2093/2999): loss=0.4178967924511425\n",
      "Log Regression(2094/2999): loss=0.4178968122550838\n",
      "Log Regression(2095/2999): loss=0.41789718779019913\n",
      "Log Regression(2096/2999): loss=0.4178972344725204\n",
      "Log Regression(2097/2999): loss=0.4178975843374446\n",
      "Log Regression(2098/2999): loss=0.41789763043538614\n",
      "Log Regression(2099/2999): loss=0.4178974720782416\n",
      "Log Regression(2100/2999): loss=0.41789741005537145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(2101/2999): loss=0.41789756623376884\n",
      "Log Regression(2102/2999): loss=0.417897603444779\n",
      "Log Regression(2103/2999): loss=0.4178981736859857\n",
      "Log Regression(2104/2999): loss=0.4178979707656627\n",
      "Log Regression(2105/2999): loss=0.41789784909228245\n",
      "Log Regression(2106/2999): loss=0.41789798844148723\n",
      "Log Regression(2107/2999): loss=0.4178981387406833\n",
      "Log Regression(2108/2999): loss=0.41789863485066425\n",
      "Log Regression(2109/2999): loss=0.41789828459060874\n",
      "Log Regression(2110/2999): loss=0.4178980733554142\n",
      "Log Regression(2111/2999): loss=0.41789799291986185\n",
      "Log Regression(2112/2999): loss=0.4178979082536862\n",
      "Log Regression(2113/2999): loss=0.4178979396560866\n",
      "Log Regression(2114/2999): loss=0.41789784586895107\n",
      "Log Regression(2115/2999): loss=0.4178979758016695\n",
      "Log Regression(2116/2999): loss=0.4178978970076023\n",
      "Log Regression(2117/2999): loss=0.4178978221926575\n",
      "Log Regression(2118/2999): loss=0.4178979264722288\n",
      "Log Regression(2119/2999): loss=0.41789790311324443\n",
      "Log Regression(2120/2999): loss=0.4178976763166731\n",
      "Log Regression(2121/2999): loss=0.4178975478153552\n",
      "Log Regression(2122/2999): loss=0.41789754166375914\n",
      "Log Regression(2123/2999): loss=0.4178976851525712\n",
      "Log Regression(2124/2999): loss=0.4178976806738555\n",
      "Log Regression(2125/2999): loss=0.41789754086659286\n",
      "Log Regression(2126/2999): loss=0.4178973954828234\n",
      "Log Regression(2127/2999): loss=0.41789720807427344\n",
      "Log Regression(2128/2999): loss=0.41789718343028176\n",
      "Log Regression(2129/2999): loss=0.41789685463226117\n",
      "Log Regression(2130/2999): loss=0.4178969478521928\n",
      "Log Regression(2131/2999): loss=0.41789682312313775\n",
      "Log Regression(2132/2999): loss=0.41789677265125047\n",
      "Log Regression(2133/2999): loss=0.4178964310123301\n",
      "Log Regression(2134/2999): loss=0.41789736227600915\n",
      "Log Regression(2135/2999): loss=0.4178975033094799\n",
      "Log Regression(2136/2999): loss=0.4178974949670943\n",
      "Log Regression(2137/2999): loss=0.4178974808682983\n",
      "Log Regression(2138/2999): loss=0.41789753392134116\n",
      "Log Regression(2139/2999): loss=0.4178981355182942\n",
      "Log Regression(2140/2999): loss=0.4178980798760567\n",
      "Log Regression(2141/2999): loss=0.41789829472775697\n",
      "Log Regression(2142/2999): loss=0.4178976559449914\n",
      "Log Regression(2143/2999): loss=0.4178979077142222\n",
      "Log Regression(2144/2999): loss=0.41789794199478786\n",
      "Log Regression(2145/2999): loss=0.41789781751029353\n",
      "Log Regression(2146/2999): loss=0.41789771125691166\n",
      "Log Regression(2147/2999): loss=0.41789777634042485\n",
      "Log Regression(2148/2999): loss=0.41789791760770656\n",
      "Log Regression(2149/2999): loss=0.4178976989476208\n",
      "Log Regression(2150/2999): loss=0.4178978427015679\n",
      "Log Regression(2151/2999): loss=0.4178979164368973\n",
      "Log Regression(2152/2999): loss=0.4178976828569719\n",
      "Log Regression(2153/2999): loss=0.4178975515434973\n",
      "Log Regression(2154/2999): loss=0.4178974962670038\n",
      "Log Regression(2155/2999): loss=0.4178976897253642\n",
      "Log Regression(2156/2999): loss=0.41789772404624737\n",
      "Log Regression(2157/2999): loss=0.4178977416461068\n",
      "Log Regression(2158/2999): loss=0.41789794546663145\n",
      "Log Regression(2159/2999): loss=0.4178976163840358\n",
      "Log Regression(2160/2999): loss=0.41789732958593334\n",
      "Log Regression(2161/2999): loss=0.41789725714738546\n",
      "Log Regression(2162/2999): loss=0.4178973395810593\n",
      "Log Regression(2163/2999): loss=0.4178971376286354\n",
      "Log Regression(2164/2999): loss=0.41789723809584306\n",
      "Log Regression(2165/2999): loss=0.41789715946357553\n",
      "Log Regression(2166/2999): loss=0.41789716919063735\n",
      "Log Regression(2167/2999): loss=0.41789732957204223\n",
      "Log Regression(2168/2999): loss=0.4178971896511608\n",
      "Log Regression(2169/2999): loss=0.41789746382355386\n",
      "Log Regression(2170/2999): loss=0.41789778022529744\n",
      "Log Regression(2171/2999): loss=0.41789745471720435\n",
      "Log Regression(2172/2999): loss=0.41789654851669134\n",
      "Log Regression(2173/2999): loss=0.4178963871242456\n",
      "Log Regression(2174/2999): loss=0.4178962236272929\n",
      "Log Regression(2175/2999): loss=0.41789659473523777\n",
      "Log Regression(2176/2999): loss=0.41789726117553827\n",
      "Log Regression(2177/2999): loss=0.4178971505749942\n",
      "Log Regression(2178/2999): loss=0.41789713058904937\n",
      "Log Regression(2179/2999): loss=0.4178972404646107\n",
      "Log Regression(2180/2999): loss=0.4178980592181914\n",
      "Log Regression(2181/2999): loss=0.41789800408041317\n",
      "Log Regression(2182/2999): loss=0.41789803373424844\n",
      "Log Regression(2183/2999): loss=0.417898000533308\n",
      "Log Regression(2184/2999): loss=0.41789765936280376\n",
      "Log Regression(2185/2999): loss=0.41789743620749753\n",
      "Log Regression(2186/2999): loss=0.4178975341261055\n",
      "Log Regression(2187/2999): loss=0.4178975949309854\n",
      "Log Regression(2188/2999): loss=0.4178972105804306\n",
      "Log Regression(2189/2999): loss=0.41789687601998465\n",
      "Log Regression(2190/2999): loss=0.41789667458553914\n",
      "Log Regression(2191/2999): loss=0.41789644184068425\n",
      "Log Regression(2192/2999): loss=0.4178962739887033\n",
      "Log Regression(2193/2999): loss=0.41789610233572366\n",
      "Log Regression(2194/2999): loss=0.41789594675427666\n",
      "Log Regression(2195/2999): loss=0.4178961049711165\n",
      "Log Regression(2196/2999): loss=0.4178962689093881\n",
      "Log Regression(2197/2999): loss=0.4178960006226256\n",
      "Log Regression(2198/2999): loss=0.4178958854041945\n",
      "Log Regression(2199/2999): loss=0.4178960602932294\n",
      "Log Regression(2200/2999): loss=0.41789565892444547\n",
      "Log Regression(2201/2999): loss=0.4178955726163414\n",
      "Log Regression(2202/2999): loss=0.41789534412805546\n",
      "Log Regression(2203/2999): loss=0.4178951407402495\n",
      "Log Regression(2204/2999): loss=0.4178952953789088\n",
      "Log Regression(2205/2999): loss=0.41789544297986214\n",
      "Log Regression(2206/2999): loss=0.41789573066522767\n",
      "Log Regression(2207/2999): loss=0.4178960114343328\n",
      "Log Regression(2208/2999): loss=0.41789590063139626\n",
      "Log Regression(2209/2999): loss=0.4178952393468676\n",
      "Log Regression(2210/2999): loss=0.41789516606455335\n",
      "Log Regression(2211/2999): loss=0.4178950920579241\n",
      "Log Regression(2212/2999): loss=0.41789503610321366\n",
      "Log Regression(2213/2999): loss=0.417895149984691\n",
      "Log Regression(2214/2999): loss=0.4178948861691001\n",
      "Log Regression(2215/2999): loss=0.41789489455229206\n",
      "Log Regression(2216/2999): loss=0.4178949133041553\n",
      "Log Regression(2217/2999): loss=0.4178950567268768\n",
      "Log Regression(2218/2999): loss=0.41789504170103964\n",
      "Log Regression(2219/2999): loss=0.4178948369372285\n",
      "Log Regression(2220/2999): loss=0.41789443247680746\n",
      "Log Regression(2221/2999): loss=0.4178948754893477\n",
      "Log Regression(2222/2999): loss=0.4178952110767977\n",
      "Log Regression(2223/2999): loss=0.4178952856302054\n",
      "Log Regression(2224/2999): loss=0.4178950365403229\n",
      "Log Regression(2225/2999): loss=0.4178950340091872\n",
      "Log Regression(2226/2999): loss=0.4178949427792535\n",
      "Log Regression(2227/2999): loss=0.41789470641454324\n",
      "Log Regression(2228/2999): loss=0.41789475960302597\n",
      "Log Regression(2229/2999): loss=0.4178952229595483\n",
      "Log Regression(2230/2999): loss=0.41789518664725384\n",
      "Log Regression(2231/2999): loss=0.4178957972514181\n",
      "Log Regression(2232/2999): loss=0.4178961816444613\n",
      "Log Regression(2233/2999): loss=0.41789610395137494\n",
      "Log Regression(2234/2999): loss=0.4178961974526354\n",
      "Log Regression(2235/2999): loss=0.41789610403971766\n",
      "Log Regression(2236/2999): loss=0.41789599423100426\n",
      "Log Regression(2237/2999): loss=0.4178962537333567\n",
      "Log Regression(2238/2999): loss=0.4178967871957549\n",
      "Log Regression(2239/2999): loss=0.41789693917476717\n",
      "Log Regression(2240/2999): loss=0.4178963818199039\n",
      "Log Regression(2241/2999): loss=0.41789652526901355\n",
      "Log Regression(2242/2999): loss=0.4178968913011269\n",
      "Log Regression(2243/2999): loss=0.41789664614114397\n",
      "Log Regression(2244/2999): loss=0.4178969159890896\n",
      "Log Regression(2245/2999): loss=0.4178970331706472\n",
      "Log Regression(2246/2999): loss=0.41789688071504516\n",
      "Log Regression(2247/2999): loss=0.41789657124931456\n",
      "Log Regression(2248/2999): loss=0.41789629158107555\n",
      "Log Regression(2249/2999): loss=0.4178964989169034\n",
      "Log Regression(2250/2999): loss=0.41789657410978925\n",
      "Log Regression(2251/2999): loss=0.4178964886228447\n",
      "Log Regression(2252/2999): loss=0.4178961385274529\n",
      "Log Regression(2253/2999): loss=0.4178961644762387\n",
      "Log Regression(2254/2999): loss=0.4178963019361805\n",
      "Log Regression(2255/2999): loss=0.4178963046836398\n",
      "Log Regression(2256/2999): loss=0.41789641006813266\n",
      "Log Regression(2257/2999): loss=0.41789617713593574\n",
      "Log Regression(2258/2999): loss=0.41789608017142676\n",
      "Log Regression(2259/2999): loss=0.41789601121800324\n",
      "Log Regression(2260/2999): loss=0.41789555118681315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(2261/2999): loss=0.41789555964911096\n",
      "Log Regression(2262/2999): loss=0.4178955216248431\n",
      "Log Regression(2263/2999): loss=0.4178950922044316\n",
      "Log Regression(2264/2999): loss=0.41789523978040444\n",
      "Log Regression(2265/2999): loss=0.41789556556208685\n",
      "Log Regression(2266/2999): loss=0.41789576573024495\n",
      "Log Regression(2267/2999): loss=0.41789591659797665\n",
      "Log Regression(2268/2999): loss=0.4178956852836581\n",
      "Log Regression(2269/2999): loss=0.4178956919315653\n",
      "Log Regression(2270/2999): loss=0.4178952805857579\n",
      "Log Regression(2271/2999): loss=0.4178953178355209\n",
      "Log Regression(2272/2999): loss=0.4178954530379642\n",
      "Log Regression(2273/2999): loss=0.4178952728891604\n",
      "Log Regression(2274/2999): loss=0.41789528294674466\n",
      "Log Regression(2275/2999): loss=0.4178955814237487\n",
      "Log Regression(2276/2999): loss=0.4178955772790133\n",
      "Log Regression(2277/2999): loss=0.41789562067365155\n",
      "Log Regression(2278/2999): loss=0.41789577724974364\n",
      "Log Regression(2279/2999): loss=0.4178959829322851\n",
      "Log Regression(2280/2999): loss=0.4178959419301611\n",
      "Log Regression(2281/2999): loss=0.4178959314488742\n",
      "Log Regression(2282/2999): loss=0.41789645654375057\n",
      "Log Regression(2283/2999): loss=0.41789630789750254\n",
      "Log Regression(2284/2999): loss=0.4178963739021818\n",
      "Log Regression(2285/2999): loss=0.4178963068821155\n",
      "Log Regression(2286/2999): loss=0.41789631501657504\n",
      "Log Regression(2287/2999): loss=0.4178963086749348\n",
      "Log Regression(2288/2999): loss=0.4178962239029835\n",
      "Log Regression(2289/2999): loss=0.4178961998890366\n",
      "Log Regression(2290/2999): loss=0.4178967997249747\n",
      "Log Regression(2291/2999): loss=0.4178968299028951\n",
      "Log Regression(2292/2999): loss=0.41789687306669454\n",
      "Log Regression(2293/2999): loss=0.4178970455156688\n",
      "Log Regression(2294/2999): loss=0.41789706925042697\n",
      "Log Regression(2295/2999): loss=0.4178972321123017\n",
      "Log Regression(2296/2999): loss=0.41789699442725503\n",
      "Log Regression(2297/2999): loss=0.41789707858741015\n",
      "Log Regression(2298/2999): loss=0.4178968467578625\n",
      "Log Regression(2299/2999): loss=0.41789692991541055\n",
      "Log Regression(2300/2999): loss=0.4178967913110822\n",
      "Log Regression(2301/2999): loss=0.41789711606713553\n",
      "Log Regression(2302/2999): loss=0.4178974147705285\n",
      "Log Regression(2303/2999): loss=0.4178973207198283\n",
      "Log Regression(2304/2999): loss=0.4178970796345324\n",
      "Log Regression(2305/2999): loss=0.41789718097649486\n",
      "Log Regression(2306/2999): loss=0.41789721984989303\n",
      "Log Regression(2307/2999): loss=0.4178971798449325\n",
      "Log Regression(2308/2999): loss=0.4178972356185314\n",
      "Log Regression(2309/2999): loss=0.4178969293743563\n",
      "Log Regression(2310/2999): loss=0.41789715335437305\n",
      "Log Regression(2311/2999): loss=0.4178975363553798\n",
      "Log Regression(2312/2999): loss=0.4178972505778872\n",
      "Log Regression(2313/2999): loss=0.4178972708813187\n",
      "Log Regression(2314/2999): loss=0.4178972643913258\n",
      "Log Regression(2315/2999): loss=0.4178976259120625\n",
      "Log Regression(2316/2999): loss=0.4178975239675913\n",
      "Log Regression(2317/2999): loss=0.41789664744964705\n",
      "Log Regression(2318/2999): loss=0.4178964895753237\n",
      "Log Regression(2319/2999): loss=0.4178966924934084\n",
      "Log Regression(2320/2999): loss=0.4178965114293029\n",
      "Log Regression(2321/2999): loss=0.41789658972821697\n",
      "Log Regression(2322/2999): loss=0.4178967386552661\n",
      "Log Regression(2323/2999): loss=0.41789703328618055\n",
      "Log Regression(2324/2999): loss=0.4178966391493662\n",
      "Log Regression(2325/2999): loss=0.4178967607381228\n",
      "Log Regression(2326/2999): loss=0.4178966729785008\n",
      "Log Regression(2327/2999): loss=0.41789655641664875\n",
      "Log Regression(2328/2999): loss=0.41789612592710523\n",
      "Log Regression(2329/2999): loss=0.41789624349803006\n",
      "Log Regression(2330/2999): loss=0.41789624908633227\n",
      "Log Regression(2331/2999): loss=0.4178960639461965\n",
      "Log Regression(2332/2999): loss=0.41789610091105406\n",
      "Log Regression(2333/2999): loss=0.417896406063917\n",
      "Log Regression(2334/2999): loss=0.4178965825267288\n",
      "Log Regression(2335/2999): loss=0.4178963349951572\n",
      "Log Regression(2336/2999): loss=0.41789628613168966\n",
      "Log Regression(2337/2999): loss=0.41789626411849956\n",
      "Log Regression(2338/2999): loss=0.41789626806054686\n",
      "Log Regression(2339/2999): loss=0.41789593183359997\n",
      "Log Regression(2340/2999): loss=0.4178957726946561\n",
      "Log Regression(2341/2999): loss=0.4178958274136608\n",
      "Log Regression(2342/2999): loss=0.41789557652543213\n",
      "Log Regression(2343/2999): loss=0.41789552472745845\n",
      "Log Regression(2344/2999): loss=0.41789534142957185\n",
      "Log Regression(2345/2999): loss=0.417895267180685\n",
      "Log Regression(2346/2999): loss=0.4178952567184695\n",
      "Log Regression(2347/2999): loss=0.41789502933632083\n",
      "Log Regression(2348/2999): loss=0.4178950577089841\n",
      "Log Regression(2349/2999): loss=0.4178951254904794\n",
      "Log Regression(2350/2999): loss=0.4178956899094549\n",
      "Log Regression(2351/2999): loss=0.4178953912078785\n",
      "Log Regression(2352/2999): loss=0.4178957260259863\n",
      "Log Regression(2353/2999): loss=0.41789587733885253\n",
      "Log Regression(2354/2999): loss=0.4178955773542951\n",
      "Log Regression(2355/2999): loss=0.4178956755849192\n",
      "Log Regression(2356/2999): loss=0.4178956208555973\n",
      "Log Regression(2357/2999): loss=0.4178959734325773\n",
      "Log Regression(2358/2999): loss=0.41789655767562195\n",
      "Log Regression(2359/2999): loss=0.41789678213027\n",
      "Log Regression(2360/2999): loss=0.41789694813612965\n",
      "Log Regression(2361/2999): loss=0.4178971378151252\n",
      "Log Regression(2362/2999): loss=0.4178971815944157\n",
      "Log Regression(2363/2999): loss=0.41789731219753473\n",
      "Log Regression(2364/2999): loss=0.4178978790225545\n",
      "Log Regression(2365/2999): loss=0.4178981795528513\n",
      "Log Regression(2366/2999): loss=0.41789821185444\n",
      "Log Regression(2367/2999): loss=0.4178981591115841\n",
      "Log Regression(2368/2999): loss=0.41789836714387557\n",
      "Log Regression(2369/2999): loss=0.41789829072119106\n",
      "Log Regression(2370/2999): loss=0.41789773380159795\n",
      "Log Regression(2371/2999): loss=0.4178978330371948\n",
      "Log Regression(2372/2999): loss=0.417897808816022\n",
      "Log Regression(2373/2999): loss=0.41789802288770034\n",
      "Log Regression(2374/2999): loss=0.41789810475071015\n",
      "Log Regression(2375/2999): loss=0.4178984564515799\n",
      "Log Regression(2376/2999): loss=0.41789845631302674\n",
      "Log Regression(2377/2999): loss=0.41789841478682566\n",
      "Log Regression(2378/2999): loss=0.4178984189056697\n",
      "Log Regression(2379/2999): loss=0.4178982864927996\n",
      "Log Regression(2380/2999): loss=0.4178981910488356\n",
      "Log Regression(2381/2999): loss=0.4178984304806073\n",
      "Log Regression(2382/2999): loss=0.4178986810680078\n",
      "Log Regression(2383/2999): loss=0.41789841827972785\n",
      "Log Regression(2384/2999): loss=0.4178981759735432\n",
      "Log Regression(2385/2999): loss=0.4178978422887368\n",
      "Log Regression(2386/2999): loss=0.41789792973551626\n",
      "Log Regression(2387/2999): loss=0.4178986030902387\n",
      "Log Regression(2388/2999): loss=0.4178988844347232\n",
      "Log Regression(2389/2999): loss=0.4178990021597804\n",
      "Log Regression(2390/2999): loss=0.41789906522217224\n",
      "Log Regression(2391/2999): loss=0.41789880384814343\n",
      "Log Regression(2392/2999): loss=0.4178988623680494\n",
      "Log Regression(2393/2999): loss=0.41789833748850647\n",
      "Log Regression(2394/2999): loss=0.41789810097791097\n",
      "Log Regression(2395/2999): loss=0.41789790857442927\n",
      "Log Regression(2396/2999): loss=0.41789788287426666\n",
      "Log Regression(2397/2999): loss=0.4178979023430791\n",
      "Log Regression(2398/2999): loss=0.41789764559183257\n",
      "Log Regression(2399/2999): loss=0.41789744137742596\n",
      "Log Regression(2400/2999): loss=0.4178973646867205\n",
      "Log Regression(2401/2999): loss=0.41789735178059323\n",
      "Log Regression(2402/2999): loss=0.41789741381508017\n",
      "Log Regression(2403/2999): loss=0.4178976024369412\n",
      "Log Regression(2404/2999): loss=0.41789763683526493\n",
      "Log Regression(2405/2999): loss=0.4178971322052372\n",
      "Log Regression(2406/2999): loss=0.41789702324992195\n",
      "Log Regression(2407/2999): loss=0.4178967670570112\n",
      "Log Regression(2408/2999): loss=0.41789661717817406\n",
      "Log Regression(2409/2999): loss=0.41789722449736927\n",
      "Log Regression(2410/2999): loss=0.41789735245774295\n",
      "Log Regression(2411/2999): loss=0.4178973205768312\n",
      "Log Regression(2412/2999): loss=0.4178975948662903\n",
      "Log Regression(2413/2999): loss=0.41789762436444505\n",
      "Log Regression(2414/2999): loss=0.41789735736278527\n",
      "Log Regression(2415/2999): loss=0.41789727778778984\n",
      "Log Regression(2416/2999): loss=0.41789702971858134\n",
      "Log Regression(2417/2999): loss=0.4178973398580426\n",
      "Log Regression(2418/2999): loss=0.4178973095079093\n",
      "Log Regression(2419/2999): loss=0.41789776023406866\n",
      "Log Regression(2420/2999): loss=0.41789762326101565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(2421/2999): loss=0.41789797567836096\n",
      "Log Regression(2422/2999): loss=0.4178978924534862\n",
      "Log Regression(2423/2999): loss=0.41789783973004807\n",
      "Log Regression(2424/2999): loss=0.4178976769076561\n",
      "Log Regression(2425/2999): loss=0.4178976217950169\n",
      "Log Regression(2426/2999): loss=0.41789750131835063\n",
      "Log Regression(2427/2999): loss=0.41789697054254915\n",
      "Log Regression(2428/2999): loss=0.41789676423283517\n",
      "Log Regression(2429/2999): loss=0.4178967389898391\n",
      "Log Regression(2430/2999): loss=0.41789653257724585\n",
      "Log Regression(2431/2999): loss=0.4178968608820071\n",
      "Log Regression(2432/2999): loss=0.4178969368871721\n",
      "Log Regression(2433/2999): loss=0.41789677145491944\n",
      "Log Regression(2434/2999): loss=0.41789654845615865\n",
      "Log Regression(2435/2999): loss=0.4178962501821396\n",
      "Log Regression(2436/2999): loss=0.4178969169873507\n",
      "Log Regression(2437/2999): loss=0.4178971845641163\n",
      "Log Regression(2438/2999): loss=0.41789723005899354\n",
      "Log Regression(2439/2999): loss=0.41789733106737975\n",
      "Log Regression(2440/2999): loss=0.4178974522969607\n",
      "Log Regression(2441/2999): loss=0.4178975852598649\n",
      "Log Regression(2442/2999): loss=0.4178976384488677\n",
      "Log Regression(2443/2999): loss=0.41789741448142576\n",
      "Log Regression(2444/2999): loss=0.4178972483031744\n",
      "Log Regression(2445/2999): loss=0.4178972400608686\n",
      "Log Regression(2446/2999): loss=0.4178973300863626\n",
      "Log Regression(2447/2999): loss=0.41789714963600794\n",
      "Log Regression(2448/2999): loss=0.41789718918594854\n",
      "Log Regression(2449/2999): loss=0.4178967271438464\n",
      "Log Regression(2450/2999): loss=0.4178964218016679\n",
      "Log Regression(2451/2999): loss=0.41789646642201544\n",
      "Log Regression(2452/2999): loss=0.41789665231864315\n",
      "Log Regression(2453/2999): loss=0.41789665257119524\n",
      "Log Regression(2454/2999): loss=0.4178970121443846\n",
      "Log Regression(2455/2999): loss=0.4178968169411904\n",
      "Log Regression(2456/2999): loss=0.41789653373914426\n",
      "Log Regression(2457/2999): loss=0.4178966925007386\n",
      "Log Regression(2458/2999): loss=0.4178966223825987\n",
      "Log Regression(2459/2999): loss=0.4178966435922329\n",
      "Log Regression(2460/2999): loss=0.4178966604303074\n",
      "Log Regression(2461/2999): loss=0.4178966323634834\n",
      "Log Regression(2462/2999): loss=0.41789621136359956\n",
      "Log Regression(2463/2999): loss=0.417896096671721\n",
      "Log Regression(2464/2999): loss=0.41789612125765235\n",
      "Log Regression(2465/2999): loss=0.41789639070452655\n",
      "Log Regression(2466/2999): loss=0.4178967553105675\n",
      "Log Regression(2467/2999): loss=0.4178969730058831\n",
      "Log Regression(2468/2999): loss=0.41789686124160846\n",
      "Log Regression(2469/2999): loss=0.41789659494735215\n",
      "Log Regression(2470/2999): loss=0.4178966653183204\n",
      "Log Regression(2471/2999): loss=0.4178968989434037\n",
      "Log Regression(2472/2999): loss=0.4178971627439585\n",
      "Log Regression(2473/2999): loss=0.4178969795766324\n",
      "Log Regression(2474/2999): loss=0.41789710747843306\n",
      "Log Regression(2475/2999): loss=0.41789722512657096\n",
      "Log Regression(2476/2999): loss=0.4178974362476456\n",
      "Log Regression(2477/2999): loss=0.4178974012409229\n",
      "Log Regression(2478/2999): loss=0.4178973123387326\n",
      "Log Regression(2479/2999): loss=0.41789722028802945\n",
      "Log Regression(2480/2999): loss=0.4178971736233576\n",
      "Log Regression(2481/2999): loss=0.4178973159869192\n",
      "Log Regression(2482/2999): loss=0.4178973204933128\n",
      "Log Regression(2483/2999): loss=0.417897384796984\n",
      "Log Regression(2484/2999): loss=0.4178973164241083\n",
      "Log Regression(2485/2999): loss=0.41789704631507635\n",
      "Log Regression(2486/2999): loss=0.4178973285529889\n",
      "Log Regression(2487/2999): loss=0.4178972547720662\n",
      "Log Regression(2488/2999): loss=0.41789692360516156\n",
      "Log Regression(2489/2999): loss=0.4178970147549726\n",
      "Log Regression(2490/2999): loss=0.4178968481407699\n",
      "Log Regression(2491/2999): loss=0.41789687378451984\n",
      "Log Regression(2492/2999): loss=0.4178972675101374\n",
      "Log Regression(2493/2999): loss=0.417897050795636\n",
      "Log Regression(2494/2999): loss=0.41789733432188736\n",
      "Log Regression(2495/2999): loss=0.4178971882447977\n",
      "Log Regression(2496/2999): loss=0.4178969679205593\n",
      "Log Regression(2497/2999): loss=0.41789687152552957\n",
      "Log Regression(2498/2999): loss=0.41789687927066904\n",
      "Log Regression(2499/2999): loss=0.4178966521073748\n",
      "Log Regression(2500/2999): loss=0.41789670404539364\n",
      "Log Regression(2501/2999): loss=0.41789689485661624\n",
      "Log Regression(2502/2999): loss=0.4178969883418186\n",
      "Log Regression(2503/2999): loss=0.41789697654736624\n",
      "Log Regression(2504/2999): loss=0.41789702524423067\n",
      "Log Regression(2505/2999): loss=0.4178972838683141\n",
      "Log Regression(2506/2999): loss=0.41789731049588985\n",
      "Log Regression(2507/2999): loss=0.4178970229441883\n",
      "Log Regression(2508/2999): loss=0.417897091459191\n",
      "Log Regression(2509/2999): loss=0.41789712175294785\n",
      "Log Regression(2510/2999): loss=0.41789706482164785\n",
      "Log Regression(2511/2999): loss=0.41789729245563023\n",
      "Log Regression(2512/2999): loss=0.417896999327691\n",
      "Log Regression(2513/2999): loss=0.4178966451027292\n",
      "Log Regression(2514/2999): loss=0.4178966322380066\n",
      "Log Regression(2515/2999): loss=0.41789624861755614\n",
      "Log Regression(2516/2999): loss=0.41789568537854777\n",
      "Log Regression(2517/2999): loss=0.41789578430198054\n",
      "Log Regression(2518/2999): loss=0.4178953645446599\n",
      "Log Regression(2519/2999): loss=0.41789526372653163\n",
      "Log Regression(2520/2999): loss=0.4178950958832406\n",
      "Log Regression(2521/2999): loss=0.4178951684548584\n",
      "Log Regression(2522/2999): loss=0.4178950135027229\n",
      "Log Regression(2523/2999): loss=0.4178947470948573\n",
      "Log Regression(2524/2999): loss=0.4178945228563025\n",
      "Log Regression(2525/2999): loss=0.4178945962241007\n",
      "Log Regression(2526/2999): loss=0.4178945721828995\n",
      "Log Regression(2527/2999): loss=0.4178944063206912\n",
      "Log Regression(2528/2999): loss=0.4178944753007949\n",
      "Log Regression(2529/2999): loss=0.41789424769067934\n",
      "Log Regression(2530/2999): loss=0.4178941097870225\n",
      "Log Regression(2531/2999): loss=0.41789423343725735\n",
      "Log Regression(2532/2999): loss=0.41789416015628106\n",
      "Log Regression(2533/2999): loss=0.41789464160088646\n",
      "Log Regression(2534/2999): loss=0.41789454143149074\n",
      "Log Regression(2535/2999): loss=0.4178947339926698\n",
      "Log Regression(2536/2999): loss=0.41789480141059376\n",
      "Log Regression(2537/2999): loss=0.417894702947616\n",
      "Log Regression(2538/2999): loss=0.41789429220220004\n",
      "Log Regression(2539/2999): loss=0.4178942339888683\n",
      "Log Regression(2540/2999): loss=0.41789392987938223\n",
      "Log Regression(2541/2999): loss=0.4178934902531674\n",
      "Log Regression(2542/2999): loss=0.4178934013176764\n",
      "Log Regression(2543/2999): loss=0.41789388011635464\n",
      "Log Regression(2544/2999): loss=0.4178938979396679\n",
      "Log Regression(2545/2999): loss=0.4178935861738437\n",
      "Log Regression(2546/2999): loss=0.41789332756374237\n",
      "Log Regression(2547/2999): loss=0.4178934108850624\n",
      "Log Regression(2548/2999): loss=0.41789348556522266\n",
      "Log Regression(2549/2999): loss=0.41789318165450023\n",
      "Log Regression(2550/2999): loss=0.41789326542846345\n",
      "Log Regression(2551/2999): loss=0.41789312785621885\n",
      "Log Regression(2552/2999): loss=0.41789328564588984\n",
      "Log Regression(2553/2999): loss=0.41789331711498723\n",
      "Log Regression(2554/2999): loss=0.41789316023100237\n",
      "Log Regression(2555/2999): loss=0.417893407879518\n",
      "Log Regression(2556/2999): loss=0.4178933400577853\n",
      "Log Regression(2557/2999): loss=0.4178931620229631\n",
      "Log Regression(2558/2999): loss=0.4178933165637768\n",
      "Log Regression(2559/2999): loss=0.41789335207274075\n",
      "Log Regression(2560/2999): loss=0.4178935611698777\n",
      "Log Regression(2561/2999): loss=0.41789373237626426\n",
      "Log Regression(2562/2999): loss=0.41789369642610485\n",
      "Log Regression(2563/2999): loss=0.41789334491259866\n",
      "Log Regression(2564/2999): loss=0.4178936595499718\n",
      "Log Regression(2565/2999): loss=0.4178934305404947\n",
      "Log Regression(2566/2999): loss=0.4178934828674526\n",
      "Log Regression(2567/2999): loss=0.4178935731878331\n",
      "Log Regression(2568/2999): loss=0.41789358086789563\n",
      "Log Regression(2569/2999): loss=0.41789327796370856\n",
      "Log Regression(2570/2999): loss=0.41789320736012003\n",
      "Log Regression(2571/2999): loss=0.4178930414775187\n",
      "Log Regression(2572/2999): loss=0.4178928863821951\n",
      "Log Regression(2573/2999): loss=0.41789244550298166\n",
      "Log Regression(2574/2999): loss=0.4178923677247783\n",
      "Log Regression(2575/2999): loss=0.4178921778229311\n",
      "Log Regression(2576/2999): loss=0.4178920851510204\n",
      "Log Regression(2577/2999): loss=0.4178919106464985\n",
      "Log Regression(2578/2999): loss=0.41789193148880005\n",
      "Log Regression(2579/2999): loss=0.41789178841245594\n",
      "Log Regression(2580/2999): loss=0.41789126588641623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(2581/2999): loss=0.41789121706847127\n",
      "Log Regression(2582/2999): loss=0.41789081412889795\n",
      "Log Regression(2583/2999): loss=0.4178910061357176\n",
      "Log Regression(2584/2999): loss=0.417891093708273\n",
      "Log Regression(2585/2999): loss=0.4178906660046244\n",
      "Log Regression(2586/2999): loss=0.4178908322969774\n",
      "Log Regression(2587/2999): loss=0.4178910647175823\n",
      "Log Regression(2588/2999): loss=0.41789128122439323\n",
      "Log Regression(2589/2999): loss=0.4178913129731089\n",
      "Log Regression(2590/2999): loss=0.41789161707918904\n",
      "Log Regression(2591/2999): loss=0.4178915035899617\n",
      "Log Regression(2592/2999): loss=0.4178914221918201\n",
      "Log Regression(2593/2999): loss=0.41789155492182195\n",
      "Log Regression(2594/2999): loss=0.4178912775707487\n",
      "Log Regression(2595/2999): loss=0.4178910701618213\n",
      "Log Regression(2596/2999): loss=0.4178910222296711\n",
      "Log Regression(2597/2999): loss=0.41789081520716176\n",
      "Log Regression(2598/2999): loss=0.41789111486435493\n",
      "Log Regression(2599/2999): loss=0.4178911729964833\n",
      "Log Regression(2600/2999): loss=0.41789121923472367\n",
      "Log Regression(2601/2999): loss=0.4178911745455572\n",
      "Log Regression(2602/2999): loss=0.41789126089551654\n",
      "Log Regression(2603/2999): loss=0.41789050604988814\n",
      "Log Regression(2604/2999): loss=0.41789076357289384\n",
      "Log Regression(2605/2999): loss=0.41789059674033924\n",
      "Log Regression(2606/2999): loss=0.41789068263080836\n",
      "Log Regression(2607/2999): loss=0.4178903755911729\n",
      "Log Regression(2608/2999): loss=0.41789012858701147\n",
      "Log Regression(2609/2999): loss=0.4178901005458511\n",
      "Log Regression(2610/2999): loss=0.4178900005577548\n",
      "Log Regression(2611/2999): loss=0.4178895078403948\n",
      "Log Regression(2612/2999): loss=0.41788958289861783\n",
      "Log Regression(2613/2999): loss=0.41788956070502825\n",
      "Log Regression(2614/2999): loss=0.4178894095920638\n",
      "Log Regression(2615/2999): loss=0.41788913245769255\n",
      "Log Regression(2616/2999): loss=0.4178890548535564\n",
      "Log Regression(2617/2999): loss=0.41788908543887465\n",
      "Log Regression(2618/2999): loss=0.4178891875654474\n",
      "Log Regression(2619/2999): loss=0.41788909744223496\n",
      "Log Regression(2620/2999): loss=0.4178890758394076\n",
      "Log Regression(2621/2999): loss=0.41788907349890897\n",
      "Log Regression(2622/2999): loss=0.41788900382103034\n",
      "Log Regression(2623/2999): loss=0.41788914793237275\n",
      "Log Regression(2624/2999): loss=0.4178891958621599\n",
      "Log Regression(2625/2999): loss=0.41788918392879615\n",
      "Log Regression(2626/2999): loss=0.41788944686728025\n",
      "Log Regression(2627/2999): loss=0.41788934205222616\n",
      "Log Regression(2628/2999): loss=0.41788935222163087\n",
      "Log Regression(2629/2999): loss=0.4178893623237593\n",
      "Log Regression(2630/2999): loss=0.41788988295020263\n",
      "Log Regression(2631/2999): loss=0.41789012884552745\n",
      "Log Regression(2632/2999): loss=0.4178900198662226\n",
      "Log Regression(2633/2999): loss=0.4178899691543017\n",
      "Log Regression(2634/2999): loss=0.4178900750483284\n",
      "Log Regression(2635/2999): loss=0.41789063340875954\n",
      "Log Regression(2636/2999): loss=0.41789055797501806\n",
      "Log Regression(2637/2999): loss=0.41789047071931085\n",
      "Log Regression(2638/2999): loss=0.4178904923536243\n",
      "Log Regression(2639/2999): loss=0.4178900524615385\n",
      "Log Regression(2640/2999): loss=0.41788954084097835\n",
      "Log Regression(2641/2999): loss=0.4178895098137376\n",
      "Log Regression(2642/2999): loss=0.4178894383373446\n",
      "Log Regression(2643/2999): loss=0.4178892422589681\n",
      "Log Regression(2644/2999): loss=0.41788925993940734\n",
      "Log Regression(2645/2999): loss=0.4178890336936843\n",
      "Log Regression(2646/2999): loss=0.4178889744704958\n",
      "Log Regression(2647/2999): loss=0.41788904475900396\n",
      "Log Regression(2648/2999): loss=0.4178891929458003\n",
      "Log Regression(2649/2999): loss=0.41788905266408854\n",
      "Log Regression(2650/2999): loss=0.4178888603034235\n",
      "Log Regression(2651/2999): loss=0.417888842510749\n",
      "Log Regression(2652/2999): loss=0.41788791947389686\n",
      "Log Regression(2653/2999): loss=0.41788769993014646\n",
      "Log Regression(2654/2999): loss=0.4178875916004181\n",
      "Log Regression(2655/2999): loss=0.41788767110791675\n",
      "Log Regression(2656/2999): loss=0.4178874402288915\n",
      "Log Regression(2657/2999): loss=0.41788741092269127\n",
      "Log Regression(2658/2999): loss=0.4178873298011821\n",
      "Log Regression(2659/2999): loss=0.41788735455237813\n",
      "Log Regression(2660/2999): loss=0.41788737083322997\n",
      "Log Regression(2661/2999): loss=0.4178876536127955\n",
      "Log Regression(2662/2999): loss=0.41788756169257113\n",
      "Log Regression(2663/2999): loss=0.4178872236085773\n",
      "Log Regression(2664/2999): loss=0.4178874373996075\n",
      "Log Regression(2665/2999): loss=0.4178872397878625\n",
      "Log Regression(2666/2999): loss=0.4178871387183909\n",
      "Log Regression(2667/2999): loss=0.4178874749853683\n",
      "Log Regression(2668/2999): loss=0.41788740180587713\n",
      "Log Regression(2669/2999): loss=0.4178875424471326\n",
      "Log Regression(2670/2999): loss=0.41788744000652517\n",
      "Log Regression(2671/2999): loss=0.4178873450708929\n",
      "Log Regression(2672/2999): loss=0.4178874347402937\n",
      "Log Regression(2673/2999): loss=0.41788749637750294\n",
      "Log Regression(2674/2999): loss=0.41788723847926723\n",
      "Log Regression(2675/2999): loss=0.4178876547800753\n",
      "Log Regression(2676/2999): loss=0.41788738888749616\n",
      "Log Regression(2677/2999): loss=0.4178872236994056\n",
      "Log Regression(2678/2999): loss=0.41788724695251567\n",
      "Log Regression(2679/2999): loss=0.41788756776112623\n",
      "Log Regression(2680/2999): loss=0.4178876528334347\n",
      "Log Regression(2681/2999): loss=0.41788797039328845\n",
      "Log Regression(2682/2999): loss=0.41788806374809756\n",
      "Log Regression(2683/2999): loss=0.4178881560959935\n",
      "Log Regression(2684/2999): loss=0.41788795521783745\n",
      "Log Regression(2685/2999): loss=0.4178878049405767\n",
      "Log Regression(2686/2999): loss=0.41788748293518924\n",
      "Log Regression(2687/2999): loss=0.41788724828849294\n",
      "Log Regression(2688/2999): loss=0.4178871276169741\n",
      "Log Regression(2689/2999): loss=0.41788700506802645\n",
      "Log Regression(2690/2999): loss=0.4178869307545017\n",
      "Log Regression(2691/2999): loss=0.4178871439436277\n",
      "Log Regression(2692/2999): loss=0.4178876050058488\n",
      "Log Regression(2693/2999): loss=0.4178873462034844\n",
      "Log Regression(2694/2999): loss=0.41788713449940834\n",
      "Log Regression(2695/2999): loss=0.4178873600618472\n",
      "Log Regression(2696/2999): loss=0.4178874431875135\n",
      "Log Regression(2697/2999): loss=0.4178873926198537\n",
      "Log Regression(2698/2999): loss=0.41788719496040183\n",
      "Log Regression(2699/2999): loss=0.41788702445577475\n",
      "Log Regression(2700/2999): loss=0.4178869676428221\n",
      "Log Regression(2701/2999): loss=0.41788684341445104\n",
      "Log Regression(2702/2999): loss=0.41788694231206175\n",
      "Log Regression(2703/2999): loss=0.4178867720154291\n",
      "Log Regression(2704/2999): loss=0.41788647001723744\n",
      "Log Regression(2705/2999): loss=0.417886708051327\n",
      "Log Regression(2706/2999): loss=0.41788676459596247\n",
      "Log Regression(2707/2999): loss=0.4178867538362163\n",
      "Log Regression(2708/2999): loss=0.41788630218323913\n",
      "Log Regression(2709/2999): loss=0.41788620176531616\n",
      "Log Regression(2710/2999): loss=0.4178861821969985\n",
      "Log Regression(2711/2999): loss=0.41788634021838367\n",
      "Log Regression(2712/2999): loss=0.4178864982409983\n",
      "Log Regression(2713/2999): loss=0.4178866284412423\n",
      "Log Regression(2714/2999): loss=0.41788664763254696\n",
      "Log Regression(2715/2999): loss=0.417886801316545\n",
      "Log Regression(2716/2999): loss=0.4178868953696124\n",
      "Log Regression(2717/2999): loss=0.41788697463540764\n",
      "Log Regression(2718/2999): loss=0.4178869923932234\n",
      "Log Regression(2719/2999): loss=0.4178873837111736\n",
      "Log Regression(2720/2999): loss=0.41788729675852726\n",
      "Log Regression(2721/2999): loss=0.41788738663949626\n",
      "Log Regression(2722/2999): loss=0.41788754412456647\n",
      "Log Regression(2723/2999): loss=0.4178873161788635\n",
      "Log Regression(2724/2999): loss=0.41788726949415883\n",
      "Log Regression(2725/2999): loss=0.4178870097065176\n",
      "Log Regression(2726/2999): loss=0.41788699800303675\n",
      "Log Regression(2727/2999): loss=0.41788702478035455\n",
      "Log Regression(2728/2999): loss=0.41788755845847814\n",
      "Log Regression(2729/2999): loss=0.41788741834195164\n",
      "Log Regression(2730/2999): loss=0.41788756960254847\n",
      "Log Regression(2731/2999): loss=0.41788764302706716\n",
      "Log Regression(2732/2999): loss=0.4178878429725414\n",
      "Log Regression(2733/2999): loss=0.41788795806344353\n",
      "Log Regression(2734/2999): loss=0.4178877299945834\n",
      "Log Regression(2735/2999): loss=0.41788779794634634\n",
      "Log Regression(2736/2999): loss=0.4178877559283328\n",
      "Log Regression(2737/2999): loss=0.41788772685897657\n",
      "Log Regression(2738/2999): loss=0.41788782912749806\n",
      "Log Regression(2739/2999): loss=0.41788804206348756\n",
      "Log Regression(2740/2999): loss=0.41788780204654563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(2741/2999): loss=0.4178876787929677\n",
      "Log Regression(2742/2999): loss=0.41788732502088854\n",
      "Log Regression(2743/2999): loss=0.4178869418059307\n",
      "Log Regression(2744/2999): loss=0.4178873097186015\n",
      "Log Regression(2745/2999): loss=0.4178873182169866\n",
      "Log Regression(2746/2999): loss=0.417887617031323\n",
      "Log Regression(2747/2999): loss=0.41788769114613955\n",
      "Log Regression(2748/2999): loss=0.41788783590117184\n",
      "Log Regression(2749/2999): loss=0.41788783478821057\n",
      "Log Regression(2750/2999): loss=0.41788783017651887\n",
      "Log Regression(2751/2999): loss=0.41788753993309347\n",
      "Log Regression(2752/2999): loss=0.41788756058746046\n",
      "Log Regression(2753/2999): loss=0.41788726327028525\n",
      "Log Regression(2754/2999): loss=0.4178878354995564\n",
      "Log Regression(2755/2999): loss=0.41788778481548233\n",
      "Log Regression(2756/2999): loss=0.4178878802542328\n",
      "Log Regression(2757/2999): loss=0.41788808387491727\n",
      "Log Regression(2758/2999): loss=0.41788746483276645\n",
      "Log Regression(2759/2999): loss=0.4178876354576054\n",
      "Log Regression(2760/2999): loss=0.4178873343216677\n",
      "Log Regression(2761/2999): loss=0.41788763755622416\n",
      "Log Regression(2762/2999): loss=0.4178876409325096\n",
      "Log Regression(2763/2999): loss=0.4178877682907101\n",
      "Log Regression(2764/2999): loss=0.41788767899119317\n",
      "Log Regression(2765/2999): loss=0.417887593530575\n",
      "Log Regression(2766/2999): loss=0.41788742739269336\n",
      "Log Regression(2767/2999): loss=0.4178874653627849\n",
      "Log Regression(2768/2999): loss=0.41788784164142606\n",
      "Log Regression(2769/2999): loss=0.41788798115371945\n",
      "Log Regression(2770/2999): loss=0.4178878723228017\n",
      "Log Regression(2771/2999): loss=0.41788779711320706\n",
      "Log Regression(2772/2999): loss=0.4178883691405968\n",
      "Log Regression(2773/2999): loss=0.41788820017278\n",
      "Log Regression(2774/2999): loss=0.41788797955758755\n",
      "Log Regression(2775/2999): loss=0.4178878836071223\n",
      "Log Regression(2776/2999): loss=0.41788771609666353\n",
      "Log Regression(2777/2999): loss=0.41788788067433347\n",
      "Log Regression(2778/2999): loss=0.41788771313754364\n",
      "Log Regression(2779/2999): loss=0.4178874266605555\n",
      "Log Regression(2780/2999): loss=0.41788752467821483\n",
      "Log Regression(2781/2999): loss=0.41788773217565744\n",
      "Log Regression(2782/2999): loss=0.4178873423382658\n",
      "Log Regression(2783/2999): loss=0.41788744174699616\n",
      "Log Regression(2784/2999): loss=0.4178872751104488\n",
      "Log Regression(2785/2999): loss=0.4178871936926885\n",
      "Log Regression(2786/2999): loss=0.41788701596615835\n",
      "Log Regression(2787/2999): loss=0.41788689886362884\n",
      "Log Regression(2788/2999): loss=0.41788661800548\n",
      "Log Regression(2789/2999): loss=0.41788639461398763\n",
      "Log Regression(2790/2999): loss=0.41788655702565564\n",
      "Log Regression(2791/2999): loss=0.4178865828229797\n",
      "Log Regression(2792/2999): loss=0.4178865343558202\n",
      "Log Regression(2793/2999): loss=0.417886485678739\n",
      "Log Regression(2794/2999): loss=0.4178867263668039\n",
      "Log Regression(2795/2999): loss=0.4178863248946212\n",
      "Log Regression(2796/2999): loss=0.4178859454065471\n",
      "Log Regression(2797/2999): loss=0.4178857476865432\n",
      "Log Regression(2798/2999): loss=0.4178850114127514\n",
      "Log Regression(2799/2999): loss=0.417884962956941\n",
      "Log Regression(2800/2999): loss=0.4178852261248216\n",
      "Log Regression(2801/2999): loss=0.41788526240670865\n",
      "Log Regression(2802/2999): loss=0.4178850012801367\n",
      "Log Regression(2803/2999): loss=0.4178850530018059\n",
      "Log Regression(2804/2999): loss=0.417884760622136\n",
      "Log Regression(2805/2999): loss=0.41788448046813653\n",
      "Log Regression(2806/2999): loss=0.4178843513405371\n",
      "Log Regression(2807/2999): loss=0.41788361759741605\n",
      "Log Regression(2808/2999): loss=0.41788378116534974\n",
      "Log Regression(2809/2999): loss=0.41788401346322984\n",
      "Log Regression(2810/2999): loss=0.4178841867050398\n",
      "Log Regression(2811/2999): loss=0.41788395697810543\n",
      "Log Regression(2812/2999): loss=0.41788371646341677\n",
      "Log Regression(2813/2999): loss=0.417883505486448\n",
      "Log Regression(2814/2999): loss=0.4178839904284466\n",
      "Log Regression(2815/2999): loss=0.4178839808931071\n",
      "Log Regression(2816/2999): loss=0.41788383086618597\n",
      "Log Regression(2817/2999): loss=0.41788378927420256\n",
      "Log Regression(2818/2999): loss=0.4178844740066032\n",
      "Log Regression(2819/2999): loss=0.4178842399894528\n",
      "Log Regression(2820/2999): loss=0.41788433823054666\n",
      "Log Regression(2821/2999): loss=0.4178841908603437\n",
      "Log Regression(2822/2999): loss=0.4178843176611474\n",
      "Log Regression(2823/2999): loss=0.41788389741970666\n",
      "Log Regression(2824/2999): loss=0.41788372132880597\n",
      "Log Regression(2825/2999): loss=0.4178834877166698\n",
      "Log Regression(2826/2999): loss=0.41788325072880067\n",
      "Log Regression(2827/2999): loss=0.41788416012271495\n",
      "Log Regression(2828/2999): loss=0.4178841919507377\n",
      "Log Regression(2829/2999): loss=0.41788396111539\n",
      "Log Regression(2830/2999): loss=0.4178840161746828\n",
      "Log Regression(2831/2999): loss=0.41788409512926633\n",
      "Log Regression(2832/2999): loss=0.4178841823505244\n",
      "Log Regression(2833/2999): loss=0.4178841696727792\n",
      "Log Regression(2834/2999): loss=0.4178841868024448\n",
      "Log Regression(2835/2999): loss=0.41788407473806816\n",
      "Log Regression(2836/2999): loss=0.41788393436777493\n",
      "Log Regression(2837/2999): loss=0.4178842757304318\n",
      "Log Regression(2838/2999): loss=0.41788470953727824\n",
      "Log Regression(2839/2999): loss=0.4178849741585737\n",
      "Log Regression(2840/2999): loss=0.4178851985149037\n",
      "Log Regression(2841/2999): loss=0.4178851784765912\n",
      "Log Regression(2842/2999): loss=0.41788493248286895\n",
      "Log Regression(2843/2999): loss=0.4178846818140582\n",
      "Log Regression(2844/2999): loss=0.4178848481978112\n",
      "Log Regression(2845/2999): loss=0.4178846761858858\n",
      "Log Regression(2846/2999): loss=0.41788511493882613\n",
      "Log Regression(2847/2999): loss=0.41788482195258636\n",
      "Log Regression(2848/2999): loss=0.4178847280709393\n",
      "Log Regression(2849/2999): loss=0.41788427699536773\n",
      "Log Regression(2850/2999): loss=0.41788436591573036\n",
      "Log Regression(2851/2999): loss=0.4178842739498053\n",
      "Log Regression(2852/2999): loss=0.41788458937383843\n",
      "Log Regression(2853/2999): loss=0.41788475562988125\n",
      "Log Regression(2854/2999): loss=0.41788496648217255\n",
      "Log Regression(2855/2999): loss=0.41788474024561\n",
      "Log Regression(2856/2999): loss=0.4178847684206372\n",
      "Log Regression(2857/2999): loss=0.4178846316131947\n",
      "Log Regression(2858/2999): loss=0.4178845855458268\n",
      "Log Regression(2859/2999): loss=0.4178843332554703\n",
      "Log Regression(2860/2999): loss=0.4178842104174611\n",
      "Log Regression(2861/2999): loss=0.4178842713787694\n",
      "Log Regression(2862/2999): loss=0.4178843711457917\n",
      "Log Regression(2863/2999): loss=0.4178845313606245\n",
      "Log Regression(2864/2999): loss=0.4178844516037196\n",
      "Log Regression(2865/2999): loss=0.4178840822463991\n",
      "Log Regression(2866/2999): loss=0.4178838237187827\n",
      "Log Regression(2867/2999): loss=0.41788407838009\n",
      "Log Regression(2868/2999): loss=0.417884285924727\n",
      "Log Regression(2869/2999): loss=0.4178842095116613\n",
      "Log Regression(2870/2999): loss=0.41788405152112407\n",
      "Log Regression(2871/2999): loss=0.41788432130690895\n",
      "Log Regression(2872/2999): loss=0.4178842148622723\n",
      "Log Regression(2873/2999): loss=0.4178841270483706\n",
      "Log Regression(2874/2999): loss=0.41788405602485357\n",
      "Log Regression(2875/2999): loss=0.41788459958461394\n",
      "Log Regression(2876/2999): loss=0.4178853510187664\n",
      "Log Regression(2877/2999): loss=0.41788498529623924\n",
      "Log Regression(2878/2999): loss=0.41788492883258443\n",
      "Log Regression(2879/2999): loss=0.417885144456695\n",
      "Log Regression(2880/2999): loss=0.4178850345335068\n",
      "Log Regression(2881/2999): loss=0.41788437636643444\n",
      "Log Regression(2882/2999): loss=0.41788446191237916\n",
      "Log Regression(2883/2999): loss=0.41788457397923606\n",
      "Log Regression(2884/2999): loss=0.41788430951748895\n",
      "Log Regression(2885/2999): loss=0.4178846350715475\n",
      "Log Regression(2886/2999): loss=0.4178852102874584\n",
      "Log Regression(2887/2999): loss=0.4178853160927861\n",
      "Log Regression(2888/2999): loss=0.41788532798424194\n",
      "Log Regression(2889/2999): loss=0.4178851122649154\n",
      "Log Regression(2890/2999): loss=0.4178850465197042\n",
      "Log Regression(2891/2999): loss=0.41788488393384315\n",
      "Log Regression(2892/2999): loss=0.4178846970179486\n",
      "Log Regression(2893/2999): loss=0.4178845685050927\n",
      "Log Regression(2894/2999): loss=0.41788458463182515\n",
      "Log Regression(2895/2999): loss=0.4178843213322309\n",
      "Log Regression(2896/2999): loss=0.41788419093822116\n",
      "Log Regression(2897/2999): loss=0.4178842839512742\n",
      "Log Regression(2898/2999): loss=0.417884193178066\n",
      "Log Regression(2899/2999): loss=0.4178841716724818\n",
      "Log Regression(2900/2999): loss=0.41788422615708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Regression(2901/2999): loss=0.4178841004878303\n",
      "Log Regression(2902/2999): loss=0.41788435910688626\n",
      "Log Regression(2903/2999): loss=0.4178845745256956\n",
      "Log Regression(2904/2999): loss=0.4178846069145999\n",
      "Log Regression(2905/2999): loss=0.4178846102357823\n",
      "Log Regression(2906/2999): loss=0.4178841645788252\n",
      "Log Regression(2907/2999): loss=0.41788406732452654\n",
      "Log Regression(2908/2999): loss=0.41788414279653274\n",
      "Log Regression(2909/2999): loss=0.41788403132063007\n",
      "Log Regression(2910/2999): loss=0.4178838233039216\n",
      "Log Regression(2911/2999): loss=0.4178841090581582\n",
      "Log Regression(2912/2999): loss=0.417884237167578\n",
      "Log Regression(2913/2999): loss=0.41788451425918716\n",
      "Log Regression(2914/2999): loss=0.4178844093631555\n",
      "Log Regression(2915/2999): loss=0.41788421700835127\n",
      "Log Regression(2916/2999): loss=0.4178842242839576\n",
      "Log Regression(2917/2999): loss=0.41788426362205533\n",
      "Log Regression(2918/2999): loss=0.41788429059640436\n",
      "Log Regression(2919/2999): loss=0.4178843573530927\n",
      "Log Regression(2920/2999): loss=0.41788384003146634\n",
      "Log Regression(2921/2999): loss=0.41788369878332704\n",
      "Log Regression(2922/2999): loss=0.4178837474409039\n",
      "Log Regression(2923/2999): loss=0.4178845538343932\n",
      "Log Regression(2924/2999): loss=0.4178848506812432\n",
      "Log Regression(2925/2999): loss=0.41788467064485063\n",
      "Log Regression(2926/2999): loss=0.4178843981977362\n",
      "Log Regression(2927/2999): loss=0.41788421747665655\n",
      "Log Regression(2928/2999): loss=0.4178840833848836\n",
      "Log Regression(2929/2999): loss=0.4178844633509149\n",
      "Log Regression(2930/2999): loss=0.4178843557852401\n",
      "Log Regression(2931/2999): loss=0.41788461758171974\n",
      "Log Regression(2932/2999): loss=0.4178846030103383\n",
      "Log Regression(2933/2999): loss=0.4178845507542439\n",
      "Log Regression(2934/2999): loss=0.41788432800354325\n",
      "Log Regression(2935/2999): loss=0.41788459855010807\n",
      "Log Regression(2936/2999): loss=0.41788452859980846\n",
      "Log Regression(2937/2999): loss=0.41788430556663675\n",
      "Log Regression(2938/2999): loss=0.4178846264688096\n",
      "Log Regression(2939/2999): loss=0.4178845677593588\n",
      "Log Regression(2940/2999): loss=0.41788454984674983\n",
      "Log Regression(2941/2999): loss=0.4178844448265001\n",
      "Log Regression(2942/2999): loss=0.4178843014511331\n",
      "Log Regression(2943/2999): loss=0.4178844422849051\n",
      "Log Regression(2944/2999): loss=0.4178843417117174\n",
      "Log Regression(2945/2999): loss=0.41788452750463057\n",
      "Log Regression(2946/2999): loss=0.4178847192052005\n",
      "Log Regression(2947/2999): loss=0.41788449941372036\n",
      "Log Regression(2948/2999): loss=0.41788457187256317\n",
      "Log Regression(2949/2999): loss=0.41788452832742723\n",
      "Log Regression(2950/2999): loss=0.41788466665627255\n",
      "Log Regression(2951/2999): loss=0.4178847497050851\n",
      "Log Regression(2952/2999): loss=0.4178848044909027\n",
      "Log Regression(2953/2999): loss=0.4178846335777912\n",
      "Log Regression(2954/2999): loss=0.41788460590311544\n",
      "Log Regression(2955/2999): loss=0.4178845841037792\n",
      "Log Regression(2956/2999): loss=0.41788486022457244\n",
      "Log Regression(2957/2999): loss=0.41788488258774936\n",
      "Log Regression(2958/2999): loss=0.4178846894630659\n",
      "Log Regression(2959/2999): loss=0.41788447411149265\n",
      "Log Regression(2960/2999): loss=0.41788431424652467\n",
      "Log Regression(2961/2999): loss=0.4178839946014897\n",
      "Log Regression(2962/2999): loss=0.41788385317190263\n",
      "Log Regression(2963/2999): loss=0.4178836600323282\n",
      "Log Regression(2964/2999): loss=0.4178831831120828\n",
      "Log Regression(2965/2999): loss=0.4178832253708533\n",
      "Log Regression(2966/2999): loss=0.4178830811185979\n",
      "Log Regression(2967/2999): loss=0.41788267132517193\n",
      "Log Regression(2968/2999): loss=0.4178825944837449\n",
      "Log Regression(2969/2999): loss=0.41788241859216574\n",
      "Log Regression(2970/2999): loss=0.4178824040704131\n",
      "Log Regression(2971/2999): loss=0.4178825162483455\n",
      "Log Regression(2972/2999): loss=0.4178824614889481\n",
      "Log Regression(2973/2999): loss=0.41788268807614387\n",
      "Log Regression(2974/2999): loss=0.417882817316932\n",
      "Log Regression(2975/2999): loss=0.41788258932986366\n",
      "Log Regression(2976/2999): loss=0.4178828990275789\n",
      "Log Regression(2977/2999): loss=0.417882855648766\n",
      "Log Regression(2978/2999): loss=0.4178828775028799\n",
      "Log Regression(2979/2999): loss=0.4178829633909791\n",
      "Log Regression(2980/2999): loss=0.4178834986228293\n",
      "Log Regression(2981/2999): loss=0.41788341349335\n",
      "Log Regression(2982/2999): loss=0.41788311932860267\n",
      "Log Regression(2983/2999): loss=0.41788287376224253\n",
      "Log Regression(2984/2999): loss=0.4178828957868146\n",
      "Log Regression(2985/2999): loss=0.41788277481570246\n",
      "Log Regression(2986/2999): loss=0.4178830287122999\n",
      "Log Regression(2987/2999): loss=0.4178831068970627\n",
      "Log Regression(2988/2999): loss=0.41788295355593763\n",
      "Log Regression(2989/2999): loss=0.4178834002574625\n",
      "Log Regression(2990/2999): loss=0.4178834514787378\n",
      "Log Regression(2991/2999): loss=0.41788332829149044\n",
      "Log Regression(2992/2999): loss=0.4178828965464035\n",
      "Log Regression(2993/2999): loss=0.417883037897197\n",
      "Log Regression(2994/2999): loss=0.4178828533971262\n",
      "Log Regression(2995/2999): loss=0.4178827018221491\n",
      "Log Regression(2996/2999): loss=0.41788248783831133\n",
      "Log Regression(2997/2999): loss=0.4178823459368321\n",
      "Log Regression(2998/2999): loss=0.41788229754508366\n",
      "Log Regression(2999/2999): loss=0.4178820945602814\n"
     ]
    }
   ],
   "source": [
    "w, mse = reg_logistic_regression(y_train_lr, tX_train_poly_std, lambda_, w, max_iters, gamma, batch_size=512, lr_decay=False, lr_decay_rate=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8088088888888889\n"
     ]
    }
   ],
   "source": [
    "y_pred1 = predict_labels(w, tX_train_poly_std)\n",
    "print(np.mean(y_train.reshape(-1,1)==y_pred1))\n",
    "y_train_LR = y_train.reshape(-1,1) >0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.81068\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict_labels(w, tX_val_poly_std)\n",
    "print(np.mean(y_val.reshape(-1,1)==y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_good = w.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('weights8106.pkl','wb') as f:\n",
    "    pickle.dump(w_good, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Capping the loss, for inf values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_test, _ = dp.fill_nan(tX_test, nan_value=-999, method='use_filler', filler=filler)\n",
    "tX_test_new, _ = dp.standardize(tX_test, norm_stats=norm_stats)\n",
    "tX_test_aug_poly = dp.poly_features(tX_test_new,6)\n",
    "tX_test_aug_poly, _ = dp.standardize(tX_test_aug_poly, norm_stats=stats2)\n",
    "tX_test_poly_std = dp.add_ones(tX_test_aug_poly)\n",
    "y_pred = predict_labels(w, tX_test_poly_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'out.csv' # TODO: fill in desired name of output file for submission\n",
    "#y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
